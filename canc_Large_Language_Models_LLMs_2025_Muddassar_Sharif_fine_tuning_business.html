<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Fine-tuning LLMs: A Cost-Benefit Analysis for Businesses</title>
    <meta name="description" content="One model, extra large, please!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Muddassar%20Sharif_llm.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Fine-tuning LLMs: A Cost-Benefit Analysis for Businesses | Conf42"/>
    <meta property="og:description" content="As businesses scramble to integrate Large Language Models (LLMs) into their workflows, a pertinent question arises: Is fine-tuning worth the investment? Fine-tuning offers domain-specific precision and improved user satisfaction, but requires tremendous upfront data, computational power, and skilled personnel. This presentation analyzes the economics of fine-tuningâ€™s intricacies, when it is a good investment, when retrieval-augmented generation (RAG) is a more suitable option, and how companies can balance their need for custom solutions against cost considerations."/>
    <meta property="og:url" content="https://conf42.com/Large_Language_Models_LLMs_2025_Muddassar_Sharif_fine_tuning_business"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/MLOPS2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        MLOps 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-09-18
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/mlops2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #CCB87B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Large Language Models (LLMs) 2025 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- One model, extra large, please!
 -->
              <script>
                const event_date = new Date("2025-03-20T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2025-03-20T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "X5o7d-Y-970"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrCnqe_gWc_lIVyDmyIx8BQG" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hey everyone, so I am moder, and this is gonna be my short talk on go", "timestamp": "00:00:00,360", "timestamp_s": 0.0}, {"text": "manifest analysis of for businesses when it comes to fine tuning LLMs or", "timestamp": "00:00:09,300", "timestamp_s": 9.0}, {"text": "deciding between fine tuning and other techniques out there in order to get", "timestamp": "00:00:15,840", "timestamp_s": 15.0}, {"text": "to the final result that the use case.", "timestamp": "00:00:22,740", "timestamp_s": 22.0}, {"text": "Company wants to solve.", "timestamp": "00:00:25,830", "timestamp_s": 25.0}, {"text": "So like the most, like in every use case, like the central question that", "timestamp": "00:00:27,270", "timestamp_s": 27.0}, {"text": "every business operations or stakeholder needs to answer is is it possible for", "timestamp": "00:00:34,619", "timestamp_s": 34.0}, {"text": "us to use just pre-trained models that are out there from open ai, from cloud?", "timestamp": "00:00:40,459", "timestamp_s": 40.0}, {"text": "The reason being is because they are out of the box.", "timestamp": "00:00:48,830", "timestamp_s": 48.0}, {"text": "There is no need to do any fine tuning there.", "timestamp": "00:00:52,400", "timestamp_s": 52.0}, {"text": "They are fast, they are affordable.", "timestamp": "00:00:56,839", "timestamp_s": 56.0}, {"text": "And the next most important question is, for example, like in our use case.", "timestamp": "00:01:00,050", "timestamp_s": 60.0}, {"text": "What is our use case?", "timestamp": "00:01:06,860", "timestamp_s": 66.0}, {"text": "And in our use case, is there a need?", "timestamp": "00:01:08,510", "timestamp_s": 68.0}, {"text": "A need for any data that is not public, that is private?", "timestamp": "00:01:12,410", "timestamp_s": 72.0}, {"text": "So you want L them to go over that data, understand patterns in that", "timestamp": "00:01:18,530", "timestamp_s": 78.0}, {"text": "data in order to answer some really important question or was or call", "timestamp": "00:01:24,410", "timestamp_s": 84.0}, {"text": "some really important function or tool if you are building an agent.", "timestamp": "00:01:29,320", "timestamp_s": 89.0}, {"text": "AI and then based on that, do something.", "timestamp": "00:01:34,244", "timestamp_s": 94.0}, {"text": "So that\u0027s where if you have your own private data do is it required or not?", "timestamp": "00:01:37,475", "timestamp_s": 97.0}, {"text": "That\u0027s a really important question that every stakeholder needs to answer.", "timestamp": "00:01:42,385", "timestamp_s": 102.0}, {"text": "And then based on this answer, let\u0027s say you need, you do need your data in", "timestamp": "00:01:46,165", "timestamp_s": 106.0}, {"text": "the entire use case you wanna resolve.", "timestamp": "00:01:52,125", "timestamp_s": 112.0}, {"text": "Then the question is do you.", "timestamp": "00:01:54,795", "timestamp_s": 114.0}, {"text": "Fine tune your model on that data, or do you just every time you wanna answer some", "timestamp": "00:01:57,255", "timestamp_s": 117.0}, {"text": "question, you go and you retrieve the most important bits and pieces of your", "timestamp": "00:02:02,965", "timestamp_s": 122.0}, {"text": "private data, and then you just see, add that in the model without fine tuning it.", "timestamp": "00:02:09,375", "timestamp_s": 129.0}, {"text": "So these are two different, I would say techniques out there where", "timestamp": "00:02:15,795", "timestamp_s": 135.0}, {"text": "you can use your private data.", "timestamp": "00:02:20,385", "timestamp_s": 140.0}, {"text": "So I feel like to answer this question on what do you choose?", "timestamp": "00:02:22,505", "timestamp_s": 142.0}, {"text": "Do we choose fine tuning or do you choose rack where you retrieve the", "timestamp": "00:02:26,845", "timestamp_s": 146.0}, {"text": "most important bits of data and then use that as context in the LLM?", "timestamp": "00:02:33,045", "timestamp_s": 153.0}, {"text": "I think like it\u0027s really important too.", "timestamp": "00:02:38,285", "timestamp_s": 158.0}, {"text": "First of all, understand what is fine tuning and what the fundamentals", "timestamp": "00:02:40,310", "timestamp_s": 160.0}, {"text": "are in order and before we can easily answer this question.", "timestamp": "00:02:44,210", "timestamp_s": 164.0}, {"text": "So the fine tuning is all about adjusting the weights of the", "timestamp": "00:02:49,190", "timestamp_s": 169.0}, {"text": "model, with the new weights.", "timestamp": "00:02:54,610", "timestamp_s": 174.0}, {"text": "And the new weights will have knowledge of that private data as well.", "timestamp": "00:02:57,170", "timestamp_s": 177.0}, {"text": "fine tuning also means, for example, you have to modify how the models behave.", "timestamp": "00:03:01,540", "timestamp_s": 181.0}, {"text": "You have the models need to unlearn something or relearn something", "timestamp": "00:03:06,240", "timestamp_s": 186.0}, {"text": "because you wanna, you want model to like, just focus on your data", "timestamp": "00:03:10,800", "timestamp_s": 190.0}, {"text": "and then forget about the past.", "timestamp": "00:03:15,060", "timestamp_s": 195.0}, {"text": "And the way I learn LS or any deep learning model models work is as", "timestamp": "00:03:17,430", "timestamp_s": 197.0}, {"text": "you train and keep on training.", "timestamp": "00:03:24,320", "timestamp_s": 204.0}, {"text": "The latest training always has a precedent over the old one.", "timestamp": "00:03:26,975", "timestamp_s": 206.0}, {"text": "So if you take a LLM or large language model that\u0027s pre-trained on some", "timestamp": "00:03:32,225", "timestamp_s": 212.0}, {"text": "natural language data, let\u0027s say if I just find, tune that on medical", "timestamp": "00:03:39,415", "timestamp_s": 219.0}, {"text": "data that LLM will eventually like.", "timestamp": "00:03:46,615", "timestamp_s": 226.0}, {"text": "Learn, be more focused or learn more stuff about medical data.", "timestamp": "00:03:50,025", "timestamp_s": 230.0}, {"text": "and in order to do that, it\u0027ll also unlearn some of the", "timestamp": "00:03:54,445", "timestamp_s": 234.0}, {"text": "stuff that it learned before.", "timestamp": "00:03:59,185", "timestamp_s": 239.0}, {"text": "So fine tuning in a sense, will update your model.", "timestamp": "00:04:00,985", "timestamp_s": 240.0}, {"text": "It waits.", "timestamp": "00:04:04,835", "timestamp_s": 244.0}, {"text": "And there are two things that, and then there, and then within", "timestamp": "00:04:06,205", "timestamp_s": 246.0}, {"text": "the fine tuning domain, there are two ways that you can fine tune.", "timestamp": "00:04:09,415", "timestamp_s": 249.0}, {"text": "Number one is like you, you update all the weights, every layer of your", "timestamp": "00:04:13,165", "timestamp_s": 253.0}, {"text": "model, which is really expensive by the way, because, let\u0027s say.", "timestamp": "00:04:20,905", "timestamp_s": 260.0}, {"text": "The models that we have right now out there.", "timestamp": "00:04:26,105", "timestamp_s": 266.0}, {"text": "So one way to fine tune is like find, just updating all the weights in, all", "timestamp": "00:04:29,345", "timestamp_s": 269.0}, {"text": "the weights right now that we have in our model, which is really expensive", "timestamp": "00:04:38,415", "timestamp_s": 278.0}, {"text": "by the way, because as I said before, large models, they are really big.", "timestamp": "00:04:42,375", "timestamp_s": 282.0}, {"text": "They have one 50 billion parameters wide.", "timestamp": "00:04:48,585", "timestamp_s": 288.0}, {"text": "So let\u0027s say if you wanna go and.", "timestamp": "00:04:51,195", "timestamp_s": 291.0}, {"text": "Update or the entire model.", "timestamp": "00:04:54,090", "timestamp_s": 294.0}, {"text": "You need a really big hardware set up, which is really expensive by the way.", "timestamp": "00:04:57,810", "timestamp_s": 297.0}, {"text": "And the other way of fine tuning, which is also provided out of the works by", "timestamp": "00:05:03,240", "timestamp_s": 303.0}, {"text": "Open AI cloud and other companies is just Fine tuning a few layers towards the end.", "timestamp": "00:05:11,100", "timestamp_s": 311.0}, {"text": "It\u0027s called, meter efficient fine tuning because like you are only, and then here", "timestamp": "00:05:18,710", "timestamp_s": 318.0}, {"text": "you only fine tuning the last few layers.", "timestamp": "00:05:25,280", "timestamp_s": 325.0}, {"text": "The advantage being is like the last few layers will be able", "timestamp": "00:05:29,120", "timestamp_s": 329.0}, {"text": "to retain the knowledge that.", "timestamp": "00:05:33,650", "timestamp_s": 333.0}, {"text": "Or learn from your own private data.", "timestamp": "00:05:36,365", "timestamp_s": 336.0}, {"text": "Whereas all the other layers in the model, which are already trained on other", "timestamp": "00:05:39,605", "timestamp_s": 339.0}, {"text": "aspects of language, for example, grammar and other stuff, and then emotions.", "timestamp": "00:05:46,205", "timestamp_s": 346.0}, {"text": "So all that knowledge you still need, right?", "timestamp": "00:05:52,625", "timestamp_s": 352.0}, {"text": "So that\u0027s the overall idea of just fine tuning a few layers and normally.", "timestamp": "00:05:55,775", "timestamp_s": 355.0}, {"text": "That\u0027s much cheaper, that\u0027s much faster.", "timestamp": "00:06:02,315", "timestamp_s": 362.0}, {"text": "You can, if you have a small amount of your data, you can easily", "timestamp": "00:06:05,165", "timestamp_s": 365.0}, {"text": "fine tune it and then the cost won\u0027t be super high at the time.", "timestamp": "00:06:09,095", "timestamp_s": 369.0}, {"text": "Requirement is also pretty low as well.", "timestamp": "00:06:13,805", "timestamp_s": 373.0}, {"text": "And then you can move forward with that.", "timestamp": "00:06:17,825", "timestamp_s": 377.0}, {"text": "So if I just recap everything right now on Prompt engineering and then, fine tuning", "timestamp": "00:06:20,191", "timestamp_s": 380.0}, {"text": "and rack, and then from scratch, as well.", "timestamp": "00:06:28,076", "timestamp_s": 388.0}, {"text": "So if you are fine tuning the entire model, then it\u0027s same as like", "timestamp": "00:06:30,996", "timestamp_s": 390.0}, {"text": "building the model from scratch.", "timestamp": "00:06:37,671", "timestamp_s": 397.0}, {"text": "It\u0027s not same as, but in the end, for example, like you, since you are.", "timestamp": "00:06:39,741", "timestamp_s": 399.0}, {"text": "It\u0027s like build, building the model from scratch because like you have", "timestamp": "00:06:43,691", "timestamp_s": 403.0}, {"text": "to update so many different layers and weight in the entire model.", "timestamp": "00:06:47,531", "timestamp_s": 407.0}, {"text": "and then you will end up having your own cluster model, which you will", "timestamp": "00:06:52,011", "timestamp_s": 412.0}, {"text": "have a full control over, but in the end, like it\u0027s gonna be really", "timestamp": "00:06:56,241", "timestamp_s": 416.0}, {"text": "expensive to train it from scratch and then maintain that on the server.", "timestamp": "00:07:00,171", "timestamp_s": 420.0}, {"text": "From scratching, train from scratch is really important pathway.", "timestamp": "00:07:09,051", "timestamp_s": 429.0}, {"text": "If you have terabytes or petabytes of large amount of data and then you know", "timestamp": "00:07:13,821", "timestamp_s": 433.0}, {"text": "that, you won\u0027t be able to fine tune, get the results by just fine tuning", "timestamp": "00:07:19,761", "timestamp_s": 439.0}, {"text": "the last few layers you need to like.", "timestamp": "00:07:25,371", "timestamp_s": 445.0}, {"text": "Train majority of your entire model.", "timestamp": "00:07:29,361", "timestamp_s": 449.0}, {"text": "And then in the end, the other important consideration here is that let\u0027s say you", "timestamp": "00:07:33,591", "timestamp_s": 453.0}, {"text": "want to keep your model private as well, so that, so from scratch or taking the", "timestamp": "00:07:40,461", "timestamp_s": 460.0}, {"text": "open source model and then retraining it on your data makes a lot of sense.", "timestamp": "00:07:46,581", "timestamp_s": 466.0}, {"text": "Let\u0027s say if you have a very small amount of data and then you are not", "timestamp": "00:07:52,351", "timestamp_s": 472.0}, {"text": "concerned about privacy, or if you are even concerned about privacy based on", "timestamp": "00:07:55,501", "timestamp_s": 475.0}, {"text": "the privacy factor, you will decide to either use an open source model or", "timestamp": "00:08:01,031", "timestamp_s": 481.0}, {"text": "you will use, the closed source one.", "timestamp": "00:08:07,211", "timestamp_s": 487.0}, {"text": "For example, open AI cloud.", "timestamp": "00:08:10,361", "timestamp_s": 490.0}, {"text": "And then when you once, and then once you decide the close and open source model,", "timestamp": "00:08:12,551", "timestamp_s": 492.0}, {"text": "since you have small amount of data.", "timestamp": "00:08:16,721", "timestamp_s": 496.0}, {"text": "Then fine tuning the last few layers makes a lot of sense.", "timestamp": "00:08:18,901", "timestamp_s": 498.0}, {"text": "And then as a result, you will see that, for example, the model will be", "timestamp": "00:08:22,441", "timestamp_s": 502.0}, {"text": "better able to answer questions in the domain of data that you have given.", "timestamp": "00:08:27,251", "timestamp_s": 507.0}, {"text": "Let\u0027s say if you have given a really specialized data on health tech,", "timestamp": "00:08:32,621", "timestamp_s": 512.0}, {"text": "for example, then you can expect a fine tuned model to answer.", "timestamp": "00:08:38,681", "timestamp_s": 518.0}, {"text": "Health tech questions better than the model.", "timestamp": "00:08:44,846", "timestamp_s": 524.0}, {"text": "That\u0027s not fine tuning.", "timestamp": "00:08:48,776", "timestamp_s": 528.0}, {"text": "and then for example, why would you not use fine tuning, let\u0027s", "timestamp": "00:08:50,586", "timestamp_s": 530.0}, {"text": "say your data is evolving a lot.", "timestamp": "00:08:55,116", "timestamp_s": 535.0}, {"text": "Just to give one more example on when you should use rag.", "timestamp": "00:08:59,116", "timestamp_s": 539.0}, {"text": "Or fine tuning is, let\u0027s say like you have a an LLM or an AI agent, which", "timestamp": "00:09:05,431", "timestamp_s": 545.0}, {"text": "buys and sells stock based on the news.", "timestamp": "00:09:12,871", "timestamp_s": 552.0}, {"text": "Since news is evolving and everything is evolving, it makes no sense just", "timestamp": "00:09:17,311", "timestamp_s": 557.0}, {"text": "to fine tune the LLM on all the news.", "timestamp": "00:09:21,661", "timestamp_s": 561.0}, {"text": "It makes more sense to just retrieve the latest news.", "timestamp": "00:09:25,351", "timestamp_s": 565.0}, {"text": "In seconds, and then use that as the context for the LLM to answer.", "timestamp": "00:09:31,161", "timestamp_s": 571.0}, {"text": "Really important question about whether to execute a trade or not.", "timestamp": "00:09:38,871", "timestamp_s": 578.0}, {"text": "So in this case, you need up to date information and fine tuning is not", "timestamp": "00:09:44,571", "timestamp_s": 584.0}, {"text": "a bit feasible because you need to execute the results really fast.", "timestamp": "00:09:51,381", "timestamp_s": 591.0}, {"text": "And then the data over here is also changing over time as well.", "timestamp": "00:09:56,361", "timestamp_s": 596.0}, {"text": "So we can talk about more about RAG as well.", "timestamp": "00:10:01,291", "timestamp_s": 601.0}, {"text": "There are multiple types of rags, which are, which work in different use cases.", "timestamp": "00:10:04,921", "timestamp_s": 604.0}, {"text": "And this is something that can be part two of this to in the future, as well.", "timestamp": "00:10:11,371", "timestamp_s": 611.0}, {"text": "and then the last point over here is like prompt engineering,", "timestamp": "00:10:18,221", "timestamp_s": 618.0}, {"text": "which is pretty much like.", "timestamp": "00:10:21,341", "timestamp_s": 621.0}, {"text": "that\u0027s all about, for example, adding some context in the prompt where you think", "timestamp": "00:10:23,121", "timestamp_s": 623.0}, {"text": "you only have a few small contexts that you want LLM to take into account when", "timestamp": "00:10:27,771", "timestamp_s": 627.0}, {"text": "answering a question or doing some task.", "timestamp": "00:10:34,011", "timestamp_s": 634.0}, {"text": "And that context is pretty small, then it makes sense.", "timestamp": "00:10:36,941", "timestamp_s": 636.0}, {"text": "Just add that context in pretty much like you are on.", "timestamp": "00:10:39,671", "timestamp_s": 639.0}, {"text": "System prompts.", "timestamp": "00:10:44,881", "timestamp_s": 644.0}, {"text": "So in short, like you add your small content and system prom, if you have", "timestamp": "00:10:46,231", "timestamp_s": 646.0}, {"text": "data that\u0027s changing a lot, then makes sense to retrieve that data through rag.", "timestamp": "00:10:53,281", "timestamp_s": 653.0}, {"text": "If you have a really, if you have a decent amount of data.", "timestamp": "00:11:01,111", "timestamp_s": 661.0}, {"text": "On some domain and then you wanna fine tune LM to, to work on that domain,", "timestamp": "00:11:05,271", "timestamp_s": 665.0}, {"text": "then fine tuning makes more sense.", "timestamp": "00:11:10,461", "timestamp_s": 670.0}, {"text": "And then for example, like if you have large amount of data, then", "timestamp": "00:11:12,921", "timestamp_s": 672.0}, {"text": "going from going for building your LM from scratch makes more sense.", "timestamp": "00:11:17,001", "timestamp_s": 677.0}, {"text": "and then of course, for example, every method that we discussed just now", "timestamp": "00:11:22,901", "timestamp_s": 682.0}, {"text": "has different costs and everything.", "timestamp": "00:11:27,851", "timestamp_s": 687.0}, {"text": "For sure.", "timestamp": "00:11:32,111", "timestamp_s": 692.0}, {"text": "If you are.", "timestamp": "00:11:32,621", "timestamp_s": 692.0}, {"text": "Training from scratch, that\u0027s gonna be a really big hassle for sure.", "timestamp": "00:11:33,551", "timestamp_s": 693.0}, {"text": "Fine tuning is a bit cheaper and fine tuning.", "timestamp": "00:11:39,551", "timestamp_s": 699.0}, {"text": "The most important thing that you need is the compute resource", "timestamp": "00:11:44,501", "timestamp_s": 704.0}, {"text": "you can easily have, right?", "timestamp": "00:11:48,271", "timestamp_s": 708.0}, {"text": "There are so many different, even if you are using open ai, they, they provide you", "timestamp": "00:11:49,861", "timestamp_s": 709.0}, {"text": "with a way to fine tune open AI models.", "timestamp": "00:11:55,901", "timestamp_s": 715.0}, {"text": "You don\u0027t need to.", "timestamp": "00:11:59,411", "timestamp_s": 719.0}, {"text": "To care about compute resources over here for sure.", "timestamp": "00:12:00,851", "timestamp_s": 720.0}, {"text": "They will charge you more per token for the model, which is fine tuned", "timestamp": "00:12:04,871", "timestamp_s": 724.0}, {"text": "versus the one is not fine tuned.", "timestamp": "00:12:10,001", "timestamp_s": 730.0}, {"text": "But there\u0027s, but in the end, like the compute resource won\u0027t be too much.", "timestamp": "00:12:12,761", "timestamp_s": 732.0}, {"text": "if you wanna op, fine tune an open source model, then for sure you have", "timestamp": "00:12:17,616", "timestamp_s": 737.0}, {"text": "to think about compute resources.", "timestamp": "00:12:21,936", "timestamp_s": 741.0}, {"text": "The most important cause.", "timestamp": "00:12:24,966", "timestamp_s": 744.0}, {"text": "Will be in data preparation because like there, if there\u0027s a certain", "timestamp": "00:12:26,991", "timestamp_s": 746.0}, {"text": "format that data needs to have, and then let\u0027s say if Q data is", "timestamp": "00:12:31,741", "timestamp_s": 751.0}, {"text": "spread around, it\u0027s in bit pieces.", "timestamp": "00:12:37,321", "timestamp_s": 757.0}, {"text": "So compiling data collection, preparation is something that takes so much of the", "timestamp": "00:12:41,311", "timestamp_s": 761.0}, {"text": "time and that is where you need also manpower and that is where you need.", "timestamp": "00:12:46,821", "timestamp_s": 766.0}, {"text": "An investment into engineering as well.", "timestamp": "00:12:53,016", "timestamp_s": 773.0}, {"text": "And then for example, it, so fine tuning is not just a one time thing.", "timestamp": "00:12:57,326", "timestamp_s": 777.0}, {"text": "It\u0027s quite possible your data is changing every three months or every four months,", "timestamp": "00:13:03,186", "timestamp_s": 783.0}, {"text": "or every six months depending on that.", "timestamp": "00:13:10,026", "timestamp_s": 790.0}, {"text": "You have to also keep on fine tuning as well, so you need to have a", "timestamp": "00:13:13,986", "timestamp_s": 793.0}, {"text": "schedule for fine tuning your LLMs.", "timestamp": "00:13:17,936", "timestamp_s": 797.0}, {"text": "And speaking about the hidden cost here, the most important thing is like the", "timestamp": "00:13:22,066", "timestamp_s": 802.0}, {"text": "delayed implementation because There, data preparation involved, and then there\u0027s", "timestamp": "00:13:27,316", "timestamp_s": 807.0}, {"text": "also fine tuning involved, which can take a few days depending on the size of data.", "timestamp": "00:13:33,076", "timestamp_s": 813.0}, {"text": "And then, for example, the most important thing is, for example,", "timestamp": "00:13:39,946", "timestamp_s": 819.0}, {"text": "maintaining that custom model as well.", "timestamp": "00:13:43,366", "timestamp_s": 823.0}, {"text": "So yeah, that all adds up.", "timestamp": "00:13:47,716", "timestamp_s": 827.0}, {"text": "But.", "timestamp": "00:13:50,566", "timestamp_s": 830.0}, {"text": "If your use case is a use case where you want your model to be an expert in a", "timestamp": "00:13:51,366", "timestamp_s": 831.0}, {"text": "really niche domain, and then that, and then you have your own private data as", "timestamp": "00:13:57,246", "timestamp_s": 837.0}, {"text": "well on that domain than fine tuning is a way to go, and then the benefits will way", "timestamp": "00:14:02,986", "timestamp_s": 842.0}, {"text": "more than the cost and everything as well.", "timestamp": "00:14:10,246", "timestamp_s": 850.0}, {"text": "as I said before, the cost reduction, pretty much like what I see everyone", "timestamp": "00:14:13,666", "timestamp_s": 853.0}, {"text": "doing, I haven\u0027t, people don\u0027t retrain or fine tune the entire LLM because,", "timestamp": "00:14:20,196", "timestamp_s": 860.0}, {"text": "just because that\u0027s super hard.", "timestamp": "00:14:27,566", "timestamp_s": 867.0}, {"text": "In fact, they always go for an approach where they only fine tune.", "timestamp": "00:14:30,296", "timestamp_s": 870.0}, {"text": "One part of the model or last few layers of the model, and then as a result,", "timestamp": "00:14:37,826", "timestamp_s": 877.0}, {"text": "we have seen that\u0027s much quicker.", "timestamp": "00:14:44,236", "timestamp_s": 884.0}, {"text": "You don\u0027t need as much hardware as you, you required before.", "timestamp": "00:14:46,936", "timestamp_s": 886.0}, {"text": "Number three, your knowledge of your custom data will be so model", "timestamp": "00:14:53,416", "timestamp_s": 893.0}, {"text": "will be better able to learn that knowledge from custom data", "timestamp": "00:14:58,351", "timestamp_s": 898.0}, {"text": "and then that knowledge will be.", "timestamp": "00:15:02,646", "timestamp_s": 902.0}, {"text": "Stored in a few layers in the model, which is gonna be, which is even better, versus", "timestamp": "00:15:04,746", "timestamp_s": 904.0}, {"text": "spreading your knowledge about your custom data around 65 billion parameter model.", "timestamp": "00:15:11,716", "timestamp_s": 911.0}, {"text": "That\u0027s pretty big.", "timestamp": "00:15:17,716", "timestamp_s": 917.0}, {"text": "And then we have, I would say, a couple of.", "timestamp": "00:15:19,116", "timestamp_s": 919.0}, {"text": "Tuning things, for example, as well, fine tuning techniques right now out there.", "timestamp": "00:15:21,806", "timestamp_s": 921.0}, {"text": "So yeah, like a last few layers is a way to go.", "timestamp": "00:15:28,166", "timestamp_s": 928.0}, {"text": "Always.", "timestamp": "00:15:32,306", "timestamp_s": 932.0}, {"text": "And then moving on to was like the ROI.", "timestamp": "00:15:33,616", "timestamp_s": 933.0}, {"text": "The ROI pretty much the entire ROI equation depends on multiple things.", "timestamp": "00:15:39,096", "timestamp_s": 939.0}, {"text": "For example, how periodically you not wanna retrain everything", "timestamp": "00:15:45,461", "timestamp_s": 945.0}, {"text": "after three months or four months.", "timestamp": "00:15:51,281", "timestamp_s": 951.0}, {"text": "What\u0027s the inference cost quite possible that if you are using LLM, the inference", "timestamp": "00:15:53,801", "timestamp_s": 953.0}, {"text": "cause of a fine tune model is way higher.", "timestamp": "00:15:59,921", "timestamp_s": 959.0}, {"text": "that\u0027s.", "timestamp": "00:16:03,591", "timestamp_s": 963.0}, {"text": "Pretty important.", "timestamp": "00:16:04,566", "timestamp_s": 964.0}, {"text": "And then the initial fine tuning where you have to prepare data", "timestamp": "00:16:05,466", "timestamp_s": 965.0}, {"text": "is really important as well.", "timestamp": "00:16:10,506", "timestamp_s": 970.0}, {"text": "So all these things, factors, once again, will go into your", "timestamp": "00:16:11,796", "timestamp_s": 971.0}, {"text": "equation to find your ROI.", "timestamp": "00:16:15,706", "timestamp_s": 975.0}, {"text": "What\u0027s the cause, what\u0027s the advantage out there as well?", "timestamp": "00:16:19,366", "timestamp_s": 979.0}, {"text": "in the end, on average, I would say.", "timestamp": "00:16:24,296", "timestamp_s": 984.0}, {"text": "it\u0027s 20% or 15%.", "timestamp": "00:16:27,896", "timestamp_s": 987.0}, {"text": "The cost is higher for a fine tune model than the other way around.", "timestamp": "00:16:30,206", "timestamp_s": 990.0}, {"text": "So if your advantages or the benefits you will get out of is more than", "timestamp": "00:16:36,246", "timestamp_s": 996.0}, {"text": "that, then it\u0027s totally makes sense.", "timestamp": "00:16:42,516", "timestamp_s": 1002.0}, {"text": "and then speaking about the benefits, they are quite a lot.", "timestamp": "00:16:45,516", "timestamp_s": 1005.0}, {"text": "for example, if you wanna, if you have a, if you wanna have LLM, be an expert in a", "timestamp": "00:16:50,051", "timestamp_s": 1010.0}, {"text": "domain, then fine tuning on that domain will give you 15 to 30% more improvement", "timestamp": "00:16:55,181", "timestamp_s": 1015.0}, {"text": "in the accuracy of results compared to.", "timestamp": "00:17:04,451", "timestamp_s": 1024.0}, {"text": "Out of the box LLMs from open AI or from cloud and from other vendors out there.", "timestamp": "00:17:08,586", "timestamp_s": 1028.0}, {"text": "The next most important thing is, for example, as the model has", "timestamp": "00:17:16,026", "timestamp_s": 1036.0}, {"text": "more knowledge about that domain and that knowledge is fed into.", "timestamp": "00:17:19,446", "timestamp_s": 1039.0}, {"text": "A few layers of the model where the model can go and fetch that information.", "timestamp": "00:17:26,701", "timestamp_s": 1046.0}, {"text": "We also see around 50, 60% less hallucination.", "timestamp": "00:17:32,581", "timestamp_s": 1052.0}, {"text": "So in the use case where you need higher accuracy, fine tuning", "timestamp": "00:17:37,351", "timestamp_s": 1057.0}, {"text": "is a way to go here as well.", "timestamp": "00:17:43,271", "timestamp_s": 1063.0}, {"text": "Yeah.", "timestamp": "00:17:45,481", "timestamp_s": 1065.0}, {"text": "moving on.", "timestamp": "00:17:45,801", "timestamp_s": 1065.0}, {"text": "For example, like since you have spent fine tuning, You don\u0027t", "timestamp": "00:17:46,491", "timestamp_s": 1066.0}, {"text": "have to pass as much context in your input or system token.", "timestamp": "00:17:49,931", "timestamp_s": 1069.0}, {"text": "So as a result, you can also, there are cases where you can expect to have", "timestamp": "00:17:56,501", "timestamp_s": 1076.0}, {"text": "less inference calls just because like you need to use less tokens and then.", "timestamp": "00:18:01,911", "timestamp_s": 1081.0}, {"text": "If you, if reasoning is really important for you since you have better, more", "timestamp": "00:18:08,961", "timestamp_s": 1088.0}, {"text": "context, and so reasoning that you will get out of LLM with the output will be", "timestamp": "00:18:13,401", "timestamp_s": 1093.0}, {"text": "also much better than before as well.", "timestamp": "00:18:20,986", "timestamp_s": 1100.0}, {"text": "And then the other benefits that are.", "timestamp": "00:18:25,356", "timestamp_s": 1105.0}, {"text": "Extremely important to consider here are like, if you have your own", "timestamp": "00:18:28,536", "timestamp_s": 1108.0}, {"text": "fine tuned LLM on your own prior, proprietary data, that also gives you", "timestamp": "00:18:33,726", "timestamp_s": 1113.0}, {"text": "an advantage because that\u0027s your own ip.", "timestamp": "00:18:40,516", "timestamp_s": 1120.0}, {"text": "that\u0027s the own unique model and expert in some domain that you own as well.", "timestamp": "00:18:43,586", "timestamp_s": 1123.0}, {"text": "This is something that.", "timestamp": "00:18:49,886", "timestamp_s": 1129.0}, {"text": "Will surely go on to your asset books of the company, something that you can easily", "timestamp": "00:18:51,916", "timestamp_s": 1131.0}, {"text": "think about leveraging or selling or.", "timestamp": "00:18:57,376", "timestamp_s": 1137.0}, {"text": "Or even like licensing in the future as well.", "timestamp": "00:19:01,986", "timestamp_s": 1141.0}, {"text": "That\u0027s also gives you a really huge advantage over, over everyone.", "timestamp": "00:19:06,876", "timestamp_s": 1146.0}, {"text": "And then for example, if you are fine tuning on the open source model, it\u0027s", "timestamp": "00:19:12,466", "timestamp_s": 1152.0}, {"text": "quite possible that the fine tune model on your domain, on the open Source", "timestamp": "00:19:17,266", "timestamp_s": 1157.0}, {"text": "one will perform much better than any out of the books top models out there.", "timestamp": "00:19:22,606", "timestamp_s": 1162.0}, {"text": "From top printers and then in that case, like you will end up owning everything", "timestamp": "00:19:30,021", "timestamp_s": 1170.0}, {"text": "that owning the entire LLM or really big thing, which is an expert in that domain.", "timestamp": "00:19:35,091", "timestamp_s": 1175.0}, {"text": "and then for example, like if you\u0027re going within an open source route as", "timestamp": "00:19:43,581", "timestamp_s": 1183.0}, {"text": "well, there\u0027s also the advantage of.", "timestamp": "00:19:48,131", "timestamp_s": 1188.0}, {"text": "Enhance security means your sensitive data.", "timestamp": "00:19:51,906", "timestamp_s": 1191.0}, {"text": "Your LLM is gonna be trained or fine tuned on who will stay within your system.", "timestamp": "00:19:55,656", "timestamp_s": 1195.0}, {"text": "And it\u0027s, and then that\u0027s a much secure way of doing things.", "timestamp": "00:20:01,626", "timestamp_s": 1201.0}, {"text": "That\u0027s really important if you are in a healthcare space or, or", "timestamp": "00:20:07,396", "timestamp_s": 1207.0}, {"text": "a government organization that.", "timestamp": "00:20:10,906", "timestamp_s": 1210.0}, {"text": "Totally cares about your data as well.", "timestamp": "00:20:13,336", "timestamp_s": 1213.0}, {"text": "And then, or if you are in the industry where data is very", "timestamp": "00:20:16,126", "timestamp_s": 1216.0}, {"text": "important, you can\u0027t share.", "timestamp": "00:20:19,336", "timestamp_s": 1219.0}, {"text": "So for compliance reasons, you have to go with the open source one as well there.", "timestamp": "00:20:20,626", "timestamp_s": 1220.0}, {"text": "Uh, and then I\u0027ve also compiled here different metrics or", "timestamp": "00:20:26,586", "timestamp_s": 1226.0}, {"text": "different that people reported.", "timestamp": "00:20:34,046", "timestamp_s": 1234.0}, {"text": "For example, like how fine tuning were, was able to help them get", "timestamp": "00:20:36,701", "timestamp_s": 1236.0}, {"text": "better results in different domain.", "timestamp": "00:20:42,521", "timestamp_s": 1242.0}, {"text": "For example, yeahs are trained over wide array of data, of language data.", "timestamp": "00:20:44,201", "timestamp_s": 1244.0}, {"text": "Legal data is a bit different.", "timestamp": "00:20:52,421", "timestamp_s": 1252.0}, {"text": "You have a different lingo.", "timestamp": "00:20:55,061", "timestamp_s": 1255.0}, {"text": "like a sim.", "timestamp": "00:20:56,681", "timestamp_s": 1256.0}, {"text": "A person who hasn\u0027t gone through a law school will surely have.", "timestamp": "00:20:57,341", "timestamp_s": 1257.0}, {"text": "Have a problem reading all really big legal documents, right?", "timestamp": "00:21:01,591", "timestamp_s": 1261.0}, {"text": "That is what, and that is also the case with out of the books at the lamps.", "timestamp": "00:21:06,271", "timestamp_s": 1266.0}, {"text": "They\u0027re really good, but they do struggle sometimes.", "timestamp": "00:21:11,701", "timestamp_s": 1271.0}, {"text": "That is where, like the legal profession has found that if you fine", "timestamp": "00:21:14,791", "timestamp_s": 1274.0}, {"text": "tune that, these models on the legal data, you do get better results.", "timestamp": "00:21:18,821", "timestamp_s": 1278.0}, {"text": "The same also goes for the healthcare.", "timestamp": "00:21:25,251", "timestamp_s": 1285.0}, {"text": "it\u0027s much better also the financial services.", "timestamp": "00:21:28,786", "timestamp_s": 1288.0}, {"text": "For example, if you have your own priority data, then you wanna train the", "timestamp": "00:21:32,686", "timestamp_s": 1292.0}, {"text": "model on, that\u0027s much better as well.", "timestamp": "00:21:37,096", "timestamp_s": 1297.0}, {"text": "Maybe you have a model to execute some trades as before, and then in the,", "timestamp": "00:21:40,656", "timestamp_s": 1300.0}, {"text": "in that model, that execute train.", "timestamp": "00:21:46,866", "timestamp_s": 1306.0}, {"text": "One really important input is like what\u0027s happening in the entire world", "timestamp": "00:21:49,821", "timestamp_s": 1309.0}, {"text": "based on news and all that stuff.", "timestamp": "00:21:54,321", "timestamp_s": 1314.0}, {"text": "And that\u0027s where can easily come in.", "timestamp": "00:21:56,301", "timestamp_s": 1316.0}, {"text": "And then the fine tuned model will be able to pick up patterns from", "timestamp": "00:21:59,031", "timestamp_s": 1319.0}, {"text": "the world much better as well.", "timestamp": "00:22:02,766", "timestamp_s": 1322.0}, {"text": "And then in manufacturing, fine tuning is super important here if you are.", "timestamp": "00:22:06,086", "timestamp_s": 1326.0}, {"text": "And then at the same time, All the AI agents provide us in", "timestamp": "00:22:13,076", "timestamp_s": 1333.0}, {"text": "customer service out there.", "timestamp": "00:22:17,996", "timestamp_s": 1337.0}, {"text": "They have also found that, for example, fine tuning LLMs to", "timestamp": "00:22:20,096", "timestamp_s": 1340.0}, {"text": "better understand a business.", "timestamp": "00:22:26,516", "timestamp_s": 1346.0}, {"text": "For a client makes the LLM perform better and gives them the comparative advantage", "timestamp": "00:22:28,311", "timestamp_s": 1348.0}, {"text": "or edge over everyone, other vendors out there because here you are building", "timestamp": "00:22:35,001", "timestamp_s": 1355.0}, {"text": "your own mini LLM that understands your business and then if you are providing", "timestamp": "00:22:39,861", "timestamp_s": 1359.0}, {"text": "your customers with that, a really personalized for customer support.", "timestamp": "00:22:45,681", "timestamp_s": 1365.0}, {"text": "They surely will get better results than they will get from other AI", "timestamp": "00:22:51,381", "timestamp_s": 1371.0}, {"text": "customer support vendors out there.", "timestamp": "00:22:57,021", "timestamp_s": 1377.0}, {"text": "And so because you have better results, which are the most", "timestamp": "00:22:59,691", "timestamp_s": 1379.0}, {"text": "important, metric out there.", "timestamp": "00:23:03,081", "timestamp_s": 1383.0}, {"text": "what are some different technical considerations?", "timestamp": "00:23:05,571", "timestamp_s": 1385.0}, {"text": "If you are a tech person, going over this fine tuning rack and other stuff, so the", "timestamp": "00:23:11,021", "timestamp_s": 1391.0}, {"text": "most important thing is as we discussed before, is open source of closed source.", "timestamp": "00:23:17,421", "timestamp_s": 1397.0}, {"text": "It\u0027s data like, do you wanna train the last few layers or the entire model?", "timestamp": "00:23:22,901", "timestamp_s": 1402.0}, {"text": "That\u0027s really important to consider as well as I discussed before.", "timestamp": "00:23:30,916", "timestamp_s": 1410.0}, {"text": "And then if you\u0027re going for the rack architecture, given", "timestamp": "00:23:34,426", "timestamp_s": 1414.0}, {"text": "that you need that data in real time, that\u0027s revolving as well.", "timestamp": "00:23:38,806", "timestamp_s": 1418.0}, {"text": "they are some of the most important techniques I\u0027ve found so far", "timestamp": "00:23:42,606", "timestamp_s": 1422.0}, {"text": "are here, there is a different concepts that you wanna understand.", "timestamp": "00:23:46,566", "timestamp_s": 1426.0}, {"text": "This is something that can be, Some other talk where I go into detail on", "timestamp": "00:23:52,776", "timestamp_s": 1432.0}, {"text": "different rag techniques out there.", "timestamp": "00:23:58,396", "timestamp_s": 1438.0}, {"text": "So lastly, on a really high level comparison between RAG and fine tuning,", "timestamp": "00:24:01,056", "timestamp_s": 1441.0}, {"text": "the cause, for fine tuning is fix.", "timestamp": "00:24:06,636", "timestamp_s": 1446.0}, {"text": "Whereas for rag, every time you call an LLM.", "timestamp": "00:24:09,836", "timestamp_s": 1449.0}, {"text": "You might wanna call a rack to fetch relevant information from your databases,", "timestamp": "00:24:15,226", "timestamp_s": 1455.0}, {"text": "or you have a real time course for sure.", "timestamp": "00:24:22,726", "timestamp_s": 1462.0}, {"text": "So initial course is really high with the fine tuning.", "timestamp": "00:24:25,956", "timestamp_s": 1465.0}, {"text": "With rack, it\u0027s much lower With fine tuning you alway, you always have to", "timestamp": "00:24:30,351", "timestamp_s": 1470.0}, {"text": "retrain or refin tune after the interval once you get more and more data.", "timestamp": "00:24:36,041", "timestamp_s": 1476.0}, {"text": "And then for rack, you just have to update your knowledge basis every time", "timestamp": "00:24:42,901", "timestamp_s": 1482.0}, {"text": "the LLM needs to extract some information from knowledge base, LM can extract.", "timestamp": "00:24:47,381", "timestamp_s": 1487.0}, {"text": "The real time up to date information as well for sure.", "timestamp": "00:24:53,906", "timestamp_s": 1493.0}, {"text": "For example, in the RAG pipeline, like you have to call rag, you have to pass", "timestamp": "00:24:58,671", "timestamp_s": 1498.0}, {"text": "that to LLM, and then when you call LLM, you also have to add rag data", "timestamp": "00:25:03,591", "timestamp_s": 1503.0}, {"text": "tokens into LLM, so you have more token usage, so you have a really higher.", "timestamp": "00:25:09,351", "timestamp_s": 1509.0}, {"text": "Latency as well compared to fine tune, reasoning, it\u0027s better or even", "timestamp": "00:25:16,241", "timestamp_s": 1516.0}, {"text": "same with the bot techniques as well.", "timestamp": "00:25:21,551", "timestamp_s": 1521.0}, {"text": "in terms of infrastructure, you need a different infrastructure in.", "timestamp": "00:25:24,651", "timestamp_s": 1524.0}, {"text": "In both cases, if you are using OP out of the box, LLM providers, there\u0027s no", "timestamp": "00:25:31,446", "timestamp_s": 1531.0}, {"text": "infrastructure required because you, they just want you to provide them", "timestamp": "00:25:37,896", "timestamp_s": 1537.0}, {"text": "with data in some certain format, and then once you upload data in open ai,", "timestamp": "00:25:43,746", "timestamp_s": 1543.0}, {"text": "it just does the fine tuning for you.", "timestamp": "00:25:49,746", "timestamp_s": 1549.0}, {"text": "There\u0027s no need to build or maintain anything at all on your side.", "timestamp": "00:25:53,586", "timestamp_s": 1553.0}, {"text": "On the other hand for the rack, you do need to build your vector", "timestamp": "00:25:59,391", "timestamp_s": 1559.0}, {"text": "database to store your embeddings.", "timestamp": "00:26:04,311", "timestamp_s": 1564.0}, {"text": "And pretty much there is some investment required data as well.", "timestamp": "00:26:07,601", "timestamp_s": 1567.0}, {"text": "I think we already recovered data sensitivity as well before.", "timestamp": "00:26:11,671", "timestamp_s": 1571.0}, {"text": "So in, in summary, you\u0027ll choose fine tuning when the knowledge", "timestamp": "00:26:16,461", "timestamp_s": 1576.0}, {"text": "is not changing rapidly.", "timestamp": "00:26:22,731", "timestamp_s": 1582.0}, {"text": "You have a really high volume of Curie, right?", "timestamp": "00:26:25,331", "timestamp_s": 1585.0}, {"text": "And then it makes sense to find, tune it versus using rack because like", "timestamp": "00:26:29,171", "timestamp_s": 1589.0}, {"text": "you have a really high cost we have.", "timestamp": "00:26:36,071", "timestamp_s": 1596.0}, {"text": "And then, you will use fine tuning more.", "timestamp": "00:26:38,041", "timestamp_s": 1598.0}, {"text": "when you have really lar large amount of domain knowledge that is required", "timestamp": "00:26:40,901", "timestamp_s": 1600.0}, {"text": "by the LLM to answer a question, right?", "timestamp": "00:26:45,821", "timestamp_s": 1605.0}, {"text": "That they\u0027re, and then in the end, for example, like you need really fast", "timestamp": "00:26:48,311", "timestamp_s": 1608.0}, {"text": "results, like you need really high accuracy reserve, that\u0027s where you will", "timestamp": "00:26:53,741", "timestamp_s": 1613.0}, {"text": "for surely go for fine tuning as well.", "timestamp": "00:26:58,181", "timestamp_s": 1618.0}, {"text": "And then you will choose rag, Where the data is evolving by a lot and", "timestamp": "00:27:01,801", "timestamp_s": 1621.0}, {"text": "you need to extract relevant data in real that\u0027s evolving in real time.", "timestamp": "00:27:11,101", "timestamp_s": 1631.0}, {"text": "You also use rag, for example, if you wanna do citations, right?", "timestamp": "00:27:18,741", "timestamp_s": 1638.0}, {"text": "Fine tuning LL M1, know which document had that information that LM learned?", "timestamp": "00:27:23,682", "timestamp_s": 1643.0}, {"text": "But with rag, since you are adding extra knowledge required by LLM.", "timestamp": "00:27:31,752", "timestamp_s": 1651.0}, {"text": "In the context, LLM will, you can easily cite as well.", "timestamp": "00:27:37,722", "timestamp_s": 1657.0}, {"text": "If you need citations, then RAG is a way to go and you will use rag.", "timestamp": "00:27:41,982", "timestamp_s": 1661.0}, {"text": "If you have only small amount of data, that fine tuning won\u0027t make any sense.", "timestamp": "00:27:47,502", "timestamp_s": 1667.0}, {"text": "You like, hey, I\u0027ll just add that data in the context as well.", "timestamp": "00:27:52,092", "timestamp_s": 1672.0}, {"text": "and then once again, like you don\u0027t wanna spend so much time.", "timestamp": "00:27:57,642", "timestamp_s": 1677.0}, {"text": "Initially fine tuning.", "timestamp": "00:28:01,797", "timestamp_s": 1681.0}, {"text": "There\u0027s no budget.", "timestamp": "00:28:03,507", "timestamp_s": 1683.0}, {"text": "That\u0027s also where fine tuning is where to go as well.", "timestamp": "00:28:05,097", "timestamp_s": 1685.0}, {"text": "I do also have a few right now case studies that I have mentioned here.", "timestamp": "00:28:08,787", "timestamp_s": 1688.0}, {"text": "I will be adding the link of this artifact as well, so", "timestamp": "00:28:14,067", "timestamp_s": 1694.0}, {"text": "people can easily learn from it.", "timestamp": "00:28:19,422", "timestamp_s": 1699.0}, {"text": "So once again, key takeaways, like we have different techniques out there.", "timestamp": "00:28:22,022", "timestamp_s": 1702.0}, {"text": "There\u0027s fine tuning.", "timestamp": "00:28:29,287", "timestamp_s": 1709.0}, {"text": "There\u0027s fine tuning.", "timestamp": "00:28:31,177", "timestamp_s": 1711.0}, {"text": "Only a few layers.", "timestamp": "00:28:32,407", "timestamp_s": 1712.0}, {"text": "There is rack.", "timestamp": "00:28:34,477", "timestamp_s": 1714.0}, {"text": "So depending on different scenarios, every technique is really important, right?", "timestamp": "00:28:36,577", "timestamp_s": 1716.0}, {"text": "So we learn that fine tuning can be something of an investment where you will", "timestamp": "00:28:44,417", "timestamp_s": 1724.0}, {"text": "end up building your own prior prietary.", "timestamp": "00:28:51,377", "timestamp_s": 1731.0}, {"text": "Model that works really well in, in the domain you operate in.", "timestamp": "00:28:55,247", "timestamp_s": 1735.0}, {"text": "This is something that you can license in the end, in the future or sell", "timestamp": "00:29:01,157", "timestamp_s": 1741.0}, {"text": "that\u0027s an asset that belongs to you.", "timestamp": "00:29:06,797", "timestamp_s": 1746.0}, {"text": "we also learned that, they are the cost.", "timestamp": "00:29:10,057", "timestamp_s": 1750.0}, {"text": "The direct course of fine tuning is the initial course that you have to spend", "timestamp": "00:29:14,147", "timestamp_s": 1754.0}, {"text": "money on where you need data preparation, and then we also learn that there is", "timestamp": "00:29:20,867", "timestamp_s": 1760.0}, {"text": "also the indirect course as well as associated with that, and then fine", "timestamp": "00:29:26,717", "timestamp_s": 1766.0}, {"text": "tuning can be done faster if you only like fine tune a small part of the model.", "timestamp": "00:29:32,357", "timestamp_s": 1772.0}, {"text": "Which would also means that your knowledge will be better learned", "timestamp": "00:29:39,972", "timestamp_s": 1779.0}, {"text": "by the model as well, because it\u0027s all confined to that small part.", "timestamp": "00:29:44,142", "timestamp_s": 1784.0}, {"text": "And then there is also a possibility that I. Your use case will need", "timestamp": "00:29:49,052", "timestamp_s": 1789.0}, {"text": "both rack and both fine tuning.", "timestamp": "00:29:55,342", "timestamp_s": 1795.0}, {"text": "It\u0027s quite possible that there are two types of information there is fixing.", "timestamp": "00:29:58,072", "timestamp_s": 1798.0}, {"text": "There\u0027s evolving.", "timestamp": "00:30:03,292", "timestamp_s": 1803.0}, {"text": "What\u0027s in evolving information can become part of the rack, what\u0027s fixed and what\u0027s", "timestamp": "00:30:04,882", "timestamp_s": 1804.0}, {"text": "really in high large quantity data.", "timestamp": "00:30:11,632", "timestamp_s": 1811.0}, {"text": "That can easily become your fine tuning can be used in fine tuning as well.", "timestamp": "00:30:15,442", "timestamp_s": 1815.0}, {"text": "so customer support.", "timestamp": "00:30:22,042", "timestamp_s": 1822.0}, {"text": "All the last tickets.", "timestamp": "00:30:24,022", "timestamp_s": 1824.0}, {"text": "That will resolve, pretty much can be used for fine tuning LLM", "timestamp": "00:30:26,232", "timestamp_s": 1826.0}, {"text": "to tell LM, Hey, that\u0027s how you resolve a customer support curie.", "timestamp": "00:30:30,882", "timestamp_s": 1830.0}, {"text": "And then the information about policies refund what\u0027s available in", "timestamp": "00:30:36,192", "timestamp_s": 1836.0}, {"text": "our data or what\u0027s available in stock.", "timestamp": "00:30:42,642", "timestamp_s": 1842.0}, {"text": "This is.", "timestamp": "00:30:46,332", "timestamp_s": 1846.0}, {"text": "In the company.", "timestamp": "00:30:47,492", "timestamp_s": 1847.0}, {"text": "This is the evolving information, and this can be part of the rag, as well.", "timestamp": "00:30:48,272", "timestamp_s": 1848.0}, {"text": "So there can be a use case where you need both of these", "timestamp": "00:30:53,292", "timestamp_s": 1853.0}, {"text": "things, depending on the context.", "timestamp": "00:30:56,982", "timestamp_s": 1856.0}, {"text": "yep.", "timestamp": "00:30:59,602", "timestamp_s": 1859.0}, {"text": "So this is on a really high level, what this is all about.", "timestamp": "00:31:00,022", "timestamp_s": 1860.0}, {"text": "why use LLM for, why use fine tuning and why is that important for every business?", "timestamp": "00:31:04,952", "timestamp_s": 1864.0}, {"text": "Say tune for more talks in the future.", "timestamp": "00:31:11,132", "timestamp_s": 1871.0}, {"text": "Bye.", "timestamp": "00:31:14,802", "timestamp_s": 1874.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'X5o7d-Y-970',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Fine-tuning LLMs: A Cost-Benefit Analysis for Businesses
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>As businesses scramble to integrate Large Language Models (LLMs) into their workflows, a pertinent question arises: Is fine-tuning worth the investment? Fine-tuning offers domain-specific precision and improved user satisfaction, but requires tremendous upfront data, computational power, and skilled personnel. This presentation analyzes the economics of fine-tuningâ€™s intricacies, when it is a good investment, when retrieval-augmented generation (RAG) is a more suitable option, and how companies can balance their need for custom solutions against cost considerations.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./srt/llms2025_Muddassar_Sharif.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:00,360'); seek(0.0)">
              Hey everyone, so I am moder, and this is gonna be my short talk on go
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:09,300'); seek(9.0)">
              manifest analysis of for businesses when it comes to fine tuning LLMs or
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:15,840'); seek(15.0)">
              deciding between fine tuning and other techniques out there in order to get
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:22,740'); seek(22.0)">
              to the final result that the use case.
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:25,830'); seek(25.0)">
              Company wants to solve.
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:27,270'); seek(27.0)">
              So like the most, like in every use case, like the central question that
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:34,619'); seek(34.0)">
              every business operations or stakeholder needs to answer is is it possible for
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:40,459'); seek(40.0)">
              us to use just pre-trained models that are out there from open ai, from cloud?
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:48,830'); seek(48.0)">
              The reason being is because they are out of the box.
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:52,400'); seek(52.0)">
              There is no need to do any fine tuning there.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:56,839'); seek(56.0)">
              They are fast, they are affordable.
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:00,050'); seek(60.0)">
              And the next most important question is, for example, like in our use case.
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:06,860'); seek(66.0)">
              What is our use case?
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:08,510'); seek(68.0)">
              And in our use case, is there a need?
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:12,410'); seek(72.0)">
              A need for any data that is not public, that is private?
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:18,530'); seek(78.0)">
              So you want L them to go over that data, understand patterns in that
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:24,410'); seek(84.0)">
              data in order to answer some really important question or was or call
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:29,320'); seek(89.0)">
              some really important function or tool if you are building an agent.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:34,244'); seek(94.0)">
              AI and then based on that, do something.
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:37,475'); seek(97.0)">
              So that's where if you have your own private data do is it required or not?
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:42,385'); seek(102.0)">
              That's a really important question that every stakeholder needs to answer.
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:46,165'); seek(106.0)">
              And then based on this answer, let's say you need, you do need your data in
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:52,125'); seek(112.0)">
              the entire use case you wanna resolve.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:54,795'); seek(114.0)">
              Then the question is do you.
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:57,255'); seek(117.0)">
              Fine tune your model on that data, or do you just every time you wanna answer some
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:02:02,965'); seek(122.0)">
              question, you go and you retrieve the most important bits and pieces of your
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:02:09,375'); seek(129.0)">
              private data, and then you just see, add that in the model without fine tuning it.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:15,795'); seek(135.0)">
              So these are two different, I would say techniques out there where
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:20,385'); seek(140.0)">
              you can use your private data.
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:22,505'); seek(142.0)">
              So I feel like to answer this question on what do you choose?
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:26,845'); seek(146.0)">
              Do we choose fine tuning or do you choose rack where you retrieve the
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:33,045'); seek(153.0)">
              most important bits of data and then use that as context in the LLM?
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:38,285'); seek(158.0)">
              I think like it's really important too.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:40,310'); seek(160.0)">
              First of all, understand what is fine tuning and what the fundamentals
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:44,210'); seek(164.0)">
              are in order and before we can easily answer this question.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:49,190'); seek(169.0)">
              So the fine tuning is all about adjusting the weights of the
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:54,610'); seek(174.0)">
              model, with the new weights.
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:57,170'); seek(177.0)">
              And the new weights will have knowledge of that private data as well.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:03:01,540'); seek(181.0)">
              fine tuning also means, for example, you have to modify how the models behave.
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:03:06,240'); seek(186.0)">
              You have the models need to unlearn something or relearn something
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:03:10,800'); seek(190.0)">
              because you wanna, you want model to like, just focus on your data
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:03:15,060'); seek(195.0)">
              and then forget about the past.
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:17,430'); seek(197.0)">
              And the way I learn LS or any deep learning model models work is as
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:24,320'); seek(204.0)">
              you train and keep on training.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:26,975'); seek(206.0)">
              The latest training always has a precedent over the old one.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:32,225'); seek(212.0)">
              So if you take a LLM or large language model that's pre-trained on some
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:39,415'); seek(219.0)">
              natural language data, let's say if I just find, tune that on medical
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:46,615'); seek(226.0)">
              data that LLM will eventually like.
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:50,025'); seek(230.0)">
              Learn, be more focused or learn more stuff about medical data.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:54,445'); seek(234.0)">
              and in order to do that, it'll also unlearn some of the
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:59,185'); seek(239.0)">
              stuff that it learned before.
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:04:00,985'); seek(240.0)">
              So fine tuning in a sense, will update your model.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:04:04,835'); seek(244.0)">
              It waits.
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:04:06,205'); seek(246.0)">
              And there are two things that, and then there, and then within
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:04:09,415'); seek(249.0)">
              the fine tuning domain, there are two ways that you can fine tune.
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:04:13,165'); seek(253.0)">
              Number one is like you, you update all the weights, every layer of your
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:04:20,905'); seek(260.0)">
              model, which is really expensive by the way, because, let's say.
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:04:26,105'); seek(266.0)">
              The models that we have right now out there.
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:29,345'); seek(269.0)">
              So one way to fine tune is like find, just updating all the weights in, all
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:38,415'); seek(278.0)">
              the weights right now that we have in our model, which is really expensive
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:42,375'); seek(282.0)">
              by the way, because as I said before, large models, they are really big.
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:48,585'); seek(288.0)">
              They have one 50 billion parameters wide.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:51,195'); seek(291.0)">
              So let's say if you wanna go and.
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:54,090'); seek(294.0)">
              Update or the entire model.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:57,810'); seek(297.0)">
              You need a really big hardware set up, which is really expensive by the way.
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:05:03,240'); seek(303.0)">
              And the other way of fine tuning, which is also provided out of the works by
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:05:11,100'); seek(311.0)">
              Open AI cloud and other companies is just Fine tuning a few layers towards the end.
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:05:18,710'); seek(318.0)">
              It's called, meter efficient fine tuning because like you are only, and then here
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:05:25,280'); seek(325.0)">
              you only fine tuning the last few layers.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:05:29,120'); seek(329.0)">
              The advantage being is like the last few layers will be able
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:05:33,650'); seek(333.0)">
              to retain the knowledge that.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:05:36,365'); seek(336.0)">
              Or learn from your own private data.
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:05:39,605'); seek(339.0)">
              Whereas all the other layers in the model, which are already trained on other
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:05:46,205'); seek(346.0)">
              aspects of language, for example, grammar and other stuff, and then emotions.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:52,625'); seek(352.0)">
              So all that knowledge you still need, right?
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:55,775'); seek(355.0)">
              So that's the overall idea of just fine tuning a few layers and normally.
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:06:02,315'); seek(362.0)">
              That's much cheaper, that's much faster.
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:06:05,165'); seek(365.0)">
              You can, if you have a small amount of your data, you can easily
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:06:09,095'); seek(369.0)">
              fine tune it and then the cost won't be super high at the time.
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:06:13,805'); seek(373.0)">
              Requirement is also pretty low as well.
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:06:17,825'); seek(377.0)">
              And then you can move forward with that.
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:06:20,191'); seek(380.0)">
              So if I just recap everything right now on Prompt engineering and then, fine tuning
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:06:28,076'); seek(388.0)">
              and rack, and then from scratch, as well.
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:06:30,996'); seek(390.0)">
              So if you are fine tuning the entire model, then it's same as like
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:06:37,671'); seek(397.0)">
              building the model from scratch.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:06:39,741'); seek(399.0)">
              It's not same as, but in the end, for example, like you, since you are.
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:06:43,691'); seek(403.0)">
              It's like build, building the model from scratch because like you have
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:06:47,531'); seek(407.0)">
              to update so many different layers and weight in the entire model.
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:06:52,011'); seek(412.0)">
              and then you will end up having your own cluster model, which you will
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:06:56,241'); seek(416.0)">
              have a full control over, but in the end, like it's gonna be really
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:07:00,171'); seek(420.0)">
              expensive to train it from scratch and then maintain that on the server.
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:07:09,051'); seek(429.0)">
              From scratching, train from scratch is really important pathway.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:07:13,821'); seek(433.0)">
              If you have terabytes or petabytes of large amount of data and then you know
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:07:19,761'); seek(439.0)">
              that, you won't be able to fine tune, get the results by just fine tuning
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:07:25,371'); seek(445.0)">
              the last few layers you need to like.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:07:29,361'); seek(449.0)">
              Train majority of your entire model.
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:07:33,591'); seek(453.0)">
              And then in the end, the other important consideration here is that let's say you
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:07:40,461'); seek(460.0)">
              want to keep your model private as well, so that, so from scratch or taking the
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:07:46,581'); seek(466.0)">
              open source model and then retraining it on your data makes a lot of sense.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:07:52,351'); seek(472.0)">
              Let's say if you have a very small amount of data and then you are not
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:07:55,501'); seek(475.0)">
              concerned about privacy, or if you are even concerned about privacy based on
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:08:01,031'); seek(481.0)">
              the privacy factor, you will decide to either use an open source model or
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:08:07,211'); seek(487.0)">
              you will use, the closed source one.
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:08:10,361'); seek(490.0)">
              For example, open AI cloud.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:08:12,551'); seek(492.0)">
              And then when you once, and then once you decide the close and open source model,
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:08:16,721'); seek(496.0)">
              since you have small amount of data.
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:08:18,901'); seek(498.0)">
              Then fine tuning the last few layers makes a lot of sense.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:08:22,441'); seek(502.0)">
              And then as a result, you will see that, for example, the model will be
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:08:27,251'); seek(507.0)">
              better able to answer questions in the domain of data that you have given.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:08:32,621'); seek(512.0)">
              Let's say if you have given a really specialized data on health tech,
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:08:38,681'); seek(518.0)">
              for example, then you can expect a fine tuned model to answer.
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:08:44,846'); seek(524.0)">
              Health tech questions better than the model.
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:08:48,776'); seek(528.0)">
              That's not fine tuning.
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:08:50,586'); seek(530.0)">
              and then for example, why would you not use fine tuning, let's
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:08:55,116'); seek(535.0)">
              say your data is evolving a lot.
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:08:59,116'); seek(539.0)">
              Just to give one more example on when you should use rag.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:09:05,431'); seek(545.0)">
              Or fine tuning is, let's say like you have a an LLM or an AI agent, which
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:09:12,871'); seek(552.0)">
              buys and sells stock based on the news.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:09:17,311'); seek(557.0)">
              Since news is evolving and everything is evolving, it makes no sense just
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:09:21,661'); seek(561.0)">
              to fine tune the LLM on all the news.
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:09:25,351'); seek(565.0)">
              It makes more sense to just retrieve the latest news.
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:09:31,161'); seek(571.0)">
              In seconds, and then use that as the context for the LLM to answer.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:09:38,871'); seek(578.0)">
              Really important question about whether to execute a trade or not.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:09:44,571'); seek(584.0)">
              So in this case, you need up to date information and fine tuning is not
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:09:51,381'); seek(591.0)">
              a bit feasible because you need to execute the results really fast.
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:09:56,361'); seek(596.0)">
              And then the data over here is also changing over time as well.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:10:01,291'); seek(601.0)">
              So we can talk about more about RAG as well.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:10:04,921'); seek(604.0)">
              There are multiple types of rags, which are, which work in different use cases.
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:10:11,371'); seek(611.0)">
              And this is something that can be part two of this to in the future, as well.
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:10:18,221'); seek(618.0)">
              and then the last point over here is like prompt engineering,
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:10:21,341'); seek(621.0)">
              which is pretty much like.
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:10:23,121'); seek(623.0)">
              that's all about, for example, adding some context in the prompt where you think
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:10:27,771'); seek(627.0)">
              you only have a few small contexts that you want LLM to take into account when
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:10:34,011'); seek(634.0)">
              answering a question or doing some task.
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:10:36,941'); seek(636.0)">
              And that context is pretty small, then it makes sense.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:10:39,671'); seek(639.0)">
              Just add that context in pretty much like you are on.
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:10:44,881'); seek(644.0)">
              System prompts.
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:10:46,231'); seek(646.0)">
              So in short, like you add your small content and system prom, if you have
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:10:53,281'); seek(653.0)">
              data that's changing a lot, then makes sense to retrieve that data through rag.
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:11:01,111'); seek(661.0)">
              If you have a really, if you have a decent amount of data.
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:11:05,271'); seek(665.0)">
              On some domain and then you wanna fine tune LM to, to work on that domain,
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:11:10,461'); seek(670.0)">
              then fine tuning makes more sense.
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:11:12,921'); seek(672.0)">
              And then for example, like if you have large amount of data, then
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:11:17,001'); seek(677.0)">
              going from going for building your LM from scratch makes more sense.
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:11:22,901'); seek(682.0)">
              and then of course, for example, every method that we discussed just now
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:11:27,851'); seek(687.0)">
              has different costs and everything.
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:11:32,111'); seek(692.0)">
              For sure.
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:11:32,621'); seek(692.0)">
              If you are.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:11:33,551'); seek(693.0)">
              Training from scratch, that's gonna be a really big hassle for sure.
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:11:39,551'); seek(699.0)">
              Fine tuning is a bit cheaper and fine tuning.
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:11:44,501'); seek(704.0)">
              The most important thing that you need is the compute resource
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:11:48,271'); seek(708.0)">
              you can easily have, right?
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:11:49,861'); seek(709.0)">
              There are so many different, even if you are using open ai, they, they provide you
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:11:55,901'); seek(715.0)">
              with a way to fine tune open AI models.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:11:59,411'); seek(719.0)">
              You don't need to.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:12:00,851'); seek(720.0)">
              To care about compute resources over here for sure.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:12:04,871'); seek(724.0)">
              They will charge you more per token for the model, which is fine tuned
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:12:10,001'); seek(730.0)">
              versus the one is not fine tuned.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:12:12,761'); seek(732.0)">
              But there's, but in the end, like the compute resource won't be too much.
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:12:17,616'); seek(737.0)">
              if you wanna op, fine tune an open source model, then for sure you have
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:12:21,936'); seek(741.0)">
              to think about compute resources.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:12:24,966'); seek(744.0)">
              The most important cause.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:12:26,991'); seek(746.0)">
              Will be in data preparation because like there, if there's a certain
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:12:31,741'); seek(751.0)">
              format that data needs to have, and then let's say if Q data is
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:12:37,321'); seek(757.0)">
              spread around, it's in bit pieces.
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:12:41,311'); seek(761.0)">
              So compiling data collection, preparation is something that takes so much of the
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:12:46,821'); seek(766.0)">
              time and that is where you need also manpower and that is where you need.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:12:53,016'); seek(773.0)">
              An investment into engineering as well.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:12:57,326'); seek(777.0)">
              And then for example, it, so fine tuning is not just a one time thing.
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:13:03,186'); seek(783.0)">
              It's quite possible your data is changing every three months or every four months,
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:13:10,026'); seek(790.0)">
              or every six months depending on that.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:13:13,986'); seek(793.0)">
              You have to also keep on fine tuning as well, so you need to have a
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:13:17,936'); seek(797.0)">
              schedule for fine tuning your LLMs.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:13:22,066'); seek(802.0)">
              And speaking about the hidden cost here, the most important thing is like the
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:13:27,316'); seek(807.0)">
              delayed implementation because There, data preparation involved, and then there's
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:13:33,076'); seek(813.0)">
              also fine tuning involved, which can take a few days depending on the size of data.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:13:39,946'); seek(819.0)">
              And then, for example, the most important thing is, for example,
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:13:43,366'); seek(823.0)">
              maintaining that custom model as well.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:13:47,716'); seek(827.0)">
              So yeah, that all adds up.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:13:50,566'); seek(830.0)">
              But.
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:13:51,366'); seek(831.0)">
              If your use case is a use case where you want your model to be an expert in a
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:13:57,246'); seek(837.0)">
              really niche domain, and then that, and then you have your own private data as
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:14:02,986'); seek(842.0)">
              well on that domain than fine tuning is a way to go, and then the benefits will way
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:14:10,246'); seek(850.0)">
              more than the cost and everything as well.
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:14:13,666'); seek(853.0)">
              as I said before, the cost reduction, pretty much like what I see everyone
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:14:20,196'); seek(860.0)">
              doing, I haven't, people don't retrain or fine tune the entire LLM because,
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:14:27,566'); seek(867.0)">
              just because that's super hard.
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:14:30,296'); seek(870.0)">
              In fact, they always go for an approach where they only fine tune.
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:14:37,826'); seek(877.0)">
              One part of the model or last few layers of the model, and then as a result,
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:14:44,236'); seek(884.0)">
              we have seen that's much quicker.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:14:46,936'); seek(886.0)">
              You don't need as much hardware as you, you required before.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:14:53,416'); seek(893.0)">
              Number three, your knowledge of your custom data will be so model
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:14:58,351'); seek(898.0)">
              will be better able to learn that knowledge from custom data
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:15:02,646'); seek(902.0)">
              and then that knowledge will be.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:15:04,746'); seek(904.0)">
              Stored in a few layers in the model, which is gonna be, which is even better, versus
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:15:11,716'); seek(911.0)">
              spreading your knowledge about your custom data around 65 billion parameter model.
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:15:17,716'); seek(917.0)">
              That's pretty big.
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:15:19,116'); seek(919.0)">
              And then we have, I would say, a couple of.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:15:21,806'); seek(921.0)">
              Tuning things, for example, as well, fine tuning techniques right now out there.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:15:28,166'); seek(928.0)">
              So yeah, like a last few layers is a way to go.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:15:32,306'); seek(932.0)">
              Always.
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:15:33,616'); seek(933.0)">
              And then moving on to was like the ROI.
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:15:39,096'); seek(939.0)">
              The ROI pretty much the entire ROI equation depends on multiple things.
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:15:45,461'); seek(945.0)">
              For example, how periodically you not wanna retrain everything
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:15:51,281'); seek(951.0)">
              after three months or four months.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:15:53,801'); seek(953.0)">
              What's the inference cost quite possible that if you are using LLM, the inference
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:15:59,921'); seek(959.0)">
              cause of a fine tune model is way higher.
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:16:03,591'); seek(963.0)">
              that's.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:16:04,566'); seek(964.0)">
              Pretty important.
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:16:05,466'); seek(965.0)">
              And then the initial fine tuning where you have to prepare data
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:16:10,506'); seek(970.0)">
              is really important as well.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:16:11,796'); seek(971.0)">
              So all these things, factors, once again, will go into your
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:16:15,706'); seek(975.0)">
              equation to find your ROI.
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:16:19,366'); seek(979.0)">
              What's the cause, what's the advantage out there as well?
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:16:24,296'); seek(984.0)">
              in the end, on average, I would say.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:16:27,896'); seek(987.0)">
              it's 20% or 15%.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:16:30,206'); seek(990.0)">
              The cost is higher for a fine tune model than the other way around.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:16:36,246'); seek(996.0)">
              So if your advantages or the benefits you will get out of is more than
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:16:42,516'); seek(1002.0)">
              that, then it's totally makes sense.
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:16:45,516'); seek(1005.0)">
              and then speaking about the benefits, they are quite a lot.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:16:50,051'); seek(1010.0)">
              for example, if you wanna, if you have a, if you wanna have LLM, be an expert in a
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:16:55,181'); seek(1015.0)">
              domain, then fine tuning on that domain will give you 15 to 30% more improvement
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:17:04,451'); seek(1024.0)">
              in the accuracy of results compared to.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:17:08,586'); seek(1028.0)">
              Out of the box LLMs from open AI or from cloud and from other vendors out there.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:17:16,026'); seek(1036.0)">
              The next most important thing is, for example, as the model has
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:17:19,446'); seek(1039.0)">
              more knowledge about that domain and that knowledge is fed into.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:17:26,701'); seek(1046.0)">
              A few layers of the model where the model can go and fetch that information.
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:17:32,581'); seek(1052.0)">
              We also see around 50, 60% less hallucination.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:17:37,351'); seek(1057.0)">
              So in the use case where you need higher accuracy, fine tuning
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:17:43,271'); seek(1063.0)">
              is a way to go here as well.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:17:45,481'); seek(1065.0)">
              Yeah.
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:17:45,801'); seek(1065.0)">
              moving on.
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:17:46,491'); seek(1066.0)">
              For example, like since you have spent fine tuning, You don't
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:17:49,931'); seek(1069.0)">
              have to pass as much context in your input or system token.
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:17:56,501'); seek(1076.0)">
              So as a result, you can also, there are cases where you can expect to have
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:18:01,911'); seek(1081.0)">
              less inference calls just because like you need to use less tokens and then.
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:18:08,961'); seek(1088.0)">
              If you, if reasoning is really important for you since you have better, more
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:18:13,401'); seek(1093.0)">
              context, and so reasoning that you will get out of LLM with the output will be
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:18:20,986'); seek(1100.0)">
              also much better than before as well.
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:18:25,356'); seek(1105.0)">
              And then the other benefits that are.
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:18:28,536'); seek(1108.0)">
              Extremely important to consider here are like, if you have your own
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:18:33,726'); seek(1113.0)">
              fine tuned LLM on your own prior, proprietary data, that also gives you
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:18:40,516'); seek(1120.0)">
              an advantage because that's your own ip.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:18:43,586'); seek(1123.0)">
              that's the own unique model and expert in some domain that you own as well.
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:18:49,886'); seek(1129.0)">
              This is something that.
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:18:51,916'); seek(1131.0)">
              Will surely go on to your asset books of the company, something that you can easily
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:18:57,376'); seek(1137.0)">
              think about leveraging or selling or.
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:19:01,986'); seek(1141.0)">
              Or even like licensing in the future as well.
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:19:06,876'); seek(1146.0)">
              That's also gives you a really huge advantage over, over everyone.
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:19:12,466'); seek(1152.0)">
              And then for example, if you are fine tuning on the open source model, it's
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:19:17,266'); seek(1157.0)">
              quite possible that the fine tune model on your domain, on the open Source
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:19:22,606'); seek(1162.0)">
              one will perform much better than any out of the books top models out there.
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:19:30,021'); seek(1170.0)">
              From top printers and then in that case, like you will end up owning everything
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:19:35,091'); seek(1175.0)">
              that owning the entire LLM or really big thing, which is an expert in that domain.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:19:43,581'); seek(1183.0)">
              and then for example, like if you're going within an open source route as
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:19:48,131'); seek(1188.0)">
              well, there's also the advantage of.
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:19:51,906'); seek(1191.0)">
              Enhance security means your sensitive data.
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:19:55,656'); seek(1195.0)">
              Your LLM is gonna be trained or fine tuned on who will stay within your system.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:20:01,626'); seek(1201.0)">
              And it's, and then that's a much secure way of doing things.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:20:07,396'); seek(1207.0)">
              That's really important if you are in a healthcare space or, or
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:20:10,906'); seek(1210.0)">
              a government organization that.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:20:13,336'); seek(1213.0)">
              Totally cares about your data as well.
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:20:16,126'); seek(1216.0)">
              And then, or if you are in the industry where data is very
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:20:19,336'); seek(1219.0)">
              important, you can't share.
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:20:20,626'); seek(1220.0)">
              So for compliance reasons, you have to go with the open source one as well there.
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:20:26,586'); seek(1226.0)">
              Uh, and then I've also compiled here different metrics or
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:20:34,046'); seek(1234.0)">
              different that people reported.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:20:36,701'); seek(1236.0)">
              For example, like how fine tuning were, was able to help them get
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:20:42,521'); seek(1242.0)">
              better results in different domain.
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:20:44,201'); seek(1244.0)">
              For example, yeahs are trained over wide array of data, of language data.
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:20:52,421'); seek(1252.0)">
              Legal data is a bit different.
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:20:55,061'); seek(1255.0)">
              You have a different lingo.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:20:56,681'); seek(1256.0)">
              like a sim.
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:20:57,341'); seek(1257.0)">
              A person who hasn't gone through a law school will surely have.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:21:01,591'); seek(1261.0)">
              Have a problem reading all really big legal documents, right?
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:21:06,271'); seek(1266.0)">
              That is what, and that is also the case with out of the books at the lamps.
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:21:11,701'); seek(1271.0)">
              They're really good, but they do struggle sometimes.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:21:14,791'); seek(1274.0)">
              That is where, like the legal profession has found that if you fine
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:21:18,821'); seek(1278.0)">
              tune that, these models on the legal data, you do get better results.
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:21:25,251'); seek(1285.0)">
              The same also goes for the healthcare.
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:21:28,786'); seek(1288.0)">
              it's much better also the financial services.
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:21:32,686'); seek(1292.0)">
              For example, if you have your own priority data, then you wanna train the
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:21:37,096'); seek(1297.0)">
              model on, that's much better as well.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:21:40,656'); seek(1300.0)">
              Maybe you have a model to execute some trades as before, and then in the,
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:21:46,866'); seek(1306.0)">
              in that model, that execute train.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:21:49,821'); seek(1309.0)">
              One really important input is like what's happening in the entire world
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:21:54,321'); seek(1314.0)">
              based on news and all that stuff.
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:21:56,301'); seek(1316.0)">
              And that's where can easily come in.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:21:59,031'); seek(1319.0)">
              And then the fine tuned model will be able to pick up patterns from
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:22:02,766'); seek(1322.0)">
              the world much better as well.
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:22:06,086'); seek(1326.0)">
              And then in manufacturing, fine tuning is super important here if you are.
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:22:13,076'); seek(1333.0)">
              And then at the same time, All the AI agents provide us in
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:22:17,996'); seek(1337.0)">
              customer service out there.
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:22:20,096'); seek(1340.0)">
              They have also found that, for example, fine tuning LLMs to
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:22:26,516'); seek(1346.0)">
              better understand a business.
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:22:28,311'); seek(1348.0)">
              For a client makes the LLM perform better and gives them the comparative advantage
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:22:35,001'); seek(1355.0)">
              or edge over everyone, other vendors out there because here you are building
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:22:39,861'); seek(1359.0)">
              your own mini LLM that understands your business and then if you are providing
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:22:45,681'); seek(1365.0)">
              your customers with that, a really personalized for customer support.
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:22:51,381'); seek(1371.0)">
              They surely will get better results than they will get from other AI
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:22:57,021'); seek(1377.0)">
              customer support vendors out there.
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:22:59,691'); seek(1379.0)">
              And so because you have better results, which are the most
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:23:03,081'); seek(1383.0)">
              important, metric out there.
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:23:05,571'); seek(1385.0)">
              what are some different technical considerations?
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:23:11,021'); seek(1391.0)">
              If you are a tech person, going over this fine tuning rack and other stuff, so the
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:23:17,421'); seek(1397.0)">
              most important thing is as we discussed before, is open source of closed source.
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:23:22,901'); seek(1402.0)">
              It's data like, do you wanna train the last few layers or the entire model?
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:23:30,916'); seek(1410.0)">
              That's really important to consider as well as I discussed before.
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:23:34,426'); seek(1414.0)">
              And then if you're going for the rack architecture, given
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:23:38,806'); seek(1418.0)">
              that you need that data in real time, that's revolving as well.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:23:42,606'); seek(1422.0)">
              they are some of the most important techniques I've found so far
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:23:46,566'); seek(1426.0)">
              are here, there is a different concepts that you wanna understand.
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:23:52,776'); seek(1432.0)">
              This is something that can be, Some other talk where I go into detail on
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:23:58,396'); seek(1438.0)">
              different rag techniques out there.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:24:01,056'); seek(1441.0)">
              So lastly, on a really high level comparison between RAG and fine tuning,
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:24:06,636'); seek(1446.0)">
              the cause, for fine tuning is fix.
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:24:09,836'); seek(1449.0)">
              Whereas for rag, every time you call an LLM.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:24:15,226'); seek(1455.0)">
              You might wanna call a rack to fetch relevant information from your databases,
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:24:22,726'); seek(1462.0)">
              or you have a real time course for sure.
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:24:25,956'); seek(1465.0)">
              So initial course is really high with the fine tuning.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:24:30,351'); seek(1470.0)">
              With rack, it's much lower With fine tuning you alway, you always have to
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:24:36,041'); seek(1476.0)">
              retrain or refin tune after the interval once you get more and more data.
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:24:42,901'); seek(1482.0)">
              And then for rack, you just have to update your knowledge basis every time
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:24:47,381'); seek(1487.0)">
              the LLM needs to extract some information from knowledge base, LM can extract.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:24:53,906'); seek(1493.0)">
              The real time up to date information as well for sure.
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:24:58,671'); seek(1498.0)">
              For example, in the RAG pipeline, like you have to call rag, you have to pass
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:25:03,591'); seek(1503.0)">
              that to LLM, and then when you call LLM, you also have to add rag data
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:25:09,351'); seek(1509.0)">
              tokens into LLM, so you have more token usage, so you have a really higher.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:25:16,241'); seek(1516.0)">
              Latency as well compared to fine tune, reasoning, it's better or even
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:25:21,551'); seek(1521.0)">
              same with the bot techniques as well.
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:25:24,651'); seek(1524.0)">
              in terms of infrastructure, you need a different infrastructure in.
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:25:31,446'); seek(1531.0)">
              In both cases, if you are using OP out of the box, LLM providers, there's no
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:25:37,896'); seek(1537.0)">
              infrastructure required because you, they just want you to provide them
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:25:43,746'); seek(1543.0)">
              with data in some certain format, and then once you upload data in open ai,
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:25:49,746'); seek(1549.0)">
              it just does the fine tuning for you.
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:25:53,586'); seek(1553.0)">
              There's no need to build or maintain anything at all on your side.
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:25:59,391'); seek(1559.0)">
              On the other hand for the rack, you do need to build your vector
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:26:04,311'); seek(1564.0)">
              database to store your embeddings.
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:26:07,601'); seek(1567.0)">
              And pretty much there is some investment required data as well.
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:26:11,671'); seek(1571.0)">
              I think we already recovered data sensitivity as well before.
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:26:16,461'); seek(1576.0)">
              So in, in summary, you'll choose fine tuning when the knowledge
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:26:22,731'); seek(1582.0)">
              is not changing rapidly.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:26:25,331'); seek(1585.0)">
              You have a really high volume of Curie, right?
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:26:29,171'); seek(1589.0)">
              And then it makes sense to find, tune it versus using rack because like
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:26:36,071'); seek(1596.0)">
              you have a really high cost we have.
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:26:38,041'); seek(1598.0)">
              And then, you will use fine tuning more.
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:26:40,901'); seek(1600.0)">
              when you have really lar large amount of domain knowledge that is required
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:26:45,821'); seek(1605.0)">
              by the LLM to answer a question, right?
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:26:48,311'); seek(1608.0)">
              That they're, and then in the end, for example, like you need really fast
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:26:53,741'); seek(1613.0)">
              results, like you need really high accuracy reserve, that's where you will
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:26:58,181'); seek(1618.0)">
              for surely go for fine tuning as well.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:27:01,801'); seek(1621.0)">
              And then you will choose rag, Where the data is evolving by a lot and
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:27:11,101'); seek(1631.0)">
              you need to extract relevant data in real that's evolving in real time.
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:27:18,741'); seek(1638.0)">
              You also use rag, for example, if you wanna do citations, right?
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:27:23,682'); seek(1643.0)">
              Fine tuning LL M1, know which document had that information that LM learned?
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:27:31,752'); seek(1651.0)">
              But with rag, since you are adding extra knowledge required by LLM.
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:27:37,722'); seek(1657.0)">
              In the context, LLM will, you can easily cite as well.
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:27:41,982'); seek(1661.0)">
              If you need citations, then RAG is a way to go and you will use rag.
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:27:47,502'); seek(1667.0)">
              If you have only small amount of data, that fine tuning won't make any sense.
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:27:52,092'); seek(1672.0)">
              You like, hey, I'll just add that data in the context as well.
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:27:57,642'); seek(1677.0)">
              and then once again, like you don't wanna spend so much time.
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:28:01,797'); seek(1681.0)">
              Initially fine tuning.
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:28:03,507'); seek(1683.0)">
              There's no budget.
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:28:05,097'); seek(1685.0)">
              That's also where fine tuning is where to go as well.
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:28:08,787'); seek(1688.0)">
              I do also have a few right now case studies that I have mentioned here.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:28:14,067'); seek(1694.0)">
              I will be adding the link of this artifact as well, so
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:28:19,422'); seek(1699.0)">
              people can easily learn from it.
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:28:22,022'); seek(1702.0)">
              So once again, key takeaways, like we have different techniques out there.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:28:29,287'); seek(1709.0)">
              There's fine tuning.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:28:31,177'); seek(1711.0)">
              There's fine tuning.
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:28:32,407'); seek(1712.0)">
              Only a few layers.
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:28:34,477'); seek(1714.0)">
              There is rack.
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:28:36,577'); seek(1716.0)">
              So depending on different scenarios, every technique is really important, right?
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:28:44,417'); seek(1724.0)">
              So we learn that fine tuning can be something of an investment where you will
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:28:51,377'); seek(1731.0)">
              end up building your own prior prietary.
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:28:55,247'); seek(1735.0)">
              Model that works really well in, in the domain you operate in.
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:29:01,157'); seek(1741.0)">
              This is something that you can license in the end, in the future or sell
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:29:06,797'); seek(1746.0)">
              that's an asset that belongs to you.
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:29:10,057'); seek(1750.0)">
              we also learned that, they are the cost.
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:29:14,147'); seek(1754.0)">
              The direct course of fine tuning is the initial course that you have to spend
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:29:20,867'); seek(1760.0)">
              money on where you need data preparation, and then we also learn that there is
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:29:26,717'); seek(1766.0)">
              also the indirect course as well as associated with that, and then fine
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:29:32,357'); seek(1772.0)">
              tuning can be done faster if you only like fine tune a small part of the model.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:29:39,972'); seek(1779.0)">
              Which would also means that your knowledge will be better learned
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:29:44,142'); seek(1784.0)">
              by the model as well, because it's all confined to that small part.
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:29:49,052'); seek(1789.0)">
              And then there is also a possibility that I. Your use case will need
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:29:55,342'); seek(1795.0)">
              both rack and both fine tuning.
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:29:58,072'); seek(1798.0)">
              It's quite possible that there are two types of information there is fixing.
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:30:03,292'); seek(1803.0)">
              There's evolving.
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:30:04,882'); seek(1804.0)">
              What's in evolving information can become part of the rack, what's fixed and what's
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:30:11,632'); seek(1811.0)">
              really in high large quantity data.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:30:15,442'); seek(1815.0)">
              That can easily become your fine tuning can be used in fine tuning as well.
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:30:22,042'); seek(1822.0)">
              so customer support.
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:30:24,022'); seek(1824.0)">
              All the last tickets.
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:30:26,232'); seek(1826.0)">
              That will resolve, pretty much can be used for fine tuning LLM
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:30:30,882'); seek(1830.0)">
              to tell LM, Hey, that's how you resolve a customer support curie.
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:30:36,192'); seek(1836.0)">
              And then the information about policies refund what's available in
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:30:42,642'); seek(1842.0)">
              our data or what's available in stock.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:30:46,332'); seek(1846.0)">
              This is.
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:30:47,492'); seek(1847.0)">
              In the company.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:30:48,272'); seek(1848.0)">
              This is the evolving information, and this can be part of the rag, as well.
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:30:53,292'); seek(1853.0)">
              So there can be a use case where you need both of these
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:30:56,982'); seek(1856.0)">
              things, depending on the context.
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:30:59,602'); seek(1859.0)">
              yep.
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:31:00,022'); seek(1860.0)">
              So this is on a really high level, what this is all about.
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:31:04,952'); seek(1864.0)">
              why use LLM for, why use fine tuning and why is that important for every business?
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:31:11,132'); seek(1871.0)">
              Say tune for more talks in the future.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:31:14,802'); seek(1874.0)">
              Bye.
            </span>
            
            </div>
          </div>
          
          

          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/llms2025" class="btn btn-sm btn-danger shadow lift" style="background-color: #CCB87B;">
                <i class="fe fe-grid me-2"></i>
                See all 40 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Muddassar%20Sharif_llm.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Muddassar Sharif
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Co-founder & CTO @ Virtuans.ai
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/muddassar-sharif-4b1b86121/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Muddassar Sharif's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Muddassar Sharif"
                  data-url="https://www.conf42.com/llms2025"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/llms2025"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Large Language Models"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>