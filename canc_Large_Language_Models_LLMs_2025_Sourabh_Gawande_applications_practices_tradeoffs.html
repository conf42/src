<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Practical tips for building AI applications using LLMs - Best practices and trade-offs</title>
    <meta name="description" content="One model, extra large, please!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Sourabh%20Gawande_llm.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Practical tips for building AI applications using LLMs - Best practices and trade-offs | Conf42"/>
    <meta property="og:description" content="Understand the problems and trade-offs you'll encounter when building an AI application on top of LLMs based on my learnings of building KushoAI, an AI agent used by 5000+ engineers to make API testing completely autonomous by leveraging LLMs."/>
    <meta property="og:url" content="https://conf42.com/Large_Language_Models_LLMs_2025_Sourabh_Gawande_applications_practices_tradeoffs"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/LLM2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Large Language Models (LLMs) 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-03-20
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/llms2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #CCB87B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Large Language Models (LLMs) 2025 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- One model, extra large, please!
 -->
              <script>
                const event_date = new Date("2025-03-20T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2025-03-20T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "aZo37Cyaprk"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrCnqe_gWc_lIVyDmyIx8BQG" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello everyone.", "timestamp": "00:00:00,240", "timestamp_s": 0.0}, {"text": "My name is Soap.", "timestamp": "00:00:01,050", "timestamp_s": 1.0}, {"text": "I am the co-founder and CTO of Ku ai and today I\u0027m going to talk about practical", "timestamp": "00:00:01,890", "timestamp_s": 1.0}, {"text": "tips for building AI applications or AI agents using LLMs at KU ai.", "timestamp": "00:00:07,080", "timestamp_s": 7.0}, {"text": "We have been, working on building AI applications and,", "timestamp": "00:00:12,510", "timestamp_s": 12.0}, {"text": "agents for the last, 18 months.", "timestamp": "00:00:16,710", "timestamp_s": 16.0}, {"text": "And, during our journey, we have identified a bunch of unique, problems.", "timestamp": "00:00:19,040", "timestamp_s": 19.0}, {"text": "That people generally face.", "timestamp": "00:00:25,355", "timestamp_s": 25.0}, {"text": "And, we have also faced, those same problems specifically while", "timestamp": "00:00:27,035", "timestamp_s": 27.0}, {"text": "building, applications using LLMs.", "timestamp": "00:00:30,904", "timestamp_s": 30.0}, {"text": "And, the, agenda for today\u0027s talk is that, we want to, educate, devs about", "timestamp": "00:00:33,425", "timestamp_s": 33.0}, {"text": "these problems, so that, when they are building apps on top of LLMs, they\u0027re", "timestamp": "00:00:40,290", "timestamp_s": 40.0}, {"text": "aware of these problems and, Also discuss what are, the solutions that", "timestamp": "00:00:45,780", "timestamp_s": 45.0}, {"text": "work for us and, the dev tools or tooling that we use to solve these problems.", "timestamp": "00:00:49,680", "timestamp_s": 49.0}, {"text": "and, by sharing this information, we want to save time, when devs", "timestamp": "00:00:54,120", "timestamp_s": 54.0}, {"text": "are, building applications on top of, for the first time.", "timestamp": "00:00:58,885", "timestamp_s": 58.0}, {"text": "The first thing that you\u0027ll need to solve when you start building", "timestamp": "00:01:02,755", "timestamp_s": 62.0}, {"text": "apps on top of, LLMs is, how to handle LLM inconsistencies.", "timestamp": "00:01:05,635", "timestamp_s": 65.0}, {"text": "you, if you have some experience building, applications using normal", "timestamp": "00:01:11,095", "timestamp_s": 71.0}, {"text": "APIs, you would\u0027ve seen that they don\u0027t.", "timestamp": "00:01:14,810", "timestamp_s": 74.0}, {"text": "fail that offer.", "timestamp": "00:01:16,875", "timestamp_s": 76.0}, {"text": "while building, general applications, you don\u0027t really worry about,", "timestamp": "00:01:17,575", "timestamp_s": 77.0}, {"text": "inconsistencies or failures, that much.", "timestamp": "00:01:21,264", "timestamp_s": 81.0}, {"text": "if an API fails, you just let, the API fail.", "timestamp": "00:01:24,164", "timestamp_s": 84.0}, {"text": "And, the user, when they refresh their page, you make another API", "timestamp": "00:01:26,455", "timestamp_s": 86.0}, {"text": "call and it\u0027ll most probably succeed.", "timestamp": "00:01:29,854", "timestamp_s": 89.0}, {"text": "but in case of LLMs.", "timestamp": "00:01:31,555", "timestamp_s": 91.0}, {"text": "this is the first thing that you\u0027ll probably need to solve, when you\u0027re", "timestamp": "00:01:33,215", "timestamp_s": 93.0}, {"text": "actually building an application because, LLMs have a much higher", "timestamp": "00:01:36,175", "timestamp_s": 96.0}, {"text": "error rate, than your normal APIs.", "timestamp": "00:01:40,055", "timestamp_s": 100.0}, {"text": "And unless you solve this particular thing, your application will have a", "timestamp": "00:01:42,765", "timestamp_s": 102.0}, {"text": "terrible ux, or user experience because, in your application, you\u0027ll generally", "timestamp": "00:01:47,265", "timestamp_s": 107.0}, {"text": "use the LLM response, somewhere else.", "timestamp": "00:01:52,925", "timestamp_s": 112.0}, {"text": "And every time the LLM gives you.", "timestamp": "00:01:55,475", "timestamp_s": 115.0}, {"text": "A wrong output, your application will also crash.", "timestamp": "00:01:58,130", "timestamp_s": 118.0}, {"text": "So this is the first, problem that, you should solve, while", "timestamp": "00:02:00,830", "timestamp_s": 120.0}, {"text": "building LLM applications.", "timestamp": "00:02:06,180", "timestamp_s": 126.0}, {"text": "now before we get into, Like why, how to solve this, particular problem.", "timestamp": "00:02:08,610", "timestamp_s": 128.0}, {"text": "let\u0027s just talk about why this even occurs, in these, in", "timestamp": "00:02:15,070", "timestamp_s": 135.0}, {"text": "these specific applications.", "timestamp": "00:02:18,895", "timestamp_s": 138.0}, {"text": "so like I mentioned earlier, if you are working with, normal", "timestamp": "00:02:20,695", "timestamp_s": 140.0}, {"text": "APIs, you generally don\u0027t worry.", "timestamp": "00:02:23,945", "timestamp_s": 143.0}, {"text": "Much about the error rate, in like fairly stable, APIs, but even the most", "timestamp": "00:02:27,135", "timestamp_s": 147.0}, {"text": "stable LLMs give you a much higher error rate, than your normal APIs.", "timestamp": "00:02:32,825", "timestamp_s": 152.0}, {"text": "And the reason for this is LLMs are inherently non-deterministic.", "timestamp": "00:02:37,035", "timestamp_s": 157.0}, {"text": "so what do you mean by that?", "timestamp": "00:02:43,305", "timestamp_s": 163.0}, {"text": "so if you look at an LLM under the hood.", "timestamp": "00:02:44,775", "timestamp_s": 164.0}, {"text": "they\u0027re essentially statistical machines, that produce token after token based", "timestamp": "00:02:47,215", "timestamp_s": 167.0}, {"text": "on, the input prompt and whatever tokens have been generated previously.", "timestamp": "00:02:53,165", "timestamp_s": 173.0}, {"text": "statistical machines are basically probabilistic and as soon as you bring", "timestamp": "00:02:57,675", "timestamp_s": 177.0}, {"text": "probability into software, you are going to get something non-deterministic.", "timestamp": "00:03:02,865", "timestamp_s": 182.0}, {"text": "Now what do we mean by non-deterministic?", "timestamp": "00:03:06,675", "timestamp_s": 186.0}, {"text": "You basically, will get a different output for the same input every time.", "timestamp": "00:03:09,895", "timestamp_s": 189.0}, {"text": "you, ask LLM for a response, you could, I, and I\u0027m pretty sure like you are,", "timestamp": "00:03:14,465", "timestamp_s": 194.0}, {"text": "you would have seen this problem while using, all the different, chat bots", "timestamp": "00:03:20,895", "timestamp_s": 200.0}, {"text": "that are available, like chat, GPT or.", "timestamp": "00:03:24,495", "timestamp_s": 204.0}, {"text": "the Deep Seeq or Claude chat, you would\u0027ve noticed that, every time you give,", "timestamp": "00:03:27,190", "timestamp_s": 207.0}, {"text": "give, give an input, for the same input.", "timestamp": "00:03:31,830", "timestamp_s": 211.0}, {"text": "Every time you hit retry, you\u0027ll get a different output.", "timestamp": "00:03:34,130", "timestamp_s": 214.0}, {"text": "that\u0027s the same thing that will happen with, the LLM", "timestamp": "00:03:36,870", "timestamp_s": 216.0}, {"text": "responses in your applications.", "timestamp": "00:03:39,050", "timestamp_s": 219.0}, {"text": "you most of the time don\u0027t, have a lot of control or, will you", "timestamp": "00:03:41,160", "timestamp_s": 221.0}, {"text": "get the exact output or not?", "timestamp": "00:03:45,034", "timestamp_s": 225.0}, {"text": "Now because of this particular problem, which is, a being non-deterministic, every", "timestamp": "00:03:46,774", "timestamp_s": 226.0}, {"text": "time you give it an input, you\u0027ll not always get the response that you want.", "timestamp": "00:03:53,850", "timestamp_s": 233.0}, {"text": "for example, if you ask an L-L-M-A-P-I to generate, JSON, which is a", "timestamp": "00:03:58,140", "timestamp_s": 238.0}, {"text": "structured output, You might get more fields than, what you asked for.", "timestamp": "00:04:03,809", "timestamp_s": 243.0}, {"text": "Sometimes you might get less fields.", "timestamp": "00:04:08,539", "timestamp_s": 248.0}, {"text": "sometimes you might have, a bracket missing based on what we have seen.", "timestamp": "00:04:10,369", "timestamp_s": 250.0}, {"text": "if you have a normal stable API, you\u0027ll see an error rate of something like 0.1%.", "timestamp": "00:04:15,159", "timestamp_s": 255.0}, {"text": "But if you are working with an LLM, even the most stable, the LLMs, which have", "timestamp": "00:04:23,169", "timestamp_s": 263.0}, {"text": "been, here for the longest amount of time, they\u0027ll give you an error rate of,", "timestamp": "00:04:27,979", "timestamp_s": 267.0}, {"text": "something like one to 5% based on what kind of task you\u0027re asking it to perform.", "timestamp": "00:04:31,889", "timestamp_s": 271.0}, {"text": "And if you are working with chain LLM responses, basically you,", "timestamp": "00:04:36,179", "timestamp_s": 276.0}, {"text": "provide, the L-L-M-A-P-I with.", "timestamp": "00:04:40,529", "timestamp_s": 280.0}, {"text": "A prompt.", "timestamp": "00:04:42,820", "timestamp_s": 282.0}, {"text": "You take that response and then you provide it with another prompt, using", "timestamp": "00:04:43,540", "timestamp_s": 283.0}, {"text": "the response, that you got earlier.", "timestamp": "00:04:47,890", "timestamp_s": 287.0}, {"text": "this is basically a chained, LLM responses.", "timestamp": "00:04:50,100", "timestamp_s": 290.0}, {"text": "you\u0027ll see that your error rate gets compounded and, this particular thing,", "timestamp": "00:04:53,680", "timestamp_s": 293.0}, {"text": "will probably not have a solution.", "timestamp": "00:04:58,520", "timestamp_s": 298.0}, {"text": "In the LLMs because of LLMs, like I mentioned, are", "timestamp": "00:05:01,450", "timestamp_s": 301.0}, {"text": "inherently non-deterministic.", "timestamp": "00:05:05,170", "timestamp_s": 305.0}, {"text": "that is how the architecture is.", "timestamp": "00:05:06,560", "timestamp_s": 306.0}, {"text": "So this is something that needs to be solved in your application.", "timestamp": "00:05:09,460", "timestamp_s": 309.0}, {"text": "you can\u0027t really wait for like LLMs to get better and, start", "timestamp": "00:05:14,250", "timestamp_s": 314.0}, {"text": "providing better responses.", "timestamp": "00:05:18,840", "timestamp_s": 318.0}, {"text": "they will definitely, get better and, Reduce the error rate, but, I think", "timestamp": "00:05:19,910", "timestamp_s": 319.0}, {"text": "as an application developer, it\u0027s your responsibility to take care of this", "timestamp": "00:05:23,755", "timestamp_s": 323.0}, {"text": "issue within your application as well.", "timestamp": "00:05:27,595", "timestamp_s": 327.0}, {"text": "So what are your options?", "timestamp": "00:05:29,965", "timestamp_s": 329.0}, {"text": "The first thing that you should definitely try out is, retries and timeouts.", "timestamp": "00:05:32,545", "timestamp_s": 332.0}, {"text": "Now, these are not new concepts.", "timestamp": "00:05:37,565", "timestamp_s": 337.0}, {"text": "if you have worked in software development for a while now,", "timestamp": "00:05:40,535", "timestamp_s": 340.0}, {"text": "you would know what a retry is.", "timestamp": "00:05:43,235", "timestamp_s": 343.0}, {"text": "Basically.", "timestamp": "00:05:44,465", "timestamp_s": 344.0}, {"text": "when an API.", "timestamp": "00:05:45,235", "timestamp_s": 345.0}, {"text": "Gives you a wrong response.", "timestamp": "00:05:46,365", "timestamp_s": 346.0}, {"text": "You try it again with some, cool down period or, maybe not depending", "timestamp": "00:05:47,594", "timestamp_s": 347.0}, {"text": "on, how the rate limits are.", "timestamp": "00:05:52,584", "timestamp_s": 352.0}, {"text": "retry is basically, you make an API call the API fails.", "timestamp": "00:05:54,374", "timestamp_s": 354.0}, {"text": "you wait for a while and then you retry it again.", "timestamp": "00:05:58,574", "timestamp_s": 358.0}, {"text": "as simple as that.", "timestamp": "00:06:01,684", "timestamp_s": 361.0}, {"text": "Now, when you.", "timestamp": "00:06:03,034", "timestamp_s": 363.0}, {"text": "Are developing, general applications, I think retries and timeouts are", "timestamp": "00:06:05,424", "timestamp_s": 365.0}, {"text": "something that, are not the first thing that you would implement.", "timestamp": "00:06:09,674", "timestamp_s": 369.0}, {"text": "Because, you just assume, you just go with the assumption that the API response", "timestamp": "00:06:13,839", "timestamp_s": 373.0}, {"text": "rate is going to be fairly reasonable.", "timestamp": "00:06:18,549", "timestamp_s": 378.0}, {"text": "they\u0027ll, most of the time work and, like not adding retries and timeouts", "timestamp": "00:06:20,419", "timestamp_s": 380.0}, {"text": "to your APIs will, not really degrade the application performance.", "timestamp": "00:06:25,199", "timestamp_s": 385.0}, {"text": "unless like you are working with very critical, applications", "timestamp": "00:06:29,139", "timestamp_s": 389.0}, {"text": "like, something in finance.", "timestamp": "00:06:32,679", "timestamp_s": 392.0}, {"text": "Or health where, the operation has to finish, in which case you\u0027ll, definitely", "timestamp": "00:06:34,685", "timestamp_s": 394.0}, {"text": "start with retries and timeouts.", "timestamp": "00:06:38,825", "timestamp_s": 398.0}, {"text": "But, in our general experience, if you\u0027re working with normal APIs, you", "timestamp": "00:06:40,175", "timestamp_s": 400.0}, {"text": "don\u0027t really worry about these things.", "timestamp": "00:06:43,435", "timestamp_s": 403.0}, {"text": "but because LLM APIs specifically have a higher error rate, retries", "timestamp": "00:06:45,385", "timestamp_s": 405.0}, {"text": "and timeouts are something that, Need to be implemented from day one.", "timestamp": "00:06:50,755", "timestamp_s": 410.0}, {"text": "timeouts again, I think, I don\u0027t need to get into this.", "timestamp": "00:06:55,475", "timestamp_s": 415.0}, {"text": "A timeout is basically, you make an API call and you wait for X", "timestamp": "00:06:57,455", "timestamp_s": 417.0}, {"text": "seconds for the API to return.", "timestamp": "00:07:03,365", "timestamp_s": 423.0}, {"text": "If it doesn\u0027t return an X seconds for whatever reason.", "timestamp": "00:07:05,225", "timestamp_s": 425.0}, {"text": "you terminate that, particular a p and you try again.", "timestamp": "00:07:07,795", "timestamp_s": 427.0}, {"text": "this basically is protection against, the server, the API server being down.", "timestamp": "00:07:11,220", "timestamp_s": 431.0}, {"text": "And, so if you don\u0027t do this, and if the API takes a minute to respond, you, your", "timestamp": "00:07:17,200", "timestamp_s": 437.0}, {"text": "application is also stuck for a minute.", "timestamp": "00:07:22,490", "timestamp_s": 442.0}, {"text": "And, so are your users.", "timestamp": "00:07:24,230", "timestamp_s": 444.0}, {"text": "So a timeout is basically protection so that if an a p doesn\u0027t return in", "timestamp": "00:07:25,860", "timestamp_s": 445.0}, {"text": "like a reasonable amount of time, you cancel that API call and you.", "timestamp": "00:07:31,860", "timestamp_s": 451.0}, {"text": "retry that API again, that\u0027s where timeout comes into picture.", "timestamp": "00:07:35,405", "timestamp_s": 455.0}, {"text": "cool.", "timestamp": "00:07:38,465", "timestamp_s": 458.0}, {"text": "So how do you implement this into your application?", "timestamp": "00:07:38,675", "timestamp_s": 458.0}, {"text": "I would suggest don\u0027t, write the word for retries and timeouts from", "timestamp": "00:07:41,775", "timestamp_s": 461.0}, {"text": "scratch because there are a bunch of, battle tested libraries available", "timestamp": "00:07:45,165", "timestamp_s": 465.0}, {"text": "in every language that you can use.", "timestamp": "00:07:49,155", "timestamp_s": 469.0}, {"text": "And, with a few lines of code add these.", "timestamp": "00:07:52,155", "timestamp_s": 472.0}, {"text": "behaviors to your application.", "timestamp": "00:07:54,715", "timestamp_s": 474.0}, {"text": "So let\u0027s look at a few examples.", "timestamp": "00:07:56,365", "timestamp_s": 476.0}, {"text": "the one that we actually use in production is, this one called", "timestamp": "00:07:59,475", "timestamp_s": 479.0}, {"text": "Tensity by, it\u0027s a Python library, and, it allows you to add retry to", "timestamp": "00:08:02,805", "timestamp_s": 482.0}, {"text": "your functions by simply doing this.", "timestamp": "00:08:08,575", "timestamp_s": 488.0}, {"text": "you add a decorator.", "timestamp": "00:08:10,705", "timestamp_s": 490.0}, {"text": "Which is provided by the, by this particular library.", "timestamp": "00:08:12,135", "timestamp_s": 492.0}, {"text": "you add it to a function and this function will be retried.", "timestamp": "00:08:14,395", "timestamp_s": 494.0}, {"text": "whenever there is an, exception or error in this particular function.", "timestamp": "00:08:18,085", "timestamp_s": 498.0}, {"text": "Now, you\u0027d ideally want more control over, how many d tries to do, how,", "timestamp": "00:08:22,375", "timestamp_s": 502.0}, {"text": "like how long to wait after, every try and those kind of things.", "timestamp": "00:08:28,925", "timestamp_s": 508.0}, {"text": "those.", "timestamp": "00:08:32,805", "timestamp_s": 512.0}, {"text": "All options are present in this library.", "timestamp": "00:08:33,975", "timestamp_s": 513.0}, {"text": "You can, give it stopping conditions where you want to stop after three retries.", "timestamp": "00:08:35,805", "timestamp_s": 515.0}, {"text": "You want to stop after, 10 seconds of retrying.", "timestamp": "00:08:40,045", "timestamp_s": 520.0}, {"text": "you can add a wait time before every retry.", "timestamp": "00:08:42,865", "timestamp_s": 522.0}, {"text": "you can add a fixed wait time.", "timestamp": "00:08:46,385", "timestamp_s": 526.0}, {"text": "You can add a random wait time.", "timestamp": "00:08:47,495", "timestamp_s": 527.0}, {"text": "all these, different kinds of, behaviors can.", "timestamp": "00:08:49,385", "timestamp_s": 529.0}, {"text": "Be added using this library, with a few lines of course.", "timestamp": "00:08:52,610", "timestamp_s": 532.0}, {"text": "if you are working in Python, this is our choice, has been working,", "timestamp": "00:08:55,200", "timestamp_s": 535.0}, {"text": "very well for us in production.", "timestamp": "00:08:59,350", "timestamp_s": 539.0}, {"text": "this is what we, have been using for a very long time.", "timestamp": "00:09:01,200", "timestamp_s": 541.0}, {"text": "So I would recommend this, if you\u0027re working in js, there is a similar library", "timestamp": "00:09:03,210", "timestamp_s": 543.0}, {"text": "called Retract, very conveniently.", "timestamp": "00:09:07,630", "timestamp_s": 547.0}, {"text": "that you can, that is available on NPM.", "timestamp": "00:09:09,725", "timestamp_s": 549.0}, {"text": "similar type of functionalities.", "timestamp": "00:09:11,905", "timestamp_s": 551.0}, {"text": "It gives you, retries and timeouts.", "timestamp": "00:09:13,525", "timestamp_s": 553.0}, {"text": "Oh, by the way, tenacity also has, timeout related decorators.", "timestamp": "00:09:16,205", "timestamp_s": 556.0}, {"text": "works the same way.", "timestamp": "00:09:19,965", "timestamp_s": 559.0}, {"text": "if you want to add a timeout to particular function, you just add", "timestamp": "00:09:20,675", "timestamp_s": 560.0}, {"text": "that decorator, specify the timeout.", "timestamp": "00:09:23,795", "timestamp_s": 563.0}, {"text": "yeah.", "timestamp": "00:09:25,935", "timestamp_s": 565.0}, {"text": "This library, if you are working with a JS application, retry is, our choice.", "timestamp": "00:09:26,645", "timestamp_s": 566.0}, {"text": "the third option is, basically, a lot of people who are, developing", "timestamp": "00:09:32,135", "timestamp_s": 572.0}, {"text": "LLM applications are using these frameworks, LLM frameworks to", "timestamp": "00:09:37,165", "timestamp_s": 577.0}, {"text": "handle, API calls retries and like a bunch of different things.", "timestamp": "00:09:40,665", "timestamp_s": 580.0}, {"text": "So the most famous LLM frameworks, framework, which a lot of", "timestamp": "00:09:44,860", "timestamp_s": 584.0}, {"text": "people are using is land chain.", "timestamp": "00:09:48,460", "timestamp_s": 588.0}, {"text": "And if you are working on top of land chain, land chain provides", "timestamp": "00:09:50,530", "timestamp_s": 590.0}, {"text": "you a, basically a, some, mechanism to retry, out of the box.", "timestamp": "00:09:54,580", "timestamp_s": 594.0}, {"text": "it\u0027s called.", "timestamp": "00:10:00,325", "timestamp_s": 600.0}, {"text": "the retry output parser, where, you can use this to make the LLM calls", "timestamp": "00:10:01,785", "timestamp_s": 601.0}, {"text": "and whenever the LLM call fails, this parser will basically, handle retry", "timestamp": "00:10:06,085", "timestamp_s": 606.0}, {"text": "on your behalf, by passing, The prompt again, and, also the previous output,", "timestamp": "00:10:11,355", "timestamp_s": 611.0}, {"text": "so that the, L-L-M-A-P has a better idea that, okay, the last output failed.", "timestamp": "00:10:15,695", "timestamp_s": 615.0}, {"text": "And, I\u0027m not supposed to, give this response again.", "timestamp": "00:10:19,339", "timestamp_s": 619.0}, {"text": "So if you\u0027re on, then it\u0027s already sorted out for you.", "timestamp": "00:10:22,149", "timestamp_s": 622.0}, {"text": "You use the retry output parcel.", "timestamp": "00:10:24,839", "timestamp_s": 624.0}, {"text": "Alright, so this sorts out how to implement retries", "timestamp": "00:10:26,494", "timestamp_s": 626.0}, {"text": "and timeouts the next most.", "timestamp": "00:10:30,099", "timestamp_s": 630.0}, {"text": "Common reason for, failure or LLM inconsistency is when you are", "timestamp": "00:10:33,059", "timestamp_s": 633.0}, {"text": "working with structured outputs.", "timestamp": "00:10:37,429", "timestamp_s": 637.0}, {"text": "So when I say structured output, something like you asked the LLM to", "timestamp": "00:10:39,319", "timestamp_s": 639.0}, {"text": "generate A-J-S-O-N or X-M-L-C-S-V, even list ra, those kind of things.", "timestamp": "00:10:42,959", "timestamp_s": 642.0}, {"text": "whenever you are asking an LLM to generate a structured output, there", "timestamp": "00:10:47,819", "timestamp_s": 647.0}, {"text": "is a slight chance that, there\u0027ll be something wrong with that structure.", "timestamp": "00:10:51,849", "timestamp_s": 651.0}, {"text": "Maybe there are some fields missing.", "timestamp": "00:10:55,064", "timestamp_s": 655.0}, {"text": "there are extra fields.", "timestamp": "00:10:56,514", "timestamp_s": 656.0}, {"text": "in case of JSONs, XMLs, there are brackets missing, might happen.", "timestamp": "00:10:57,954", "timestamp_s": 657.0}, {"text": "So how do you handle that?", "timestamp": "00:11:01,869", "timestamp_s": 661.0}, {"text": "the simplest way to do that is to, is to integrate a schema library instead", "timestamp": "00:11:04,639", "timestamp_s": 664.0}, {"text": "of doing it on your own every time.", "timestamp": "00:11:10,599", "timestamp_s": 670.0}, {"text": "a schema library could be something like pedantic and, this is what.", "timestamp": "00:11:12,339", "timestamp_s": 672.0}, {"text": "We use, in our production.", "timestamp": "00:11:16,519", "timestamp_s": 676.0}, {"text": "PTECH is basically, the most commonly used data validation library in Python.", "timestamp": "00:11:19,359", "timestamp_s": 679.0}, {"text": "And what it does is it allows you to, create classes, in which you describe", "timestamp": "00:11:24,549", "timestamp_s": 684.0}, {"text": "the structure of your response, and then you use this particular class to,", "timestamp": "00:11:31,209", "timestamp_s": 691.0}, {"text": "check whether the LRM response fits.", "timestamp": "00:11:37,339", "timestamp_s": 697.0}, {"text": "This particular structure or not.", "timestamp": "00:11:40,039", "timestamp_s": 700.0}, {"text": "it\u0027ll check for, fields, extra fields, or less fields.", "timestamp": "00:11:41,849", "timestamp_s": 701.0}, {"text": "It\u0027ll check for data types, and a bunch of other options.", "timestamp": "00:11:46,549", "timestamp_s": 706.0}, {"text": "on Python, just go for Edan Tech.", "timestamp": "00:11:49,859", "timestamp_s": 709.0}, {"text": "it is a tried and tested library, and, it\u0027ll make the data validation", "timestamp": "00:11:51,819", "timestamp_s": 711.0}, {"text": "part when you\u0027re working with structured outputs, hassle free.", "timestamp": "00:11:56,859", "timestamp_s": 716.0}, {"text": "similarly, if you are working with NPM, there\u0027s something called Yap, same", "timestamp": "00:12:00,199", "timestamp_s": 720.0}, {"text": "stuff as pedantic, data validation.", "timestamp": "00:12:03,779", "timestamp_s": 723.0}, {"text": "you essentially, define the shape of your output and, yap basically uses that shape,", "timestamp": "00:12:06,469", "timestamp_s": 726.0}, {"text": "which is essentially a class, JS class, or a JS object to, Check or enforce,", "timestamp": "00:12:12,499", "timestamp_s": 732.0}, {"text": "the structure of your LLM responses.", "timestamp": "00:12:18,519", "timestamp_s": 738.0}, {"text": "and the idea is to use, these, these, data validation libraries,", "timestamp": "00:12:20,619", "timestamp_s": 740.0}, {"text": "along with, retries and timeouts.", "timestamp": "00:12:25,019", "timestamp_s": 745.0}, {"text": "what you basically do is when you make an L-L-M-E-P-I call, and you", "timestamp": "00:12:26,789", "timestamp_s": 746.0}, {"text": "get a response, you pass it through pedantic or, whatever data validation.", "timestamp": "00:12:29,929", "timestamp_s": 749.0}, {"text": "Letter you are using, and if you get an error, you use the retry, to", "timestamp": "00:12:34,839", "timestamp_s": 754.0}, {"text": "like basically let the LLM generate that structured output again, most", "timestamp": "00:12:39,214", "timestamp_s": 759.0}, {"text": "of the time, you will see that, a couple of retries sorts it out.", "timestamp": "00:12:43,144", "timestamp_s": 763.0}, {"text": "it\u0027s not like every API call will feel in the same way.", "timestamp": "00:12:48,054", "timestamp_s": 768.0}, {"text": "So if, let\u0027s say there are a few things missing.", "timestamp": "00:12:51,294", "timestamp_s": 771.0}, {"text": "In your structured output the first time when you do a retry, the next", "timestamp": "00:12:54,404", "timestamp_s": 774.0}, {"text": "time you\u0027ll get the correct output.", "timestamp": "00:12:57,784", "timestamp_s": 777.0}, {"text": "but just as a general advice, if you see that there are particular", "timestamp": "00:12:59,734", "timestamp_s": 779.0}, {"text": "kind of issues happening again and again, you should mention that,", "timestamp": "00:13:05,774", "timestamp_s": 785.0}, {"text": "instruction in the prompt itself.", "timestamp": "00:13:09,694", "timestamp_s": 789.0}, {"text": "Because what happens is that, when you do retry, an API call, which", "timestamp": "00:13:11,494", "timestamp_s": 791.0}, {"text": "was supposed to take five seconds, might end up taking 15 to 20 seconds.", "timestamp": "00:13:16,334", "timestamp_s": 796.0}, {"text": "And, it\u0027ll make your, make your application feel laggy.", "timestamp": "00:13:19,874", "timestamp_s": 799.0}, {"text": "because, at the end of that API call, you\u0027re going to provide", "timestamp": "00:13:23,844", "timestamp_s": 803.0}, {"text": "some output to your users and, they\u0027re waiting for that output.", "timestamp": "00:13:26,924", "timestamp_s": 806.0}, {"text": "so if, that there are particular kind of, problems that are happening again", "timestamp": "00:13:30,054", "timestamp_s": 810.0}, {"text": "and again, like for example, If, if you are, generating JSON, using, JSON,", "timestamp": "00:13:34,229", "timestamp_s": 814.0}, {"text": "using an L-L-M-A-P-I, and you\u0027ll see that, like the LLM is always using", "timestamp": "00:13:39,889", "timestamp_s": 819.0}, {"text": "single code sensor or double code, which will generally cause issues, you should", "timestamp": "00:13:43,499", "timestamp_s": 823.0}, {"text": "specify that as an important point in your prompt so that, you get the correct", "timestamp": "00:13:47,319", "timestamp_s": 827.0}, {"text": "output in the first, attempt itself.", "timestamp": "00:13:51,369", "timestamp_s": 831.0}, {"text": "this is just an additional level of check, but the idea is that the first response", "timestamp": "00:13:54,129", "timestamp_s": 834.0}, {"text": "should itself give you the correct output.", "timestamp": "00:13:59,079", "timestamp_s": 839.0}, {"text": "anything that is, that is known, should be mentioned in the prompt", "timestamp": "00:14:01,159", "timestamp_s": 841.0}, {"text": "as a special instruction, so that you don\u0027t keep retrying and you", "timestamp": "00:14:04,449", "timestamp_s": 844.0}, {"text": "use this an waiting for an output.", "timestamp": "00:14:07,669", "timestamp_s": 847.0}, {"text": "one, one additional option worth, one special mention here is, The structured", "timestamp": "00:14:09,629", "timestamp_s": 849.0}, {"text": "output capabilities provided by open ai.", "timestamp": "00:14:15,119", "timestamp_s": 855.0}, {"text": "if you\u0027re using, GPT models, and open AI APIs, what you can do is", "timestamp": "00:14:17,829", "timestamp_s": 857.0}, {"text": "there is a response format field where you can specify a class and,", "timestamp": "00:14:23,594", "timestamp_s": 863.0}, {"text": "the open air APIs themselves will, try to enforce the structure.", "timestamp": "00:14:27,794", "timestamp_s": 867.0}, {"text": "but this one problem here, which is if you want to switch out.", "timestamp": "00:14:33,134", "timestamp_s": 873.0}, {"text": "The model and, use something else like Claude Orrock.", "timestamp": "00:14:37,184", "timestamp_s": 877.0}, {"text": "then you have basically, lost the structured output capabilities", "timestamp": "00:14:40,364", "timestamp_s": 880.0}, {"text": "because those are not, available right now in, other LLM APIs.", "timestamp": "00:14:44,654", "timestamp_s": 884.0}, {"text": "my suggestion is to just handle the, schema enforcing and checking in", "timestamp": "00:14:48,854", "timestamp_s": 888.0}, {"text": "your application itself so that like it\u0027s easy for you to switch out,", "timestamp": "00:14:52,724", "timestamp_s": 892.0}, {"text": "the models and use different models.", "timestamp": "00:14:56,004", "timestamp_s": 896.0}, {"text": "That\u0027s all for, handling LLM inconsistencies.", "timestamp": "00:14:57,794", "timestamp_s": 897.0}, {"text": "two main things, retries and timeouts.", "timestamp": "00:15:01,114", "timestamp_s": 901.0}, {"text": "use them from the start.", "timestamp": "00:15:02,954", "timestamp_s": 902.0}, {"text": "if you are working with structured outputs, use a data validation", "timestamp": "00:15:05,234", "timestamp_s": 905.0}, {"text": "library to, check the structure.", "timestamp": "00:15:08,874", "timestamp_s": 908.0}, {"text": "you see the options here.", "timestamp": "00:15:11,424", "timestamp_s": 911.0}, {"text": "any of these are good.", "timestamp": "00:15:12,484", "timestamp_s": 912.0}, {"text": "The next thing that you should, start thinking about is how to implement", "timestamp": "00:15:13,914", "timestamp_s": 913.0}, {"text": "streaming in your LLM application.", "timestamp": "00:15:17,974", "timestamp_s": 917.0}, {"text": "generally when you develop APIs, you, you implement, you", "timestamp": "00:15:20,854", "timestamp_s": 920.0}, {"text": "implement normal request response.", "timestamp": "00:15:24,894", "timestamp_s": 924.0}, {"text": "you get an a p call and, the server does some work, and then you, then you", "timestamp": "00:15:26,274", "timestamp_s": 926.0}, {"text": "return the entire response in one go.", "timestamp": "00:15:31,194", "timestamp_s": 931.0}, {"text": "in.", "timestamp": "00:15:33,564", "timestamp_s": 933.0}, {"text": "In case of, LLMs, what happens is sometimes it might take a long time", "timestamp": "00:15:34,524", "timestamp_s": 934.0}, {"text": "for the l LM to generate a response.", "timestamp": "00:15:39,414", "timestamp_s": 939.0}, {"text": "That\u0027s where streaming comes into picture.", "timestamp": "00:15:41,964", "timestamp_s": 941.0}, {"text": "Streaming of your responses allow you to start returning partial responses,", "timestamp": "00:15:43,524", "timestamp_s": 943.0}, {"text": "to the client, even when, the LLM is not done, done with the generation.", "timestamp": "00:15:49,324", "timestamp_s": 949.0}, {"text": "let\u0027s look at why.", "timestamp": "00:15:54,764", "timestamp_s": 954.0}, {"text": "Streaming is so important.", "timestamp": "00:15:55,974", "timestamp_s": 955.0}, {"text": "while building LLM applications, like I mentioned, LLMs might take long", "timestamp": "00:15:58,094", "timestamp_s": 958.0}, {"text": "time for, for generation, to complete.", "timestamp": "00:16:03,544", "timestamp_s": 963.0}, {"text": "Now, when your user is, using your application, most", "timestamp": "00:16:06,699", "timestamp_s": 966.0}, {"text": "users are very impatient.", "timestamp": "00:16:10,289", "timestamp_s": 970.0}, {"text": "you.", "timestamp": "00:16:11,369", "timestamp_s": 971.0}, {"text": "can\u0027t ask them to wait for, seconds.", "timestamp": "00:16:12,369", "timestamp_s": 972.0}, {"text": "like I\u0027m not even talking about minutes.", "timestamp": "00:16:15,349", "timestamp_s": 975.0}, {"text": "if you have a ten second delay in showing the response, you", "timestamp": "00:16:17,159", "timestamp_s": 977.0}, {"text": "might see a lot of drop off.", "timestamp": "00:16:20,279", "timestamp_s": 980.0}, {"text": "Um, and like most of the LMS that you would work with would take five to 10", "timestamp": "00:16:21,869", "timestamp_s": 981.0}, {"text": "seconds for even the simplest, prompts.", "timestamp": "00:16:25,929", "timestamp_s": 985.0}, {"text": "So how do you improve the ux?", "timestamp": "00:16:30,029", "timestamp_s": 990.0}, {"text": "and make sure that your users don\u0027t drop off.", "timestamp": "00:16:33,984", "timestamp_s": 993.0}, {"text": "that\u0027s where streaming comes into picture.", "timestamp": "00:16:36,694", "timestamp_s": 996.0}, {"text": "what streaming allows you to do is, LLMs generate.", "timestamp": "00:16:38,714", "timestamp_s": 998.0}, {"text": "Response is token by token.", "timestamp": "00:16:42,134", "timestamp_s": 1002.0}, {"text": "they\u0027ll generate it word by word.", "timestamp": "00:16:43,654", "timestamp_s": 1003.0}, {"text": "And, what streaming allows you to do is, you don\u0027t need to wait for the LM to", "timestamp": "00:16:45,304", "timestamp_s": 1005.0}, {"text": "generate the entire response or, output.", "timestamp": "00:16:48,934", "timestamp_s": 1008.0}, {"text": "What you can do is as soon as it is done generating a few words, you can send", "timestamp": "00:16:51,664", "timestamp_s": 1011.0}, {"text": "them to the client and, start displaying them on the UI or, what your client", "timestamp": "00:16:55,174", "timestamp_s": 1015.0}, {"text": "you\u0027re using, in this way, the user.", "timestamp": "00:16:59,064", "timestamp_s": 1019.0}, {"text": "Doesn\u0027t really feel the lag, that, LM generation results in.", "timestamp": "00:17:03,059", "timestamp_s": 1023.0}, {"text": "what they see is that, as soon as they type out a prompt, immediately", "timestamp": "00:17:07,879", "timestamp_s": 1027.0}, {"text": "they start seeing some response and they can start reading it out.", "timestamp": "00:17:11,409", "timestamp_s": 1031.0}, {"text": "you, this is a very common pattern in any chat or LLM", "timestamp": "00:17:14,749", "timestamp_s": 1034.0}, {"text": "application that you would\u0027ve used.", "timestamp": "00:17:19,699", "timestamp_s": 1039.0}, {"text": "As soon as you type something out or you do an action, you start seeing", "timestamp": "00:17:21,559", "timestamp_s": 1041.0}, {"text": "partial results on your UI that is implemented through streaming.", "timestamp": "00:17:25,009", "timestamp_s": 1045.0}, {"text": "the most common or the most, used way, to implement streaming is web sockets.", "timestamp": "00:17:28,619", "timestamp_s": 1048.0}, {"text": "web sockets allow you to send, generated tokens or words in real time.", "timestamp": "00:17:35,529", "timestamp_s": 1055.0}, {"text": "The connection is established, between client and the server, and then, until", "timestamp": "00:17:40,999", "timestamp_s": 1060.0}, {"text": "the entire generation is completed, or, as long as the user is, live on", "timestamp": "00:17:45,939", "timestamp_s": 1065.0}, {"text": "the ui, you can just like reuse that connection to keep sending response,", "timestamp": "00:17:51,369", "timestamp_s": 1071.0}, {"text": "as and when it gets, generated.", "timestamp": "00:17:56,389", "timestamp_s": 1076.0}, {"text": "this is also a bidirectional, um.", "timestamp": "00:17:58,739", "timestamp_s": 1078.0}, {"text": "communication method, so you can use the same method to get", "timestamp": "00:18:02,244", "timestamp_s": 1082.0}, {"text": "some input from the client.", "timestamp": "00:18:05,994", "timestamp_s": 1085.0}, {"text": "Also, no One drawback of, web sockets is that they need", "timestamp": "00:18:08,544", "timestamp_s": 1088.0}, {"text": "some custom, implementation.", "timestamp": "00:18:12,724", "timestamp_s": 1092.0}, {"text": "you can\u0027t just take like your simple HDP rest server and,", "timestamp": "00:18:14,794", "timestamp_s": 1094.0}, {"text": "convert it into web socket.", "timestamp": "00:18:19,424", "timestamp_s": 1099.0}, {"text": "You\u0027ll need to redo your implementation.", "timestamp": "00:18:20,564", "timestamp_s": 1100.0}, {"text": "use new libraries.", "timestamp": "00:18:23,724", "timestamp_s": 1103.0}, {"text": "probably even new use a new language.", "timestamp": "00:18:25,294", "timestamp_s": 1105.0}, {"text": "for example, if you are working on Python, Python is not, very, efficient,", "timestamp": "00:18:27,124", "timestamp_s": 1107.0}, {"text": "way for implementing web sockets.", "timestamp": "00:18:32,704", "timestamp_s": 1112.0}, {"text": "You probably want to move to a different language which handles, threads or", "timestamp": "00:18:34,204", "timestamp_s": 1114.0}, {"text": "multi-processing in a much better way than Python, like Golan or Java, or c plus.", "timestamp": "00:18:39,634", "timestamp_s": 1119.0}, {"text": "so generally web socket implementation.", "timestamp": "00:18:45,154", "timestamp_s": 1125.0}, {"text": "Is a considerable effort.", "timestamp": "00:18:48,454", "timestamp_s": 1128.0}, {"text": "And, if all you want to do is stream LLM responses, it probably", "timestamp": "00:18:50,194", "timestamp_s": 1130.0}, {"text": "is not the best way to do it.", "timestamp": "00:18:54,194", "timestamp_s": 1134.0}, {"text": "there is another, solution for streaming, over SGDP, which is", "timestamp": "00:18:56,114", "timestamp_s": 1136.0}, {"text": "called Server Set Events, which basically uses, your, your, um.", "timestamp": "00:19:01,794", "timestamp_s": 1141.0}, {"text": "as server itself, like basically if you are on Python and you\u0027re using", "timestamp": "00:19:09,164", "timestamp_s": 1149.0}, {"text": "Flask or Fast API, you won\u0027t need to do a lot of changes to start", "timestamp": "00:19:12,374", "timestamp_s": 1152.0}, {"text": "streaming, using server sentiments, code-wise or implementation-wise.", "timestamp": "00:19:17,224", "timestamp_s": 1157.0}, {"text": "this is a minimal effort.", "timestamp": "00:19:21,194", "timestamp_s": 1161.0}, {"text": "what this essentially does is, It\u0027ll use the same FTTP, connection,", "timestamp": "00:19:22,604", "timestamp_s": 1162.0}, {"text": "which your STPI call utilizes.", "timestamp": "00:19:28,484", "timestamp_s": 1168.0}, {"text": "but instead of, sending the entire response in one shot, you can", "timestamp": "00:19:31,494", "timestamp_s": 1171.0}, {"text": "send the response in chunks and, on your client side, you can,", "timestamp": "00:19:36,234", "timestamp_s": 1176.0}, {"text": "receive it in and start displaying.", "timestamp": "00:19:40,384", "timestamp_s": 1180.0}, {"text": "now this is a unidirectional, flow.", "timestamp": "00:19:42,504", "timestamp_s": 1182.0}, {"text": "it works exactly as a rest API call.", "timestamp": "00:19:45,764", "timestamp_s": 1185.0}, {"text": "but instead of, the client waiting for the entire response, to come, the client", "timestamp": "00:19:48,159", "timestamp_s": 1188.0}, {"text": "starts showing chunks that have been sent from server, using server sentiments.", "timestamp": "00:19:54,009", "timestamp_s": 1194.0}, {"text": "implementation wise, it\u0027s very simple.", "timestamp": "00:20:00,309", "timestamp_s": 1200.0}, {"text": "Like you, just need to maybe implement the generator, if you\u0027re using Python", "timestamp": "00:20:02,369", "timestamp_s": 1202.0}, {"text": "and, Maybe add a couple of headers.", "timestamp": "00:20:07,459", "timestamp_s": 1207.0}, {"text": "we won\u0027t get into specific details because these are, things that you", "timestamp": "00:20:10,299", "timestamp_s": 1210.0}, {"text": "can easily Google and, find out.", "timestamp": "00:20:13,059", "timestamp_s": 1213.0}, {"text": "But, our recommendation if you want to implement streaming in your", "timestamp": "00:20:14,979", "timestamp_s": 1214.0}, {"text": "application and you already have a rest, set up ready on the backend.", "timestamp": "00:20:18,809", "timestamp_s": 1218.0}, {"text": "just go for server set events, much, faster implementation,", "timestamp": "00:20:22,599", "timestamp_s": 1222.0}, {"text": "also much easier to implement.", "timestamp": "00:20:26,069", "timestamp_s": 1226.0}, {"text": "web sockets is a bit heavy.", "timestamp": "00:20:28,389", "timestamp_s": 1228.0}, {"text": "And unless you have a specific use case for, web sockets, I won\u0027t", "timestamp": "00:20:30,399", "timestamp_s": 1230.0}, {"text": "recommend, going that, on that path", "timestamp": "00:20:35,389", "timestamp_s": 1235.0}, {"text": "streaming is a good solution.", "timestamp": "00:20:38,394", "timestamp_s": 1238.0}, {"text": "If, the particular task that an LLM is handling, gets over in a few", "timestamp": "00:20:39,954", "timestamp_s": 1239.0}, {"text": "seconds, like five to 10 seconds.", "timestamp": "00:20:45,034", "timestamp_s": 1245.0}, {"text": "but if your task.", "timestamp": "00:20:46,934", "timestamp_s": 1246.0}, {"text": "It is going to take minutes.", "timestamp": "00:20:48,179", "timestamp_s": 1248.0}, {"text": "streaming probably is not a good option.", "timestamp": "00:20:50,369", "timestamp_s": 1250.0}, {"text": "that\u0027s where background jobs come into picture.", "timestamp": "00:20:53,229", "timestamp_s": 1253.0}, {"text": "if you have a task which can be done in five to 10 seconds, probably use streaming", "timestamp": "00:20:56,229", "timestamp_s": 1256.0}, {"text": "and, it\u0027s a good way to start showing, an output, to the user on client side.", "timestamp": "00:21:00,439", "timestamp_s": 1260.0}, {"text": "but if you have a task which is going to take minutes.", "timestamp": "00:21:05,709", "timestamp_s": 1265.0}, {"text": "it is better to handle it asynchronously instead of", "timestamp": "00:21:08,929", "timestamp_s": 1268.0}, {"text": "synchronously in your, backend server.", "timestamp": "00:21:11,809", "timestamp_s": 1271.0}, {"text": "and background jobs help you do that.", "timestamp": "00:21:14,029", "timestamp_s": 1274.0}, {"text": "So what are these particular use cases where you, might want to use", "timestamp": "00:21:15,969", "timestamp_s": 1275.0}, {"text": "background jobs instead of swimming?", "timestamp": "00:21:22,299", "timestamp_s": 1282.0}, {"text": "think of it this way.", "timestamp": "00:21:24,159", "timestamp_s": 1284.0}, {"text": "Let\u0027s say if you, if you are building, something like.", "timestamp": "00:21:25,359", "timestamp_s": 1285.0}, {"text": "An essay generator, and you allow the user to, enter essay topics in bulk.", "timestamp": "00:21:30,009", "timestamp_s": 1290.0}, {"text": "So if someone, gives you a single essay topic, probably, you\u0027ll finish", "timestamp": "00:21:35,979", "timestamp_s": 1295.0}, {"text": "the generation in a few seconds.", "timestamp": "00:21:39,949", "timestamp_s": 1299.0}, {"text": "And, streaming is the way to go.", "timestamp": "00:21:41,329", "timestamp_s": 1301.0}, {"text": "But let\u0027s say if someone, Gives you a hundred essay topics, for generation.", "timestamp": "00:21:43,319", "timestamp_s": 1303.0}, {"text": "I know that this particular task, doesn\u0027t matter how fast the LLM is going to", "timestamp": "00:21:48,669", "timestamp_s": 1308.0}, {"text": "take minutes at least a few minutes.", "timestamp": "00:21:52,269", "timestamp_s": 1312.0}, {"text": "And, if you use streaming for this, streaming, will do all the work", "timestamp": "00:21:54,559", "timestamp_s": 1314.0}, {"text": "in your backend server, and, until this particular task is completed,", "timestamp": "00:21:59,059", "timestamp_s": 1319.0}, {"text": "which is going to be few minutes.", "timestamp": "00:22:03,459", "timestamp_s": 1323.0}, {"text": "your backend server resources are going to get, hogged or are going to be, tied", "timestamp": "00:22:05,259", "timestamp_s": 1325.0}, {"text": "up in this particular task, which is very inefficient because, like your", "timestamp": "00:22:11,819", "timestamp_s": 1331.0}, {"text": "backend server\u0027s job is basically take a request, process in a few seconds", "timestamp": "00:22:16,199", "timestamp_s": 1336.0}, {"text": "and, send it back to the client.", "timestamp": "00:22:20,409", "timestamp_s": 1340.0}, {"text": "if you start doing things which take minutes.", "timestamp": "00:22:22,699", "timestamp_s": 1342.0}, {"text": "You will see that, if you have a lot of concurrent users, you, your backend", "timestamp": "00:22:25,189", "timestamp_s": 1345.0}, {"text": "server will be busy and it\u0027ll not be able to, handle tasks, which take a few", "timestamp": "00:22:30,249", "timestamp_s": 1350.0}, {"text": "seconds and, your APIs will start getting blocked and, your, your, application", "timestamp": "00:22:35,329", "timestamp_s": 1355.0}, {"text": "performance will start to degrade.", "timestamp": "00:22:40,239", "timestamp_s": 1360.0}, {"text": "So what\u0027s the solution here?", "timestamp": "00:22:41,949", "timestamp_s": 1361.0}, {"text": "You, the solution is, you don\u0027t handle.", "timestamp": "00:22:44,699", "timestamp_s": 1364.0}, {"text": "long running tasks in backend server synchronously.", "timestamp": "00:22:48,284", "timestamp_s": 1368.0}, {"text": "You handle them in background jobs asynchronously.", "timestamp": "00:22:52,334", "timestamp_s": 1372.0}, {"text": "Basically.", "timestamp": "00:22:55,454", "timestamp_s": 1375.0}, {"text": "when a user gives you a task, which is going to take minutes, you log", "timestamp": "00:22:56,204", "timestamp_s": 1376.0}, {"text": "it in a database, a background job will pick that task up till then, you", "timestamp": "00:22:59,194", "timestamp_s": 1379.0}, {"text": "tell the, you basically communicate to the user that, okay, this is", "timestamp": "00:23:03,984", "timestamp_s": 1383.0}, {"text": "going to take a few minutes, once.", "timestamp": "00:23:06,654", "timestamp_s": 1386.0}, {"text": "The task is completed, you\u0027ll get a notification, probably", "timestamp": "00:23:09,139", "timestamp_s": 1389.0}, {"text": "as an email, or on Slack.", "timestamp": "00:23:12,719", "timestamp_s": 1392.0}, {"text": "And, what you do is you use a background job to, pick up the task, process it, and", "timestamp": "00:23:15,129", "timestamp_s": 1395.0}, {"text": "once it\u0027s ready, send out a notification.", "timestamp": "00:23:20,549", "timestamp_s": 1400.0}, {"text": "easiest way to implement this is CR Jobs.", "timestamp": "00:23:22,989", "timestamp_s": 1402.0}, {"text": "crown Jobs have been here for, I don\u0027t know for a very long time.", "timestamp": "00:23:25,829", "timestamp_s": 1405.0}, {"text": "very easy to implement, on any Unix-based, server, which is", "timestamp": "00:23:29,799", "timestamp_s": 1409.0}, {"text": "probably, what will be used in most of, production backing servers.", "timestamp": "00:23:35,459", "timestamp_s": 1415.0}, {"text": "all you need to do is set up a CR job, which does the processing.", "timestamp": "00:23:39,189", "timestamp_s": 1419.0}, {"text": "and the CR job runs every few minutes, checks the database", "timestamp": "00:23:43,519", "timestamp_s": 1423.0}, {"text": "if there are in pending tasks.", "timestamp": "00:23:46,939", "timestamp_s": 1426.0}, {"text": "now when your user, comes to you.", "timestamp": "00:23:48,599", "timestamp_s": 1428.0}, {"text": "With, with a task, you just put it in a DB and, mark it as pending.", "timestamp": "00:23:52,209", "timestamp_s": 1432.0}, {"text": "when the crown job wakes up in a few minutes, it will check for any", "timestamp": "00:23:57,749", "timestamp_s": 1437.0}, {"text": "pending task and start the processing.", "timestamp": "00:24:00,749", "timestamp_s": 1440.0}, {"text": "And, on the US side you can probably, implement some sort of polling.", "timestamp": "00:24:03,269", "timestamp_s": 1443.0}, {"text": "To check if the task is completed or not.", "timestamp": "00:24:07,604", "timestamp_s": 1447.0}, {"text": "And once it is completed, you can display that on the ui.", "timestamp": "00:24:09,344", "timestamp_s": 1449.0}, {"text": "But, this is an optional thing.", "timestamp": "00:24:12,554", "timestamp_s": 1452.0}, {"text": "Ideally, if you\u0027re using background jobs, you should also, sorry.", "timestamp": "00:24:14,004", "timestamp_s": 1454.0}, {"text": "You should also, Separately communicate, that the task is completed with the user", "timestamp": "00:24:18,524", "timestamp_s": 1458.0}, {"text": "because, the general, idea is that, when you, when a task is going to take a few", "timestamp": "00:24:23,694", "timestamp_s": 1463.0}, {"text": "minutes, your users will probably come to your platform, submit the task, and", "timestamp": "00:24:27,834", "timestamp_s": 1467.0}, {"text": "they will, move away from your platform.", "timestamp": "00:24:32,564", "timestamp_s": 1472.0}, {"text": "So they\u0027re not looking at.", "timestamp": "00:24:34,254", "timestamp_s": 1474.0}, {"text": "the UI of your application.", "timestamp": "00:24:35,884", "timestamp_s": 1475.0}, {"text": "So you should probably communicate that the task is completed through", "timestamp": "00:24:37,444", "timestamp_s": 1477.0}, {"text": "an email or a Slack notification.", "timestamp": "00:24:41,584", "timestamp_s": 1481.0}, {"text": "so the users who have moved away from, the UI also know that okay, that,", "timestamp": "00:24:43,964", "timestamp_s": 1483.0}, {"text": "that generation has been completed.", "timestamp": "00:24:47,744", "timestamp_s": 1487.0}, {"text": "this works very well, minimal setup.", "timestamp": "00:24:51,029", "timestamp_s": 1491.0}, {"text": "Nothing new that you probably need to learn, nothing new", "timestamp": "00:24:53,269", "timestamp_s": 1493.0}, {"text": "that you need to install.", "timestamp": "00:24:55,789", "timestamp_s": 1495.0}, {"text": "for the initial stages of your LLM application, just go for a crown job.", "timestamp": "00:24:57,269", "timestamp_s": 1497.0}, {"text": "what happens is that as your, application grows, you\u0027ll probably need to scale", "timestamp": "00:25:02,099", "timestamp_s": 1502.0}, {"text": "this now, if you run multiple crown jobs, you need to handle which crown job.", "timestamp": "00:25:08,779", "timestamp_s": 1508.0}, {"text": "pick up which task you need to implement some sort of, distributed locking and,", "timestamp": "00:25:13,854", "timestamp_s": 1513.0}, {"text": "all those complexities come into picture.", "timestamp": "00:25:19,034", "timestamp_s": 1519.0}, {"text": "Basically, crown jobs are good for the initial stages, but, like", "timestamp": "00:25:20,924", "timestamp_s": 1520.0}, {"text": "we also started with crown jobs.", "timestamp": "00:25:26,084", "timestamp_s": 1526.0}, {"text": "we still use crown jobs for some simple tasks, but there will be a", "timestamp": "00:25:27,784", "timestamp_s": 1527.0}, {"text": "stage, When you\u0027ll need to move away from crown jobs for scalability, and", "timestamp": "00:25:31,504", "timestamp_s": 1531.0}, {"text": "for, better retrain mechanisms, that\u0027s where task queue come into picture.", "timestamp": "00:25:36,204", "timestamp_s": 1536.0}, {"text": "So basically think of task queue as crown jobs with like more intelligence, where", "timestamp": "00:25:41,654", "timestamp_s": 1541.0}, {"text": "all the, task management that needs to be done, is handled by the task queue itself.", "timestamp": "00:25:47,454", "timestamp_s": 1547.0}, {"text": "when I say task management, on a very high level, what.", "timestamp": "00:25:53,204", "timestamp_s": 1553.0}, {"text": "It means is that, you submit a task to the task queue.", "timestamp": "00:25:56,344", "timestamp_s": 1556.0}, {"text": "generally a task queue is backed by some storage, like Redis or some other cache.", "timestamp": "00:25:59,504", "timestamp_s": 1559.0}, {"text": "the task is stored over there, and then the task queue handles, basically a", "timestamp": "00:26:04,914", "timestamp_s": 1564.0}, {"text": "task queue will have a bunch of workers running and, a ta the task queue will", "timestamp": "00:26:09,634", "timestamp_s": 1569.0}, {"text": "then handle, how to allocate that work.", "timestamp": "00:26:13,884", "timestamp_s": 1573.0}, {"text": "To which worker based on like a bunch of different mechanisms.", "timestamp": "00:26:17,264", "timestamp_s": 1577.0}, {"text": "Like you can have, priority queues, you can have a bunch of different", "timestamp": "00:26:20,114", "timestamp_s": 1580.0}, {"text": "retry mechanisms, and all those things.", "timestamp": "00:26:24,204", "timestamp_s": 1584.0}, {"text": "two good things about using task queue.", "timestamp": "00:26:26,954", "timestamp_s": 1586.0}, {"text": "task queues are much easier to scale.", "timestamp": "00:26:29,184", "timestamp_s": 1589.0}, {"text": "in Aron job, if you go from one to two to 10 crown jobs, you have to handle a", "timestamp": "00:26:31,004", "timestamp_s": 1591.0}, {"text": "bunch of, Locking related stuff yourself.", "timestamp": "00:26:35,204", "timestamp_s": 1595.0}, {"text": "in task queue, it\u0027s already, implemented for you.", "timestamp": "00:26:37,954", "timestamp_s": 1597.0}, {"text": "So all you can do is increase the number of workers in a task queue.", "timestamp": "00:26:40,724", "timestamp_s": 1600.0}, {"text": "And, if you start getting more, tasks or workload, the, you can just it\u0027s as easy", "timestamp": "00:26:44,504", "timestamp_s": 1604.0}, {"text": "as just changing a number on a dashboard, to increase the number of workers.", "timestamp": "00:26:50,954", "timestamp_s": 1610.0}, {"text": "again, like all the, additional handling.", "timestamp": "00:26:54,744", "timestamp_s": 1614.0}, {"text": "For race conditions, retries, timeouts, it\u0027s already taken care of.", "timestamp": "00:26:58,644", "timestamp_s": 1618.0}, {"text": "All you need to do is, provide some configuration.", "timestamp": "00:27:01,614", "timestamp_s": 1621.0}, {"text": "you also get better monitoring with task use.", "timestamp": "00:27:04,574", "timestamp_s": 1624.0}, {"text": "you, every task you comes with some sort of, monitoring mechanism", "timestamp": "00:27:06,744", "timestamp_s": 1626.0}, {"text": "or, dashboard where you can see what are the task currently", "timestamp": "00:27:11,894", "timestamp_s": 1631.0}, {"text": "running, how much resources there.", "timestamp": "00:27:15,434", "timestamp_s": 1635.0}, {"text": "Eating up, which tasks are failing, start or restart tasks", "timestamp": "00:27:17,414", "timestamp_s": 1637.0}, {"text": "and all those kind of things.", "timestamp": "00:27:21,344", "timestamp_s": 1641.0}, {"text": "once you start scaling your application, go for task use.", "timestamp": "00:27:22,874", "timestamp_s": 1642.0}, {"text": "The task queue that we use in our production is called rq, which stands", "timestamp": "00:27:26,384", "timestamp_s": 1646.0}, {"text": "for Redis Q. And, as the name suggest, it\u0027s backed by Redis, and it\u0027s a", "timestamp": "00:27:32,544", "timestamp_s": 1652.0}, {"text": "very simple, library for Qing and processing background jobs with workers.", "timestamp": "00:27:37,759", "timestamp_s": 1657.0}, {"text": "very easy setup.", "timestamp": "00:27:43,959", "timestamp_s": 1663.0}, {"text": "Hardly takes 15 minutes to set it up.", "timestamp": "00:27:45,039", "timestamp_s": 1665.0}, {"text": "If you already have a Redis, you don\u0027t even need to, set, set up", "timestamp": "00:27:47,169", "timestamp_s": 1667.0}, {"text": "a red, for RQ and, very simple.", "timestamp": "00:27:50,869", "timestamp_s": 1670.0}, {"text": "Uh.", "timestamp": "00:27:56,329", "timestamp_s": 1676.0}, {"text": "mechanism for queuing and processing.", "timestamp": "00:27:57,699", "timestamp_s": 1677.0}, {"text": "All you need to do is create a queue, provide it a red connection so that,", "timestamp": "00:28:00,429", "timestamp_s": 1680.0}, {"text": "it has a place to store the tasks.", "timestamp": "00:28:04,079", "timestamp_s": 1684.0}, {"text": "when you get a task, queue and queue, you can, and, this is basically a", "timestamp": "00:28:06,519", "timestamp_s": 1686.0}, {"text": "function which is going to get called in the worker to process your tasks.", "timestamp": "00:28:10,249", "timestamp_s": 1690.0}, {"text": "it\u0027s this simple and you can also provide some arguments for that function and.", "timestamp": "00:28:14,559", "timestamp_s": 1694.0}, {"text": "The worker for the worker, you just need to start it like", "timestamp": "00:28:20,279", "timestamp_s": 1700.0}, {"text": "this, on your command line.", "timestamp": "00:28:24,119", "timestamp_s": 1704.0}, {"text": "And, it consumes tasks from Redis and, process them.", "timestamp": "00:28:26,329", "timestamp_s": 1706.0}, {"text": "If you, want to increase the number of workers, you just start 10", "timestamp": "00:28:29,899", "timestamp_s": 1709.0}, {"text": "different workers, connect them to the same Redis, and, RQ will itself", "timestamp": "00:28:34,229", "timestamp_s": 1714.0}, {"text": "handle all the, all the complexities.", "timestamp": "00:28:38,869", "timestamp_s": 1718.0}, {"text": "Of, managing which worker gets what task, and all those kind of things.", "timestamp": "00:28:42,134", "timestamp_s": 1722.0}, {"text": "if you\u0027re on Python, RQ is the way to go.", "timestamp": "00:28:46,669", "timestamp_s": 1726.0}, {"text": "Salary provides you with, similar, functionality.", "timestamp": "00:28:50,039", "timestamp_s": 1730.0}, {"text": "but we just found that, there were a bunch of things in salary,", "timestamp": "00:28:54,399", "timestamp_s": 1734.0}, {"text": "which we did not really need.", "timestamp": "00:28:56,979", "timestamp_s": 1736.0}, {"text": "and it seemed like an overkill.", "timestamp": "00:28:59,279", "timestamp_s": 1739.0}, {"text": "so we decided to go with RQH was much simpler to set up on our end from", "timestamp": "00:29:01,074", "timestamp_s": 1741.0}, {"text": "thread, what inputs are not working, what models are working, what models", "timestamp": "00:29:05,324", "timestamp_s": 1745.0}, {"text": "are not working, and things like that.", "timestamp": "00:29:07,934", "timestamp_s": 1747.0}, {"text": "if you want an analogy, you can think of evals as unit testing.", "timestamp": "00:29:09,994", "timestamp_s": 1749.0}, {"text": "think of it as unit testing for your prompts.", "timestamp": "00:29:14,754", "timestamp_s": 1754.0}, {"text": "So this allows you to take a prompt template.", "timestamp": "00:29:16,704", "timestamp_s": 1756.0}, {"text": "And individually just test out that template with a bunch of different values.", "timestamp": "00:29:20,169", "timestamp_s": 1760.0}, {"text": "and, you can, there are a bunch of, reasons why you should ideally, use", "timestamp": "00:29:26,079", "timestamp_s": 1766.0}, {"text": "evals with your prompt templates.", "timestamp": "00:29:32,129", "timestamp_s": 1772.0}, {"text": "one, it allows you to just test out the prompts in I isolation,", "timestamp": "00:29:34,219", "timestamp_s": 1774.0}, {"text": "which makes it very fast.", "timestamp": "00:29:37,459", "timestamp_s": 1777.0}, {"text": "the same way unit tests are fast, because you are just checking one function", "timestamp": "00:29:38,469", "timestamp_s": 1778.0}, {"text": "against different types of inputs.", "timestamp": "00:29:41,719", "timestamp_s": 1781.0}, {"text": "Using prompts, using, sorry, I\u0027m sorry.", "timestamp": "00:29:43,579", "timestamp_s": 1783.0}, {"text": "using evals, you will be able to figure out different things like which", "timestamp": "00:29:45,828", "timestamp_s": 1785.0}, {"text": "input works, which input doesn\u0027t work, which model works for a particular", "timestamp": "00:29:51,178", "timestamp_s": 1791.0}, {"text": "task, which model does not work.", "timestamp": "00:29:55,018", "timestamp_s": 1795.0}, {"text": "you\u0027ll be able to compare, costs of different models for different", "timestamp": "00:29:57,168", "timestamp_s": 1797.0}, {"text": "types of inputs and so on.", "timestamp": "00:30:00,758", "timestamp_s": 1800.0}, {"text": "an additional, benefit of using evals is that.", "timestamp": "00:30:03,018", "timestamp_s": 1803.0}, {"text": "You can directly, integrate them with your CICD pipeline so that,", "timestamp": "00:30:07,368", "timestamp_s": 1807.0}, {"text": "you don\u0027t need to manually keep checking before every release if your", "timestamp": "00:30:12,478", "timestamp_s": 1812.0}, {"text": "prompts are still working the way, they\u0027re working just like unit test.", "timestamp": "00:30:15,628", "timestamp_s": 1815.0}, {"text": "You just, hook it up to your CICD pipeline and, before every commit or I\u0027m sorry,", "timestamp": "00:30:19,178", "timestamp_s": 1819.0}, {"text": "after every commit or, after every build, you, straight up run the evals.", "timestamp": "00:30:23,903", "timestamp_s": 1823.0}, {"text": "And, similar to, assertions in unit tests, evals also have assertions or checks", "timestamp": "00:30:28,778", "timestamp_s": 1828.0}, {"text": "where you can check the response, and, specify whether it is as expected or not", "timestamp": "00:30:34,458", "timestamp_s": 1834.0}, {"text": "as expected, and pass or fail an eval.", "timestamp": "00:30:40,128", "timestamp_s": 1840.0}, {"text": "That\u0027s how on a very high level evals work.", "timestamp": "00:30:42,858", "timestamp_s": 1842.0}, {"text": "we have tried out a bunch of different, eval libraries.", "timestamp": "00:30:46,478", "timestamp_s": 1846.0}, {"text": "the one we like the most is Profu.", "timestamp": "00:30:50,208", "timestamp_s": 1850.0}, {"text": "very easy to set up.", "timestamp": "00:30:52,948", "timestamp_s": 1852.0}, {"text": "simply works using YAML files.", "timestamp": "00:30:54,668", "timestamp_s": 1854.0}, {"text": "basically you, you create a YAML file where you specify your prompt template", "timestamp": "00:30:58,968", "timestamp_s": 1858.0}, {"text": "and you press, specify a bunch of inputs, for that prompt template.", "timestamp": "00:31:03,718", "timestamp_s": 1863.0}, {"text": "And, using, profu is an open source, tool.", "timestamp": "00:31:07,863", "timestamp_s": 1867.0}, {"text": "So you can just like, straight up install it from NPM, brand it in your CLI.", "timestamp": "00:31:11,093", "timestamp_s": 1871.0}, {"text": "and at the end of the Evalue you get a nice, graph like this.", "timestamp": "00:31:16,493", "timestamp_s": 1876.0}, {"text": "Which will show you for different types of inputs, whether the", "timestamp": "00:31:21,748", "timestamp_s": 1881.0}, {"text": "output has passed the condition.", "timestamp": "00:31:26,198", "timestamp_s": 1886.0}, {"text": "it\u0027ll also allow you to compare different, models and, there is", "timestamp": "00:31:28,488", "timestamp_s": 1888.0}, {"text": "some way to compare cost as well.", "timestamp": "00:31:33,538", "timestamp_s": 1893.0}, {"text": "I don\u0027t think they have displayed it here, but yeah.", "timestamp": "00:31:35,428", "timestamp_s": 1895.0}, {"text": "cost comparison is also something that you\u0027ll get in the same dashboard", "timestamp": "00:31:37,418", "timestamp_s": 1897.0}, {"text": "and, You can start off with the open source version of Profu, but they", "timestamp": "00:31:40,808", "timestamp_s": 1900.0}, {"text": "also have a cloud hosted version.", "timestamp": "00:31:45,258", "timestamp_s": 1905.0}, {"text": "So if you want more reliability or don\u0027t want to manage your own instance,", "timestamp": "00:31:47,028", "timestamp_s": 1907.0}, {"text": "that option is also available.", "timestamp": "00:31:51,058", "timestamp_s": 1911.0}, {"text": "before we end the talk, let\u0027s do a quick walkthrough of all the different", "timestamp": "00:31:53,028", "timestamp_s": 1913.0}, {"text": "foundational models, or foundational model APIs that are available for public use.", "timestamp": "00:31:56,898", "timestamp_s": 1916.0}, {"text": "The reason for doing this is basically, this landscape is changing very fast.", "timestamp": "00:32:02,018", "timestamp_s": 1922.0}, {"text": "So the last time you had gone over all the available models, I\u0027m pretty sure", "timestamp": "00:32:06,878", "timestamp_s": 1926.0}, {"text": "that, by now the list of models and also their, comparisons have changed.", "timestamp": "00:32:12,198", "timestamp_s": 1932.0}, {"text": "probably the models you thought, are not that great have", "timestamp": "00:32:17,698", "timestamp_s": 1937.0}, {"text": "become very good, and so on.", "timestamp": "00:32:21,048", "timestamp_s": 1941.0}, {"text": "So let\u0027s do a quick run through of all the available models, what are they good at?", "timestamp": "00:32:22,828", "timestamp_s": 1942.0}, {"text": "What are.", "timestamp": "00:32:26,968", "timestamp_s": 1946.0}, {"text": "They\u0027re not good at what kind of use cases?", "timestamp": "00:32:27,763", "timestamp_s": 1947.0}, {"text": "Um, you what case, what kind of use cases work with a particular kind of model?", "timestamp": "00:32:29,623", "timestamp_s": 1949.0}, {"text": "let\u0027s start with the oldest player OpenAI.", "timestamp": "00:32:35,983", "timestamp_s": 1955.0}, {"text": "OpenAI has, three main families of models, which is GPT 4 0 4 O, and o,", "timestamp": "00:32:38,413", "timestamp_s": 1958.0}, {"text": "which are available for public use.", "timestamp": "00:32:45,453", "timestamp_s": 1965.0}, {"text": "I think they\u0027ve deprecated their three and 3.5 models.", "timestamp": "00:32:47,263", "timestamp_s": 1967.0}, {"text": "so these are the models that are available right now.", "timestamp": "00:32:50,043", "timestamp_s": 1970.0}, {"text": "If you don\u0027t know what to use, just go with open air.", "timestamp": "00:32:53,343", "timestamp_s": 1973.0}, {"text": "these are the most versatile.", "timestamp": "00:32:56,743", "timestamp_s": 1976.0}, {"text": "Models, work with wide, they work very well with wide, wide variety", "timestamp": "00:32:59,533", "timestamp_s": 1979.0}, {"text": "of tasks, within these models.", "timestamp": "00:33:03,693", "timestamp_s": 1983.0}, {"text": "between four oh and, four oh mini, the difference is mainly, the trade off", "timestamp": "00:33:06,813", "timestamp_s": 1986.0}, {"text": "between, Cost and latency versus accuracy.", "timestamp": "00:33:11,203", "timestamp_s": 1991.0}, {"text": "So if you have a complex task or something that requires a bit", "timestamp": "00:33:14,553", "timestamp_s": 1994.0}, {"text": "more of reasoning, go for four.", "timestamp": "00:33:18,813", "timestamp_s": 1998.0}, {"text": "if you are worried about cost or if you\u0027re worried about, how fast the response is", "timestamp": "00:33:21,513", "timestamp_s": 2001.0}, {"text": "going to be, go for four oh mini, but, it\u0027ll basically, give you lesser accuracy.", "timestamp": "00:33:25,383", "timestamp_s": 2005.0}, {"text": "O is something that I\u0027ve not tried out.", "timestamp": "00:33:30,948", "timestamp_s": 2010.0}, {"text": "these are supposed to be, open as flagship models.", "timestamp": "00:33:32,718", "timestamp_s": 2012.0}, {"text": "but from.", "timestamp": "00:33:36,478", "timestamp_s": 2016.0}, {"text": "What I\u0027ve heard, these are like fairly new.", "timestamp": "00:33:37,673", "timestamp_s": 2017.0}, {"text": "before you put it in production, maybe, test them out thoroughly.", "timestamp": "00:33:39,603", "timestamp_s": 2019.0}, {"text": "four O and four O Mini have been around for a while now, so I think, you should", "timestamp": "00:33:42,983", "timestamp_s": 2022.0}, {"text": "not see a lot of problems, with them.", "timestamp": "00:33:47,023", "timestamp_s": 2027.0}, {"text": "Also, like reliability wise, as, according to us, open air APIs", "timestamp": "00:33:49,013", "timestamp_s": 2029.0}, {"text": "have been the most reliable.", "timestamp": "00:33:54,193", "timestamp_s": 2034.0}, {"text": "so you don\u0027t need to worry about, downtime or, having to, handle switching models", "timestamp": "00:33:55,728", "timestamp_s": 2035.0}, {"text": "because, this provider is not working.", "timestamp": "00:34:02,348", "timestamp_s": 2042.0}, {"text": "The next provider is, philanthropic.", "timestamp": "00:34:04,678", "timestamp_s": 2044.0}, {"text": "I think for a while, these guys were working mostly on the, chat.", "timestamp": "00:34:08,958", "timestamp_s": 2048.0}, {"text": "the APIs were not publicly available as far as I know.", "timestamp": "00:34:13,838", "timestamp_s": 2053.0}, {"text": "but I think in the last few months, I think that has changed.", "timestamp": "00:34:18,168", "timestamp_s": 2058.0}, {"text": "the APIs are available.", "timestamp": "00:34:20,618", "timestamp_s": 2060.0}, {"text": "You can just directly, and they\u0027re completely self serve.", "timestamp": "00:34:21,758", "timestamp_s": 2061.0}, {"text": "You can just directly go, on, anthropics, anthropics console and, create an API key.", "timestamp": "00:34:24,228", "timestamp_s": 2064.0}, {"text": "Load up some credit and get started with it.", "timestamp": "00:34:29,418", "timestamp_s": 2069.0}, {"text": "If you have any coding related use case, Claude APIs are your best choice.", "timestamp": "00:34:32,648", "timestamp_s": 2072.0}, {"text": "I think, as far as coding is concerned, coding as a particular task, Claude, works", "timestamp": "00:34:38,118", "timestamp_s": 2078.0}, {"text": "much better than, all the other models.", "timestamp": "00:34:43,988", "timestamp_s": 2083.0}, {"text": "which is also why you would\u0027ve seen that.", "timestamp": "00:34:46,688", "timestamp_s": 2086.0}, {"text": "everyone is using Claude with, They\u0027re, code editors as well, like cursor.", "timestamp": "00:34:48,408", "timestamp_s": 2088.0}, {"text": "so yeah, if code is what you want, work with clo.", "timestamp": "00:34:53,138", "timestamp_s": 2093.0}, {"text": "Next up is gr not to be confused with x gr.", "timestamp": "00:34:56,658", "timestamp_s": 2096.0}, {"text": "So Grok is, essentially, a company that is building, Special purpose chips.", "timestamp": "00:35:01,398", "timestamp_s": 2101.0}, {"text": "They call them pus, for running LLMs, which, makes,", "timestamp": "00:35:08,708", "timestamp_s": 2108.0}, {"text": "their inference time very low.", "timestamp": "00:35:12,658", "timestamp_s": 2112.0}, {"text": "probably, even the inference cost, So if latency is what you\u0027re trying to", "timestamp": "00:35:14,968", "timestamp_s": 2114.0}, {"text": "optimize, tryout, grok, grok, cloud, which is their API, which are their, LLM APIs.", "timestamp": "00:35:20,628", "timestamp_s": 2120.0}, {"text": "they generally host, most of the commonly used open source models.", "timestamp": "00:35:26,848", "timestamp_s": 2126.0}, {"text": "so you have llama, extra Gemma available.", "timestamp": "00:35:30,983", "timestamp_s": 2130.0}, {"text": "apart from that, a bunch of other things, Latency wise, they are much faster", "timestamp": "00:35:33,643", "timestamp_s": 2133.0}, {"text": "than, all the other model providers.", "timestamp": "00:35:39,133", "timestamp_s": 2139.0}, {"text": "So if you are optimizing for latency and, these models work for", "timestamp": "00:35:40,863", "timestamp_s": 2140.0}, {"text": "your particular task, go for it.", "timestamp": "00:35:45,413", "timestamp_s": 2145.0}, {"text": "Alright, so AWS, mainly works like rock.", "timestamp": "00:35:47,493", "timestamp_s": 2147.0}, {"text": "They host a lot of open source models.", "timestamp": "00:35:52,473", "timestamp_s": 2152.0}, {"text": "On.", "timestamp": "00:35:55,203", "timestamp_s": 2155.0}, {"text": "And along with that, I think they also have, their own models,", "timestamp": "00:35:55,743", "timestamp_s": 2155.0}, {"text": "which, we have not tried out yet.", "timestamp": "00:35:58,813", "timestamp_s": 2158.0}, {"text": "but the biggest USP of using AWS bedrock would be if you\u0027re already", "timestamp": "00:36:00,633", "timestamp_s": 2160.0}, {"text": "in the AWS, ecosystem and you are, worried about, your sensitive data,", "timestamp": "00:36:07,013", "timestamp_s": 2167.0}, {"text": "Getting out of your infra and you don\u0027t want to like, send it to open AI", "timestamp": "00:36:12,973", "timestamp_s": 2172.0}, {"text": "or cloud or any other model provider.", "timestamp": "00:36:16,618", "timestamp_s": 2176.0}, {"text": "in that case, bedrock should be your choice.", "timestamp": "00:36:18,823", "timestamp_s": 2178.0}, {"text": "one good thing is Bedrock also hosts cloud APIs.", "timestamp": "00:36:21,503", "timestamp_s": 2181.0}, {"text": "the limits are lower.", "timestamp": "00:36:24,993", "timestamp_s": 2184.0}, {"text": "as far as I know.", "timestamp": "00:36:26,953", "timestamp_s": 2186.0}, {"text": "I think you\u0027ll need to talk to the support and, get your service quota increased.", "timestamp": "00:36:27,733", "timestamp_s": 2187.0}, {"text": "But, If you are worried about, sensitive data and you\u0027re okay with cloud,", "timestamp": "00:36:30,903", "timestamp_s": 2190.0}, {"text": "bedrock should work for you very well.", "timestamp": "00:36:35,458", "timestamp_s": 2195.0}, {"text": "and along with that, they also host LA and Mixture and a few", "timestamp": "00:36:37,528", "timestamp_s": 2197.0}, {"text": "other, APIs, multimodal APIs.", "timestamp": "00:36:40,918", "timestamp_s": 2200.0}, {"text": "Azure is, the last time I checked Azure is hosting GPT models.", "timestamp": "00:36:43,318", "timestamp_s": 2203.0}, {"text": "separately, the hosting, which OpenAI does is separate from Azure.", "timestamp": "00:36:48,733", "timestamp_s": 2208.0}, {"text": "And, the last time we checked, Azure GPT APIs were a bit more faster than open air.", "timestamp": "00:36:52,483", "timestamp_s": 2212.0}, {"text": "So again, oh, if you want to use open AI APIs and you, want, a slightly", "timestamp": "00:36:58,373", "timestamp_s": 2218.0}, {"text": "better latency, tryout as Azure, but, they\u0027ll make you fill a bunch of forms.", "timestamp": "00:37:04,513", "timestamp_s": 2224.0}, {"text": "I think these APIs or, these models are not publicly", "timestamp": "00:37:08,513", "timestamp_s": 2228.0}, {"text": "available on Azure for everyone.", "timestamp": "00:37:11,003", "timestamp_s": 2231.0}, {"text": "Of now, GCP, I\u0027ve not tried out.", "timestamp": "00:37:13,208", "timestamp_s": 2233.0}, {"text": "again, I think the setup was a bit complex, we didn\u0027t get a chance to give", "timestamp": "00:37:15,798", "timestamp_s": 2235.0}, {"text": "it a try, but from what we\u0027ve heard, the developer experience is much better now.", "timestamp": "00:37:18,768", "timestamp_s": 2238.0}, {"text": "So someday we\u0027ll give it a try again.", "timestamp": "00:37:23,628", "timestamp_s": 2243.0}, {"text": "But GCP has, Gemini and the.", "timestamp": "00:37:26,118", "timestamp_s": 2246.0}, {"text": "Latest, the newest scale on the block is Deeps sec. if you are active", "timestamp": "00:37:30,408", "timestamp_s": 2250.0}, {"text": "on Twitter, you would\u0027ve already heard, about deeps, SEC\u0027s, APIs.", "timestamp": "00:37:35,318", "timestamp_s": 2255.0}, {"text": "from the chatter, it seems as if they are at par with own APIs.", "timestamp": "00:37:39,828", "timestamp_s": 2259.0}, {"text": "again, having tried it out, give it a try.", "timestamp": "00:37:45,568", "timestamp_s": 2265.0}, {"text": "one concern could be, the hosting, which is in China, but, um.", "timestamp": "00:37:47,988", "timestamp_s": 2267.0}, {"text": "definitely give it a try.", "timestamp": "00:37:52,858", "timestamp_s": 2272.0}, {"text": "probably you might find it, to be a good fit for your use case.", "timestamp": "00:37:54,228", "timestamp_s": 2274.0}, {"text": "And, one more thing, deep seeks models are also open source so", "timestamp": "00:37:58,198", "timestamp_s": 2278.0}, {"text": "you can host them on your own.", "timestamp": "00:38:02,238", "timestamp_s": 2282.0}, {"text": "And that\u0027s all from me.", "timestamp": "00:38:03,638", "timestamp_s": 2283.0}, {"text": "I hope you find the information shared in the stock useful, and it speeds up", "timestamp": "00:38:04,808", "timestamp_s": 2284.0}, {"text": "your development process when you are building LM applications and, AI agents.", "timestamp": "00:38:10,998", "timestamp_s": 2290.0}, {"text": "if you have any, queries or if you want to, talk more", "timestamp": "00:38:16,068", "timestamp_s": 2296.0}, {"text": "about this, drop us an email.", "timestamp": "00:38:19,288", "timestamp_s": 2299.0}, {"text": "You can find our email.", "timestamp": "00:38:20,608", "timestamp_s": 2300.0}, {"text": "On Ku AI\u0027s landing page, or just, send me a message on LinkedIn.", "timestamp": "00:38:22,288", "timestamp_s": 2302.0}, {"text": "happy to chat about this and, go with something.", "timestamp": "00:38:27,598", "timestamp_s": 2307.0}, {"text": "Awesome.", "timestamp": "00:38:30,528", "timestamp_s": 2310.0}, {"text": "Bye.", "timestamp": "00:38:30,798", "timestamp_s": 2310.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'aZo37Cyaprk',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Practical tips for building AI applications using LLMs - Best practices and trade-offs
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Understand the problems and trade-offs you&rsquo;ll encounter when building an AI application on top of LLMs based on my learnings of building KushoAI, an AI agent used by 5000+ engineers to make API testing completely autonomous by leveraging LLMs.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./srt/llms2025_Sourabh_Gawande.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:00,240'); seek(0.0)">
              Hello everyone.
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:01,050'); seek(1.0)">
              My name is Soap.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:01,890'); seek(1.0)">
              I am the co-founder and CTO of Ku ai and today I'm going to talk about practical
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:07,080'); seek(7.0)">
              tips for building AI applications or AI agents using LLMs at KU ai.
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:12,510'); seek(12.0)">
              We have been, working on building AI applications and,
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:16,710'); seek(16.0)">
              agents for the last, 18 months.
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:19,040'); seek(19.0)">
              And, during our journey, we have identified a bunch of unique, problems.
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:25,355'); seek(25.0)">
              That people generally face.
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:27,035'); seek(27.0)">
              And, we have also faced, those same problems specifically while
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:30,904'); seek(30.0)">
              building, applications using LLMs.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:33,425'); seek(33.0)">
              And, the, agenda for today's talk is that, we want to, educate, devs about
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:00:40,290'); seek(40.0)">
              these problems, so that, when they are building apps on top of LLMs, they're
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:00:45,780'); seek(45.0)">
              aware of these problems and, Also discuss what are, the solutions that
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:00:49,680'); seek(49.0)">
              work for us and, the dev tools or tooling that we use to solve these problems.
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:00:54,120'); seek(54.0)">
              and, by sharing this information, we want to save time, when devs
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:00:58,885'); seek(58.0)">
              are, building applications on top of, for the first time.
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:02,755'); seek(62.0)">
              The first thing that you'll need to solve when you start building
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:05,635'); seek(65.0)">
              apps on top of, LLMs is, how to handle LLM inconsistencies.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:11,095'); seek(71.0)">
              you, if you have some experience building, applications using normal
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:14,810'); seek(74.0)">
              APIs, you would've seen that they don't.
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:16,875'); seek(76.0)">
              fail that offer.
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:17,575'); seek(77.0)">
              while building, general applications, you don't really worry about,
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:21,264'); seek(81.0)">
              inconsistencies or failures, that much.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:24,164'); seek(84.0)">
              if an API fails, you just let, the API fail.
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:26,455'); seek(86.0)">
              And, the user, when they refresh their page, you make another API
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:29,854'); seek(89.0)">
              call and it'll most probably succeed.
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:31,555'); seek(91.0)">
              but in case of LLMs.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:33,215'); seek(93.0)">
              this is the first thing that you'll probably need to solve, when you're
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:01:36,175'); seek(96.0)">
              actually building an application because, LLMs have a much higher
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:01:40,055'); seek(100.0)">
              error rate, than your normal APIs.
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:01:42,765'); seek(102.0)">
              And unless you solve this particular thing, your application will have a
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:01:47,265'); seek(107.0)">
              terrible ux, or user experience because, in your application, you'll generally
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:01:52,925'); seek(112.0)">
              use the LLM response, somewhere else.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:01:55,475'); seek(115.0)">
              And every time the LLM gives you.
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:01:58,130'); seek(118.0)">
              A wrong output, your application will also crash.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:00,830'); seek(120.0)">
              So this is the first, problem that, you should solve, while
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:06,180'); seek(126.0)">
              building LLM applications.
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:08,610'); seek(128.0)">
              now before we get into, Like why, how to solve this, particular problem.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:15,070'); seek(135.0)">
              let's just talk about why this even occurs, in these, in
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:18,895'); seek(138.0)">
              these specific applications.
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:20,695'); seek(140.0)">
              so like I mentioned earlier, if you are working with, normal
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:23,945'); seek(143.0)">
              APIs, you generally don't worry.
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:27,135'); seek(147.0)">
              Much about the error rate, in like fairly stable, APIs, but even the most
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:32,825'); seek(152.0)">
              stable LLMs give you a much higher error rate, than your normal APIs.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:02:37,035'); seek(157.0)">
              And the reason for this is LLMs are inherently non-deterministic.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:02:43,305'); seek(163.0)">
              so what do you mean by that?
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:02:44,775'); seek(164.0)">
              so if you look at an LLM under the hood.
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:02:47,215'); seek(167.0)">
              they're essentially statistical machines, that produce token after token based
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:02:53,165'); seek(173.0)">
              on, the input prompt and whatever tokens have been generated previously.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:02:57,675'); seek(177.0)">
              statistical machines are basically probabilistic and as soon as you bring
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:02,865'); seek(182.0)">
              probability into software, you are going to get something non-deterministic.
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:06,675'); seek(186.0)">
              Now what do we mean by non-deterministic?
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:09,895'); seek(189.0)">
              You basically, will get a different output for the same input every time.
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:14,465'); seek(194.0)">
              you, ask LLM for a response, you could, I, and I'm pretty sure like you are,
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:20,895'); seek(200.0)">
              you would have seen this problem while using, all the different, chat bots
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:24,495'); seek(204.0)">
              that are available, like chat, GPT or.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:27,190'); seek(207.0)">
              the Deep Seeq or Claude chat, you would've noticed that, every time you give,
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:31,830'); seek(211.0)">
              give, give an input, for the same input.
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:34,130'); seek(214.0)">
              Every time you hit retry, you'll get a different output.
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:36,870'); seek(216.0)">
              that's the same thing that will happen with, the LLM
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:03:39,050'); seek(219.0)">
              responses in your applications.
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:03:41,160'); seek(221.0)">
              you most of the time don't, have a lot of control or, will you
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:03:45,034'); seek(225.0)">
              get the exact output or not?
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:03:46,774'); seek(226.0)">
              Now because of this particular problem, which is, a being non-deterministic, every
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:03:53,850'); seek(233.0)">
              time you give it an input, you'll not always get the response that you want.
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:03:58,140'); seek(238.0)">
              for example, if you ask an L-L-M-A-P-I to generate, JSON, which is a
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:03,809'); seek(243.0)">
              structured output, You might get more fields than, what you asked for.
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:08,539'); seek(248.0)">
              Sometimes you might get less fields.
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:10,369'); seek(250.0)">
              sometimes you might have, a bracket missing based on what we have seen.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:15,159'); seek(255.0)">
              if you have a normal stable API, you'll see an error rate of something like 0.1%.
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:23,169'); seek(263.0)">
              But if you are working with an LLM, even the most stable, the LLMs, which have
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:27,979'); seek(267.0)">
              been, here for the longest amount of time, they'll give you an error rate of,
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:31,889'); seek(271.0)">
              something like one to 5% based on what kind of task you're asking it to perform.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:36,179'); seek(276.0)">
              And if you are working with chain LLM responses, basically you,
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:40,529'); seek(280.0)">
              provide, the L-L-M-A-P-I with.
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:42,820'); seek(282.0)">
              A prompt.
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:04:43,540'); seek(283.0)">
              You take that response and then you provide it with another prompt, using
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:04:47,890'); seek(287.0)">
              the response, that you got earlier.
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:04:50,100'); seek(290.0)">
              this is basically a chained, LLM responses.
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:04:53,680'); seek(293.0)">
              you'll see that your error rate gets compounded and, this particular thing,
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:04:58,520'); seek(298.0)">
              will probably not have a solution.
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:01,450'); seek(301.0)">
              In the LLMs because of LLMs, like I mentioned, are
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:05,170'); seek(305.0)">
              inherently non-deterministic.
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:06,560'); seek(306.0)">
              that is how the architecture is.
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:09,460'); seek(309.0)">
              So this is something that needs to be solved in your application.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:14,250'); seek(314.0)">
              you can't really wait for like LLMs to get better and, start
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:18,840'); seek(318.0)">
              providing better responses.
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:19,910'); seek(319.0)">
              they will definitely, get better and, Reduce the error rate, but, I think
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:23,755'); seek(323.0)">
              as an application developer, it's your responsibility to take care of this
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:27,595'); seek(327.0)">
              issue within your application as well.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:29,965'); seek(329.0)">
              So what are your options?
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:32,545'); seek(332.0)">
              The first thing that you should definitely try out is, retries and timeouts.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:05:37,565'); seek(337.0)">
              Now, these are not new concepts.
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:05:40,535'); seek(340.0)">
              if you have worked in software development for a while now,
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:05:43,235'); seek(343.0)">
              you would know what a retry is.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:05:44,465'); seek(344.0)">
              Basically.
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:05:45,235'); seek(345.0)">
              when an API.
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:05:46,365'); seek(346.0)">
              Gives you a wrong response.
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:05:47,594'); seek(347.0)">
              You try it again with some, cool down period or, maybe not depending
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:05:52,584'); seek(352.0)">
              on, how the rate limits are.
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:05:54,374'); seek(354.0)">
              retry is basically, you make an API call the API fails.
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:05:58,574'); seek(358.0)">
              you wait for a while and then you retry it again.
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:01,684'); seek(361.0)">
              as simple as that.
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:03,034'); seek(363.0)">
              Now, when you.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:05,424'); seek(365.0)">
              Are developing, general applications, I think retries and timeouts are
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:09,674'); seek(369.0)">
              something that, are not the first thing that you would implement.
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:13,839'); seek(373.0)">
              Because, you just assume, you just go with the assumption that the API response
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:18,549'); seek(378.0)">
              rate is going to be fairly reasonable.
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:06:20,419'); seek(380.0)">
              they'll, most of the time work and, like not adding retries and timeouts
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:06:25,199'); seek(385.0)">
              to your APIs will, not really degrade the application performance.
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:06:29,139'); seek(389.0)">
              unless like you are working with very critical, applications
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:06:32,679'); seek(392.0)">
              like, something in finance.
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:06:34,685'); seek(394.0)">
              Or health where, the operation has to finish, in which case you'll, definitely
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:06:38,825'); seek(398.0)">
              start with retries and timeouts.
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:06:40,175'); seek(400.0)">
              But, in our general experience, if you're working with normal APIs, you
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:06:43,435'); seek(403.0)">
              don't really worry about these things.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:06:45,385'); seek(405.0)">
              but because LLM APIs specifically have a higher error rate, retries
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:06:50,755'); seek(410.0)">
              and timeouts are something that, Need to be implemented from day one.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:06:55,475'); seek(415.0)">
              timeouts again, I think, I don't need to get into this.
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:06:57,455'); seek(417.0)">
              A timeout is basically, you make an API call and you wait for X
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:03,365'); seek(423.0)">
              seconds for the API to return.
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:05,225'); seek(425.0)">
              If it doesn't return an X seconds for whatever reason.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:07,795'); seek(427.0)">
              you terminate that, particular a p and you try again.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:11,220'); seek(431.0)">
              this basically is protection against, the server, the API server being down.
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:07:17,200'); seek(437.0)">
              And, so if you don't do this, and if the API takes a minute to respond, you, your
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:07:22,490'); seek(442.0)">
              application is also stuck for a minute.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:07:24,230'); seek(444.0)">
              And, so are your users.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:07:25,860'); seek(445.0)">
              So a timeout is basically protection so that if an a p doesn't return in
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:07:31,860'); seek(451.0)">
              like a reasonable amount of time, you cancel that API call and you.
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:07:35,405'); seek(455.0)">
              retry that API again, that's where timeout comes into picture.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:07:38,465'); seek(458.0)">
              cool.
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:07:38,675'); seek(458.0)">
              So how do you implement this into your application?
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:07:41,775'); seek(461.0)">
              I would suggest don't, write the word for retries and timeouts from
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:07:45,165'); seek(465.0)">
              scratch because there are a bunch of, battle tested libraries available
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:07:49,155'); seek(469.0)">
              in every language that you can use.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:07:52,155'); seek(472.0)">
              And, with a few lines of code add these.
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:07:54,715'); seek(474.0)">
              behaviors to your application.
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:07:56,365'); seek(476.0)">
              So let's look at a few examples.
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:07:59,475'); seek(479.0)">
              the one that we actually use in production is, this one called
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:02,805'); seek(482.0)">
              Tensity by, it's a Python library, and, it allows you to add retry to
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:08:08,575'); seek(488.0)">
              your functions by simply doing this.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:08:10,705'); seek(490.0)">
              you add a decorator.
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:08:12,135'); seek(492.0)">
              Which is provided by the, by this particular library.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:08:14,395'); seek(494.0)">
              you add it to a function and this function will be retried.
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:08:18,085'); seek(498.0)">
              whenever there is an, exception or error in this particular function.
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:08:22,375'); seek(502.0)">
              Now, you'd ideally want more control over, how many d tries to do, how,
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:08:28,925'); seek(508.0)">
              like how long to wait after, every try and those kind of things.
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:08:32,805'); seek(512.0)">
              those.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:08:33,975'); seek(513.0)">
              All options are present in this library.
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:08:35,805'); seek(515.0)">
              You can, give it stopping conditions where you want to stop after three retries.
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:08:40,045'); seek(520.0)">
              You want to stop after, 10 seconds of retrying.
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:08:42,865'); seek(522.0)">
              you can add a wait time before every retry.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:08:46,385'); seek(526.0)">
              you can add a fixed wait time.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:08:47,495'); seek(527.0)">
              You can add a random wait time.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:08:49,385'); seek(529.0)">
              all these, different kinds of, behaviors can.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:08:52,610'); seek(532.0)">
              Be added using this library, with a few lines of course.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:08:55,200'); seek(535.0)">
              if you are working in Python, this is our choice, has been working,
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:08:59,350'); seek(539.0)">
              very well for us in production.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:09:01,200'); seek(541.0)">
              this is what we, have been using for a very long time.
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:09:03,210'); seek(543.0)">
              So I would recommend this, if you're working in js, there is a similar library
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:09:07,630'); seek(547.0)">
              called Retract, very conveniently.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:09:09,725'); seek(549.0)">
              that you can, that is available on NPM.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:09:11,905'); seek(551.0)">
              similar type of functionalities.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:09:13,525'); seek(553.0)">
              It gives you, retries and timeouts.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:09:16,205'); seek(556.0)">
              Oh, by the way, tenacity also has, timeout related decorators.
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:09:19,965'); seek(559.0)">
              works the same way.
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:09:20,675'); seek(560.0)">
              if you want to add a timeout to particular function, you just add
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:09:23,795'); seek(563.0)">
              that decorator, specify the timeout.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:09:25,935'); seek(565.0)">
              yeah.
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:09:26,645'); seek(566.0)">
              This library, if you are working with a JS application, retry is, our choice.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:09:32,135'); seek(572.0)">
              the third option is, basically, a lot of people who are, developing
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:09:37,165'); seek(577.0)">
              LLM applications are using these frameworks, LLM frameworks to
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:09:40,665'); seek(580.0)">
              handle, API calls retries and like a bunch of different things.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:09:44,860'); seek(584.0)">
              So the most famous LLM frameworks, framework, which a lot of
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:09:48,460'); seek(588.0)">
              people are using is land chain.
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:09:50,530'); seek(590.0)">
              And if you are working on top of land chain, land chain provides
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:09:54,580'); seek(594.0)">
              you a, basically a, some, mechanism to retry, out of the box.
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:10:00,325'); seek(600.0)">
              it's called.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:10:01,785'); seek(601.0)">
              the retry output parser, where, you can use this to make the LLM calls
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:10:06,085'); seek(606.0)">
              and whenever the LLM call fails, this parser will basically, handle retry
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:10:11,355'); seek(611.0)">
              on your behalf, by passing, The prompt again, and, also the previous output,
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:10:15,695'); seek(615.0)">
              so that the, L-L-M-A-P has a better idea that, okay, the last output failed.
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:10:19,339'); seek(619.0)">
              And, I'm not supposed to, give this response again.
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:10:22,149'); seek(622.0)">
              So if you're on, then it's already sorted out for you.
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:10:24,839'); seek(624.0)">
              You use the retry output parcel.
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:10:26,494'); seek(626.0)">
              Alright, so this sorts out how to implement retries
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:10:30,099'); seek(630.0)">
              and timeouts the next most.
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:10:33,059'); seek(633.0)">
              Common reason for, failure or LLM inconsistency is when you are
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:10:37,429'); seek(637.0)">
              working with structured outputs.
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:10:39,319'); seek(639.0)">
              So when I say structured output, something like you asked the LLM to
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:10:42,959'); seek(642.0)">
              generate A-J-S-O-N or X-M-L-C-S-V, even list ra, those kind of things.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:10:47,819'); seek(647.0)">
              whenever you are asking an LLM to generate a structured output, there
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:10:51,849'); seek(651.0)">
              is a slight chance that, there'll be something wrong with that structure.
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:10:55,064'); seek(655.0)">
              Maybe there are some fields missing.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:10:56,514'); seek(656.0)">
              there are extra fields.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:10:57,954'); seek(657.0)">
              in case of JSONs, XMLs, there are brackets missing, might happen.
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:11:01,869'); seek(661.0)">
              So how do you handle that?
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:11:04,639'); seek(664.0)">
              the simplest way to do that is to, is to integrate a schema library instead
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:11:10,599'); seek(670.0)">
              of doing it on your own every time.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:11:12,339'); seek(672.0)">
              a schema library could be something like pedantic and, this is what.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:11:16,519'); seek(676.0)">
              We use, in our production.
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:11:19,359'); seek(679.0)">
              PTECH is basically, the most commonly used data validation library in Python.
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:11:24,549'); seek(684.0)">
              And what it does is it allows you to, create classes, in which you describe
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:11:31,209'); seek(691.0)">
              the structure of your response, and then you use this particular class to,
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:11:37,339'); seek(697.0)">
              check whether the LRM response fits.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:11:40,039'); seek(700.0)">
              This particular structure or not.
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:11:41,849'); seek(701.0)">
              it'll check for, fields, extra fields, or less fields.
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:11:46,549'); seek(706.0)">
              It'll check for data types, and a bunch of other options.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:11:49,859'); seek(709.0)">
              on Python, just go for Edan Tech.
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:11:51,819'); seek(711.0)">
              it is a tried and tested library, and, it'll make the data validation
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:11:56,859'); seek(716.0)">
              part when you're working with structured outputs, hassle free.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:12:00,199'); seek(720.0)">
              similarly, if you are working with NPM, there's something called Yap, same
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:12:03,779'); seek(723.0)">
              stuff as pedantic, data validation.
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:12:06,469'); seek(726.0)">
              you essentially, define the shape of your output and, yap basically uses that shape,
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:12:12,499'); seek(732.0)">
              which is essentially a class, JS class, or a JS object to, Check or enforce,
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:12:18,519'); seek(738.0)">
              the structure of your LLM responses.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:12:20,619'); seek(740.0)">
              and the idea is to use, these, these, data validation libraries,
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:12:25,019'); seek(745.0)">
              along with, retries and timeouts.
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:12:26,789'); seek(746.0)">
              what you basically do is when you make an L-L-M-E-P-I call, and you
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:12:29,929'); seek(749.0)">
              get a response, you pass it through pedantic or, whatever data validation.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:12:34,839'); seek(754.0)">
              Letter you are using, and if you get an error, you use the retry, to
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:12:39,214'); seek(759.0)">
              like basically let the LLM generate that structured output again, most
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:12:43,144'); seek(763.0)">
              of the time, you will see that, a couple of retries sorts it out.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:12:48,054'); seek(768.0)">
              it's not like every API call will feel in the same way.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:12:51,294'); seek(771.0)">
              So if, let's say there are a few things missing.
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:12:54,404'); seek(774.0)">
              In your structured output the first time when you do a retry, the next
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:12:57,784'); seek(777.0)">
              time you'll get the correct output.
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:12:59,734'); seek(779.0)">
              but just as a general advice, if you see that there are particular
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:13:05,774'); seek(785.0)">
              kind of issues happening again and again, you should mention that,
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:13:09,694'); seek(789.0)">
              instruction in the prompt itself.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:13:11,494'); seek(791.0)">
              Because what happens is that, when you do retry, an API call, which
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:13:16,334'); seek(796.0)">
              was supposed to take five seconds, might end up taking 15 to 20 seconds.
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:13:19,874'); seek(799.0)">
              And, it'll make your, make your application feel laggy.
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:13:23,844'); seek(803.0)">
              because, at the end of that API call, you're going to provide
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:13:26,924'); seek(806.0)">
              some output to your users and, they're waiting for that output.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:13:30,054'); seek(810.0)">
              so if, that there are particular kind of, problems that are happening again
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:13:34,229'); seek(814.0)">
              and again, like for example, If, if you are, generating JSON, using, JSON,
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:13:39,889'); seek(819.0)">
              using an L-L-M-A-P-I, and you'll see that, like the LLM is always using
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:13:43,499'); seek(823.0)">
              single code sensor or double code, which will generally cause issues, you should
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:13:47,319'); seek(827.0)">
              specify that as an important point in your prompt so that, you get the correct
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:13:51,369'); seek(831.0)">
              output in the first, attempt itself.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:13:54,129'); seek(834.0)">
              this is just an additional level of check, but the idea is that the first response
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:13:59,079'); seek(839.0)">
              should itself give you the correct output.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:14:01,159'); seek(841.0)">
              anything that is, that is known, should be mentioned in the prompt
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:14:04,449'); seek(844.0)">
              as a special instruction, so that you don't keep retrying and you
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:14:07,669'); seek(847.0)">
              use this an waiting for an output.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:14:09,629'); seek(849.0)">
              one, one additional option worth, one special mention here is, The structured
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:14:15,119'); seek(855.0)">
              output capabilities provided by open ai.
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:14:17,829'); seek(857.0)">
              if you're using, GPT models, and open AI APIs, what you can do is
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:14:23,594'); seek(863.0)">
              there is a response format field where you can specify a class and,
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:14:27,794'); seek(867.0)">
              the open air APIs themselves will, try to enforce the structure.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:14:33,134'); seek(873.0)">
              but this one problem here, which is if you want to switch out.
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:14:37,184'); seek(877.0)">
              The model and, use something else like Claude Orrock.
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:14:40,364'); seek(880.0)">
              then you have basically, lost the structured output capabilities
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:14:44,654'); seek(884.0)">
              because those are not, available right now in, other LLM APIs.
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:14:48,854'); seek(888.0)">
              my suggestion is to just handle the, schema enforcing and checking in
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:14:52,724'); seek(892.0)">
              your application itself so that like it's easy for you to switch out,
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:14:56,004'); seek(896.0)">
              the models and use different models.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:14:57,794'); seek(897.0)">
              That's all for, handling LLM inconsistencies.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:15:01,114'); seek(901.0)">
              two main things, retries and timeouts.
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:15:02,954'); seek(902.0)">
              use them from the start.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:15:05,234'); seek(905.0)">
              if you are working with structured outputs, use a data validation
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:15:08,874'); seek(908.0)">
              library to, check the structure.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:15:11,424'); seek(911.0)">
              you see the options here.
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:15:12,484'); seek(912.0)">
              any of these are good.
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:15:13,914'); seek(913.0)">
              The next thing that you should, start thinking about is how to implement
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:15:17,974'); seek(917.0)">
              streaming in your LLM application.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:15:20,854'); seek(920.0)">
              generally when you develop APIs, you, you implement, you
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:15:24,894'); seek(924.0)">
              implement normal request response.
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:15:26,274'); seek(926.0)">
              you get an a p call and, the server does some work, and then you, then you
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:15:31,194'); seek(931.0)">
              return the entire response in one go.
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:15:33,564'); seek(933.0)">
              in.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:15:34,524'); seek(934.0)">
              In case of, LLMs, what happens is sometimes it might take a long time
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:15:39,414'); seek(939.0)">
              for the l LM to generate a response.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:15:41,964'); seek(941.0)">
              That's where streaming comes into picture.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:15:43,524'); seek(943.0)">
              Streaming of your responses allow you to start returning partial responses,
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:15:49,324'); seek(949.0)">
              to the client, even when, the LLM is not done, done with the generation.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:15:54,764'); seek(954.0)">
              let's look at why.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:15:55,974'); seek(955.0)">
              Streaming is so important.
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:15:58,094'); seek(958.0)">
              while building LLM applications, like I mentioned, LLMs might take long
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:16:03,544'); seek(963.0)">
              time for, for generation, to complete.
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:16:06,699'); seek(966.0)">
              Now, when your user is, using your application, most
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:16:10,289'); seek(970.0)">
              users are very impatient.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:16:11,369'); seek(971.0)">
              you.
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:16:12,369'); seek(972.0)">
              can't ask them to wait for, seconds.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:16:15,349'); seek(975.0)">
              like I'm not even talking about minutes.
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:16:17,159'); seek(977.0)">
              if you have a ten second delay in showing the response, you
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:16:20,279'); seek(980.0)">
              might see a lot of drop off.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:16:21,869'); seek(981.0)">
              Um, and like most of the LMS that you would work with would take five to 10
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:16:25,929'); seek(985.0)">
              seconds for even the simplest, prompts.
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:16:30,029'); seek(990.0)">
              So how do you improve the ux?
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:16:33,984'); seek(993.0)">
              and make sure that your users don't drop off.
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:16:36,694'); seek(996.0)">
              that's where streaming comes into picture.
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:16:38,714'); seek(998.0)">
              what streaming allows you to do is, LLMs generate.
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:16:42,134'); seek(1002.0)">
              Response is token by token.
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:16:43,654'); seek(1003.0)">
              they'll generate it word by word.
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:16:45,304'); seek(1005.0)">
              And, what streaming allows you to do is, you don't need to wait for the LM to
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:16:48,934'); seek(1008.0)">
              generate the entire response or, output.
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:16:51,664'); seek(1011.0)">
              What you can do is as soon as it is done generating a few words, you can send
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:16:55,174'); seek(1015.0)">
              them to the client and, start displaying them on the UI or, what your client
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:16:59,064'); seek(1019.0)">
              you're using, in this way, the user.
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:17:03,059'); seek(1023.0)">
              Doesn't really feel the lag, that, LM generation results in.
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:17:07,879'); seek(1027.0)">
              what they see is that, as soon as they type out a prompt, immediately
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:17:11,409'); seek(1031.0)">
              they start seeing some response and they can start reading it out.
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:17:14,749'); seek(1034.0)">
              you, this is a very common pattern in any chat or LLM
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:17:19,699'); seek(1039.0)">
              application that you would've used.
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:17:21,559'); seek(1041.0)">
              As soon as you type something out or you do an action, you start seeing
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:17:25,009'); seek(1045.0)">
              partial results on your UI that is implemented through streaming.
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:17:28,619'); seek(1048.0)">
              the most common or the most, used way, to implement streaming is web sockets.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:17:35,529'); seek(1055.0)">
              web sockets allow you to send, generated tokens or words in real time.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:17:40,999'); seek(1060.0)">
              The connection is established, between client and the server, and then, until
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:17:45,939'); seek(1065.0)">
              the entire generation is completed, or, as long as the user is, live on
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:17:51,369'); seek(1071.0)">
              the ui, you can just like reuse that connection to keep sending response,
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:17:56,389'); seek(1076.0)">
              as and when it gets, generated.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:17:58,739'); seek(1078.0)">
              this is also a bidirectional, um.
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:18:02,244'); seek(1082.0)">
              communication method, so you can use the same method to get
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:18:05,994'); seek(1085.0)">
              some input from the client.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:18:08,544'); seek(1088.0)">
              Also, no One drawback of, web sockets is that they need
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:18:12,724'); seek(1092.0)">
              some custom, implementation.
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:18:14,794'); seek(1094.0)">
              you can't just take like your simple HDP rest server and,
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:18:19,424'); seek(1099.0)">
              convert it into web socket.
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:18:20,564'); seek(1100.0)">
              You'll need to redo your implementation.
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:18:23,724'); seek(1103.0)">
              use new libraries.
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:18:25,294'); seek(1105.0)">
              probably even new use a new language.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:18:27,124'); seek(1107.0)">
              for example, if you are working on Python, Python is not, very, efficient,
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:18:32,704'); seek(1112.0)">
              way for implementing web sockets.
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:18:34,204'); seek(1114.0)">
              You probably want to move to a different language which handles, threads or
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:18:39,634'); seek(1119.0)">
              multi-processing in a much better way than Python, like Golan or Java, or c plus.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:18:45,154'); seek(1125.0)">
              so generally web socket implementation.
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:18:48,454'); seek(1128.0)">
              Is a considerable effort.
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:18:50,194'); seek(1130.0)">
              And, if all you want to do is stream LLM responses, it probably
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:18:54,194'); seek(1134.0)">
              is not the best way to do it.
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:18:56,114'); seek(1136.0)">
              there is another, solution for streaming, over SGDP, which is
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:19:01,794'); seek(1141.0)">
              called Server Set Events, which basically uses, your, your, um.
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:19:09,164'); seek(1149.0)">
              as server itself, like basically if you are on Python and you're using
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:19:12,374'); seek(1152.0)">
              Flask or Fast API, you won't need to do a lot of changes to start
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:19:17,224'); seek(1157.0)">
              streaming, using server sentiments, code-wise or implementation-wise.
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:19:21,194'); seek(1161.0)">
              this is a minimal effort.
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:19:22,604'); seek(1162.0)">
              what this essentially does is, It'll use the same FTTP, connection,
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:19:28,484'); seek(1168.0)">
              which your STPI call utilizes.
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:19:31,494'); seek(1171.0)">
              but instead of, sending the entire response in one shot, you can
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:19:36,234'); seek(1176.0)">
              send the response in chunks and, on your client side, you can,
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:19:40,384'); seek(1180.0)">
              receive it in and start displaying.
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:19:42,504'); seek(1182.0)">
              now this is a unidirectional, flow.
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:19:45,764'); seek(1185.0)">
              it works exactly as a rest API call.
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:19:48,159'); seek(1188.0)">
              but instead of, the client waiting for the entire response, to come, the client
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:19:54,009'); seek(1194.0)">
              starts showing chunks that have been sent from server, using server sentiments.
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:20:00,309'); seek(1200.0)">
              implementation wise, it's very simple.
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:20:02,369'); seek(1202.0)">
              Like you, just need to maybe implement the generator, if you're using Python
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:20:07,459'); seek(1207.0)">
              and, Maybe add a couple of headers.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:20:10,299'); seek(1210.0)">
              we won't get into specific details because these are, things that you
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:20:13,059'); seek(1213.0)">
              can easily Google and, find out.
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:20:14,979'); seek(1214.0)">
              But, our recommendation if you want to implement streaming in your
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:20:18,809'); seek(1218.0)">
              application and you already have a rest, set up ready on the backend.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:20:22,599'); seek(1222.0)">
              just go for server set events, much, faster implementation,
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:20:26,069'); seek(1226.0)">
              also much easier to implement.
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:20:28,389'); seek(1228.0)">
              web sockets is a bit heavy.
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:20:30,399'); seek(1230.0)">
              And unless you have a specific use case for, web sockets, I won't
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:20:35,389'); seek(1235.0)">
              recommend, going that, on that path
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:20:38,394'); seek(1238.0)">
              streaming is a good solution.
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:20:39,954'); seek(1239.0)">
              If, the particular task that an LLM is handling, gets over in a few
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:20:45,034'); seek(1245.0)">
              seconds, like five to 10 seconds.
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:20:46,934'); seek(1246.0)">
              but if your task.
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:20:48,179'); seek(1248.0)">
              It is going to take minutes.
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:20:50,369'); seek(1250.0)">
              streaming probably is not a good option.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:20:53,229'); seek(1253.0)">
              that's where background jobs come into picture.
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:20:56,229'); seek(1256.0)">
              if you have a task which can be done in five to 10 seconds, probably use streaming
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:21:00,439'); seek(1260.0)">
              and, it's a good way to start showing, an output, to the user on client side.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:21:05,709'); seek(1265.0)">
              but if you have a task which is going to take minutes.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:21:08,929'); seek(1268.0)">
              it is better to handle it asynchronously instead of
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:21:11,809'); seek(1271.0)">
              synchronously in your, backend server.
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:21:14,029'); seek(1274.0)">
              and background jobs help you do that.
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:21:15,969'); seek(1275.0)">
              So what are these particular use cases where you, might want to use
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:21:22,299'); seek(1282.0)">
              background jobs instead of swimming?
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:21:24,159'); seek(1284.0)">
              think of it this way.
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:21:25,359'); seek(1285.0)">
              Let's say if you, if you are building, something like.
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:21:30,009'); seek(1290.0)">
              An essay generator, and you allow the user to, enter essay topics in bulk.
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:21:35,979'); seek(1295.0)">
              So if someone, gives you a single essay topic, probably, you'll finish
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:21:39,949'); seek(1299.0)">
              the generation in a few seconds.
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:21:41,329'); seek(1301.0)">
              And, streaming is the way to go.
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:21:43,319'); seek(1303.0)">
              But let's say if someone, Gives you a hundred essay topics, for generation.
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:21:48,669'); seek(1308.0)">
              I know that this particular task, doesn't matter how fast the LLM is going to
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:21:52,269'); seek(1312.0)">
              take minutes at least a few minutes.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:21:54,559'); seek(1314.0)">
              And, if you use streaming for this, streaming, will do all the work
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:21:59,059'); seek(1319.0)">
              in your backend server, and, until this particular task is completed,
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:22:03,459'); seek(1323.0)">
              which is going to be few minutes.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:22:05,259'); seek(1325.0)">
              your backend server resources are going to get, hogged or are going to be, tied
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:22:11,819'); seek(1331.0)">
              up in this particular task, which is very inefficient because, like your
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:22:16,199'); seek(1336.0)">
              backend server's job is basically take a request, process in a few seconds
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:22:20,409'); seek(1340.0)">
              and, send it back to the client.
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:22:22,699'); seek(1342.0)">
              if you start doing things which take minutes.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:22:25,189'); seek(1345.0)">
              You will see that, if you have a lot of concurrent users, you, your backend
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:22:30,249'); seek(1350.0)">
              server will be busy and it'll not be able to, handle tasks, which take a few
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:22:35,329'); seek(1355.0)">
              seconds and, your APIs will start getting blocked and, your, your, application
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:22:40,239'); seek(1360.0)">
              performance will start to degrade.
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:22:41,949'); seek(1361.0)">
              So what's the solution here?
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:22:44,699'); seek(1364.0)">
              You, the solution is, you don't handle.
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:22:48,284'); seek(1368.0)">
              long running tasks in backend server synchronously.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:22:52,334'); seek(1372.0)">
              You handle them in background jobs asynchronously.
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:22:55,454'); seek(1375.0)">
              Basically.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:22:56,204'); seek(1376.0)">
              when a user gives you a task, which is going to take minutes, you log
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:22:59,194'); seek(1379.0)">
              it in a database, a background job will pick that task up till then, you
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:23:03,984'); seek(1383.0)">
              tell the, you basically communicate to the user that, okay, this is
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:23:06,654'); seek(1386.0)">
              going to take a few minutes, once.
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:23:09,139'); seek(1389.0)">
              The task is completed, you'll get a notification, probably
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:23:12,719'); seek(1392.0)">
              as an email, or on Slack.
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:23:15,129'); seek(1395.0)">
              And, what you do is you use a background job to, pick up the task, process it, and
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:23:20,549'); seek(1400.0)">
              once it's ready, send out a notification.
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:23:22,989'); seek(1402.0)">
              easiest way to implement this is CR Jobs.
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:23:25,829'); seek(1405.0)">
              crown Jobs have been here for, I don't know for a very long time.
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:23:29,799'); seek(1409.0)">
              very easy to implement, on any Unix-based, server, which is
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:23:35,459'); seek(1415.0)">
              probably, what will be used in most of, production backing servers.
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:23:39,189'); seek(1419.0)">
              all you need to do is set up a CR job, which does the processing.
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:23:43,519'); seek(1423.0)">
              and the CR job runs every few minutes, checks the database
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:23:46,939'); seek(1426.0)">
              if there are in pending tasks.
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:23:48,599'); seek(1428.0)">
              now when your user, comes to you.
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:23:52,209'); seek(1432.0)">
              With, with a task, you just put it in a DB and, mark it as pending.
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:23:57,749'); seek(1437.0)">
              when the crown job wakes up in a few minutes, it will check for any
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:24:00,749'); seek(1440.0)">
              pending task and start the processing.
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:24:03,269'); seek(1443.0)">
              And, on the US side you can probably, implement some sort of polling.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:24:07,604'); seek(1447.0)">
              To check if the task is completed or not.
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:24:09,344'); seek(1449.0)">
              And once it is completed, you can display that on the ui.
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:24:12,554'); seek(1452.0)">
              But, this is an optional thing.
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:24:14,004'); seek(1454.0)">
              Ideally, if you're using background jobs, you should also, sorry.
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:24:18,524'); seek(1458.0)">
              You should also, Separately communicate, that the task is completed with the user
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:24:23,694'); seek(1463.0)">
              because, the general, idea is that, when you, when a task is going to take a few
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:24:27,834'); seek(1467.0)">
              minutes, your users will probably come to your platform, submit the task, and
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:24:32,564'); seek(1472.0)">
              they will, move away from your platform.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:24:34,254'); seek(1474.0)">
              So they're not looking at.
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:24:35,884'); seek(1475.0)">
              the UI of your application.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:24:37,444'); seek(1477.0)">
              So you should probably communicate that the task is completed through
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:24:41,584'); seek(1481.0)">
              an email or a Slack notification.
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:24:43,964'); seek(1483.0)">
              so the users who have moved away from, the UI also know that okay, that,
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:24:47,744'); seek(1487.0)">
              that generation has been completed.
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:24:51,029'); seek(1491.0)">
              this works very well, minimal setup.
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:24:53,269'); seek(1493.0)">
              Nothing new that you probably need to learn, nothing new
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:24:55,789'); seek(1495.0)">
              that you need to install.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:24:57,269'); seek(1497.0)">
              for the initial stages of your LLM application, just go for a crown job.
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:25:02,099'); seek(1502.0)">
              what happens is that as your, application grows, you'll probably need to scale
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:25:08,779'); seek(1508.0)">
              this now, if you run multiple crown jobs, you need to handle which crown job.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:25:13,854'); seek(1513.0)">
              pick up which task you need to implement some sort of, distributed locking and,
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:25:19,034'); seek(1519.0)">
              all those complexities come into picture.
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:25:20,924'); seek(1520.0)">
              Basically, crown jobs are good for the initial stages, but, like
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:25:26,084'); seek(1526.0)">
              we also started with crown jobs.
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:25:27,784'); seek(1527.0)">
              we still use crown jobs for some simple tasks, but there will be a
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:25:31,504'); seek(1531.0)">
              stage, When you'll need to move away from crown jobs for scalability, and
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:25:36,204'); seek(1536.0)">
              for, better retrain mechanisms, that's where task queue come into picture.
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:25:41,654'); seek(1541.0)">
              So basically think of task queue as crown jobs with like more intelligence, where
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:25:47,454'); seek(1547.0)">
              all the, task management that needs to be done, is handled by the task queue itself.
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:25:53,204'); seek(1553.0)">
              when I say task management, on a very high level, what.
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:25:56,344'); seek(1556.0)">
              It means is that, you submit a task to the task queue.
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:25:59,504'); seek(1559.0)">
              generally a task queue is backed by some storage, like Redis or some other cache.
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:26:04,914'); seek(1564.0)">
              the task is stored over there, and then the task queue handles, basically a
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:26:09,634'); seek(1569.0)">
              task queue will have a bunch of workers running and, a ta the task queue will
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:26:13,884'); seek(1573.0)">
              then handle, how to allocate that work.
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:26:17,264'); seek(1577.0)">
              To which worker based on like a bunch of different mechanisms.
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:26:20,114'); seek(1580.0)">
              Like you can have, priority queues, you can have a bunch of different
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:26:24,204'); seek(1584.0)">
              retry mechanisms, and all those things.
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:26:26,954'); seek(1586.0)">
              two good things about using task queue.
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:26:29,184'); seek(1589.0)">
              task queues are much easier to scale.
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:26:31,004'); seek(1591.0)">
              in Aron job, if you go from one to two to 10 crown jobs, you have to handle a
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:26:35,204'); seek(1595.0)">
              bunch of, Locking related stuff yourself.
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:26:37,954'); seek(1597.0)">
              in task queue, it's already, implemented for you.
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:26:40,724'); seek(1600.0)">
              So all you can do is increase the number of workers in a task queue.
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:26:44,504'); seek(1604.0)">
              And, if you start getting more, tasks or workload, the, you can just it's as easy
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:26:50,954'); seek(1610.0)">
              as just changing a number on a dashboard, to increase the number of workers.
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:26:54,744'); seek(1614.0)">
              again, like all the, additional handling.
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:26:58,644'); seek(1618.0)">
              For race conditions, retries, timeouts, it's already taken care of.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:27:01,614'); seek(1621.0)">
              All you need to do is, provide some configuration.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:27:04,574'); seek(1624.0)">
              you also get better monitoring with task use.
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:27:06,744'); seek(1626.0)">
              you, every task you comes with some sort of, monitoring mechanism
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:27:11,894'); seek(1631.0)">
              or, dashboard where you can see what are the task currently
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:27:15,434'); seek(1635.0)">
              running, how much resources there.
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:27:17,414'); seek(1637.0)">
              Eating up, which tasks are failing, start or restart tasks
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:27:21,344'); seek(1641.0)">
              and all those kind of things.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:27:22,874'); seek(1642.0)">
              once you start scaling your application, go for task use.
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:27:26,384'); seek(1646.0)">
              The task queue that we use in our production is called rq, which stands
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:27:32,544'); seek(1652.0)">
              for Redis Q. And, as the name suggest, it's backed by Redis, and it's a
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:27:37,759'); seek(1657.0)">
              very simple, library for Qing and processing background jobs with workers.
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:27:43,959'); seek(1663.0)">
              very easy setup.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:27:45,039'); seek(1665.0)">
              Hardly takes 15 minutes to set it up.
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:27:47,169'); seek(1667.0)">
              If you already have a Redis, you don't even need to, set, set up
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:27:50,869'); seek(1670.0)">
              a red, for RQ and, very simple.
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:27:56,329'); seek(1676.0)">
              Uh.
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:27:57,699'); seek(1677.0)">
              mechanism for queuing and processing.
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:28:00,429'); seek(1680.0)">
              All you need to do is create a queue, provide it a red connection so that,
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:28:04,079'); seek(1684.0)">
              it has a place to store the tasks.
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:28:06,519'); seek(1686.0)">
              when you get a task, queue and queue, you can, and, this is basically a
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:28:10,249'); seek(1690.0)">
              function which is going to get called in the worker to process your tasks.
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:28:14,559'); seek(1694.0)">
              it's this simple and you can also provide some arguments for that function and.
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:28:20,279'); seek(1700.0)">
              The worker for the worker, you just need to start it like
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:28:24,119'); seek(1704.0)">
              this, on your command line.
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:28:26,329'); seek(1706.0)">
              And, it consumes tasks from Redis and, process them.
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:28:29,899'); seek(1709.0)">
              If you, want to increase the number of workers, you just start 10
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:28:34,229'); seek(1714.0)">
              different workers, connect them to the same Redis, and, RQ will itself
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:28:38,869'); seek(1718.0)">
              handle all the, all the complexities.
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:28:42,134'); seek(1722.0)">
              Of, managing which worker gets what task, and all those kind of things.
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:28:46,669'); seek(1726.0)">
              if you're on Python, RQ is the way to go.
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:28:50,039'); seek(1730.0)">
              Salary provides you with, similar, functionality.
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:28:54,399'); seek(1734.0)">
              but we just found that, there were a bunch of things in salary,
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:28:56,979'); seek(1736.0)">
              which we did not really need.
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:28:59,279'); seek(1739.0)">
              and it seemed like an overkill.
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:29:01,074'); seek(1741.0)">
              so we decided to go with RQH was much simpler to set up on our end from
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:29:05,324'); seek(1745.0)">
              thread, what inputs are not working, what models are working, what models
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:29:07,934'); seek(1747.0)">
              are not working, and things like that.
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:29:09,994'); seek(1749.0)">
              if you want an analogy, you can think of evals as unit testing.
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:29:14,754'); seek(1754.0)">
              think of it as unit testing for your prompts.
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:29:16,704'); seek(1756.0)">
              So this allows you to take a prompt template.
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:29:20,169'); seek(1760.0)">
              And individually just test out that template with a bunch of different values.
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:29:26,079'); seek(1766.0)">
              and, you can, there are a bunch of, reasons why you should ideally, use
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:29:32,129'); seek(1772.0)">
              evals with your prompt templates.
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:29:34,219'); seek(1774.0)">
              one, it allows you to just test out the prompts in I isolation,
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:29:37,459'); seek(1777.0)">
              which makes it very fast.
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:29:38,469'); seek(1778.0)">
              the same way unit tests are fast, because you are just checking one function
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:29:41,719'); seek(1781.0)">
              against different types of inputs.
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:29:43,579'); seek(1783.0)">
              Using prompts, using, sorry, I'm sorry.
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:29:45,828'); seek(1785.0)">
              using evals, you will be able to figure out different things like which
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:29:51,178'); seek(1791.0)">
              input works, which input doesn't work, which model works for a particular
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:29:55,018'); seek(1795.0)">
              task, which model does not work.
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:29:57,168'); seek(1797.0)">
              you'll be able to compare, costs of different models for different
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:30:00,758'); seek(1800.0)">
              types of inputs and so on.
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:30:03,018'); seek(1803.0)">
              an additional, benefit of using evals is that.
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:30:07,368'); seek(1807.0)">
              You can directly, integrate them with your CICD pipeline so that,
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:30:12,478'); seek(1812.0)">
              you don't need to manually keep checking before every release if your
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:30:15,628'); seek(1815.0)">
              prompts are still working the way, they're working just like unit test.
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:30:19,178'); seek(1819.0)">
              You just, hook it up to your CICD pipeline and, before every commit or I'm sorry,
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:30:23,903'); seek(1823.0)">
              after every commit or, after every build, you, straight up run the evals.
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:30:28,778'); seek(1828.0)">
              And, similar to, assertions in unit tests, evals also have assertions or checks
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:30:34,458'); seek(1834.0)">
              where you can check the response, and, specify whether it is as expected or not
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:30:40,128'); seek(1840.0)">
              as expected, and pass or fail an eval.
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:30:42,858'); seek(1842.0)">
              That's how on a very high level evals work.
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:30:46,478'); seek(1846.0)">
              we have tried out a bunch of different, eval libraries.
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:30:50,208'); seek(1850.0)">
              the one we like the most is Profu.
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:30:52,948'); seek(1852.0)">
              very easy to set up.
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:30:54,668'); seek(1854.0)">
              simply works using YAML files.
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:30:58,968'); seek(1858.0)">
              basically you, you create a YAML file where you specify your prompt template
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:31:03,718'); seek(1863.0)">
              and you press, specify a bunch of inputs, for that prompt template.
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:31:07,863'); seek(1867.0)">
              And, using, profu is an open source, tool.
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:31:11,093'); seek(1871.0)">
              So you can just like, straight up install it from NPM, brand it in your CLI.
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:31:16,493'); seek(1876.0)">
              and at the end of the Evalue you get a nice, graph like this.
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:31:21,748'); seek(1881.0)">
              Which will show you for different types of inputs, whether the
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:31:26,198'); seek(1886.0)">
              output has passed the condition.
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:31:28,488'); seek(1888.0)">
              it'll also allow you to compare different, models and, there is
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:31:33,538'); seek(1893.0)">
              some way to compare cost as well.
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:31:35,428'); seek(1895.0)">
              I don't think they have displayed it here, but yeah.
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:31:37,418'); seek(1897.0)">
              cost comparison is also something that you'll get in the same dashboard
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:31:40,808'); seek(1900.0)">
              and, You can start off with the open source version of Profu, but they
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:31:45,258'); seek(1905.0)">
              also have a cloud hosted version.
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:31:47,028'); seek(1907.0)">
              So if you want more reliability or don't want to manage your own instance,
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:31:51,058'); seek(1911.0)">
              that option is also available.
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:31:53,028'); seek(1913.0)">
              before we end the talk, let's do a quick walkthrough of all the different
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:31:56,898'); seek(1916.0)">
              foundational models, or foundational model APIs that are available for public use.
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:32:02,018'); seek(1922.0)">
              The reason for doing this is basically, this landscape is changing very fast.
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:32:06,878'); seek(1926.0)">
              So the last time you had gone over all the available models, I'm pretty sure
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:32:12,198'); seek(1932.0)">
              that, by now the list of models and also their, comparisons have changed.
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:32:17,698'); seek(1937.0)">
              probably the models you thought, are not that great have
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:32:21,048'); seek(1941.0)">
              become very good, and so on.
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:32:22,828'); seek(1942.0)">
              So let's do a quick run through of all the available models, what are they good at?
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:32:26,968'); seek(1946.0)">
              What are.
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:32:27,763'); seek(1947.0)">
              They're not good at what kind of use cases?
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:32:29,623'); seek(1949.0)">
              Um, you what case, what kind of use cases work with a particular kind of model?
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:32:35,983'); seek(1955.0)">
              let's start with the oldest player OpenAI.
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:32:38,413'); seek(1958.0)">
              OpenAI has, three main families of models, which is GPT 4 0 4 O, and o,
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:32:45,453'); seek(1965.0)">
              which are available for public use.
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:32:47,263'); seek(1967.0)">
              I think they've deprecated their three and 3.5 models.
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:32:50,043'); seek(1970.0)">
              so these are the models that are available right now.
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:32:53,343'); seek(1973.0)">
              If you don't know what to use, just go with open air.
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:32:56,743'); seek(1976.0)">
              these are the most versatile.
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:32:59,533'); seek(1979.0)">
              Models, work with wide, they work very well with wide, wide variety
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:33:03,693'); seek(1983.0)">
              of tasks, within these models.
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:33:06,813'); seek(1986.0)">
              between four oh and, four oh mini, the difference is mainly, the trade off
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:33:11,203'); seek(1991.0)">
              between, Cost and latency versus accuracy.
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:33:14,553'); seek(1994.0)">
              So if you have a complex task or something that requires a bit
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:33:18,813'); seek(1998.0)">
              more of reasoning, go for four.
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:33:21,513'); seek(2001.0)">
              if you are worried about cost or if you're worried about, how fast the response is
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:33:25,383'); seek(2005.0)">
              going to be, go for four oh mini, but, it'll basically, give you lesser accuracy.
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:33:30,948'); seek(2010.0)">
              O is something that I've not tried out.
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:33:32,718'); seek(2012.0)">
              these are supposed to be, open as flagship models.
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:33:36,478'); seek(2016.0)">
              but from.
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:33:37,673'); seek(2017.0)">
              What I've heard, these are like fairly new.
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:33:39,603'); seek(2019.0)">
              before you put it in production, maybe, test them out thoroughly.
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:33:42,983'); seek(2022.0)">
              four O and four O Mini have been around for a while now, so I think, you should
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:33:47,023'); seek(2027.0)">
              not see a lot of problems, with them.
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:33:49,013'); seek(2029.0)">
              Also, like reliability wise, as, according to us, open air APIs
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:33:54,193'); seek(2034.0)">
              have been the most reliable.
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:33:55,728'); seek(2035.0)">
              so you don't need to worry about, downtime or, having to, handle switching models
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:34:02,348'); seek(2042.0)">
              because, this provider is not working.
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:34:04,678'); seek(2044.0)">
              The next provider is, philanthropic.
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:34:08,958'); seek(2048.0)">
              I think for a while, these guys were working mostly on the, chat.
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:34:13,838'); seek(2053.0)">
              the APIs were not publicly available as far as I know.
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:34:18,168'); seek(2058.0)">
              but I think in the last few months, I think that has changed.
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:34:20,618'); seek(2060.0)">
              the APIs are available.
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:34:21,758'); seek(2061.0)">
              You can just directly, and they're completely self serve.
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:34:24,228'); seek(2064.0)">
              You can just directly go, on, anthropics, anthropics console and, create an API key.
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:34:29,418'); seek(2069.0)">
              Load up some credit and get started with it.
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:34:32,648'); seek(2072.0)">
              If you have any coding related use case, Claude APIs are your best choice.
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:34:38,118'); seek(2078.0)">
              I think, as far as coding is concerned, coding as a particular task, Claude, works
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:34:43,988'); seek(2083.0)">
              much better than, all the other models.
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:34:46,688'); seek(2086.0)">
              which is also why you would've seen that.
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:34:48,408'); seek(2088.0)">
              everyone is using Claude with, They're, code editors as well, like cursor.
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:34:53,138'); seek(2093.0)">
              so yeah, if code is what you want, work with clo.
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:34:56,658'); seek(2096.0)">
              Next up is gr not to be confused with x gr.
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:35:01,398'); seek(2101.0)">
              So Grok is, essentially, a company that is building, Special purpose chips.
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:35:08,708'); seek(2108.0)">
              They call them pus, for running LLMs, which, makes,
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:35:12,658'); seek(2112.0)">
              their inference time very low.
            </span>
            
            <span id="chunk-605" class="transcript-chunks" onclick="console.log('00:35:14,968'); seek(2114.0)">
              probably, even the inference cost, So if latency is what you're trying to
            </span>
            
            <span id="chunk-606" class="transcript-chunks" onclick="console.log('00:35:20,628'); seek(2120.0)">
              optimize, tryout, grok, grok, cloud, which is their API, which are their, LLM APIs.
            </span>
            
            <span id="chunk-607" class="transcript-chunks" onclick="console.log('00:35:26,848'); seek(2126.0)">
              they generally host, most of the commonly used open source models.
            </span>
            
            <span id="chunk-608" class="transcript-chunks" onclick="console.log('00:35:30,983'); seek(2130.0)">
              so you have llama, extra Gemma available.
            </span>
            
            <span id="chunk-609" class="transcript-chunks" onclick="console.log('00:35:33,643'); seek(2133.0)">
              apart from that, a bunch of other things, Latency wise, they are much faster
            </span>
            
            <span id="chunk-610" class="transcript-chunks" onclick="console.log('00:35:39,133'); seek(2139.0)">
              than, all the other model providers.
            </span>
            
            <span id="chunk-611" class="transcript-chunks" onclick="console.log('00:35:40,863'); seek(2140.0)">
              So if you are optimizing for latency and, these models work for
            </span>
            
            <span id="chunk-612" class="transcript-chunks" onclick="console.log('00:35:45,413'); seek(2145.0)">
              your particular task, go for it.
            </span>
            
            <span id="chunk-613" class="transcript-chunks" onclick="console.log('00:35:47,493'); seek(2147.0)">
              Alright, so AWS, mainly works like rock.
            </span>
            
            <span id="chunk-614" class="transcript-chunks" onclick="console.log('00:35:52,473'); seek(2152.0)">
              They host a lot of open source models.
            </span>
            
            <span id="chunk-615" class="transcript-chunks" onclick="console.log('00:35:55,203'); seek(2155.0)">
              On.
            </span>
            
            <span id="chunk-616" class="transcript-chunks" onclick="console.log('00:35:55,743'); seek(2155.0)">
              And along with that, I think they also have, their own models,
            </span>
            
            <span id="chunk-617" class="transcript-chunks" onclick="console.log('00:35:58,813'); seek(2158.0)">
              which, we have not tried out yet.
            </span>
            
            <span id="chunk-618" class="transcript-chunks" onclick="console.log('00:36:00,633'); seek(2160.0)">
              but the biggest USP of using AWS bedrock would be if you're already
            </span>
            
            <span id="chunk-619" class="transcript-chunks" onclick="console.log('00:36:07,013'); seek(2167.0)">
              in the AWS, ecosystem and you are, worried about, your sensitive data,
            </span>
            
            <span id="chunk-620" class="transcript-chunks" onclick="console.log('00:36:12,973'); seek(2172.0)">
              Getting out of your infra and you don't want to like, send it to open AI
            </span>
            
            <span id="chunk-621" class="transcript-chunks" onclick="console.log('00:36:16,618'); seek(2176.0)">
              or cloud or any other model provider.
            </span>
            
            <span id="chunk-622" class="transcript-chunks" onclick="console.log('00:36:18,823'); seek(2178.0)">
              in that case, bedrock should be your choice.
            </span>
            
            <span id="chunk-623" class="transcript-chunks" onclick="console.log('00:36:21,503'); seek(2181.0)">
              one good thing is Bedrock also hosts cloud APIs.
            </span>
            
            <span id="chunk-624" class="transcript-chunks" onclick="console.log('00:36:24,993'); seek(2184.0)">
              the limits are lower.
            </span>
            
            <span id="chunk-625" class="transcript-chunks" onclick="console.log('00:36:26,953'); seek(2186.0)">
              as far as I know.
            </span>
            
            <span id="chunk-626" class="transcript-chunks" onclick="console.log('00:36:27,733'); seek(2187.0)">
              I think you'll need to talk to the support and, get your service quota increased.
            </span>
            
            <span id="chunk-627" class="transcript-chunks" onclick="console.log('00:36:30,903'); seek(2190.0)">
              But, If you are worried about, sensitive data and you're okay with cloud,
            </span>
            
            <span id="chunk-628" class="transcript-chunks" onclick="console.log('00:36:35,458'); seek(2195.0)">
              bedrock should work for you very well.
            </span>
            
            <span id="chunk-629" class="transcript-chunks" onclick="console.log('00:36:37,528'); seek(2197.0)">
              and along with that, they also host LA and Mixture and a few
            </span>
            
            <span id="chunk-630" class="transcript-chunks" onclick="console.log('00:36:40,918'); seek(2200.0)">
              other, APIs, multimodal APIs.
            </span>
            
            <span id="chunk-631" class="transcript-chunks" onclick="console.log('00:36:43,318'); seek(2203.0)">
              Azure is, the last time I checked Azure is hosting GPT models.
            </span>
            
            <span id="chunk-632" class="transcript-chunks" onclick="console.log('00:36:48,733'); seek(2208.0)">
              separately, the hosting, which OpenAI does is separate from Azure.
            </span>
            
            <span id="chunk-633" class="transcript-chunks" onclick="console.log('00:36:52,483'); seek(2212.0)">
              And, the last time we checked, Azure GPT APIs were a bit more faster than open air.
            </span>
            
            <span id="chunk-634" class="transcript-chunks" onclick="console.log('00:36:58,373'); seek(2218.0)">
              So again, oh, if you want to use open AI APIs and you, want, a slightly
            </span>
            
            <span id="chunk-635" class="transcript-chunks" onclick="console.log('00:37:04,513'); seek(2224.0)">
              better latency, tryout as Azure, but, they'll make you fill a bunch of forms.
            </span>
            
            <span id="chunk-636" class="transcript-chunks" onclick="console.log('00:37:08,513'); seek(2228.0)">
              I think these APIs or, these models are not publicly
            </span>
            
            <span id="chunk-637" class="transcript-chunks" onclick="console.log('00:37:11,003'); seek(2231.0)">
              available on Azure for everyone.
            </span>
            
            <span id="chunk-638" class="transcript-chunks" onclick="console.log('00:37:13,208'); seek(2233.0)">
              Of now, GCP, I've not tried out.
            </span>
            
            <span id="chunk-639" class="transcript-chunks" onclick="console.log('00:37:15,798'); seek(2235.0)">
              again, I think the setup was a bit complex, we didn't get a chance to give
            </span>
            
            <span id="chunk-640" class="transcript-chunks" onclick="console.log('00:37:18,768'); seek(2238.0)">
              it a try, but from what we've heard, the developer experience is much better now.
            </span>
            
            <span id="chunk-641" class="transcript-chunks" onclick="console.log('00:37:23,628'); seek(2243.0)">
              So someday we'll give it a try again.
            </span>
            
            <span id="chunk-642" class="transcript-chunks" onclick="console.log('00:37:26,118'); seek(2246.0)">
              But GCP has, Gemini and the.
            </span>
            
            <span id="chunk-643" class="transcript-chunks" onclick="console.log('00:37:30,408'); seek(2250.0)">
              Latest, the newest scale on the block is Deeps sec. if you are active
            </span>
            
            <span id="chunk-644" class="transcript-chunks" onclick="console.log('00:37:35,318'); seek(2255.0)">
              on Twitter, you would've already heard, about deeps, SEC's, APIs.
            </span>
            
            <span id="chunk-645" class="transcript-chunks" onclick="console.log('00:37:39,828'); seek(2259.0)">
              from the chatter, it seems as if they are at par with own APIs.
            </span>
            
            <span id="chunk-646" class="transcript-chunks" onclick="console.log('00:37:45,568'); seek(2265.0)">
              again, having tried it out, give it a try.
            </span>
            
            <span id="chunk-647" class="transcript-chunks" onclick="console.log('00:37:47,988'); seek(2267.0)">
              one concern could be, the hosting, which is in China, but, um.
            </span>
            
            <span id="chunk-648" class="transcript-chunks" onclick="console.log('00:37:52,858'); seek(2272.0)">
              definitely give it a try.
            </span>
            
            <span id="chunk-649" class="transcript-chunks" onclick="console.log('00:37:54,228'); seek(2274.0)">
              probably you might find it, to be a good fit for your use case.
            </span>
            
            <span id="chunk-650" class="transcript-chunks" onclick="console.log('00:37:58,198'); seek(2278.0)">
              And, one more thing, deep seeks models are also open source so
            </span>
            
            <span id="chunk-651" class="transcript-chunks" onclick="console.log('00:38:02,238'); seek(2282.0)">
              you can host them on your own.
            </span>
            
            <span id="chunk-652" class="transcript-chunks" onclick="console.log('00:38:03,638'); seek(2283.0)">
              And that's all from me.
            </span>
            
            <span id="chunk-653" class="transcript-chunks" onclick="console.log('00:38:04,808'); seek(2284.0)">
              I hope you find the information shared in the stock useful, and it speeds up
            </span>
            
            <span id="chunk-654" class="transcript-chunks" onclick="console.log('00:38:10,998'); seek(2290.0)">
              your development process when you are building LM applications and, AI agents.
            </span>
            
            <span id="chunk-655" class="transcript-chunks" onclick="console.log('00:38:16,068'); seek(2296.0)">
              if you have any, queries or if you want to, talk more
            </span>
            
            <span id="chunk-656" class="transcript-chunks" onclick="console.log('00:38:19,288'); seek(2299.0)">
              about this, drop us an email.
            </span>
            
            <span id="chunk-657" class="transcript-chunks" onclick="console.log('00:38:20,608'); seek(2300.0)">
              You can find our email.
            </span>
            
            <span id="chunk-658" class="transcript-chunks" onclick="console.log('00:38:22,288'); seek(2302.0)">
              On Ku AI's landing page, or just, send me a message on LinkedIn.
            </span>
            
            <span id="chunk-659" class="transcript-chunks" onclick="console.log('00:38:27,598'); seek(2307.0)">
              happy to chat about this and, go with something.
            </span>
            
            <span id="chunk-660" class="transcript-chunks" onclick="console.log('00:38:30,528'); seek(2310.0)">
              Awesome.
            </span>
            
            <span id="chunk-661" class="transcript-chunks" onclick="console.log('00:38:30,798'); seek(2310.0)">
              Bye.
            </span>
            
            </div>
          </div>
          
          

          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/llms2025" class="btn btn-sm btn-danger shadow lift" style="background-color: #CCB87B;">
                <i class="fe fe-grid me-2"></i>
                See all 38 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Sourabh%20Gawande_llm.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Sourabh Gawande
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Co-founder @ Kusho
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/sourabhgawande/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Sourabh Gawande's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Sourabh Gawande"
                  data-url="https://www.conf42.com/llms2025"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/llms2025"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Large Language Models"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>