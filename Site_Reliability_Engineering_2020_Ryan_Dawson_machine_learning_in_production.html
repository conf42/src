<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Machine Learning in Production: An Intro to MLOps</title>
    <meta name="description" content="Building reliable computer systems has never been that easy!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/sre_ryand.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Machine Learning in Production: An Intro to MLOps | Conf42"/>
    <meta property="og:description" content="Reliably deploying and maintaining machine learning applications is complex. There's a dizzying array of tools and they look different from the usual DevOps tools.   To apply SRE skils to ML, we need to understand the specific challenges of ML build-deploy-monitor workflows. We'll use reference examples to understand the cycle in terms of data prep, training, rollout and monitoring. We'll see that some key challenges relate to training models from slices of large and varying data domains - a problem alien to the mainstream DevOps world."/>
    <meta property="og:url" content="https://conf42.com/Site_Reliability_Engineering_2020_Ryan_Dawson_machine_learning_in_production"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/CE2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Chaos Engineering 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-02-15
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/ce2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/aiml2024">
                            Artificial Intelligence & Machine Learning (AI & ML)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday London
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2023
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2023">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2023">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2023">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2023">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2023">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2023">
                            Site Reliability Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2023">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2023">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2023">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2023">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2023">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2023">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2023">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2023">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2023">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2023">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/DnyHgrC7jC" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #E36414;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Site Reliability Engineering 2020 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2020-08-27">August 27 2020</time>
              
              - premiere 5PM GMT
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <span id="localtime"></span>
              
              <!-- Building reliable computer systems has never been that easy!
 -->
              <script>
                const event_date = new Date("2020-08-27T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2020-08-27T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "UlmCJl8ZsT4"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBwt4jpyTpSRYMh-MYxvXGB" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Machine learning in production this session", "timestamp": "00:00:36,610", "timestamp_s": 36.0}, {"text": "is an introduction to running machine learning in production, which is", "timestamp": "00:00:40,258", "timestamp_s": 40.0}, {"text": "being called MlOps. I\u0027m Ryan Dawson and I\u0027m an", "timestamp": "00:00:44,332", "timestamp_s": 44.0}, {"text": "engineer working on Mlops solutions at Seldon.", "timestamp": "00:00:48,108", "timestamp_s": 48.0}, {"text": "The MLops scene is complex and new.", "timestamp": "00:00:51,970", "timestamp_s": 51.0}, {"text": "It\u0027s distinct from mainstream DevOps. So we\u0027ll start by comparing", "timestamp": "00:00:55,124", "timestamp_s": 55.0}, {"text": "mlops to DevOps. To understand why it\u0027s so different,", "timestamp": "00:00:58,442", "timestamp_s": 58.0}, {"text": "we need to understand how data science is different from programming.", "timestamp": "00:01:02,100", "timestamp_s": 62.0}, {"text": "We\u0027ll find out that the difference centers on how data is used.", "timestamp": "00:01:05,910", "timestamp_s": 65.0}, {"text": "When we\u0027re clear about that difference, then we\u0027ll look at how the build", "timestamp": "00:01:09,830", "timestamp_s": 69.0}, {"text": "deploy monitor workflows for DevOps differ from Mlops.", "timestamp": "00:01:13,720", "timestamp_s": 73.0}, {"text": "From there, we\u0027ll be able to go deeper on particular steps in the mlops", "timestamp": "00:01:18,970", "timestamp_s": 78.0}, {"text": "build deploy monitor workflow. I\u0027ll try to explain", "timestamp": "00:01:23,106", "timestamp_s": 83.0}, {"text": "that MlOps challenges vary by use case, and that some use cases", "timestamp": "00:01:26,492", "timestamp_s": 86.0}, {"text": "have especially advanced challenges. Lastly,", "timestamp": "00:01:30,534", "timestamp_s": 90.0}, {"text": "I\u0027ll go into some of the advanced challenges and how they relate to the", "timestamp": "00:01:34,374", "timestamp_s": 94.0}, {"text": "topic of governance for running machine learning.", "timestamp": "00:01:37,584", "timestamp_s": 97.0}, {"text": "So before we try to understand mlops, let\u0027s make sure we\u0027re clear about DevOps.", "timestamp": "00:01:42,290", "timestamp_s": 102.0}, {"text": "As I see it, DevOps is all about making the build deploy monitor", "timestamp": "00:01:47,330", "timestamp_s": 107.0}, {"text": "workflow for applications as smooth as possible. It tends to", "timestamp": "00:01:51,338", "timestamp_s": 111.0}, {"text": "focus on CI, CD and infrastructure SRE", "timestamp": "00:01:55,128", "timestamp_s": 115.0}, {"text": "or site reliability engineering. As I see, it is an overlapping role,", "timestamp": "00:02:00,014", "timestamp_s": 120.0}, {"text": "but with a bit more focus on the monitoring stage of the workflows.", "timestamp": "00:02:04,510", "timestamp_s": 124.0}, {"text": "This whole workflows is a key enabler for software projects.", "timestamp": "00:02:10,250", "timestamp_s": 130.0}, {"text": "Fortunately, there\u0027s some great tools in the space that have become pretty well", "timestamp": "00:02:14,250", "timestamp_s": 134.0}, {"text": "established across the industry, tools like git, Jenkins,", "timestamp": "00:02:17,452", "timestamp_s": 137.0}, {"text": "Docker, Ansible, Prometheus, et cetera.", "timestamp": "00:02:20,710", "timestamp_s": 140.0}, {"text": "MLOPS is in a very different space right now.", "timestamp": "00:02:26,190", "timestamp_s": 146.0}, {"text": "There\u0027s surveys suggesting that 80% to 90% of machine learning", "timestamp": "00:02:29,550", "timestamp_s": 149.0}, {"text": "models never make it to live, and at least part of that is", "timestamp": "00:02:32,832", "timestamp_s": 152.0}, {"text": "due to the complexity of running machine learning in production.", "timestamp": "00:02:36,148", "timestamp_s": 156.0}, {"text": "There\u0027s a famous paper called hidden technical debt in machine learning", "timestamp": "00:02:39,570", "timestamp_s": 159.0}, {"text": "systems, and it explains about all the effort that goes into", "timestamp": "00:02:43,076", "timestamp_s": 163.0}, {"text": "running production grade machine training systems. It has", "timestamp": "00:02:47,096", "timestamp_s": 167.0}, {"text": "a diagram with boxes showing the relative size of different tasks,", "timestamp": "00:02:50,568", "timestamp_s": 170.0}, {"text": "and there\u0027s this tiny little box for ML code and", "timestamp": "00:02:54,206", "timestamp_s": 174.0}, {"text": "really big boxes for data collection, data processing,", "timestamp": "00:02:57,452", "timestamp_s": 177.0}, {"text": "runtime, infrastructure monitoring.", "timestamp": "00:03:01,122", "timestamp_s": 181.0}, {"text": "The Linux foundation for AI have tried to help by producing a diagram of", "timestamp": "00:03:06,650", "timestamp_s": 186.0}, {"text": "the whole mlops tool landscape. It\u0027s great, but it", "timestamp": "00:03:10,608", "timestamp_s": 190.0}, {"text": "has loads of tools in loads of sections, and even", "timestamp": "00:03:14,208", "timestamp_s": 194.0}, {"text": "the section titles won\u0027t make much sense for newcomers to mlops.", "timestamp": "00:03:17,632", "timestamp_s": 197.0}, {"text": "But let\u0027s try to understand more about the fundamentals of Mlops and where it\u0027s coming", "timestamp": "00:03:21,730", "timestamp_s": 201.0}, {"text": "from.", "timestamp": "00:03:25,716", "timestamp_s": 205.0}, {"text": "Fundamentally, MLops is different from DevOps because machine learning is", "timestamp": "00:03:28,770", "timestamp_s": 208.0}, {"text": "different from programming. Traditional programming codifies rules", "timestamp": "00:03:32,388", "timestamp_s": 212.0}, {"text": "explicitly, rules that say how to respond to inputs.", "timestamp": "00:03:36,238", "timestamp_s": 216.0}, {"text": "Machine learning does not codify explicitly. Instead,", "timestamp": "00:03:40,470", "timestamp_s": 220.0}, {"text": "rules are set indirectly by capturing patterns from data and", "timestamp": "00:03:43,976", "timestamp_s": 223.0}, {"text": "reapplying the extracted patterns to new input data.", "timestamp": "00:03:47,996", "timestamp_s": 227.0}, {"text": "This makes machine learning more applicable to problems that center on data,", "timestamp": "00:03:52,410", "timestamp_s": 232.0}, {"text": "especially focused numerical problems.", "timestamp": "00:03:56,172", "timestamp_s": 236.0}, {"text": "So with traditional programming, we\u0027ve got applications that respond", "timestamp": "00:04:00,750", "timestamp_s": 240.0}, {"text": "directly to user inputs, such as terminal systems or GUI", "timestamp": "00:04:03,942", "timestamp_s": 243.0}, {"text": "based systems. You code these by starting with hello", "timestamp": "00:04:07,622", "timestamp_s": 247.0}, {"text": "world and adding more control structures.", "timestamp": "00:04:11,156", "timestamp_s": 251.0}, {"text": "Data science problems fall into classification problems.", "timestamp": "00:04:14,290", "timestamp_s": 254.0}, {"text": "Regression problems classification", "timestamp": "00:04:17,668", "timestamp_s": 257.0}, {"text": "problems put data into categories. An example would be,", "timestamp": "00:04:21,354", "timestamp_s": 261.0}, {"text": "is this image a cat or not a cat?", "timestamp": "00:04:25,352", "timestamp_s": 265.0}, {"text": "Regression problems look for numerical output, for example,", "timestamp": "00:04:29,430", "timestamp_s": 269.0}, {"text": "predicting sales revenue from how advertising spend is", "timestamp": "00:04:32,792", "timestamp_s": 272.0}, {"text": "directed. The hello world of data science", "timestamp": "00:04:35,944", "timestamp_s": 275.0}, {"text": "is the mnist dataset, which is a data set", "timestamp": "00:04:39,218", "timestamp_s": 279.0}, {"text": "of handwritten digits. And the problem is to categorize each", "timestamp": "00:04:42,492", "timestamp_s": 282.0}, {"text": "handwritten sample correctly as the number that it represents.", "timestamp": "00:04:45,740", "timestamp_s": 285.0}, {"text": "When I think of machine learning as capturing patterns from data,", "timestamp": "00:04:51,790", "timestamp_s": 291.0}, {"text": "I think about fitting for regression problems basically", "timestamp": "00:04:55,856", "timestamp_s": 295.0}, {"text": "have data points on a graph, and you draw a line through the", "timestamp": "00:04:59,360", "timestamp_s": 299.0}, {"text": "data points and try to get the line as close to as many of the", "timestamp": "00:05:03,104", "timestamp_s": 303.0}, {"text": "data points as possible. The distance from each data", "timestamp": "00:05:06,484", "timestamp_s": 306.0}, {"text": "point to the line is called the error, and you keep adjusting the", "timestamp": "00:05:10,116", "timestamp_s": 310.0}, {"text": "equation of the line to minimize the total error.", "timestamp": "00:05:13,828", "timestamp_s": 313.0}, {"text": "The coefficients of the equation of the line correspond to the weights of a", "timestamp": "00:05:17,670", "timestamp_s": 317.0}, {"text": "machine learning model, and you then use that to make new predictions.", "timestamp": "00:05:21,624", "timestamp_s": 321.0}, {"text": "Of course, the machine learning training process is more complex than the way I\u0027m explaining", "timestamp": "00:05:28,510", "timestamp_s": 328.0}, {"text": "it. For example, there\u0027s more to the process of adjusting", "timestamp": "00:05:32,538", "timestamp_s": 332.0}, {"text": "the weights than to try to get the line to fit the data.", "timestamp": "00:05:36,154", "timestamp_s": 336.0}, {"text": "It\u0027s done promogrammatically by using an algorithm called", "timestamp": "00:05:40,710", "timestamp_s": 340.0}, {"text": "gradient descent. Essentially randomly pick", "timestamp": "00:05:43,944", "timestamp_s": 343.0}, {"text": "a way to shift the line, but it\u0027s only pseudoram, as it will", "timestamp": "00:05:47,672", "timestamp_s": 347.0}, {"text": "take a step in a given direction and then check whether that", "timestamp": "00:05:51,208", "timestamp_s": 351.0}, {"text": "reduced the error before deciding whether to keep going that way or go", "timestamp": "00:05:54,540", "timestamp_s": 354.0}, {"text": "a different direction. That step size", "timestamp": "00:05:57,788", "timestamp_s": 357.0}, {"text": "can be tweaked and you can get different results,", "timestamp": "00:06:01,692", "timestamp_s": 361.0}, {"text": "so the overall process is tunable.", "timestamp": "00:06:05,042", "timestamp_s": 365.0}, {"text": "So basically, data scientists are looking for patterns in data and trying to find", "timestamp": "00:06:10,510", "timestamp_s": 370.0}, {"text": "which methods are best for capturing those patterns in models.", "timestamp": "00:06:14,576", "timestamp_s": 374.0}, {"text": "This is an exploratory process, and the tools data scientists", "timestamp": "00:06:18,530", "timestamp_s": 378.0}, {"text": "use reflect this. Jupyter notebooks, for example, are great for", "timestamp": "00:06:22,042", "timestamp_s": 382.0}, {"text": "playing around with slices of data and visualizing patterns.", "timestamp": "00:06:26,228", "timestamp_s": 386.0}, {"text": "These differences between programming and machine learning have implications for", "timestamp": "00:06:32,180", "timestamp_s": 392.0}, {"text": "how we can best build, deploying, and run machine learning", "timestamp": "00:06:35,658", "timestamp_s": 395.0}, {"text": "systems. So let\u0027s get into more detail about how different", "timestamp": "00:06:39,530", "timestamp_s": 399.0}, {"text": "these build deploy monitoring journeys are.", "timestamp": "00:06:43,146", "timestamp_s": 403.0}, {"text": "Let\u0027s go on an imaginary development journey. We can", "timestamp": "00:06:48,330", "timestamp_s": 408.0}, {"text": "start with a user story. Let\u0027s say we\u0027re building", "timestamp": "00:06:51,772", "timestamp_s": 411.0}, {"text": "a calculator and our user story says that our lazy users want to", "timestamp": "00:06:55,180", "timestamp_s": 415.0}, {"text": "put numerical operations into a screen so they don\u0027t have", "timestamp": "00:06:59,168", "timestamp_s": 419.0}, {"text": "to work out the answers.", "timestamp": "00:07:02,608", "timestamp_s": 422.0}, {"text": "We could write a Java program to satisfy the story,", "timestamp": "00:07:06,860", "timestamp_s": 426.0}, {"text": "compile it, and distribute it as a", "timestamp": "00:07:11,040", "timestamp_s": 431.0}, {"text": "binary. But this", "timestamp": "00:07:14,578", "timestamp_s": 434.0}, {"text": "is 2020, so we\u0027ll more likely package the code to", "timestamp": "00:07:18,322", "timestamp_s": 438.0}, {"text": "run as a web server so that users will interact with it via", "timestamp": "00:07:22,002", "timestamp_s": 442.0}, {"text": "a browser. Most likely we\u0027ll also dockerize", "timestamp": "00:07:25,548", "timestamp_s": 445.0}, {"text": "the web app and run it on some cloud infrastructure.", "timestamp": "00:07:29,132", "timestamp_s": 449.0}, {"text": "Now let\u0027s think of a machine learning build journey.", "timestamp": "00:07:35,850", "timestamp_s": 455.0}, {"text": "This is more likely to start with some data and maybe a question.", "timestamp": "00:07:39,610", "timestamp_s": 459.0}, {"text": "Let\u0027s say we\u0027ve got data on employees and their experience and", "timestamp": "00:07:43,870", "timestamp_s": 463.0}, {"text": "skills and salaries, and we want to see whether we", "timestamp": "00:07:47,312", "timestamp_s": 467.0}, {"text": "could clean whether we could use it to benchmark salaries", "timestamp": "00:07:51,088", "timestamp_s": 471.0}, {"text": "for other employees during a pay review.", "timestamp": "00:07:55,082", "timestamp_s": 475.0}, {"text": "Let\u0027s assume the data is already available and clean, though this", "timestamp": "00:07:58,850", "timestamp_s": 478.0}, {"text": "is a pretty big assumption. But let\u0027s assume", "timestamp": "00:08:02,708", "timestamp_s": 482.0}, {"text": "we\u0027ve got good data and we can create a regression models that maps employee", "timestamp": "00:08:05,966", "timestamp_s": 485.0}, {"text": "experience to pay,", "timestamp": "00:08:09,966", "timestamp_s": 489.0}, {"text": "maybe using scikitlearn.", "timestamp": "00:08:13,350", "timestamp_s": 493.0}, {"text": "So we train the model and then", "timestamp": "00:08:16,650", "timestamp_s": 496.0}, {"text": "it can be used to make a production for any given", "timestamp": "00:08:19,868", "timestamp_s": 499.0}, {"text": "employee a", "timestamp": "00:08:23,628", "timestamp_s": 503.0}, {"text": "prediction about what the salary benchmark would be.", "timestamp": "00:08:27,468", "timestamp_s": 507.0}, {"text": "So let\u0027s say we give our predictions for a particular set of employees", "timestamp": "00:08:30,830", "timestamp_s": 510.0}, {"text": "to the business and they\u0027re happy with that.", "timestamp": "00:08:34,998", "timestamp_s": 514.0}, {"text": "So happy that they want to use it again next year or more", "timestamp": "00:08:38,510", "timestamp_s": 518.0}, {"text": "regularly. Then our situation changes.", "timestamp": "00:08:42,068", "timestamp_s": 522.0}, {"text": "Because then what we want isn\u0027t just a prediction but", "timestamp": "00:08:45,508", "timestamp_s": 525.0}, {"text": "a predict function as", "timestamp": "00:08:49,508", "timestamp_s": 529.0}, {"text": "we might not want to have to rerun the training process every time the", "timestamp": "00:08:54,542", "timestamp_s": 534.0}, {"text": "business has some new employees to check.", "timestamp": "00:08:57,858", "timestamp_s": 537.0}, {"text": "This problem would be magnified if another department says that they want to", "timestamp": "00:09:02,640", "timestamp_s": 542.0}, {"text": "make predictions too. Actually, that would add extra complication", "timestamp": "00:09:06,418", "timestamp_s": 546.0}, {"text": "as even if we know that the patterns from our", "timestamp": "00:09:10,876", "timestamp_s": 550.0}, {"text": "training data are applicable to our department, we don\u0027t necessarily", "timestamp": "00:09:14,118", "timestamp_s": 554.0}, {"text": "know about the new department. But let\u0027s assume", "timestamp": "00:09:17,916", "timestamp_s": 557.0}, {"text": "that it is applicable. Then our main problem is a problem", "timestamp": "00:09:21,712", "timestamp_s": 561.0}, {"text": "of scaling. How do we make all these predictions", "timestamp": "00:09:25,322", "timestamp_s": 565.0}, {"text": "without burning ourselves out?", "timestamp": "00:09:28,832", "timestamp_s": 568.0}, {"text": "Probably we\u0027re going to be interested in using the machine learning model in", "timestamp": "00:09:32,040", "timestamp_s": 572.0}, {"text": "a web app.", "timestamp": "00:09:35,502", "timestamp_s": 575.0}, {"text": "So maybe we add a rest API around our python code and", "timestamp": "00:09:38,540", "timestamp_s": 578.0}, {"text": "look to run it as a web application. We might naturally package", "timestamp": "00:09:42,318", "timestamp_s": 582.0}, {"text": "it in a docker container like we would for a traditional web app.", "timestamp": "00:09:45,828", "timestamp_s": 585.0}, {"text": "This is a valid and common approach, but it\u0027s just one approach", "timestamp": "00:09:50,640", "timestamp_s": 590.0}, {"text": "with machine learning deploying it does present a challenge", "timestamp": "00:09:53,832", "timestamp_s": 593.0}, {"text": "about how to dockerize the predict function without including the training data in the docker", "timestamp": "00:09:57,752", "timestamp_s": 597.0}, {"text": "image. So it\u0027s also common to", "timestamp": "00:10:02,028", "timestamp_s": 602.0}, {"text": "separate the model from the data by taking", "timestamp": "00:10:05,478", "timestamp_s": 605.0}, {"text": "the Python variable for the model production and", "timestamp": "00:10:08,806", "timestamp_s": 608.0}, {"text": "serializing that to a file using Python pickling.", "timestamp": "00:10:12,346", "timestamp_s": 612.0}, {"text": "Then the file can be loaded into another training Python application server.", "timestamp": "00:10:16,600", "timestamp_s": 616.0}, {"text": "So if we load the model into a suitable Python", "timestamp": "00:10:21,720", "timestamp_s": 621.0}, {"text": "web server app, then we can serve predictions that way.", "timestamp": "00:10:25,088", "timestamp_s": 625.0}, {"text": "This varies a little from framework to framework, and can vary", "timestamp": "00:10:29,900", "timestamp_s": 629.0}, {"text": "quite a lot if the language is not Python.", "timestamp": "00:10:33,588", "timestamp_s": 633.0}, {"text": "But basically this is a good picture for the machine learning lifecycle.", "timestamp": "00:10:37,280", "timestamp_s": 637.0}, {"text": "Get data, clean it, experiment with it, train a model,", "timestamp": "00:10:41,320", "timestamp_s": 641.0}, {"text": "package the model into something that can serve predictions.", "timestamp": "00:10:45,522", "timestamp_s": 645.0}, {"text": "And there are tools pitched at each stage of this lifecycle", "timestamp": "00:10:50,340", "timestamp_s": 650.0}, {"text": "for data storage and prep. There\u0027s tools like s three and", "timestamp": "00:10:54,420", "timestamp_s": 654.0}, {"text": "Hadoop training can use", "timestamp": "00:10:58,486", "timestamp_s": 658.0}, {"text": "a lot of compute resource and take a long time.", "timestamp": "00:11:02,374", "timestamp_s": 662.0}, {"text": "So there\u0027s tools that help with running long running training jobs,", "timestamp": "00:11:05,642", "timestamp_s": 665.0}, {"text": "and also tools for training the operations performed during training.", "timestamp": "00:11:09,136", "timestamp_s": 669.0}, {"text": "There are tools specifically aimed at helping make batch", "timestamp": "00:11:15,100", "timestamp_s": 675.0}, {"text": "predictions on a regular cycle,", "timestamp": "00:11:18,852", "timestamp_s": 678.0}, {"text": "say for just getting predictions every month or whatever the cycle", "timestamp": "00:11:23,500", "timestamp_s": 683.0}, {"text": "is that the business works to.", "timestamp": "00:11:26,952", "timestamp_s": 686.0}, {"text": "Or predictions could be needed at any time.", "timestamp": "00:11:29,760", "timestamp_s": 689.0}, {"text": "And then there\u0027s tools for real time serving of", "timestamp": "00:11:33,314", "timestamp_s": 693.0}, {"text": "predictions using a rest API.", "timestamp": "00:11:37,074", "timestamp_s": 697.0}, {"text": "Some real time serving tools are specific to the framework", "timestamp": "00:11:40,980", "timestamp_s": 700.0}, {"text": "and some SRe more general. I personally work on seldon", "timestamp": "00:11:44,300", "timestamp_s": 704.0}, {"text": "core, which is a framework agnostic open source serving", "timestamp": "00:11:48,028", "timestamp_s": 708.0}, {"text": "tool. The seldon team also collaborates on another tool", "timestamp": "00:11:51,568", "timestamp_s": 711.0}, {"text": "called KF serving. Both of these are part", "timestamp": "00:11:55,386", "timestamp_s": 715.0}, {"text": "of the Kubeflow ecosystem, which is an end to end", "timestamp": "00:11:58,762", "timestamp_s": 718.0}, {"text": "platform. That\u0027s another space of tools,", "timestamp": "00:12:02,282", "timestamp_s": 722.0}, {"text": "end to end platforms that try to join up the whole journey.", "timestamp": "00:12:05,760", "timestamp_s": 725.0}, {"text": "Platforms can save you the effort of stitching together several different", "timestamp": "00:12:09,660", "timestamp_s": 729.0}, {"text": "tools, but platforms are also opinionated", "timestamp": "00:12:13,566", "timestamp_s": 733.0}, {"text": "can be, so they don\u0027t necessarily fit every use", "timestamp": "00:12:17,496", "timestamp_s": 737.0}, {"text": "case. I\u0027m listing these types of tools because I think", "timestamp": "00:12:20,802", "timestamp_s": 740.0}, {"text": "it helps to divide the machine training lifecycle up like this into", "timestamp": "00:12:24,418", "timestamp_s": 744.0}, {"text": "data prep, training and serving.", "timestamp": "00:12:27,922", "timestamp_s": 747.0}, {"text": "This helps us make sense of the concept landscape of mlops", "timestamp": "00:12:30,980", "timestamp_s": 750.0}, {"text": "tools out there, as we can then put them into categories mapped to the", "timestamp": "00:12:34,348", "timestamp_s": 754.0}, {"text": "lifecycle. There\u0027s also the monitoring part of", "timestamp": "00:12:38,214", "timestamp_s": 758.0}, {"text": "the lifecycle, but we\u0027ll get to that later.", "timestamp": "00:12:41,594", "timestamp_s": 761.0}, {"text": "For now, the key point to see is that mlops is different from DevOps,", "timestamp": "00:12:46,920", "timestamp_s": 766.0}, {"text": "mostly because of the role of data. In particular,", "timestamp": "00:12:50,624", "timestamp_s": 770.0}, {"text": "models are built by extracting patterns from data using", "timestamp": "00:12:54,430", "timestamp_s": 774.0}, {"text": "code, so that the training data is", "timestamp": "00:12:58,222", "timestamp_s": 778.0}, {"text": "a key part of the model. The training", "timestamp": "00:13:02,460", "timestamp_s": 782.0}, {"text": "data volumes can be large,", "timestamp": "00:13:06,130", "timestamp_s": 786.0}, {"text": "and that leads to complexity in storing and processing the data,", "timestamp": "00:13:09,490", "timestamp_s": 789.0}, {"text": "which there\u0027s specialized tools to help with.", "timestamp": "00:13:13,360", "timestamp_s": 793.0}, {"text": "You also get different toolkits for building machine learning models,", "timestamp": "00:13:18,880", "timestamp_s": 798.0}, {"text": "which results in models for different formats and adds", "timestamp": "00:13:22,636", "timestamp_s": 802.0}, {"text": "some complexity to the space of tools for getting predictions out of models,", "timestamp": "00:13:26,108", "timestamp_s": 806.0}, {"text": "space called serving. So the complexity of the", "timestamp": "00:13:30,044", "timestamp_s": 810.0}, {"text": "way the ML build deploying monitor lifecycle uses", "timestamp": "00:13:33,514", "timestamp_s": 813.0}, {"text": "data has knock on effects to the tool landscape.", "timestamp": "00:13:36,608", "timestamp_s": 816.0}, {"text": "We\u0027ve not talked about the post deployment stage yet,", "timestamp": "00:13:40,920", "timestamp_s": 820.0}, {"text": "but there\u0027s also complexity there. For example,", "timestamp": "00:13:43,898", "timestamp_s": 823.0}, {"text": "you can sometimes need to retrain your model,", "timestamp": "00:13:47,310", "timestamp_s": 827.0}, {"text": "your running model, not because of any bugs in it, but because the data", "timestamp": "00:13:50,622", "timestamp_s": 830.0}, {"text": "coming in from the outside world changes.", "timestamp": "00:13:54,446", "timestamp_s": 834.0}, {"text": "Think, for example, of how fashion is seasonal.", "timestamp": "00:13:58,080", "timestamp_s": 838.0}, {"text": "Let\u0027s say you\u0027ve got a model trained to recommend clothes for an online fashion", "timestamp": "00:14:01,288", "timestamp_s": 841.0}, {"text": "store, and you trained it based on purchases made in winter.", "timestamp": "00:14:05,128", "timestamp_s": 845.0}, {"text": "Then it might perform great in winter and make lots of money.", "timestamp": "00:14:10,080", "timestamp_s": 850.0}, {"text": "But when it comes to summer, it\u0027s still going to be recommending coats when", "timestamp": "00:14:13,494", "timestamp_s": 853.0}, {"text": "people are looking for summer clothes.", "timestamp": "00:14:17,382", "timestamp_s": 857.0}, {"text": "So you would need to be regularly updating the model with new data and", "timestamp": "00:14:20,500", "timestamp_s": 860.0}, {"text": "ideally checking that it\u0027s leading to sales. That\u0027s a", "timestamp": "00:14:24,346", "timestamp_s": 864.0}, {"text": "complex you don\u0027t normally get with traditional software.", "timestamp": "00:14:28,394", "timestamp_s": 868.0}, {"text": "These complexities about handling data, they ripple", "timestamp": "00:14:31,504", "timestamp_s": 871.0}, {"text": "all the way through the whole mlops lifecycle.", "timestamp": "00:14:35,076", "timestamp_s": 875.0}, {"text": "We\u0027ve talked about this at a high level so far, but let\u0027s now think", "timestamp": "00:14:38,940", "timestamp_s": 878.0}, {"text": "about the individual steps of the workflow and the tools used in them.", "timestamp": "00:14:42,478", "timestamp_s": 882.0}, {"text": "So let\u0027s just remind ourselves of the workflows steps with traditional", "timestamp": "00:14:50,120", "timestamp_s": 890.0}, {"text": "DevOps. We\u0027ll start with the user story", "timestamp": "00:14:53,792", "timestamp_s": 893.0}, {"text": "specifying a business need. From that a developer will write", "timestamp": "00:14:57,354", "timestamp_s": 897.0}, {"text": "code and submit a pull request. Hopefully test will", "timestamp": "00:15:00,926", "timestamp_s": 900.0}, {"text": "run automatically on the pull request. Somebody will", "timestamp": "00:15:04,862", "timestamp_s": 904.0}, {"text": "review it and merge. It gets to merged to master there,", "timestamp": "00:15:08,046", "timestamp_s": 908.0}, {"text": "our pipeline will build a new version of the app and deploy that to", "timestamp": "00:15:11,966", "timestamp_s": 911.0}, {"text": "the test environment. Perhaps further tests", "timestamp": "00:15:15,602", "timestamp_s": 915.0}, {"text": "will be run and it\u0027ll get promoted to the next environment where there", "timestamp": "00:15:19,768", "timestamp_s": 919.0}, {"text": "might be more deeper tests, and then it\u0027ll go to", "timestamp": "00:15:23,778", "timestamp_s": 923.0}, {"text": "production. And in production we\u0027ll monitor for anything going", "timestamp": "00:15:27,062", "timestamp_s": 927.0}, {"text": "wrong, probably in the form of stack traces or error codes.", "timestamp": "00:15:30,742", "timestamp_s": 930.0}, {"text": "The pipeline producing these builds and running the tests will most likely be", "timestamp": "00:15:35,940", "timestamp_s": 935.0}, {"text": "a CI system like Jenkins. The driver for the pipeline", "timestamp": "00:15:39,258", "timestamp_s": 939.0}, {"text": "will most likely be a code change in git. The artifact", "timestamp": "00:15:42,912", "timestamp_s": 942.0}, {"text": "we\u0027ll be promoting will probably be an executable inside a", "timestamp": "00:15:46,752", "timestamp_s": 946.0}, {"text": "docker image.", "timestamp": "00:15:50,494", "timestamp_s": 950.0}, {"text": "ML workflows are different. The driver for", "timestamp": "00:15:54,780", "timestamp_s": 954.0}, {"text": "automation might be a code change, or it might be new data,", "timestamp": "00:15:58,350", "timestamp_s": 958.0}, {"text": "and the data probably won\u0027t be in git as git isn\u0027t a great store for", "timestamp": "00:16:02,062", "timestamp_s": 962.0}, {"text": "data getting into the gigabytes,", "timestamp": "00:16:05,778", "timestamp_s": 965.0}, {"text": "the workflows are more experimental and data driven.", "timestamp": "00:16:09,600", "timestamp_s": 969.0}, {"text": "You start with a data set and need to experiment to", "timestamp": "00:16:12,960", "timestamp_s": 972.0}, {"text": "find usable patterns in the data set that can", "timestamp": "00:16:16,258", "timestamp_s": 976.0}, {"text": "be captured in a model. When you\u0027ve got a model, then it", "timestamp": "00:16:19,398", "timestamp_s": 979.0}, {"text": "might not be enough to just check it for past fail conditions and monitor for", "timestamp": "00:16:22,934", "timestamp_s": 982.0}, {"text": "errors like you would. Traditional software likely have", "timestamp": "00:16:26,182", "timestamp_s": 986.0}, {"text": "to check how well it performs against the data. In numerical terms,", "timestamp": "00:16:29,558", "timestamp_s": 989.0}, {"text": "there can be quite a bit of variation with ML workflows.", "timestamp": "00:16:33,240", "timestamp_s": 993.0}, {"text": "One major point of variation is whether the model is trained offline or", "timestamp": "00:16:36,600", "timestamp_s": 996.0}, {"text": "online. With online learning,", "timestamp": "00:16:40,382", "timestamp_s": 1000.0}, {"text": "a model is constantly being updated by adjusting itself through each", "timestamp": "00:16:43,740", "timestamp_s": 1003.0}, {"text": "new data point that it sees. So every prediction it makes also adjusts", "timestamp": "00:16:47,502", "timestamp_s": 1007.0}, {"text": "the model. Whereas with offline learning, the training is", "timestamp": "00:16:51,524", "timestamp_s": 1011.0}, {"text": "done separately from prediction. You train the model and deploying", "timestamp": "00:16:54,978", "timestamp_s": 1014.0}, {"text": "it, and when you want to update the model, you need to train a new", "timestamp": "00:16:58,408", "timestamp_s": 1018.0}, {"text": "one. We have to pick somewhere to", "timestamp": "00:17:01,122", "timestamp_s": 1021.0}, {"text": "focus, and offline learning is probably the more common case. So let\u0027s", "timestamp": "00:17:04,838", "timestamp_s": 1024.0}, {"text": "focus on offline training workflows.", "timestamp": "00:17:08,428", "timestamp_s": 1028.0}, {"text": "As we\u0027ve talked about already, an ML workflow starts with data.", "timestamp": "00:17:13,540", "timestamp_s": 1033.0}, {"text": "It can be very large and typically needs to be cleaned and processed.", "timestamp": "00:17:17,462", "timestamp_s": 1037.0}, {"text": "A slice of that data can be taken so that the data scientists can", "timestamp": "00:17:21,640", "timestamp_s": 1041.0}, {"text": "work with it locally to explore the data on their own machine.", "timestamp": "00:17:25,194", "timestamp_s": 1045.0}, {"text": "When the data scientist has started to make some progress, then they might", "timestamp": "00:17:30,520", "timestamp_s": 1050.0}, {"text": "move to a hosted training environment to run some longer running experiments", "timestamp": "00:17:34,622", "timestamp_s": 1054.0}, {"text": "on a larger sample of the data. There will", "timestamp": "00:17:39,012", "timestamp_s": 1059.0}, {"text": "likely be collaboration with other data scientists, most likely using", "timestamp": "00:17:42,578", "timestamp_s": 1062.0}, {"text": "Jupyter notebooks. The artifact produced will", "timestamp": "00:17:46,002", "timestamp_s": 1066.0}, {"text": "be a model, commonly a model that\u0027s pickled", "timestamp": "00:17:49,762", "timestamp_s": 1069.0}, {"text": "or serialized to a file. That model can be", "timestamp": "00:17:53,592", "timestamp_s": 1073.0}, {"text": "integrated into a running app to serve real time production through HTTP.", "timestamp": "00:17:57,014", "timestamp_s": 1077.0}, {"text": "There will probably be a consumer of those predictions, which may be", "timestamp": "00:18:03,380", "timestamp_s": 1083.0}, {"text": "another app, perhaps a traditional web app. So you may need to integration", "timestamp": "00:18:07,078", "timestamp_s": 1087.0}, {"text": "test the SRE model against the consumer.", "timestamp": "00:18:11,152", "timestamp_s": 1091.0}, {"text": "And when you roll out the model to a production, you want to", "timestamp": "00:18:14,760", "timestamp_s": 1094.0}, {"text": "monitor the model by picking some metrics that represent how well performing", "timestamp": "00:18:18,218", "timestamp_s": 1098.0}, {"text": "against the live data,", "timestamp": "00:18:22,516", "timestamp_s": 1102.0}, {"text": "the rollout and monitoring phases of the workflow can be linked.", "timestamp": "00:18:27,260", "timestamp_s": 1107.0}, {"text": "An example might help to understand this. Say we\u0027ve", "timestamp": "00:18:31,268", "timestamp_s": 1111.0}, {"text": "got an online store with ecommerce. A common way to", "timestamp": "00:18:35,124", "timestamp_s": 1115.0}, {"text": "roll out new versions of a model is an A B test.", "timestamp": "00:18:39,138", "timestamp_s": 1119.0}, {"text": "With an A B test, you\u0027d have a live version that\u0027s already", "timestamp": "00:18:43,360", "timestamp_s": 1123.0}, {"text": "running and that\u0027s called the control. And then you run", "timestamp": "00:18:46,770", "timestamp_s": 1126.0}, {"text": "other versions alongside it. Let\u0027s call them version a and version b.", "timestamp": "00:18:50,502", "timestamp_s": 1130.0}, {"text": "So we\u0027re running three versions of the model in parallel, each training a", "timestamp": "00:18:55,060", "timestamp_s": 1135.0}, {"text": "bit differently to see which gives the best results.", "timestamp": "00:18:58,854", "timestamp_s": 1138.0}, {"text": "You can do that by splitting the traffic between the versions", "timestamp": "00:19:02,680", "timestamp_s": 1142.0}, {"text": "to minimize the risk. We\u0027d send most of the traffic to the control version.", "timestamp": "00:19:06,256", "timestamp_s": 1146.0}, {"text": "A subset of the traffic will go to a and to b, and we\u0027ll", "timestamp": "00:19:11,560", "timestamp_s": 1151.0}, {"text": "run that splitting process for a while until we\u0027ve got a statistically significant", "timestamp": "00:19:15,828", "timestamp_s": 1155.0}, {"text": "sample. Let\u0027s say that variation a has the highest", "timestamp": "00:19:20,132", "timestamp_s": 1160.0}, {"text": "conversion rate, so a higher proportion of the recommendations", "timestamp": "00:19:23,892", "timestamp_s": 1163.0}, {"text": "lead to sales. That\u0027s a useful metric, and it", "timestamp": "00:19:27,416", "timestamp_s": 1167.0}, {"text": "might be enough for us to choose variation a, but it", "timestamp": "00:19:31,058", "timestamp_s": 1171.0}, {"text": "might not be the only metric. These situations can get", "timestamp": "00:19:34,722", "timestamp_s": 1174.0}, {"text": "complex. For example, it might be that model a", "timestamp": "00:19:38,182", "timestamp_s": 1178.0}, {"text": "is recommending controversial products. So some customers might really", "timestamp": "00:19:41,862", "timestamp_s": 1181.0}, {"text": "like the recommendations and buy the products, but other customers are", "timestamp": "00:19:45,862", "timestamp_s": 1185.0}, {"text": "really put off and they just go to a different website.", "timestamp": "00:19:49,478", "timestamp_s": 1189.0}, {"text": "So there are trade offs to consider, and monitoring", "timestamp": "00:19:54,040", "timestamp_s": 1194.0}, {"text": "can need more than one metric depending on the use case.", "timestamp": "00:19:57,872", "timestamp_s": 1197.0}, {"text": "So we\u0027re seeing that MLOPs is complex, and in many organizations", "timestamp": "00:20:03,760", "timestamp_s": 1203.0}, {"text": "right now, the complexity is enhanced by challenges from", "timestamp": "00:20:07,868", "timestamp_s": 1207.0}, {"text": "organizational silos. You can find data scientists", "timestamp": "00:20:11,206", "timestamp_s": 1211.0}, {"text": "that work just in a world of Jupyter notebooks and model accuracy", "timestamp": "00:20:15,052", "timestamp_s": 1215.0}, {"text": "on training data that then gets handed over to", "timestamp": "00:20:19,072", "timestamp_s": 1219.0}, {"text": "a traditional DevOps team with the expectation they\u0027ll", "timestamp": "00:20:22,538", "timestamp_s": 1222.0}, {"text": "be able to take this work and build it into a production system.", "timestamp": "00:20:25,808", "timestamp_s": 1225.0}, {"text": "Without proper context. The traditional DevOps team is likely", "timestamp": "00:20:29,660", "timestamp_s": 1229.0}, {"text": "to look at those notebooks and just react like, what is this stuff?", "timestamp": "00:20:32,804", "timestamp_s": 1232.0}, {"text": "In a more mature setup, you might have better understood handoffs.", "timestamp": "00:20:40,590", "timestamp_s": 1240.0}, {"text": "For example, you might have data engineers who deal with", "timestamp": "00:20:44,262", "timestamp_s": 1244.0}, {"text": "obtaining the data and getting it into the right state for the data scientists.", "timestamp": "00:20:47,508", "timestamp_s": 1247.0}, {"text": "Once the data is ready for the data scientists, then they can take over and", "timestamp": "00:20:51,890", "timestamp_s": 1251.0}, {"text": "build the models. And from there, data science will have an", "timestamp": "00:20:55,828", "timestamp_s": 1255.0}, {"text": "understood handoff to ML engineers. And the ML engineers", "timestamp": "00:20:59,128", "timestamp_s": 1259.0}, {"text": "might still be a DevOps team, but a DevOps team that knows about the context", "timestamp": "00:21:03,422", "timestamp_s": 1263.0}, {"text": "of this particular machine learning application and knows how to run it in", "timestamp": "00:21:06,862", "timestamp_s": 1266.0}, {"text": "the production.", "timestamp": "00:21:10,204", "timestamp_s": 1270.0}, {"text": "This is new territory. There are special challenges for mlops", "timestamp": "00:21:14,290", "timestamp_s": 1274.0}, {"text": "that are not a normal part of DevOps, at least not right now.", "timestamp": "00:21:18,042", "timestamp_s": 1278.0}, {"text": "Now that we\u0027ve got a high level understanding of where MLOPs is coming", "timestamp": "00:21:22,290", "timestamp_s": 1282.0}, {"text": "from, we can next go into more detail on particular MLOps topics.", "timestamp": "00:21:25,512", "timestamp_s": 1285.0}, {"text": "So let\u0027s take these in order and go first into training,", "timestamp": "00:21:29,910", "timestamp_s": 1289.0}, {"text": "then serving, finally rollout and monitoring.", "timestamp": "00:21:32,744", "timestamp_s": 1292.0}, {"text": "So there\u0027s tools that are pitched, particularly at the training space,", "timestamp": "00:21:38,090", "timestamp_s": 1298.0}, {"text": "to name a few examples. There\u0027s Kubeflow pipelines,", "timestamp": "00:21:41,452", "timestamp_s": 1301.0}, {"text": "MLflow Polyaxon.", "timestamp": "00:21:44,498", "timestamp_s": 1304.0}, {"text": "These are all about making it easy to run long running training jobs", "timestamp": "00:21:47,550", "timestamp_s": 1307.0}, {"text": "on a hosted environment. Typically, that means providing", "timestamp": "00:21:50,758", "timestamp_s": 1310.0}, {"text": "some manifest that specifies which steps sre to be done and", "timestamp": "00:21:55,190", "timestamp_s": 1315.0}, {"text": "in which order. That\u0027s a manifest for a training", "timestamp": "00:21:58,388", "timestamp_s": 1318.0}, {"text": "pipeline. As an example,", "timestamp": "00:22:02,036", "timestamp_s": 1322.0}, {"text": "a pipeline might have as its step an action to", "timestamp": "00:22:06,228", "timestamp_s": 1326.0}, {"text": "download data from wherever it\u0027s stored. That could be the first step.", "timestamp": "00:22:09,892", "timestamp_s": 1329.0}, {"text": "Then it gets split into training and validation data.", "timestamp": "00:22:14,356", "timestamp_s": 1334.0}, {"text": "The training data will then be used to train the model, and the validation", "timestamp": "00:22:17,910", "timestamp_s": 1337.0}, {"text": "data will be used as a check on the quality of the model\u0027s predictions.", "timestamp": "00:22:21,422", "timestamp_s": 1341.0}, {"text": "When we check the quality of the predictions, we\u0027ll want to record those checks", "timestamp": "00:22:26,330", "timestamp_s": 1346.0}, {"text": "somewhere and ideally also have an automated way", "timestamp": "00:22:30,178", "timestamp_s": 1350.0}, {"text": "to decide whether we should consider this as a good model or not.", "timestamp": "00:22:33,420", "timestamp_s": 1353.0}, {"text": "If we do consider it a good model, then we\u0027ll probably want to serialize it", "timestamp": "00:22:37,710", "timestamp_s": 1357.0}, {"text": "so that the serialized model would be available for promotion to", "timestamp": "00:22:41,584", "timestamp_s": 1361.0}, {"text": "a running environment. This is", "timestamp": "00:22:44,768", "timestamp_s": 1364.0}, {"text": "probably sounding rather like continuous integration pipelines. It is", "timestamp": "00:22:47,968", "timestamp_s": 1367.0}, {"text": "similar, but also different. The difference can be seen in the specialized", "timestamp": "00:22:51,492", "timestamp_s": 1371.0}, {"text": "tools dedicated to training. One tool", "timestamp": "00:22:55,322", "timestamp_s": 1375.0}, {"text": "for handling training is Kubeflow pipelines.", "timestamp": "00:22:59,092", "timestamp_s": 1379.0}, {"text": "In Kubeflow pipelines, you can define your pipeline with all its steps,", "timestamp": "00:23:02,070", "timestamp_s": 1382.0}, {"text": "and also visualize it and watch it progress and see any steps that fail.", "timestamp": "00:23:06,270", "timestamp_s": 1386.0}, {"text": "But the pipelines aren\u0027t only called pipelines,", "timestamp": "00:23:10,630", "timestamp_s": 1390.0}, {"text": "they\u0027re also called experiments, and they\u0027re parameterized,", "timestamp": "00:23:14,066", "timestamp_s": 1394.0}, {"text": "so there\u0027s options in its UI where you can enter parameters.", "timestamp": "00:23:17,930", "timestamp_s": 1397.0}, {"text": "Remember I mentioned before that the process can be", "timestamp": "00:23:21,690", "timestamp_s": 1401.0}, {"text": "tunable. There are tunable parameters on training, such as", "timestamp": "00:23:25,052", "timestamp_s": 1405.0}, {"text": "the step size, so you can kick off runs", "timestamp": "00:23:29,008", "timestamp_s": 1409.0}, {"text": "in parallel of the same pipeline using different parameters to", "timestamp": "00:23:32,518", "timestamp_s": 1412.0}, {"text": "see which parameters might result in the best model.", "timestamp": "00:23:36,144", "timestamp_s": 1416.0}, {"text": "Cube flow pipelines is not alone in having this idea of being able to kick", "timestamp": "00:23:40,750", "timestamp_s": 1420.0}, {"text": "off runs of an experiment with different parameters.", "timestamp": "00:23:44,218", "timestamp_s": 1424.0}, {"text": "MLflow, for example, uses the same terminology and has", "timestamp": "00:23:47,010", "timestamp_s": 1427.0}, {"text": "a similar interface. So there\u0027s", "timestamp": "00:23:50,548", "timestamp_s": 1430.0}, {"text": "similarity here with traditional CI systems, as the training platforms", "timestamp": "00:23:54,558", "timestamp_s": 1434.0}, {"text": "execute a series of steps and an artifact gets built.", "timestamp": "00:23:57,918", "timestamp_s": 1437.0}, {"text": "But it\u0027s different, as you\u0027ve also got this idea of running experiments with", "timestamp": "00:24:01,510", "timestamp_s": 1441.0}, {"text": "different parameters to see which is best.", "timestamp": "00:24:05,388", "timestamp_s": 1445.0}, {"text": "That means you have to have a definition of which is good,", "timestamp": "00:24:08,570", "timestamp_s": 1448.0}, {"text": "which is the best model,", "timestamp": "00:24:12,188", "timestamp_s": 1452.0}, {"text": "whereas traditionally with continuous integration you would just be building from master,", "timestamp": "00:24:14,890", "timestamp_s": 1454.0}, {"text": "and if it passes the tests, then you\u0027re good to promote.", "timestamp": "00:24:19,078", "timestamp_s": 1459.0}, {"text": "But so long as you can automate, which counts as the best model,", "timestamp": "00:24:22,910", "timestamp_s": 1462.0}, {"text": "then your training can build an artifact from a promotion, much like", "timestamp": "00:24:26,592", "timestamp_s": 1466.0}, {"text": "with CI. And sometimes these CI systems do have integrations", "timestamp": "00:24:30,292", "timestamp_s": 1470.0}, {"text": "to rather, sometimes these training systems do have integrations", "timestamp": "00:24:34,314", "timestamp_s": 1474.0}, {"text": "available to CI systems let\u0027s", "timestamp": "00:24:38,810", "timestamp_s": 1478.0}, {"text": "say we\u0027ve got a way of building our model and we want to be able", "timestamp": "00:24:43,914", "timestamp_s": 1483.0}, {"text": "to serve it. So we want to make predictions available in real time", "timestamp": "00:24:46,488", "timestamp_s": 1486.0}, {"text": "via HTTP, perhaps using a rest API. We might", "timestamp": "00:24:50,296", "timestamp_s": 1490.0}, {"text": "use a serving solution, as there\u0027s a range of them", "timestamp": "00:24:54,492", "timestamp_s": 1494.0}, {"text": "out there, some that are particular to a machine learning toolkit", "timestamp": "00:24:57,772", "timestamp_s": 1497.0}, {"text": "such as Tensorflow serving, Tensorflow or Torch SRE", "timestamp": "00:25:01,602", "timestamp_s": 1501.0}, {"text": "or Pytorch. There\u0027s also serving solutions provided by cloud providers", "timestamp": "00:25:05,474", "timestamp_s": 1505.0}, {"text": "as well, some that are more toolkit agnostic.", "timestamp": "00:25:09,542", "timestamp_s": 1509.0}, {"text": "For example, there\u0027s the toolkit agnostic open source offering that", "timestamp": "00:25:15,150", "timestamp_s": 1515.0}, {"text": "I work on. Seldom. Typically,", "timestamp": "00:25:18,868", "timestamp_s": 1518.0}, {"text": "serving solutions use the idea of a model being packaged and hosted,", "timestamp": "00:25:21,994", "timestamp_s": 1521.0}, {"text": "perhaps in a storage bucket or a disk location,", "timestamp": "00:25:25,690", "timestamp_s": 1525.0}, {"text": "so the serving solution can then obtain the model from that location and", "timestamp": "00:25:29,570", "timestamp_s": 1529.0}, {"text": "run it. Serving solutions often come with support for rollout", "timestamp": "00:25:33,048", "timestamp_s": 1533.0}, {"text": "and some support from monitoring as well.", "timestamp": "00:25:36,702", "timestamp_s": 1536.0}, {"text": "As an example of a serving solution, I\u0027ll explain the concept", "timestamp": "00:25:41,030", "timestamp_s": 1541.0}, {"text": "behind Seldon and how it\u0027s used. Seldon is aimed", "timestamp": "00:25:44,338", "timestamp_s": 1544.0}, {"text": "in particular at serving on kubernetes, and the models", "timestamp": "00:25:47,858", "timestamp_s": 1547.0}, {"text": "are served by creating a Kubernetes custom resource. The manifest", "timestamp": "00:25:51,138", "timestamp_s": 1551.0}, {"text": "of the custom resource is designed to make it simple to plug in a", "timestamp": "00:25:55,874", "timestamp_s": 1555.0}, {"text": "URI to a storage bucket containing a serialized model.", "timestamp": "00:25:59,168", "timestamp_s": 1559.0}, {"text": "So at a minimum, you can just put in the URI to the storage bucket", "timestamp": "00:26:02,990", "timestamp_s": 1562.0}, {"text": "and specify which toolkit was used to build", "timestamp": "00:26:06,550", "timestamp_s": 1566.0}, {"text": "the model. Then you submit that manifest to Kubernetes", "timestamp": "00:26:09,732", "timestamp_s": 1569.0}, {"text": "and it will create the lower level Kubernetes resources necessary", "timestamp": "00:26:13,738", "timestamp_s": 1573.0}, {"text": "to expose an API and serve the model\u0027s HTTP traffic.", "timestamp": "00:26:17,546", "timestamp_s": 1577.0}, {"text": "There\u0027s also a docker option to serve a model from a custom image.", "timestamp": "00:26:22,130", "timestamp_s": 1582.0}, {"text": "I\u0027m emphasizing the serialized or pickled models in this talk,", "timestamp": "00:26:26,030", "timestamp_s": 1586.0}, {"text": "mostly because it\u0027s common to see those with serving solutions,", "timestamp": "00:26:29,512", "timestamp_s": 1589.0}, {"text": "and it\u0027s not very common outside of the mlops space.", "timestamp": "00:26:33,086", "timestamp_s": 1593.0}, {"text": "The serving stage links into rollout and monitoring I\u0027ve", "timestamp": "00:26:39,890", "timestamp_s": 1599.0}, {"text": "talked a little bit already about a b testing as a rollout strategy.", "timestamp": "00:26:43,498", "timestamp_s": 1603.0}, {"text": "With that strategy, the traffic during the rollout is split between", "timestamp": "00:26:47,450", "timestamp_s": 1607.0}, {"text": "different versions, and you monitor that over a period of time until", "timestamp": "00:26:50,776", "timestamp_s": 1610.0}, {"text": "you\u0027ve got enough data to be able to decide which is best.", "timestamp": "00:26:54,296", "timestamp_s": 1614.0}, {"text": "There\u0027s a more simple rollout strategy, which also involves splitting the traffic", "timestamp": "00:26:58,710", "timestamp_s": 1618.0}, {"text": "between different versions. With the Canary strategy,", "timestamp": "00:27:02,338", "timestamp_s": 1622.0}, {"text": "you split traffic between the live version of a model and a new version", "timestamp": "00:27:05,970", "timestamp_s": 1625.0}, {"text": "that you\u0027re evaluating. But typically with a canary,", "timestamp": "00:27:09,730", "timestamp_s": 1629.0}, {"text": "you just have one new model, and you evaluate it over a shorter", "timestamp": "00:27:13,554", "timestamp_s": 1633.0}, {"text": "period of time than with the A B test. It\u0027s more of", "timestamp": "00:27:16,854", "timestamp_s": 1636.0}, {"text": "a sanity check than an in depth evaluation,", "timestamp": "00:27:20,448", "timestamp_s": 1640.0}, {"text": "and you just promote if everything looks okay.", "timestamp": "00:27:23,318", "timestamp_s": 1643.0}, {"text": "Another strategy is shadowing.", "timestamp": "00:27:28,210", "timestamp_s": 1648.0}, {"text": "With shadowing, all of the traffic goes to both the new and the", "timestamp": "00:27:31,010", "timestamp_s": 1651.0}, {"text": "old model, but it\u0027s only the responses from the older", "timestamp": "00:27:34,708", "timestamp_s": 1654.0}, {"text": "model, the live model\u0027s responses that are used and which go back to", "timestamp": "00:27:38,138", "timestamp_s": 1658.0}, {"text": "the consumer. The new model is called the shadow version,", "timestamp": "00:27:42,008", "timestamp_s": 1662.0}, {"text": "and its responses are just stored. They don\u0027t go back to any live consumers.", "timestamp": "00:27:45,758", "timestamp_s": 1665.0}, {"text": "The reason for doing this is to monitor the shadow and compare it against the", "timestamp": "00:27:50,550", "timestamp_s": 1670.0}, {"text": "live version so it makes sense to be storing the shadow\u0027s output", "timestamp": "00:27:54,108", "timestamp_s": 1674.0}, {"text": "for later evaluation.", "timestamp": "00:27:58,258", "timestamp_s": 1678.0}, {"text": "Serving solutions have some support for rollout strategies. In the", "timestamp": "00:28:02,010", "timestamp_s": 1682.0}, {"text": "case of Seldon, for example, you can create a Kubernetes manifest", "timestamp": "00:28:05,968", "timestamp_s": 1685.0}, {"text": "with two sections, one for the main model and one for the Canary.", "timestamp": "00:28:09,350", "timestamp_s": 1689.0}, {"text": "The traffic will automatically be split between these two models by", "timestamp": "00:28:13,366", "timestamp_s": 1693.0}, {"text": "default. Seldom will split traffic evenly between the models. In a manifest,", "timestamp": "00:28:17,808", "timestamp_s": 1697.0}, {"text": "you can set a field against each model called traffic.", "timestamp": "00:28:21,890", "timestamp_s": 1701.0}, {"text": "That field takes a numeric percentage that tells seldom how much", "timestamp": "00:28:25,002", "timestamp_s": 1705.0}, {"text": "of the traffic each model should get.", "timestamp": "00:28:28,212", "timestamp_s": 1708.0}, {"text": "Each rollout strategies involve gathering metrics on running models.", "timestamp": "00:28:34,050", "timestamp_s": 1714.0}, {"text": "With seldom, there\u0027s out of the box integration available for Prometheus,", "timestamp": "00:28:38,254", "timestamp_s": 1718.0}, {"text": "and some Grafana dashboards are provided.", "timestamp": "00:28:41,854", "timestamp_s": 1721.0}, {"text": "These cover general metrics like frequency of requests and latency.", "timestamp": "00:28:44,970", "timestamp_s": 1724.0}, {"text": "You may also want to monitor for metrics that are specific to your use case,", "timestamp": "00:28:49,130", "timestamp_s": 1729.0}, {"text": "and there are defined interfaces so that extra metrics can be exposed in", "timestamp": "00:28:53,164", "timestamp_s": 1733.0}, {"text": "the usual Prometheus way.", "timestamp": "00:28:56,768", "timestamp_s": 1736.0}, {"text": "I mentioned earlier that in the shadow use case, you might want to be", "timestamp": "00:29:01,390", "timestamp_s": 1741.0}, {"text": "recording the predictions that the shadow is making so that you can compare its performance", "timestamp": "00:29:04,736", "timestamp_s": 1744.0}, {"text": "against the live model. This can be handled through", "timestamp": "00:29:08,922", "timestamp_s": 1748.0}, {"text": "logging all of the requests and responses to a database.", "timestamp": "00:29:12,372", "timestamp_s": 1752.0}, {"text": "There are other use cases as well where recording all predictions can be", "timestamp": "00:29:16,850", "timestamp_s": 1756.0}, {"text": "useful. For example, if you\u0027re working in a compliance heavy industry", "timestamp": "00:29:20,488", "timestamp_s": 1760.0}, {"text": "and an auditor requires to know of every prediction that\u0027s been made", "timestamp": "00:29:24,462", "timestamp_s": 1764.0}, {"text": "in the shadow use case, you\u0027d use that database then", "timestamp": "00:29:28,950", "timestamp_s": 1768.0}, {"text": "to run queries against the data and compare the shadow\u0027s performance against", "timestamp": "00:29:32,648", "timestamp_s": 1772.0}, {"text": "that of the live model.", "timestamp": "00:29:36,172", "timestamp_s": 1776.0}, {"text": "In the case of Seldon, there\u0027s an out of the box integration", "timestamp": "00:29:38,970", "timestamp_s": 1778.0}, {"text": "which provides a way to asynchronously log everything to elasticsearch", "timestamp": "00:29:43,450", "timestamp_s": 1783.0}, {"text": "so that everything can then be made available for running queries on later, but without", "timestamp": "00:29:48,110", "timestamp_s": 1788.0}, {"text": "slowing down the request path of the live models.", "timestamp": "00:29:51,856", "timestamp_s": 1791.0}, {"text": "This idea of taking the live request and asynchronously sending", "timestamp": "00:29:57,830", "timestamp_s": 1797.0}, {"text": "it elsewhere can also be useful for some monitoring use cases,", "timestamp": "00:30:01,298", "timestamp_s": 1801.0}, {"text": "and not just for audit. In particular, there\u0027s some", "timestamp": "00:30:05,282", "timestamp_s": 1805.0}, {"text": "advanced monitoring use cases that relate to the data that\u0027s coming", "timestamp": "00:30:08,828", "timestamp_s": 1808.0}, {"text": "into the live model, how well it matches to the training data.", "timestamp": "00:30:12,188", "timestamp_s": 1812.0}, {"text": "If the live data doesn\u0027t fall within the distribution of the training data,", "timestamp": "00:30:17,230", "timestamp_s": 1817.0}, {"text": "then you can\u0027t be sure that your model will perform well on that data.", "timestamp": "00:30:21,008", "timestamp_s": 1821.0}, {"text": "Your model is based on patterns from the training data.", "timestamp": "00:30:24,930", "timestamp_s": 1824.0}, {"text": "So data that doesn\u0027t fit that training distribution might have different patterns.", "timestamp": "00:30:28,370", "timestamp_s": 1828.0}, {"text": "One thing we can do about this is to send all the request data", "timestamp": "00:30:33,970", "timestamp_s": 1833.0}, {"text": "through to detector components that will look for anything that", "timestamp": "00:30:37,316", "timestamp_s": 1837.0}, {"text": "might be going wrong so that we can flag those predictions if we need to.", "timestamp": "00:30:40,968", "timestamp_s": 1840.0}, {"text": "So let\u0027s drill a bit, a little bit further into what we might need to", "timestamp": "00:30:45,670", "timestamp_s": 1845.0}, {"text": "detect.", "timestamp": "00:30:48,828", "timestamp_s": 1848.0}, {"text": "One thing we might need to detect is an outlier. This is when", "timestamp": "00:30:52,090", "timestamp_s": 1852.0}, {"text": "there\u0027s the occasional data point which is significantly outside of the training", "timestamp": "00:30:55,852", "timestamp_s": 1855.0}, {"text": "data distribution, even though most of the data does fall", "timestamp": "00:30:59,772", "timestamp_s": 1859.0}, {"text": "within the distribution. Sometimes models express", "timestamp": "00:31:03,552", "timestamp_s": 1863.0}, {"text": "their predictions using a score. So, for example,", "timestamp": "00:31:07,222", "timestamp_s": 1867.0}, {"text": "classifiers often give a probability of how likely a data point is to", "timestamp": "00:31:09,984", "timestamp_s": 1869.0}, {"text": "be of a certain class for the model.", "timestamp": "00:31:13,908", "timestamp_s": 1873.0}, {"text": "You might expect to get a lower probability on everything when", "timestamp": "00:31:17,810", "timestamp_s": 1877.0}, {"text": "the data points are outliers.", "timestamp": "00:31:22,852", "timestamp_s": 1882.0}, {"text": "Unfortunately, it doesn\u0027t work that way. And for outliers, sometimes models can", "timestamp": "00:31:27,670", "timestamp_s": 1887.0}, {"text": "give very high probabilities for data points that they\u0027re getting completely", "timestamp": "00:31:31,208", "timestamp_s": 1891.0}, {"text": "wrong. This is called overconfidence. So if", "timestamp": "00:31:34,520", "timestamp_s": 1894.0}, {"text": "your live data has outliers and your use case has risk associated", "timestamp": "00:31:38,488", "timestamp_s": 1898.0}, {"text": "with those, then you might want to detect and track outliers.", "timestamp": "00:31:42,002", "timestamp_s": 1902.0}, {"text": "Depending on your use case, you might choose to make it part of your business", "timestamp": "00:31:46,490", "timestamp_s": 1906.0}, {"text": "logic, for example, to handle outlier cases differently,", "timestamp": "00:31:49,580", "timestamp_s": 1909.0}, {"text": "perhaps scheduling a manual review on them.", "timestamp": "00:31:52,982", "timestamp_s": 1912.0}, {"text": "Worse than the outlier case is when the whole data distribution is different", "timestamp": "00:31:58,490", "timestamp_s": 1918.0}, {"text": "from the training data. It can even start", "timestamp": "00:32:01,772", "timestamp_s": 1921.0}, {"text": "out similar to the training data and then shift over time.", "timestamp": "00:32:08,496", "timestamp_s": 1928.0}, {"text": "Think, for example, of the fashion recommendation.", "timestamp": "00:32:12,190", "timestamp_s": 1932.0}, {"text": "The example that we mentioned earlier was trained on data from", "timestamp": "00:32:15,350", "timestamp_s": 1935.0}, {"text": "winter, and then you continue using it into the summer. Then it\u0027s", "timestamp": "00:32:19,236", "timestamp_s": 1939.0}, {"text": "recommending coats when it should be recommending t shirts.", "timestamp": "00:32:23,402", "timestamp_s": 1943.0}, {"text": "If you have a component that knows the distribution of the training data,", "timestamp": "00:32:27,090", "timestamp_s": 1947.0}, {"text": "then you can asynchronously feed it all of the live requests,", "timestamp": "00:32:30,676", "timestamp_s": 1950.0}, {"text": "feed them into that component, and keep a watch. That component", "timestamp": "00:32:34,390", "timestamp_s": 1954.0}, {"text": "will keep a watch so you can use it to set up notifications in case", "timestamp": "00:32:38,558", "timestamp_s": 1958.0}, {"text": "the distribution shifts. You could then use", "timestamp": "00:32:42,552", "timestamp_s": 1962.0}, {"text": "that notification to decide if you need to train a new version of the model", "timestamp": "00:32:45,628", "timestamp_s": 1965.0}, {"text": "using updated data. Or perhaps you\u0027ve", "timestamp": "00:32:49,164", "timestamp_s": 1969.0}, {"text": "got other metrics that let you track model performance, and if", "timestamp": "00:32:52,434", "timestamp_s": 1972.0}, {"text": "they\u0027re still showing as good, you might just choose to check those metrics more", "timestamp": "00:32:55,724", "timestamp_s": 1975.0}, {"text": "frequently while you look more closely into what\u0027s happening with live data distribution.", "timestamp": "00:32:59,088", "timestamp_s": 1979.0}, {"text": "These monitoring and prediction quality concerns also feed into", "timestamp": "00:33:05,860", "timestamp_s": 1985.0}, {"text": "governance for machine learning. It\u0027s a big topic and we", "timestamp": "00:33:09,382", "timestamp_s": 1989.0}, {"text": "can\u0027t go into everything in detail, but I want to give an impression", "timestamp": "00:33:12,778", "timestamp_s": 1992.0}, {"text": "of the area, so I\u0027ll at least mention of a few things I\u0027ve", "timestamp": "00:33:16,352", "timestamp_s": 1996.0}, {"text": "talked about. Detection for data drift and outliers", "timestamp": "00:33:20,596", "timestamp_s": 2000.0}, {"text": "another thing detectors might be applicable for is adversarial attacks.", "timestamp": "00:33:24,724", "timestamp_s": 2004.0}, {"text": "These are when manipulated data is", "timestamp": "00:33:28,860", "timestamp_s": 2008.0}, {"text": "fed to a model in order to trick the model. Think, for example,", "timestamp": "00:33:32,190", "timestamp_s": 2012.0}, {"text": "of how face recognition systems can sometimes be tricked by somebody wearing a mask.", "timestamp": "00:33:35,970", "timestamp_s": 2015.0}, {"text": "That\u0027s a big problem in high security situations,", "timestamp": "00:33:40,728", "timestamp_s": 2020.0}, {"text": "and there are analogous attacks that have appeared for other use cases.", "timestamp": "00:33:43,672", "timestamp_s": 2023.0}, {"text": "I also mentioned that in high compliance situations you might", "timestamp": "00:33:50,160", "timestamp_s": 2030.0}, {"text": "want to record all of the predictions in case you need to review them later.", "timestamp": "00:33:53,458", "timestamp_s": 2033.0}, {"text": "This can also be relevant for dealing with customer domains.", "timestamp": "00:33:57,120", "timestamp_s": 2037.0}, {"text": "This relates to the topic of explainability. For example,", "timestamp": "00:34:00,820", "timestamp_s": 2040.0}, {"text": "if you\u0027ve got a system that makes decisions on whether to approve loans,", "timestamp": "00:34:04,390", "timestamp_s": 2044.0}, {"text": "you\u0027re deploying somebody a loan. Then you might want to be able to explain why", "timestamp": "00:34:08,260", "timestamp_s": 2048.0}, {"text": "you denied them the loan. You\u0027ll want to be able to", "timestamp": "00:34:11,878", "timestamp_s": 2051.0}, {"text": "revisit exactly what was fed into the model. The explainability part", "timestamp": "00:34:15,258", "timestamp_s": 2055.0}, {"text": "is a data challenges data science challenge in itself, but it links", "timestamp": "00:34:19,402", "timestamp_s": 2059.0}, {"text": "into mlops because you\u0027ll need to know what data to", "timestamp": "00:34:23,264", "timestamp_s": 2063.0}, {"text": "get explanations for and what model was being used to", "timestamp": "00:34:26,846", "timestamp_s": 2066.0}, {"text": "make the original loan decision.", "timestamp": "00:34:30,398", "timestamp_s": 2070.0}, {"text": "The topic of explainability also relates to concerns about bias and ethics.", "timestamp": "00:34:33,500", "timestamp_s": 2073.0}, {"text": "Let\u0027s imagine that your model is biased and is unfairly denying", "timestamp": "00:34:38,320", "timestamp_s": 2078.0}, {"text": "loans to certain groups.", "timestamp": "00:34:41,768", "timestamp_s": 2081.0}, {"text": "You\u0027ll have a better chance of discovering that bias if you can", "timestamp": "00:34:44,960", "timestamp_s": 2084.0}, {"text": "explain which data points are contributing most towards its decisions.", "timestamp": "00:34:48,338", "timestamp_s": 2088.0}, {"text": "There\u0027s also a big governance question around being able to say exactly what", "timestamp": "00:34:54,100", "timestamp_s": 2094.0}, {"text": "was training and when. In traditional DevOps, it\u0027s a familiar", "timestamp": "00:34:57,638", "timestamp_s": 2097.0}, {"text": "idea that we\u0027d want to be able to say which version of the software was", "timestamp": "00:35:01,308", "timestamp_s": 2101.0}, {"text": "running at a given point in time and what code it was built from,", "timestamp": "00:35:04,902", "timestamp_s": 2104.0}, {"text": "so that we can delve into that code and build it again if we need", "timestamp": "00:35:08,602", "timestamp_s": 2108.0}, {"text": "to. This can", "timestamp": "00:35:11,562", "timestamp_s": 2111.0}, {"text": "be much more difficult to achieve with mlOps, as it would also require", "timestamp": "00:35:15,018", "timestamp_s": 2115.0}, {"text": "being able to get access to all the data was used to train the model,", "timestamp": "00:35:18,868", "timestamp_s": 2118.0}, {"text": "and likely also being able to reproduce all of the transformations that were", "timestamp": "00:35:22,046", "timestamp_s": 2122.0}, {"text": "performed on the data and the parameters that were used in the training run.", "timestamp": "00:35:25,582", "timestamp_s": 2125.0}, {"text": "Even then, there can be elements of randomness in the training process", "timestamp": "00:35:30,160", "timestamp_s": 2130.0}, {"text": "that can scupper reproducibility. You don\u0027t plan for them.", "timestamp": "00:35:33,986", "timestamp_s": 2133.0}, {"text": "So let\u0027s finish up by summarizing what we\u0027ve learned.", "timestamp": "00:35:40,430", "timestamp_s": 2140.0}, {"text": "MLOPs is a new terrain. ML workflows", "timestamp": "00:35:44,110", "timestamp_s": 2144.0}, {"text": "are more exploratory and data driven than traditional dev workflows.", "timestamp": "00:35:48,182", "timestamp_s": 2148.0}, {"text": "MLOPs enables ML workflows. It provides", "timestamp": "00:35:53,730", "timestamp_s": 2153.0}, {"text": "tools and practices for enabling training runs and experiments", "timestamp": "00:35:57,402", "timestamp_s": 2157.0}, {"text": "that are very data intensive and which use a lot of compute resources.", "timestamp": "00:36:01,882", "timestamp_s": 2161.0}, {"text": "Provides facilities for tracking artifacts produced and", "timestamp": "00:36:07,350", "timestamp_s": 2167.0}, {"text": "operations on data during those training runs.", "timestamp": "00:36:10,632", "timestamp_s": 2170.0}, {"text": "There are MLOps tools specifically for serving machine learning models", "timestamp": "00:36:14,630", "timestamp_s": 2174.0}, {"text": "and specialized strategies for safely rolling out new models for serving", "timestamp": "00:36:18,290", "timestamp_s": 2178.0}, {"text": "in a production environment. There\u0027s also tools", "timestamp": "00:36:21,874", "timestamp_s": 2181.0}, {"text": "and approaches for monitoring models running in a production environment and", "timestamp": "00:36:26,018", "timestamp_s": 2186.0}, {"text": "checking that model performance stays acceptable, or at least", "timestamp": "00:36:30,252", "timestamp_s": 2190.0}, {"text": "that you find out if something does go wrong.", "timestamp": "00:36:33,388", "timestamp_s": 2193.0}, {"text": "So that\u0027s my perspective on the field of mlops right at", "timestamp": "00:36:37,370", "timestamp_s": 2197.0}, {"text": "least as it is right now. Thanks very much for listening.", "timestamp": "00:36:41,068", "timestamp_s": 2201.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'UlmCJl8ZsT4',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Machine Learning in Production: An Intro to MLOps
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Reliably deploying and maintaining machine learning applications is complex. There&rsquo;s a dizzying array of tools and they look different from the usual DevOps tools. </p>
<p>To apply SRE skils to ML, we need to understand the specific challenges of ML build-deploy-monitor workflows. We&rsquo;ll use reference examples to understand the cycle in terms of data prep, training, rollout and monitoring. We&rsquo;ll see that some key challenges relate to training models from slices of large and varying data domains - a problem alien to the mainstream DevOps world.</p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                This session is an introduction to running machine learning in production. The MLops scene is complex and new. It's distinct from mainstream DevOps. Challenges vary by use case. Some use cases have especially advanced challenges.

              </li>
              
              <li>
                Machine learning is different from DevOps because it captures patterns from data. This makes machine learning more applicable to problems that center on data. These differences have implications for how we can best build, deploy, and run machine learning systems.

              </li>
              
              <li>
                A machine learning build journey starts with some data and maybe a question. Get data, clean it, experiment with it, train a model, package the model into something that can serve predictions. There are tools pitched at each stage of this lifecycle for data storage and prep. Platforms can save time and effort stitching together several different tools.

              </li>
              
              <li>
                An ML workflow starts with data. It can be very large and typically needs to be cleaned and processed. That model can be integrated into a running app to serve real time production. Monitoring can need more than one metric depending on the use case.

              </li>
              
              <li>
                In many organizations right now, the complexity is enhanced by challenges from organizational silos. There are special challenges for mlops that are not a normal part of DevOps, at least not right now. This is new territory.

              </li>
              
              <li>
                First into training, then serving, finally rollout and monitoring. There's tools that are pitched, particularly at the training space. Serving solutions often come with support for rollout and some support from monitoring as well.

              </li>
              
              <li>
                This idea of taking the live request and asynchronously sending it elsewhere can also be useful for some monitoring use cases. One thing we might need to detect is an outlier. This is when there's the occasional data point which is significantly outside of the training data distribution. If your use case has risk associated with those, then you might want to detect and track outliers.

              </li>
              
              <li>
                These monitoring and prediction quality concerns also feed into governance for machine learning. Detection for data drift and outliers another thing detectors might be applicable for is adversarial attacks. This relates to the topic of explainability.

              </li>
              
              <li>
                MLOPs enables ML workflows. It provides tools and practices for enabling training runs and experiments that are very data intensive. There are tools and approaches for monitoring models running in a production environment. That's my perspective on the field of mlops right at least as it is right now.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/assemblyai/UlmCJl8ZsT4.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:36,610'); seek(36.0)">
              Machine learning in production this session
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:40,258'); seek(40.0)">
              is an introduction to running machine learning in production, which is
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:44,332'); seek(44.0)">
              being called MlOps. I'm Ryan Dawson and I'm an
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:48,108'); seek(48.0)">
              engineer working on Mlops solutions at Seldon.
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:51,970'); seek(51.0)">
              The MLops scene is complex and new.
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:55,124'); seek(55.0)">
              It's distinct from mainstream DevOps. So we'll start by comparing
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:58,442'); seek(58.0)">
              mlops to DevOps. To understand why it's so different,
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:01:02,100'); seek(62.0)">
              we need to understand how data science is different from programming.
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:01:05,910'); seek(65.0)">
              We'll find out that the difference centers on how data is used.
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:01:09,830'); seek(69.0)">
              When we're clear about that difference, then we'll look at how the build
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:01:13,720'); seek(73.0)">
              deploy monitor workflows for DevOps differ from Mlops.
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:18,970'); seek(78.0)">
              From there, we'll be able to go deeper on particular steps in the mlops
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:23,106'); seek(83.0)">
              build deploy monitor workflow. I'll try to explain
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:26,492'); seek(86.0)">
              that MlOps challenges vary by use case, and that some use cases
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:30,534'); seek(90.0)">
              have especially advanced challenges. Lastly,
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:34,374'); seek(94.0)">
              I'll go into some of the advanced challenges and how they relate to the
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:37,584'); seek(97.0)">
              topic of governance for running machine learning.
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:42,290'); seek(102.0)">
              So before we try to understand mlops, let's make sure we're clear about DevOps.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:47,330'); seek(107.0)">
              As I see it, DevOps is all about making the build deploy monitor
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:51,338'); seek(111.0)">
              workflow for applications as smooth as possible. It tends to
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:55,128'); seek(115.0)">
              focus on CI, CD and infrastructure SRE
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:02:00,014'); seek(120.0)">
              or site reliability engineering. As I see, it is an overlapping role,
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:02:04,510'); seek(124.0)">
              but with a bit more focus on the monitoring stage of the workflows.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:02:10,250'); seek(130.0)">
              This whole workflows is a key enabler for software projects.
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:02:14,250'); seek(134.0)">
              Fortunately, there's some great tools in the space that have become pretty well
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:02:17,452'); seek(137.0)">
              established across the industry, tools like git, Jenkins,
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:02:20,710'); seek(140.0)">
              Docker, Ansible, Prometheus, et cetera.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:26,190'); seek(146.0)">
              MLOPS is in a very different space right now.
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:29,550'); seek(149.0)">
              There's surveys suggesting that 80% to 90% of machine learning
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:32,832'); seek(152.0)">
              models never make it to live, and at least part of that is
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:36,148'); seek(156.0)">
              due to the complexity of running machine learning in production.
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:39,570'); seek(159.0)">
              There's a famous paper called hidden technical debt in machine learning
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:43,076'); seek(163.0)">
              systems, and it explains about all the effort that goes into
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:47,096'); seek(167.0)">
              running production grade machine training systems. It has
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:50,568'); seek(170.0)">
              a diagram with boxes showing the relative size of different tasks,
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:54,206'); seek(174.0)">
              and there's this tiny little box for ML code and
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:57,452'); seek(177.0)">
              really big boxes for data collection, data processing,
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:03:01,122'); seek(181.0)">
              runtime, infrastructure monitoring.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:03:06,650'); seek(186.0)">
              The Linux foundation for AI have tried to help by producing a diagram of
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:03:10,608'); seek(190.0)">
              the whole mlops tool landscape. It's great, but it
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:03:14,208'); seek(194.0)">
              has loads of tools in loads of sections, and even
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:03:17,632'); seek(197.0)">
              the section titles won't make much sense for newcomers to mlops.
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:21,730'); seek(201.0)">
              But let's try to understand more about the fundamentals of Mlops and where it's coming
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:25,716'); seek(205.0)">
              from.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:28,770'); seek(208.0)">
              Fundamentally, MLops is different from DevOps because machine learning is
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:32,388'); seek(212.0)">
              different from programming. Traditional programming codifies rules
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:36,238'); seek(216.0)">
              explicitly, rules that say how to respond to inputs.
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:40,470'); seek(220.0)">
              Machine learning does not codify explicitly. Instead,
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:43,976'); seek(223.0)">
              rules are set indirectly by capturing patterns from data and
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:47,996'); seek(227.0)">
              reapplying the extracted patterns to new input data.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:52,410'); seek(232.0)">
              This makes machine learning more applicable to problems that center on data,
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:56,172'); seek(236.0)">
              especially focused numerical problems.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:04:00,750'); seek(240.0)">
              So with traditional programming, we've got applications that respond
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:04:03,942'); seek(243.0)">
              directly to user inputs, such as terminal systems or GUI
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:04:07,622'); seek(247.0)">
              based systems. You code these by starting with hello
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:04:11,156'); seek(251.0)">
              world and adding more control structures.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:04:14,290'); seek(254.0)">
              Data science problems fall into classification problems.
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:04:17,668'); seek(257.0)">
              Regression problems classification
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:21,354'); seek(261.0)">
              problems put data into categories. An example would be,
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:25,352'); seek(265.0)">
              is this image a cat or not a cat?
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:29,430'); seek(269.0)">
              Regression problems look for numerical output, for example,
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:32,792'); seek(272.0)">
              predicting sales revenue from how advertising spend is
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:35,944'); seek(275.0)">
              directed. The hello world of data science
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:39,218'); seek(279.0)">
              is the mnist dataset, which is a data set
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:42,492'); seek(282.0)">
              of handwritten digits. And the problem is to categorize each
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:45,740'); seek(285.0)">
              handwritten sample correctly as the number that it represents.
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:51,790'); seek(291.0)">
              When I think of machine learning as capturing patterns from data,
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:55,856'); seek(295.0)">
              I think about fitting for regression problems basically
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:59,360'); seek(299.0)">
              have data points on a graph, and you draw a line through the
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:05:03,104'); seek(303.0)">
              data points and try to get the line as close to as many of the
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:05:06,484'); seek(306.0)">
              data points as possible. The distance from each data
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:05:10,116'); seek(310.0)">
              point to the line is called the error, and you keep adjusting the
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:05:13,828'); seek(313.0)">
              equation of the line to minimize the total error.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:05:17,670'); seek(317.0)">
              The coefficients of the equation of the line correspond to the weights of a
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:21,624'); seek(321.0)">
              machine learning model, and you then use that to make new predictions.
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:28,510'); seek(328.0)">
              Of course, the machine learning training process is more complex than the way I'm explaining
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:32,538'); seek(332.0)">
              it. For example, there's more to the process of adjusting
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:36,154'); seek(336.0)">
              the weights than to try to get the line to fit the data.
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:40,710'); seek(340.0)">
              It's done promogrammatically by using an algorithm called
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:43,944'); seek(343.0)">
              gradient descent. Essentially randomly pick
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:47,672'); seek(347.0)">
              a way to shift the line, but it's only pseudoram, as it will
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:51,208'); seek(351.0)">
              take a step in a given direction and then check whether that
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:54,540'); seek(354.0)">
              reduced the error before deciding whether to keep going that way or go
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:57,788'); seek(357.0)">
              a different direction. That step size
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:06:01,692'); seek(361.0)">
              can be tweaked and you can get different results,
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:06:05,042'); seek(365.0)">
              so the overall process is tunable.
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:06:10,510'); seek(370.0)">
              So basically, data scientists are looking for patterns in data and trying to find
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:06:14,576'); seek(374.0)">
              which methods are best for capturing those patterns in models.
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:06:18,530'); seek(378.0)">
              This is an exploratory process, and the tools data scientists
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:06:22,042'); seek(382.0)">
              use reflect this. Jupyter notebooks, for example, are great for
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:26,228'); seek(386.0)">
              playing around with slices of data and visualizing patterns.
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:32,180'); seek(392.0)">
              These differences between programming and machine learning have implications for
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:35,658'); seek(395.0)">
              how we can best build, deploying, and run machine learning
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:39,530'); seek(399.0)">
              systems. So let's get into more detail about how different
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:43,146'); seek(403.0)">
              these build deploy monitoring journeys are.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:48,330'); seek(408.0)">
              Let's go on an imaginary development journey. We can
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:51,772'); seek(411.0)">
              start with a user story. Let's say we're building
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:55,180'); seek(415.0)">
              a calculator and our user story says that our lazy users want to
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:59,168'); seek(419.0)">
              put numerical operations into a screen so they don't have
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:07:02,608'); seek(422.0)">
              to work out the answers.
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:07:06,860'); seek(426.0)">
              We could write a Java program to satisfy the story,
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:07:11,040'); seek(431.0)">
              compile it, and distribute it as a
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:07:14,578'); seek(434.0)">
              binary. But this
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:07:18,322'); seek(438.0)">
              is 2020, so we'll more likely package the code to
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:07:22,002'); seek(442.0)">
              run as a web server so that users will interact with it via
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:25,548'); seek(445.0)">
              a browser. Most likely we'll also dockerize
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:29,132'); seek(449.0)">
              the web app and run it on some cloud infrastructure.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:35,850'); seek(455.0)">
              Now let's think of a machine learning build journey.
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:39,610'); seek(459.0)">
              This is more likely to start with some data and maybe a question.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:43,870'); seek(463.0)">
              Let's say we've got data on employees and their experience and
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:47,312'); seek(467.0)">
              skills and salaries, and we want to see whether we
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:51,088'); seek(471.0)">
              could clean whether we could use it to benchmark salaries
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:55,082'); seek(475.0)">
              for other employees during a pay review.
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:58,850'); seek(478.0)">
              Let's assume the data is already available and clean, though this
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:08:02,708'); seek(482.0)">
              is a pretty big assumption. But let's assume
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:08:05,966'); seek(485.0)">
              we've got good data and we can create a regression models that maps employee
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:08:09,966'); seek(489.0)">
              experience to pay,
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:08:13,350'); seek(493.0)">
              maybe using scikitlearn.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:08:16,650'); seek(496.0)">
              So we train the model and then
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:08:19,868'); seek(499.0)">
              it can be used to make a production for any given
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:08:23,628'); seek(503.0)">
              employee a
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:27,468'); seek(507.0)">
              prediction about what the salary benchmark would be.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:30,830'); seek(510.0)">
              So let's say we give our predictions for a particular set of employees
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:34,998'); seek(514.0)">
              to the business and they're happy with that.
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:38,510'); seek(518.0)">
              So happy that they want to use it again next year or more
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:42,068'); seek(522.0)">
              regularly. Then our situation changes.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:45,508'); seek(525.0)">
              Because then what we want isn't just a prediction but
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:49,508'); seek(529.0)">
              a predict function as
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:54,542'); seek(534.0)">
              we might not want to have to rerun the training process every time the
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:57,858'); seek(537.0)">
              business has some new employees to check.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:09:02,640'); seek(542.0)">
              This problem would be magnified if another department says that they want to
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:09:06,418'); seek(546.0)">
              make predictions too. Actually, that would add extra complication
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:09:10,876'); seek(550.0)">
              as even if we know that the patterns from our
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:09:14,118'); seek(554.0)">
              training data are applicable to our department, we don't necessarily
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:09:17,916'); seek(557.0)">
              know about the new department. But let's assume
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:09:21,712'); seek(561.0)">
              that it is applicable. Then our main problem is a problem
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:09:25,322'); seek(565.0)">
              of scaling. How do we make all these predictions
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:28,832'); seek(568.0)">
              without burning ourselves out?
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:32,040'); seek(572.0)">
              Probably we're going to be interested in using the machine learning model in
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:35,502'); seek(575.0)">
              a web app.
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:38,540'); seek(578.0)">
              So maybe we add a rest API around our python code and
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:42,318'); seek(582.0)">
              look to run it as a web application. We might naturally package
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:45,828'); seek(585.0)">
              it in a docker container like we would for a traditional web app.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:50,640'); seek(590.0)">
              This is a valid and common approach, but it's just one approach
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:53,832'); seek(593.0)">
              with machine learning deploying it does present a challenge
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:57,752'); seek(597.0)">
              about how to dockerize the predict function without including the training data in the docker
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:10:02,028'); seek(602.0)">
              image. So it's also common to
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:10:05,478'); seek(605.0)">
              separate the model from the data by taking
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:10:08,806'); seek(608.0)">
              the Python variable for the model production and
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:10:12,346'); seek(612.0)">
              serializing that to a file using Python pickling.
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:10:16,600'); seek(616.0)">
              Then the file can be loaded into another training Python application server.
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:10:21,720'); seek(621.0)">
              So if we load the model into a suitable Python
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:10:25,088'); seek(625.0)">
              web server app, then we can serve predictions that way.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:29,900'); seek(629.0)">
              This varies a little from framework to framework, and can vary
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:33,588'); seek(633.0)">
              quite a lot if the language is not Python.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:37,280'); seek(637.0)">
              But basically this is a good picture for the machine learning lifecycle.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:41,320'); seek(641.0)">
              Get data, clean it, experiment with it, train a model,
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:45,522'); seek(645.0)">
              package the model into something that can serve predictions.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:50,340'); seek(650.0)">
              And there are tools pitched at each stage of this lifecycle
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:54,420'); seek(654.0)">
              for data storage and prep. There's tools like s three and
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:58,486'); seek(658.0)">
              Hadoop training can use
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:11:02,374'); seek(662.0)">
              a lot of compute resource and take a long time.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:11:05,642'); seek(665.0)">
              So there's tools that help with running long running training jobs,
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:11:09,136'); seek(669.0)">
              and also tools for training the operations performed during training.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:11:15,100'); seek(675.0)">
              There are tools specifically aimed at helping make batch
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:11:18,852'); seek(678.0)">
              predictions on a regular cycle,
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:11:23,500'); seek(683.0)">
              say for just getting predictions every month or whatever the cycle
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:11:26,952'); seek(686.0)">
              is that the business works to.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:11:29,760'); seek(689.0)">
              Or predictions could be needed at any time.
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:11:33,314'); seek(693.0)">
              And then there's tools for real time serving of
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:37,074'); seek(697.0)">
              predictions using a rest API.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:40,980'); seek(700.0)">
              Some real time serving tools are specific to the framework
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:44,300'); seek(704.0)">
              and some SRe more general. I personally work on seldon
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:48,028'); seek(708.0)">
              core, which is a framework agnostic open source serving
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:51,568'); seek(711.0)">
              tool. The seldon team also collaborates on another tool
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:55,386'); seek(715.0)">
              called KF serving. Both of these are part
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:58,762'); seek(718.0)">
              of the Kubeflow ecosystem, which is an end to end
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:12:02,282'); seek(722.0)">
              platform. That's another space of tools,
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:12:05,760'); seek(725.0)">
              end to end platforms that try to join up the whole journey.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:12:09,660'); seek(729.0)">
              Platforms can save you the effort of stitching together several different
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:12:13,566'); seek(733.0)">
              tools, but platforms are also opinionated
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:12:17,496'); seek(737.0)">
              can be, so they don't necessarily fit every use
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:12:20,802'); seek(740.0)">
              case. I'm listing these types of tools because I think
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:12:24,418'); seek(744.0)">
              it helps to divide the machine training lifecycle up like this into
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:12:27,922'); seek(747.0)">
              data prep, training and serving.
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:30,980'); seek(750.0)">
              This helps us make sense of the concept landscape of mlops
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:34,348'); seek(754.0)">
              tools out there, as we can then put them into categories mapped to the
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:38,214'); seek(758.0)">
              lifecycle. There's also the monitoring part of
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:41,594'); seek(761.0)">
              the lifecycle, but we'll get to that later.
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:46,920'); seek(766.0)">
              For now, the key point to see is that mlops is different from DevOps,
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:50,624'); seek(770.0)">
              mostly because of the role of data. In particular,
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:54,430'); seek(774.0)">
              models are built by extracting patterns from data using
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:58,222'); seek(778.0)">
              code, so that the training data is
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:13:02,460'); seek(782.0)">
              a key part of the model. The training
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:13:06,130'); seek(786.0)">
              data volumes can be large,
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:13:09,490'); seek(789.0)">
              and that leads to complexity in storing and processing the data,
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:13:13,360'); seek(793.0)">
              which there's specialized tools to help with.
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:13:18,880'); seek(798.0)">
              You also get different toolkits for building machine learning models,
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:13:22,636'); seek(802.0)">
              which results in models for different formats and adds
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:13:26,108'); seek(806.0)">
              some complexity to the space of tools for getting predictions out of models,
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:13:30,044'); seek(810.0)">
              space called serving. So the complexity of the
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:13:33,514'); seek(813.0)">
              way the ML build deploying monitor lifecycle uses
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:36,608'); seek(816.0)">
              data has knock on effects to the tool landscape.
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:40,920'); seek(820.0)">
              We've not talked about the post deployment stage yet,
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:43,898'); seek(823.0)">
              but there's also complexity there. For example,
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:47,310'); seek(827.0)">
              you can sometimes need to retrain your model,
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:50,622'); seek(830.0)">
              your running model, not because of any bugs in it, but because the data
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:54,446'); seek(834.0)">
              coming in from the outside world changes.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:58,080'); seek(838.0)">
              Think, for example, of how fashion is seasonal.
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:14:01,288'); seek(841.0)">
              Let's say you've got a model trained to recommend clothes for an online fashion
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:14:05,128'); seek(845.0)">
              store, and you trained it based on purchases made in winter.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:14:10,080'); seek(850.0)">
              Then it might perform great in winter and make lots of money.
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:14:13,494'); seek(853.0)">
              But when it comes to summer, it's still going to be recommending coats when
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:14:17,382'); seek(857.0)">
              people are looking for summer clothes.
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:14:20,500'); seek(860.0)">
              So you would need to be regularly updating the model with new data and
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:14:24,346'); seek(864.0)">
              ideally checking that it's leading to sales. That's a
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:14:28,394'); seek(868.0)">
              complex you don't normally get with traditional software.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:14:31,504'); seek(871.0)">
              These complexities about handling data, they ripple
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:35,076'); seek(875.0)">
              all the way through the whole mlops lifecycle.
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:38,940'); seek(878.0)">
              We've talked about this at a high level so far, but let's now think
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:42,478'); seek(882.0)">
              about the individual steps of the workflow and the tools used in them.
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:50,120'); seek(890.0)">
              So let's just remind ourselves of the workflows steps with traditional
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:53,792'); seek(893.0)">
              DevOps. We'll start with the user story
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:57,354'); seek(897.0)">
              specifying a business need. From that a developer will write
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:15:00,926'); seek(900.0)">
              code and submit a pull request. Hopefully test will
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:15:04,862'); seek(904.0)">
              run automatically on the pull request. Somebody will
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:15:08,046'); seek(908.0)">
              review it and merge. It gets to merged to master there,
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:15:11,966'); seek(911.0)">
              our pipeline will build a new version of the app and deploy that to
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:15:15,602'); seek(915.0)">
              the test environment. Perhaps further tests
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:15:19,768'); seek(919.0)">
              will be run and it'll get promoted to the next environment where there
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:15:23,778'); seek(923.0)">
              might be more deeper tests, and then it'll go to
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:15:27,062'); seek(927.0)">
              production. And in production we'll monitor for anything going
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:15:30,742'); seek(930.0)">
              wrong, probably in the form of stack traces or error codes.
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:15:35,940'); seek(935.0)">
              The pipeline producing these builds and running the tests will most likely be
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:39,258'); seek(939.0)">
              a CI system like Jenkins. The driver for the pipeline
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:42,912'); seek(942.0)">
              will most likely be a code change in git. The artifact
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:46,752'); seek(946.0)">
              we'll be promoting will probably be an executable inside a
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:50,494'); seek(950.0)">
              docker image.
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:54,780'); seek(954.0)">
              ML workflows are different. The driver for
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:58,350'); seek(958.0)">
              automation might be a code change, or it might be new data,
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:16:02,062'); seek(962.0)">
              and the data probably won't be in git as git isn't a great store for
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:16:05,778'); seek(965.0)">
              data getting into the gigabytes,
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:16:09,600'); seek(969.0)">
              the workflows are more experimental and data driven.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:16:12,960'); seek(972.0)">
              You start with a data set and need to experiment to
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:16:16,258'); seek(976.0)">
              find usable patterns in the data set that can
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:16:19,398'); seek(979.0)">
              be captured in a model. When you've got a model, then it
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:16:22,934'); seek(982.0)">
              might not be enough to just check it for past fail conditions and monitor for
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:16:26,182'); seek(986.0)">
              errors like you would. Traditional software likely have
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:16:29,558'); seek(989.0)">
              to check how well it performs against the data. In numerical terms,
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:16:33,240'); seek(993.0)">
              there can be quite a bit of variation with ML workflows.
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:16:36,600'); seek(996.0)">
              One major point of variation is whether the model is trained offline or
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:40,382'); seek(1000.0)">
              online. With online learning,
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:43,740'); seek(1003.0)">
              a model is constantly being updated by adjusting itself through each
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:47,502'); seek(1007.0)">
              new data point that it sees. So every prediction it makes also adjusts
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:51,524'); seek(1011.0)">
              the model. Whereas with offline learning, the training is
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:54,978'); seek(1014.0)">
              done separately from prediction. You train the model and deploying
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:58,408'); seek(1018.0)">
              it, and when you want to update the model, you need to train a new
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:17:01,122'); seek(1021.0)">
              one. We have to pick somewhere to
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:17:04,838'); seek(1024.0)">
              focus, and offline learning is probably the more common case. So let's
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:17:08,428'); seek(1028.0)">
              focus on offline training workflows.
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:17:13,540'); seek(1033.0)">
              As we've talked about already, an ML workflow starts with data.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:17:17,462'); seek(1037.0)">
              It can be very large and typically needs to be cleaned and processed.
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:17:21,640'); seek(1041.0)">
              A slice of that data can be taken so that the data scientists can
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:17:25,194'); seek(1045.0)">
              work with it locally to explore the data on their own machine.
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:17:30,520'); seek(1050.0)">
              When the data scientist has started to make some progress, then they might
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:17:34,622'); seek(1054.0)">
              move to a hosted training environment to run some longer running experiments
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:17:39,012'); seek(1059.0)">
              on a larger sample of the data. There will
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:42,578'); seek(1062.0)">
              likely be collaboration with other data scientists, most likely using
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:46,002'); seek(1066.0)">
              Jupyter notebooks. The artifact produced will
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:49,762'); seek(1069.0)">
              be a model, commonly a model that's pickled
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:53,592'); seek(1073.0)">
              or serialized to a file. That model can be
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:57,014'); seek(1077.0)">
              integrated into a running app to serve real time production through HTTP.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:18:03,380'); seek(1083.0)">
              There will probably be a consumer of those predictions, which may be
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:18:07,078'); seek(1087.0)">
              another app, perhaps a traditional web app. So you may need to integration
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:18:11,152'); seek(1091.0)">
              test the SRE model against the consumer.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:18:14,760'); seek(1094.0)">
              And when you roll out the model to a production, you want to
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:18:18,218'); seek(1098.0)">
              monitor the model by picking some metrics that represent how well performing
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:18:22,516'); seek(1102.0)">
              against the live data,
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:18:27,260'); seek(1107.0)">
              the rollout and monitoring phases of the workflow can be linked.
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:18:31,268'); seek(1111.0)">
              An example might help to understand this. Say we've
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:18:35,124'); seek(1115.0)">
              got an online store with ecommerce. A common way to
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:18:39,138'); seek(1119.0)">
              roll out new versions of a model is an A B test.
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:18:43,360'); seek(1123.0)">
              With an A B test, you'd have a live version that's already
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:46,770'); seek(1126.0)">
              running and that's called the control. And then you run
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:50,502'); seek(1130.0)">
              other versions alongside it. Let's call them version a and version b.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:55,060'); seek(1135.0)">
              So we're running three versions of the model in parallel, each training a
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:58,854'); seek(1138.0)">
              bit differently to see which gives the best results.
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:19:02,680'); seek(1142.0)">
              You can do that by splitting the traffic between the versions
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:19:06,256'); seek(1146.0)">
              to minimize the risk. We'd send most of the traffic to the control version.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:19:11,560'); seek(1151.0)">
              A subset of the traffic will go to a and to b, and we'll
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:19:15,828'); seek(1155.0)">
              run that splitting process for a while until we've got a statistically significant
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:19:20,132'); seek(1160.0)">
              sample. Let's say that variation a has the highest
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:19:23,892'); seek(1163.0)">
              conversion rate, so a higher proportion of the recommendations
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:19:27,416'); seek(1167.0)">
              lead to sales. That's a useful metric, and it
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:19:31,058'); seek(1171.0)">
              might be enough for us to choose variation a, but it
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:19:34,722'); seek(1174.0)">
              might not be the only metric. These situations can get
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:19:38,182'); seek(1178.0)">
              complex. For example, it might be that model a
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:19:41,862'); seek(1181.0)">
              is recommending controversial products. So some customers might really
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:19:45,862'); seek(1185.0)">
              like the recommendations and buy the products, but other customers are
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:49,478'); seek(1189.0)">
              really put off and they just go to a different website.
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:54,040'); seek(1194.0)">
              So there are trade offs to consider, and monitoring
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:57,872'); seek(1197.0)">
              can need more than one metric depending on the use case.
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:20:03,760'); seek(1203.0)">
              So we're seeing that MLOPs is complex, and in many organizations
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:20:07,868'); seek(1207.0)">
              right now, the complexity is enhanced by challenges from
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:20:11,206'); seek(1211.0)">
              organizational silos. You can find data scientists
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:20:15,052'); seek(1215.0)">
              that work just in a world of Jupyter notebooks and model accuracy
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:20:19,072'); seek(1219.0)">
              on training data that then gets handed over to
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:20:22,538'); seek(1222.0)">
              a traditional DevOps team with the expectation they'll
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:20:25,808'); seek(1225.0)">
              be able to take this work and build it into a production system.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:20:29,660'); seek(1229.0)">
              Without proper context. The traditional DevOps team is likely
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:20:32,804'); seek(1232.0)">
              to look at those notebooks and just react like, what is this stuff?
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:20:40,590'); seek(1240.0)">
              In a more mature setup, you might have better understood handoffs.
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:20:44,262'); seek(1244.0)">
              For example, you might have data engineers who deal with
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:20:47,508'); seek(1247.0)">
              obtaining the data and getting it into the right state for the data scientists.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:20:51,890'); seek(1251.0)">
              Once the data is ready for the data scientists, then they can take over and
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:20:55,828'); seek(1255.0)">
              build the models. And from there, data science will have an
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:59,128'); seek(1259.0)">
              understood handoff to ML engineers. And the ML engineers
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:21:03,422'); seek(1263.0)">
              might still be a DevOps team, but a DevOps team that knows about the context
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:21:06,862'); seek(1266.0)">
              of this particular machine learning application and knows how to run it in
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:21:10,204'); seek(1270.0)">
              the production.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:21:14,290'); seek(1274.0)">
              This is new territory. There are special challenges for mlops
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:21:18,042'); seek(1278.0)">
              that are not a normal part of DevOps, at least not right now.
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:21:22,290'); seek(1282.0)">
              Now that we've got a high level understanding of where MLOPs is coming
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:21:25,512'); seek(1285.0)">
              from, we can next go into more detail on particular MLOps topics.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:21:29,910'); seek(1289.0)">
              So let's take these in order and go first into training,
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:21:32,744'); seek(1292.0)">
              then serving, finally rollout and monitoring.
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:21:38,090'); seek(1298.0)">
              So there's tools that are pitched, particularly at the training space,
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:21:41,452'); seek(1301.0)">
              to name a few examples. There's Kubeflow pipelines,
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:21:44,498'); seek(1304.0)">
              MLflow Polyaxon.
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:21:47,550'); seek(1307.0)">
              These are all about making it easy to run long running training jobs
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:21:50,758'); seek(1310.0)">
              on a hosted environment. Typically, that means providing
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:21:55,190'); seek(1315.0)">
              some manifest that specifies which steps sre to be done and
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:21:58,388'); seek(1318.0)">
              in which order. That's a manifest for a training
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:22:02,036'); seek(1322.0)">
              pipeline. As an example,
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:22:06,228'); seek(1326.0)">
              a pipeline might have as its step an action to
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:22:09,892'); seek(1329.0)">
              download data from wherever it's stored. That could be the first step.
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:22:14,356'); seek(1334.0)">
              Then it gets split into training and validation data.
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:22:17,910'); seek(1337.0)">
              The training data will then be used to train the model, and the validation
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:22:21,422'); seek(1341.0)">
              data will be used as a check on the quality of the model's predictions.
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:22:26,330'); seek(1346.0)">
              When we check the quality of the predictions, we'll want to record those checks
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:22:30,178'); seek(1350.0)">
              somewhere and ideally also have an automated way
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:22:33,420'); seek(1353.0)">
              to decide whether we should consider this as a good model or not.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:22:37,710'); seek(1357.0)">
              If we do consider it a good model, then we'll probably want to serialize it
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:22:41,584'); seek(1361.0)">
              so that the serialized model would be available for promotion to
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:22:44,768'); seek(1364.0)">
              a running environment. This is
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:22:47,968'); seek(1367.0)">
              probably sounding rather like continuous integration pipelines. It is
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:22:51,492'); seek(1371.0)">
              similar, but also different. The difference can be seen in the specialized
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:22:55,322'); seek(1375.0)">
              tools dedicated to training. One tool
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:22:59,092'); seek(1379.0)">
              for handling training is Kubeflow pipelines.
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:23:02,070'); seek(1382.0)">
              In Kubeflow pipelines, you can define your pipeline with all its steps,
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:23:06,270'); seek(1386.0)">
              and also visualize it and watch it progress and see any steps that fail.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:23:10,630'); seek(1390.0)">
              But the pipelines aren't only called pipelines,
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:23:14,066'); seek(1394.0)">
              they're also called experiments, and they're parameterized,
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:23:17,930'); seek(1397.0)">
              so there's options in its UI where you can enter parameters.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:23:21,690'); seek(1401.0)">
              Remember I mentioned before that the process can be
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:23:25,052'); seek(1405.0)">
              tunable. There are tunable parameters on training, such as
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:23:29,008'); seek(1409.0)">
              the step size, so you can kick off runs
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:23:32,518'); seek(1412.0)">
              in parallel of the same pipeline using different parameters to
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:23:36,144'); seek(1416.0)">
              see which parameters might result in the best model.
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:23:40,750'); seek(1420.0)">
              Cube flow pipelines is not alone in having this idea of being able to kick
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:23:44,218'); seek(1424.0)">
              off runs of an experiment with different parameters.
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:23:47,010'); seek(1427.0)">
              MLflow, for example, uses the same terminology and has
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:23:50,548'); seek(1430.0)">
              a similar interface. So there's
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:23:54,558'); seek(1434.0)">
              similarity here with traditional CI systems, as the training platforms
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:57,918'); seek(1437.0)">
              execute a series of steps and an artifact gets built.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:24:01,510'); seek(1441.0)">
              But it's different, as you've also got this idea of running experiments with
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:24:05,388'); seek(1445.0)">
              different parameters to see which is best.
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:24:08,570'); seek(1448.0)">
              That means you have to have a definition of which is good,
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:24:12,188'); seek(1452.0)">
              which is the best model,
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:24:14,890'); seek(1454.0)">
              whereas traditionally with continuous integration you would just be building from master,
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:24:19,078'); seek(1459.0)">
              and if it passes the tests, then you're good to promote.
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:24:22,910'); seek(1462.0)">
              But so long as you can automate, which counts as the best model,
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:24:26,592'); seek(1466.0)">
              then your training can build an artifact from a promotion, much like
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:24:30,292'); seek(1470.0)">
              with CI. And sometimes these CI systems do have integrations
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:24:34,314'); seek(1474.0)">
              to rather, sometimes these training systems do have integrations
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:24:38,810'); seek(1478.0)">
              available to CI systems let's
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:24:43,914'); seek(1483.0)">
              say we've got a way of building our model and we want to be able
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:24:46,488'); seek(1486.0)">
              to serve it. So we want to make predictions available in real time
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:24:50,296'); seek(1490.0)">
              via HTTP, perhaps using a rest API. We might
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:24:54,492'); seek(1494.0)">
              use a serving solution, as there's a range of them
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:57,772'); seek(1497.0)">
              out there, some that are particular to a machine learning toolkit
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:25:01,602'); seek(1501.0)">
              such as Tensorflow serving, Tensorflow or Torch SRE
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:25:05,474'); seek(1505.0)">
              or Pytorch. There's also serving solutions provided by cloud providers
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:25:09,542'); seek(1509.0)">
              as well, some that are more toolkit agnostic.
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:25:15,150'); seek(1515.0)">
              For example, there's the toolkit agnostic open source offering that
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:25:18,868'); seek(1518.0)">
              I work on. Seldom. Typically,
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:25:21,994'); seek(1521.0)">
              serving solutions use the idea of a model being packaged and hosted,
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:25:25,690'); seek(1525.0)">
              perhaps in a storage bucket or a disk location,
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:25:29,570'); seek(1529.0)">
              so the serving solution can then obtain the model from that location and
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:25:33,048'); seek(1533.0)">
              run it. Serving solutions often come with support for rollout
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:25:36,702'); seek(1536.0)">
              and some support from monitoring as well.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:25:41,030'); seek(1541.0)">
              As an example of a serving solution, I'll explain the concept
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:25:44,338'); seek(1544.0)">
              behind Seldon and how it's used. Seldon is aimed
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:25:47,858'); seek(1547.0)">
              in particular at serving on kubernetes, and the models
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:25:51,138'); seek(1551.0)">
              are served by creating a Kubernetes custom resource. The manifest
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:25:55,874'); seek(1555.0)">
              of the custom resource is designed to make it simple to plug in a
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:25:59,168'); seek(1559.0)">
              URI to a storage bucket containing a serialized model.
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:26:02,990'); seek(1562.0)">
              So at a minimum, you can just put in the URI to the storage bucket
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:26:06,550'); seek(1566.0)">
              and specify which toolkit was used to build
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:26:09,732'); seek(1569.0)">
              the model. Then you submit that manifest to Kubernetes
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:26:13,738'); seek(1573.0)">
              and it will create the lower level Kubernetes resources necessary
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:26:17,546'); seek(1577.0)">
              to expose an API and serve the model's HTTP traffic.
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:26:22,130'); seek(1582.0)">
              There's also a docker option to serve a model from a custom image.
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:26:26,030'); seek(1586.0)">
              I'm emphasizing the serialized or pickled models in this talk,
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:26:29,512'); seek(1589.0)">
              mostly because it's common to see those with serving solutions,
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:26:33,086'); seek(1593.0)">
              and it's not very common outside of the mlops space.
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:26:39,890'); seek(1599.0)">
              The serving stage links into rollout and monitoring I've
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:26:43,498'); seek(1603.0)">
              talked a little bit already about a b testing as a rollout strategy.
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:26:47,450'); seek(1607.0)">
              With that strategy, the traffic during the rollout is split between
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:26:50,776'); seek(1610.0)">
              different versions, and you monitor that over a period of time until
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:26:54,296'); seek(1614.0)">
              you've got enough data to be able to decide which is best.
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:26:58,710'); seek(1618.0)">
              There's a more simple rollout strategy, which also involves splitting the traffic
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:27:02,338'); seek(1622.0)">
              between different versions. With the Canary strategy,
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:27:05,970'); seek(1625.0)">
              you split traffic between the live version of a model and a new version
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:27:09,730'); seek(1629.0)">
              that you're evaluating. But typically with a canary,
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:27:13,554'); seek(1633.0)">
              you just have one new model, and you evaluate it over a shorter
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:27:16,854'); seek(1636.0)">
              period of time than with the A B test. It's more of
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:27:20,448'); seek(1640.0)">
              a sanity check than an in depth evaluation,
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:27:23,318'); seek(1643.0)">
              and you just promote if everything looks okay.
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:27:28,210'); seek(1648.0)">
              Another strategy is shadowing.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:27:31,010'); seek(1651.0)">
              With shadowing, all of the traffic goes to both the new and the
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:27:34,708'); seek(1654.0)">
              old model, but it's only the responses from the older
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:27:38,138'); seek(1658.0)">
              model, the live model's responses that are used and which go back to
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:27:42,008'); seek(1662.0)">
              the consumer. The new model is called the shadow version,
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:27:45,758'); seek(1665.0)">
              and its responses are just stored. They don't go back to any live consumers.
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:27:50,550'); seek(1670.0)">
              The reason for doing this is to monitor the shadow and compare it against the
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:27:54,108'); seek(1674.0)">
              live version so it makes sense to be storing the shadow's output
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:27:58,258'); seek(1678.0)">
              for later evaluation.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:28:02,010'); seek(1682.0)">
              Serving solutions have some support for rollout strategies. In the
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:28:05,968'); seek(1685.0)">
              case of Seldon, for example, you can create a Kubernetes manifest
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:28:09,350'); seek(1689.0)">
              with two sections, one for the main model and one for the Canary.
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:28:13,366'); seek(1693.0)">
              The traffic will automatically be split between these two models by
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:28:17,808'); seek(1697.0)">
              default. Seldom will split traffic evenly between the models. In a manifest,
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:28:21,890'); seek(1701.0)">
              you can set a field against each model called traffic.
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:28:25,002'); seek(1705.0)">
              That field takes a numeric percentage that tells seldom how much
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:28:28,212'); seek(1708.0)">
              of the traffic each model should get.
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:28:34,050'); seek(1714.0)">
              Each rollout strategies involve gathering metrics on running models.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:28:38,254'); seek(1718.0)">
              With seldom, there's out of the box integration available for Prometheus,
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:28:41,854'); seek(1721.0)">
              and some Grafana dashboards are provided.
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:28:44,970'); seek(1724.0)">
              These cover general metrics like frequency of requests and latency.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:28:49,130'); seek(1729.0)">
              You may also want to monitor for metrics that are specific to your use case,
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:28:53,164'); seek(1733.0)">
              and there are defined interfaces so that extra metrics can be exposed in
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:28:56,768'); seek(1736.0)">
              the usual Prometheus way.
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:29:01,390'); seek(1741.0)">
              I mentioned earlier that in the shadow use case, you might want to be
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:29:04,736'); seek(1744.0)">
              recording the predictions that the shadow is making so that you can compare its performance
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:29:08,922'); seek(1748.0)">
              against the live model. This can be handled through
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:29:12,372'); seek(1752.0)">
              logging all of the requests and responses to a database.
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:29:16,850'); seek(1756.0)">
              There are other use cases as well where recording all predictions can be
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:29:20,488'); seek(1760.0)">
              useful. For example, if you're working in a compliance heavy industry
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:29:24,462'); seek(1764.0)">
              and an auditor requires to know of every prediction that's been made
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:29:28,950'); seek(1768.0)">
              in the shadow use case, you'd use that database then
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:29:32,648'); seek(1772.0)">
              to run queries against the data and compare the shadow's performance against
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:29:36,172'); seek(1776.0)">
              that of the live model.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:29:38,970'); seek(1778.0)">
              In the case of Seldon, there's an out of the box integration
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:29:43,450'); seek(1783.0)">
              which provides a way to asynchronously log everything to elasticsearch
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:29:48,110'); seek(1788.0)">
              so that everything can then be made available for running queries on later, but without
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:29:51,856'); seek(1791.0)">
              slowing down the request path of the live models.
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:29:57,830'); seek(1797.0)">
              This idea of taking the live request and asynchronously sending
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:30:01,298'); seek(1801.0)">
              it elsewhere can also be useful for some monitoring use cases,
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:30:05,282'); seek(1805.0)">
              and not just for audit. In particular, there's some
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:30:08,828'); seek(1808.0)">
              advanced monitoring use cases that relate to the data that's coming
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:30:12,188'); seek(1812.0)">
              into the live model, how well it matches to the training data.
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:30:17,230'); seek(1817.0)">
              If the live data doesn't fall within the distribution of the training data,
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:30:21,008'); seek(1821.0)">
              then you can't be sure that your model will perform well on that data.
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:30:24,930'); seek(1824.0)">
              Your model is based on patterns from the training data.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:30:28,370'); seek(1828.0)">
              So data that doesn't fit that training distribution might have different patterns.
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:30:33,970'); seek(1833.0)">
              One thing we can do about this is to send all the request data
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:30:37,316'); seek(1837.0)">
              through to detector components that will look for anything that
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:30:40,968'); seek(1840.0)">
              might be going wrong so that we can flag those predictions if we need to.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:30:45,670'); seek(1845.0)">
              So let's drill a bit, a little bit further into what we might need to
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:30:48,828'); seek(1848.0)">
              detect.
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:30:52,090'); seek(1852.0)">
              One thing we might need to detect is an outlier. This is when
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:30:55,852'); seek(1855.0)">
              there's the occasional data point which is significantly outside of the training
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:30:59,772'); seek(1859.0)">
              data distribution, even though most of the data does fall
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:31:03,552'); seek(1863.0)">
              within the distribution. Sometimes models express
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:31:07,222'); seek(1867.0)">
              their predictions using a score. So, for example,
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:31:09,984'); seek(1869.0)">
              classifiers often give a probability of how likely a data point is to
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:31:13,908'); seek(1873.0)">
              be of a certain class for the model.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:31:17,810'); seek(1877.0)">
              You might expect to get a lower probability on everything when
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:31:22,852'); seek(1882.0)">
              the data points are outliers.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:31:27,670'); seek(1887.0)">
              Unfortunately, it doesn't work that way. And for outliers, sometimes models can
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:31:31,208'); seek(1891.0)">
              give very high probabilities for data points that they're getting completely
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:31:34,520'); seek(1894.0)">
              wrong. This is called overconfidence. So if
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:31:38,488'); seek(1898.0)">
              your live data has outliers and your use case has risk associated
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:31:42,002'); seek(1902.0)">
              with those, then you might want to detect and track outliers.
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:31:46,490'); seek(1906.0)">
              Depending on your use case, you might choose to make it part of your business
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:31:49,580'); seek(1909.0)">
              logic, for example, to handle outlier cases differently,
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:31:52,982'); seek(1912.0)">
              perhaps scheduling a manual review on them.
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:31:58,490'); seek(1918.0)">
              Worse than the outlier case is when the whole data distribution is different
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:32:01,772'); seek(1921.0)">
              from the training data. It can even start
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:32:08,496'); seek(1928.0)">
              out similar to the training data and then shift over time.
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:32:12,190'); seek(1932.0)">
              Think, for example, of the fashion recommendation.
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:32:15,350'); seek(1935.0)">
              The example that we mentioned earlier was trained on data from
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:32:19,236'); seek(1939.0)">
              winter, and then you continue using it into the summer. Then it's
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:32:23,402'); seek(1943.0)">
              recommending coats when it should be recommending t shirts.
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:32:27,090'); seek(1947.0)">
              If you have a component that knows the distribution of the training data,
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:32:30,676'); seek(1950.0)">
              then you can asynchronously feed it all of the live requests,
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:32:34,390'); seek(1954.0)">
              feed them into that component, and keep a watch. That component
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:32:38,558'); seek(1958.0)">
              will keep a watch so you can use it to set up notifications in case
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:32:42,552'); seek(1962.0)">
              the distribution shifts. You could then use
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:32:45,628'); seek(1965.0)">
              that notification to decide if you need to train a new version of the model
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:32:49,164'); seek(1969.0)">
              using updated data. Or perhaps you've
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:32:52,434'); seek(1972.0)">
              got other metrics that let you track model performance, and if
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:32:55,724'); seek(1975.0)">
              they're still showing as good, you might just choose to check those metrics more
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:32:59,088'); seek(1979.0)">
              frequently while you look more closely into what's happening with live data distribution.
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:33:05,860'); seek(1985.0)">
              These monitoring and prediction quality concerns also feed into
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:33:09,382'); seek(1989.0)">
              governance for machine learning. It's a big topic and we
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:33:12,778'); seek(1992.0)">
              can't go into everything in detail, but I want to give an impression
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:33:16,352'); seek(1996.0)">
              of the area, so I'll at least mention of a few things I've
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:33:20,596'); seek(2000.0)">
              talked about. Detection for data drift and outliers
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:33:24,724'); seek(2004.0)">
              another thing detectors might be applicable for is adversarial attacks.
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:33:28,860'); seek(2008.0)">
              These are when manipulated data is
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:33:32,190'); seek(2012.0)">
              fed to a model in order to trick the model. Think, for example,
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:33:35,970'); seek(2015.0)">
              of how face recognition systems can sometimes be tricked by somebody wearing a mask.
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:33:40,728'); seek(2020.0)">
              That's a big problem in high security situations,
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:33:43,672'); seek(2023.0)">
              and there are analogous attacks that have appeared for other use cases.
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:33:50,160'); seek(2030.0)">
              I also mentioned that in high compliance situations you might
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:33:53,458'); seek(2033.0)">
              want to record all of the predictions in case you need to review them later.
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:33:57,120'); seek(2037.0)">
              This can also be relevant for dealing with customer domains.
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:34:00,820'); seek(2040.0)">
              This relates to the topic of explainability. For example,
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:34:04,390'); seek(2044.0)">
              if you've got a system that makes decisions on whether to approve loans,
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:34:08,260'); seek(2048.0)">
              you're deploying somebody a loan. Then you might want to be able to explain why
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:34:11,878'); seek(2051.0)">
              you denied them the loan. You'll want to be able to
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:34:15,258'); seek(2055.0)">
              revisit exactly what was fed into the model. The explainability part
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:34:19,402'); seek(2059.0)">
              is a data challenges data science challenge in itself, but it links
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:34:23,264'); seek(2063.0)">
              into mlops because you'll need to know what data to
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:34:26,846'); seek(2066.0)">
              get explanations for and what model was being used to
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:34:30,398'); seek(2070.0)">
              make the original loan decision.
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:34:33,500'); seek(2073.0)">
              The topic of explainability also relates to concerns about bias and ethics.
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:34:38,320'); seek(2078.0)">
              Let's imagine that your model is biased and is unfairly denying
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:34:41,768'); seek(2081.0)">
              loans to certain groups.
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:34:44,960'); seek(2084.0)">
              You'll have a better chance of discovering that bias if you can
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:34:48,338'); seek(2088.0)">
              explain which data points are contributing most towards its decisions.
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:34:54,100'); seek(2094.0)">
              There's also a big governance question around being able to say exactly what
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:34:57,638'); seek(2097.0)">
              was training and when. In traditional DevOps, it's a familiar
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:35:01,308'); seek(2101.0)">
              idea that we'd want to be able to say which version of the software was
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:35:04,902'); seek(2104.0)">
              running at a given point in time and what code it was built from,
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:35:08,602'); seek(2108.0)">
              so that we can delve into that code and build it again if we need
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:35:11,562'); seek(2111.0)">
              to. This can
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:35:15,018'); seek(2115.0)">
              be much more difficult to achieve with mlOps, as it would also require
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:35:18,868'); seek(2118.0)">
              being able to get access to all the data was used to train the model,
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:35:22,046'); seek(2122.0)">
              and likely also being able to reproduce all of the transformations that were
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:35:25,582'); seek(2125.0)">
              performed on the data and the parameters that were used in the training run.
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:35:30,160'); seek(2130.0)">
              Even then, there can be elements of randomness in the training process
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:35:33,986'); seek(2133.0)">
              that can scupper reproducibility. You don't plan for them.
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:35:40,430'); seek(2140.0)">
              So let's finish up by summarizing what we've learned.
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:35:44,110'); seek(2144.0)">
              MLOPs is a new terrain. ML workflows
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:35:48,182'); seek(2148.0)">
              are more exploratory and data driven than traditional dev workflows.
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:35:53,730'); seek(2153.0)">
              MLOPs enables ML workflows. It provides
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:35:57,402'); seek(2157.0)">
              tools and practices for enabling training runs and experiments
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:36:01,882'); seek(2161.0)">
              that are very data intensive and which use a lot of compute resources.
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:36:07,350'); seek(2167.0)">
              Provides facilities for tracking artifacts produced and
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:36:10,632'); seek(2170.0)">
              operations on data during those training runs.
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:36:14,630'); seek(2174.0)">
              There are MLOps tools specifically for serving machine learning models
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:36:18,290'); seek(2178.0)">
              and specialized strategies for safely rolling out new models for serving
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:36:21,874'); seek(2181.0)">
              in a production environment. There's also tools
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:36:26,018'); seek(2186.0)">
              and approaches for monitoring models running in a production environment and
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:36:30,252'); seek(2190.0)">
              checking that model performance stays acceptable, or at least
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:36:33,388'); seek(2193.0)">
              that you find out if something does go wrong.
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:36:37,370'); seek(2197.0)">
              So that's my perspective on the field of mlops right at
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:36:41,068'); seek(2201.0)">
              least as it is right now. Thanks very much for listening.
            </span>
            
            </div>
          </div>
          

          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/sre2020" class="btn btn-sm btn-danger shadow lift" style="background-color: #E36414;">
                <i class="fe fe-grid me-2"></i>
                See all 10 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/sre_ryand.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Ryan Dawson
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Core Member @ Seldon Open Source Team
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/ryan-dawson-501ab9123/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Ryan Dawson's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/@ryandawsongb" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Ryan Dawson's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @ryandawsongb"
                  data-url="https://www.conf42.com/sre2020"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/sre2020"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Site Reliability Engineering"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/aiml2024">
                  Artificial Intelligence & Machine Learning (AI & ML) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday London 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

  </body>
</html>