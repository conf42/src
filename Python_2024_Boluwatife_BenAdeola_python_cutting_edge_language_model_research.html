<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Use of Python for Cutting edge Language Model research</title>
    <meta name="description" content="Get inspired by fellow Pythonistas, Snakes and Pandas united!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Boluwatife%20Ben-Adeola_python.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Use of Python for Cutting edge Language Model research | Conf42"/>
    <meta property="og:description" content="Join me as I share insights - from Financial Technology at Bloomberg to leading projects at Palantir Tech. Explore the pivot to LLMs, with a focus on Mechanistic Interpretability. Learn how Python, with its versatility, is the key to unraveling the potential of AI in shaping the future of humanity."/>
    <meta property="og:url" content="https://conf42.com/Python_2024_Boluwatife_BenAdeola_python_cutting_edge_language_model_research"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/DEVSECOPS2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        DevSecOps 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-12-05
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/devsecops2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #69811f;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Python 2024 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2024-02-29">February 29 2024</time>
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Get inspired by fellow Pythonistas, Snakes and Pandas united!
 -->
              <script>
                const event_date = new Date("2024-02-29T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-02-29T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "uv8CnN9lcpc"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "u0jByw3M9Jw"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrDgQLm-W_f3TJi5UuR2KXfc" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hi everyone, I\u0027m Bolu and I am", "timestamp": "00:00:20,730", "timestamp_s": 20.0}, {"text": "an independent AI researcher. And today I\u0027m going", "timestamp": "00:00:23,892", "timestamp_s": 23.0}, {"text": "to talk about using Python for large language models research.", "timestamp": "00:00:27,468", "timestamp_s": 27.0}, {"text": "Specifically, I\u0027ll be speaking on the insights Python library", "timestamp": "00:00:32,490", "timestamp_s": 32.0}, {"text": "that enables mechanistic interpretability research.", "timestamp": "00:00:36,810", "timestamp_s": 36.0}, {"text": "So a bit of a primer on this field", "timestamp": "00:00:40,650", "timestamp_s": 40.0}, {"text": "of inquiry. What is mechanistic interpretability?", "timestamp": "00:00:43,964", "timestamp_s": 43.0}, {"text": "Some things we can hopefully all agree on. First is that neural networks", "timestamp": "00:00:48,570", "timestamp_s": 48.0}, {"text": "solve an increase in number of important tasks.", "timestamp": "00:00:52,506", "timestamp_s": 52.0}, {"text": "And the second is that it would be at least interesting and probably important", "timestamp": "00:00:55,890", "timestamp_s": 55.0}, {"text": "to understand how they do that is interesting in the sense", "timestamp": "00:01:00,324", "timestamp_s": 60.0}, {"text": "of, if you feel any sense of curiosity, to basically", "timestamp": "00:01:03,956", "timestamp_s": 63.0}, {"text": "look inside this whole world that is currently black box to most people", "timestamp": "00:01:07,384", "timestamp_s": 67.0}, {"text": "just out of. Because these models arrive at solutions", "timestamp": "00:01:11,510", "timestamp_s": 71.0}, {"text": "that no person could write a program for. So, out of curiosity,", "timestamp": "00:01:15,278", "timestamp_s": 75.0}, {"text": "it\u0027d be interesting to know what are the algorithms being implemented and", "timestamp": "00:01:18,722", "timestamp_s": 78.0}, {"text": "hopefully describing them in a human understandable way,", "timestamp": "00:01:22,652", "timestamp_s": 82.0}, {"text": "and important in a sense that any sufficiently powerful", "timestamp": "00:01:26,970", "timestamp_s": 86.0}, {"text": "system that is being put in strategic places", "timestamp": "00:01:30,950", "timestamp_s": 90.0}, {"text": "of great importance in society has", "timestamp": "00:01:35,334", "timestamp_s": 95.0}, {"text": "to have a certain level of transparency and understanding before we", "timestamp": "00:01:39,152", "timestamp_s": 99.0}, {"text": "as a society can trust it to be deployed. Any effort to", "timestamp": "00:01:42,292", "timestamp_s": 102.0}, {"text": "understand how these models work will definitely continue", "timestamp": "00:01:45,812", "timestamp_s": 105.0}, {"text": "to be increasingly important in the future. Now,", "timestamp": "00:01:49,476", "timestamp_s": 109.0}, {"text": "mechanistic interpretability, or mechinterp, as I\u0027ll call it going", "timestamp": "00:01:52,292", "timestamp_s": 112.0}, {"text": "forward, because as you can imagine, it\u0027s a bit of a mouthful, is a", "timestamp": "00:01:55,988", "timestamp_s": 115.0}, {"text": "field of research that tackles this problem starting at a very granular", "timestamp": "00:01:59,704", "timestamp_s": 119.0}, {"text": "level of the models. And what does that mean by", "timestamp": "00:02:03,246", "timestamp_s": 123.0}, {"text": "granular level? The typical mechanistic interpretability", "timestamp": "00:02:06,588", "timestamp_s": 126.0}, {"text": "result provides a mechanistic model that basically means", "timestamp": "00:02:10,578", "timestamp_s": 130.0}, {"text": "a causal model describing how different discrete components", "timestamp": "00:02:14,396", "timestamp_s": 134.0}, {"text": "in a very large model arrive at some", "timestamp": "00:02:18,306", "timestamp_s": 138.0}, {"text": "observed behavior. So we have some observed behavior, and the question is, can you,", "timestamp": "00:02:22,012", "timestamp_s": 142.0}, {"text": "through experimental processes, arrive at", "timestamp": "00:02:25,792", "timestamp_s": 145.0}, {"text": "an explanation for how these observations come to", "timestamp": "00:02:29,232", "timestamp_s": 149.0}, {"text": "be? That is the mechanistic approach to it. Again, this is identifying", "timestamp": "00:02:32,788", "timestamp_s": 152.0}, {"text": "mechinterp in a much larger field of interpretability,", "timestamp": "00:02:37,130", "timestamp_s": 157.0}, {"text": "which can have different flavors to it. But mechanistic", "timestamp": "00:02:40,794", "timestamp_s": 160.0}, {"text": "interpretability is unique in taking this granular causal", "timestamp": "00:02:44,398", "timestamp_s": 164.0}, {"text": "model of trying to drill as deep as possible and hoping", "timestamp": "00:02:47,998", "timestamp_s": 167.0}, {"text": "to build on larger and larger abstractions,", "timestamp": "00:02:51,454", "timestamp_s": 171.0}, {"text": "but starting from the very granular level. And for", "timestamp": "00:02:54,862", "timestamp_s": 174.0}, {"text": "today\u0027s talk, we\u0027re going to be picking one item out of the mechinterp", "timestamp": "00:02:58,028", "timestamp_s": 178.0}, {"text": "toolkit, which is that of causal interventions.", "timestamp": "00:03:01,698", "timestamp_s": 181.0}, {"text": "So basically the idea is if we abstract the entire network", "timestamp": "00:03:05,586", "timestamp_s": 185.0}, {"text": "to be a computational graph, that is, again, we forget that", "timestamp": "00:03:10,190", "timestamp_s": 190.0}, {"text": "this is machine learning. Just imagine this has been any abstract computational", "timestamp": "00:03:13,808", "timestamp_s": 193.0}, {"text": "graph, and the current state of", "timestamp": "00:03:17,846", "timestamp_s": 197.0}, {"text": "not understanding simply means we don\u0027t know", "timestamp": "00:03:21,890", "timestamp_s": 201.0}, {"text": "what computation each of the nodes are running and how they interact with each other.", "timestamp": "00:03:25,428", "timestamp_s": 205.0}, {"text": "So from that perspective, if we\u0027re curious about knowing how", "timestamp": "00:03:29,252", "timestamp_s": 209.0}, {"text": "one component that is, either it is an attention head,", "timestamp": "00:03:33,236", "timestamp_s": 213.0}, {"text": "or an MLP", "timestamp": "00:03:36,552", "timestamp_s": 216.0}, {"text": "layer, or a linear layer or an embedding unit, again, you don\u0027t have to worry", "timestamp": "00:03:40,142", "timestamp_s": 220.0}, {"text": "about any of these means. You can just abstract these as being any node in", "timestamp": "00:03:43,810", "timestamp_s": 223.0}, {"text": "some compute graph. But if you do, it\u0027ll help to paint a picture better.", "timestamp": "00:03:46,952", "timestamp_s": 226.0}, {"text": "So if we\u0027re curious to know what any of these nodes contribute to", "timestamp": "00:03:51,100", "timestamp_s": 231.0}, {"text": "start with, even knowing if they contribute anything to start with,", "timestamp": "00:03:55,852", "timestamp_s": 235.0}, {"text": "one way of doing that is simply taking the node and", "timestamp": "00:03:59,310", "timestamp_s": 239.0}, {"text": "observing some behavior that we find interesting", "timestamp": "00:04:04,190", "timestamp_s": 244.0}, {"text": "and then changing the impute to that node to see if the downstream", "timestamp": "00:04:07,872", "timestamp_s": 247.0}, {"text": "impact for the observed behavior is noteworthy.", "timestamp": "00:04:11,466", "timestamp_s": 251.0}, {"text": "That is, if this node, in this example, the node d,", "timestamp": "00:04:15,370", "timestamp_s": 255.0}, {"text": "is very vital to some observed behavior downstream.", "timestamp": "00:04:20,210", "timestamp_s": 260.0}, {"text": "If we mess with it a bit, that\u0027s if we perturb the node,", "timestamp": "00:04:23,578", "timestamp_s": 263.0}, {"text": "the observed result should change. That means, okay, this node", "timestamp": "00:04:26,878", "timestamp_s": 266.0}, {"text": "is on the critical path from input to output for this observed", "timestamp": "00:04:30,638", "timestamp_s": 270.0}, {"text": "behavior. Of course, we expect some part", "timestamp": "00:04:34,158", "timestamp_s": 274.0}, {"text": "of the model to change if you mess with anything. But the whole point", "timestamp": "00:04:37,576", "timestamp_s": 277.0}, {"text": "of this is that we have to first of all settle on some observed behavior,", "timestamp": "00:04:41,068", "timestamp_s": 281.0}, {"text": "and then we tweak the value of some node of interest and", "timestamp": "00:04:44,322", "timestamp_s": 284.0}, {"text": "then we observe downstream. If, however, it doesn\u0027t have any impact,", "timestamp": "00:04:47,804", "timestamp_s": 287.0}, {"text": "then that means this node is not that important and then we", "timestamp": "00:04:51,618", "timestamp_s": 291.0}, {"text": "can ignore it. But if it is, then we know that we can drill deeper.", "timestamp": "00:04:54,864", "timestamp_s": 294.0}, {"text": "So I think in the rest of the course, I\u0027m going to speak on a", "timestamp": "00:04:58,710", "timestamp_s": 298.0}, {"text": "practical example in very recent research that", "timestamp": "00:05:01,748", "timestamp_s": 301.0}, {"text": "uses this kind of intervention to try to understand how", "timestamp": "00:05:05,012", "timestamp_s": 305.0}, {"text": "a model achieves some outcomes. The topic", "timestamp": "00:05:09,252", "timestamp_s": 309.0}, {"text": "of interest today is that of function vectors. So this is a very", "timestamp": "00:05:13,146", "timestamp_s": 313.0}, {"text": "recent paper, I think just published last year,", "timestamp": "00:05:16,872", "timestamp_s": 316.0}, {"text": "October, from a group from Northeastern University,", "timestamp": "00:05:19,416", "timestamp_s": 319.0}, {"text": "from Corey College of Computer Science.", "timestamp": "00:05:24,710", "timestamp_s": 324.0}, {"text": "Basically, it is a mechanistic interoperability research effort", "timestamp": "00:05:27,670", "timestamp_s": 327.0}, {"text": "that tries to observe some behavior in large language", "timestamp": "00:05:31,770", "timestamp_s": 331.0}, {"text": "models, and that behavior is", "timestamp": "00:05:35,378", "timestamp_s": 335.0}, {"text": "described thus. So the question is the hypothesis,", "timestamp": "00:05:38,880", "timestamp_s": 338.0}, {"text": "is it possible to have some", "timestamp": "00:05:42,910", "timestamp_s": 342.0}, {"text": "functional components of large language models?", "timestamp": "00:05:46,336", "timestamp_s": 346.0}, {"text": "That is, we can all agree that if I gave,", "timestamp": "00:05:50,430", "timestamp_s": 350.0}, {"text": "looking at the top left section here of a string", "timestamp": "00:05:53,972", "timestamp_s": 353.0}, {"text": "of input, that is arrive column, depart small column,", "timestamp": "00:05:57,738", "timestamp_s": 357.0}, {"text": "big common column, if I", "timestamp": "00:06:01,162", "timestamp_s": 361.0}, {"text": "gave this input to something like, say, chad GPT, I think we can", "timestamp": "00:06:04,484", "timestamp_s": 364.0}, {"text": "all agree that it will figure out, okay, this is a simple word and opposite", "timestamp": "00:06:07,768", "timestamp_s": 367.0}, {"text": "game. That is the first example at the top.", "timestamp": "00:06:11,118", "timestamp_s": 371.0}, {"text": "And the second example, I believe is converting to Spanish.", "timestamp": "00:06:14,008", "timestamp_s": 374.0}, {"text": "I think we can all agree that something like chat, GBT and", "timestamp": "00:06:17,930", "timestamp_s": 377.0}, {"text": "similar large language models are able to do such a thing.", "timestamp": "00:06:21,292", "timestamp_s": 381.0}, {"text": "Is it possible for me to take some kernel", "timestamp": "00:06:25,290", "timestamp_s": 385.0}, {"text": "of this function of opposite, say again, taking the first example,", "timestamp": "00:06:29,014", "timestamp_s": 389.0}, {"text": "and transfer it to a completely different context and", "timestamp": "00:06:32,720", "timestamp_s": 392.0}, {"text": "have that same behavior operate on", "timestamp": "00:06:36,432", "timestamp_s": 396.0}, {"text": "a token in this new context? What that means is on", "timestamp": "00:06:40,256", "timestamp_s": 400.0}, {"text": "the right you see the direction of the arrow. The example, the counterpart on the", "timestamp": "00:06:43,828", "timestamp_s": 403.0}, {"text": "right, simply says the word fast means.", "timestamp": "00:06:47,364", "timestamp_s": 407.0}, {"text": "Now, under the normal operation of a large language module", "timestamp": "00:06:51,490", "timestamp_s": 411.0}, {"text": "model, trying to predict the next token, you can say something like the word fast", "timestamp": "00:06:55,674", "timestamp_s": 415.0}, {"text": "means quick, or it means going quickly or any reasonable", "timestamp": "00:06:59,524", "timestamp_s": 419.0}, {"text": "thing to follow. However, if this hypothesis", "timestamp": "00:07:03,498", "timestamp_s": 423.0}, {"text": "of portability of functions, we should", "timestamp": "00:07:08,162", "timestamp_s": 428.0}, {"text": "be able to move something from this context on the left", "timestamp": "00:07:11,388", "timestamp_s": 431.0}, {"text": "that clearly is about word and opposite into a completely new", "timestamp": "00:07:15,452", "timestamp_s": 435.0}, {"text": "context that has no conception of word and opposite", "timestamp": "00:07:18,928", "timestamp_s": 438.0}, {"text": "as an objective and achieve the result of", "timestamp": "00:07:22,742", "timestamp_s": 442.0}, {"text": "flipping the word fast into slow. I know", "timestamp": "00:07:26,608", "timestamp_s": 446.0}, {"text": "it seems very almost crazy to", "timestamp": "00:07:30,592", "timestamp_s": 450.0}, {"text": "expect this is true, but let\u0027s just assume this is the leading hypothesis.", "timestamp": "00:07:34,068", "timestamp_s": 454.0}, {"text": "And of course we\u0027re going to discuss what exactly this thing will be exporting is.", "timestamp": "00:07:37,802", "timestamp_s": 457.0}, {"text": "We see there the letter a average layer activation.", "timestamp": "00:07:42,790", "timestamp_s": 462.0}, {"text": "What the hypothesis says is this thing in quote that we", "timestamp": "00:07:46,862", "timestamp_s": 466.0}, {"text": "plan to port over is simply the", "timestamp": "00:07:50,968", "timestamp_s": 470.0}, {"text": "average activation over a series of", "timestamp": "00:07:54,872", "timestamp_s": 474.0}, {"text": "projects for a given task. Again, I\u0027m going to break that down a bit.", "timestamp": "00:07:58,152", "timestamp_s": 478.0}, {"text": "So again, let\u0027s say our task is simple or an opposite. So we have three", "timestamp": "00:08:01,532", "timestamp_s": 481.0}, {"text": "different examples. Old young, vanish,", "timestamp": "00:08:04,876", "timestamp_s": 484.0}, {"text": "appear, dark. Colin and I", "timestamp": "00:08:07,938", "timestamp_s": 487.0}, {"text": "guess something like bright or dark and light will follow. And the", "timestamp": "00:08:11,248", "timestamp_s": 491.0}, {"text": "second example, the same thing. Awake, sleep, future, past,", "timestamp": "00:08:14,848", "timestamp_s": 494.0}, {"text": "joy. Colin at the very", "timestamp": "00:08:18,512", "timestamp_s": 498.0}, {"text": "end of all these contexts, these like query", "timestamp": "00:08:21,716", "timestamp_s": 501.0}, {"text": "inputs, the neural network is right on the", "timestamp": "00:08:25,354", "timestamp_s": 505.0}, {"text": "verge of doing the thing called flip opposite the", "timestamp": "00:08:28,868", "timestamp_s": 508.0}, {"text": "last thing I saw before my column. So the hypothesis is", "timestamp": "00:08:32,884", "timestamp_s": 512.0}, {"text": "if we can take that activation state and in", "timestamp": "00:08:37,112", "timestamp_s": 517.0}, {"text": "the section b you see there, simply add it", "timestamp": "00:08:41,512", "timestamp_s": 521.0}, {"text": "to a completely unrelated context,", "timestamp": "00:08:44,632", "timestamp_s": 524.0}, {"text": "would it be possible to observe the same behavior? Because again on", "timestamp": "00:08:47,534", "timestamp_s": 527.0}, {"text": "the right we see fair simple. In the absence of this", "timestamp": "00:08:50,988", "timestamp_s": 530.0}, {"text": "intervention, we have no reason to expect the model will", "timestamp": "00:08:54,572", "timestamp_s": 534.0}, {"text": "say anything other than simple. Then something", "timestamp": "00:08:58,272", "timestamp_s": 538.0}, {"text": "like simple, easy, or whatever the model finds appropriate to", "timestamp": "00:09:02,256", "timestamp_s": 542.0}, {"text": "follow simple. But if indeed our intervention is important,", "timestamp": "00:09:06,272", "timestamp_s": 546.0}, {"text": "we expect to observe something like simple colon", "timestamp": "00:09:09,840", "timestamp_s": 549.0}, {"text": "complex or at the bottom there encode becomes decode", "timestamp": "00:09:13,178", "timestamp_s": 553.0}, {"text": "just magically by intervening with this average", "timestamp": "00:09:16,554", "timestamp_s": 556.0}, {"text": "activation state. Again, I would explain what we mean by activation state in the", "timestamp": "00:09:21,010", "timestamp_s": 561.0}, {"text": "following line, but I hope you just get the general thesis of what this is", "timestamp": "00:09:25,044", "timestamp_s": 565.0}, {"text": "meant to be. That is the question is, is there a portable", "timestamp": "00:09:27,864", "timestamp_s": 567.0}, {"text": "component of operations", "timestamp": "00:09:31,246", "timestamp_s": 571.0}, {"text": "and functions inside of neural networks", "timestamp": "00:09:35,182", "timestamp_s": 575.0}, {"text": "and more specifically large language models? All right, so I", "timestamp": "00:09:38,594", "timestamp_s": 578.0}, {"text": "guess to give a bit of shape what I mean", "timestamp": "00:09:42,044", "timestamp_s": 582.0}, {"text": "by activation vector and what is being ported left and right.", "timestamp": "00:09:45,916", "timestamp_s": 585.0}, {"text": "So here, this is just like a typical one layer example", "timestamp": "00:09:49,152", "timestamp_s": 589.0}, {"text": "of an LLM decoder, only what", "timestamp": "00:09:53,632", "timestamp_s": 593.0}, {"text": "we have here is at the very bottom, we input a token,", "timestamp": "00:09:57,328", "timestamp_s": 597.0}, {"text": "a sequence of tokens, right? That is like the on colon,", "timestamp": "00:10:01,210", "timestamp_s": 601.0}, {"text": "off wet column, dry old colon.", "timestamp": "00:10:04,586", "timestamp_s": 604.0}, {"text": "And as we see, the expectation is as this", "timestamp": "00:10:08,290", "timestamp_s": 608.0}, {"text": "input passes through subsequent layers in,", "timestamp": "00:10:12,036", "timestamp_s": 612.0}, {"text": "there\u0027s one single set of vectors that are going to keep being updated and", "timestamp": "00:10:16,550", "timestamp_s": 616.0}, {"text": "changed and added on. And again, due to some specifics", "timestamp": "00:10:20,312", "timestamp_s": 620.0}, {"text": "to the neural network architectures, more specifically the", "timestamp": "00:10:23,438", "timestamp_s": 623.0}, {"text": "skip connections, which I won\u0027t get too much into right now, each subsequent", "timestamp": "00:10:26,712", "timestamp_s": 626.0}, {"text": "layer adds additional context that is literally just added", "timestamp": "00:10:30,482", "timestamp_s": 630.0}, {"text": "on top the last. But in any way that\u0027s not really important for now.", "timestamp": "00:10:34,082", "timestamp_s": 634.0}, {"text": "So let\u0027s just think of it. For example, again,", "timestamp": "00:10:36,876", "timestamp_s": 636.0}, {"text": "looking at the journey of the column, the very last column,", "timestamp": "00:10:39,790", "timestamp_s": 639.0}, {"text": "when it goes to the embedding layer, it has some vector", "timestamp": "00:10:44,590", "timestamp_s": 644.0}, {"text": "that represents okay, cool. This is how the", "timestamp": "00:10:48,422", "timestamp_s": 648.0}, {"text": "neural network\u0027s embedding layer represents the token of", "timestamp": "00:10:51,892", "timestamp_s": 651.0}, {"text": "a column. And again, so we can kind of anthropomorphize,", "timestamp": "00:10:55,572", "timestamp_s": 655.0}, {"text": "pretend it\u0027s like self aware almost to say", "timestamp": "00:11:00,370", "timestamp_s": 660.0}, {"text": "I am a column. Because technically, if you took that embedding vector", "timestamp": "00:11:04,052", "timestamp_s": 664.0}, {"text": "and you put it through the unembedding vector on the other", "timestamp": "00:11:08,958", "timestamp_s": 668.0}, {"text": "side, it would come out as column is really likely to come,", "timestamp": "00:11:12,232", "timestamp_s": 672.0}, {"text": "right? So we might as well just see this as the model", "timestamp": "00:11:15,656", "timestamp_s": 675.0}, {"text": "being what information the model has for that position in", "timestamp": "00:11:18,940", "timestamp_s": 678.0}, {"text": "the sequence. So somewhere between starting from I am", "timestamp": "00:11:22,844", "timestamp_s": 682.0}, {"text": "a column in the beginning to the very end", "timestamp": "00:11:26,588", "timestamp_s": 686.0}, {"text": "of the thing that follows me is the word new.", "timestamp": "00:11:29,948", "timestamp_s": 689.0}, {"text": "The model has learned some interesting things, right? By definition, like how", "timestamp": "00:11:34,400", "timestamp_s": 694.0}, {"text": "else would it know? Again, because it\u0027s still", "timestamp": "00:11:38,352", "timestamp_s": 698.0}, {"text": "that same column vector that has been", "timestamp": "00:11:41,572", "timestamp_s": 701.0}, {"text": "updated for the sequence position of the token column. So the", "timestamp": "00:11:45,412", "timestamp_s": 705.0}, {"text": "conjecture here for the hypothesis of portable functions is", "timestamp": "00:11:49,028", "timestamp_s": 709.0}, {"text": "that somewhere in between or containing that vector is information", "timestamp": "00:11:52,308", "timestamp_s": 712.0}, {"text": "on amicolum, of course, which it had before. And it", "timestamp": "00:11:56,148", "timestamp_s": 716.0}, {"text": "also has my next is new. That is, my next token", "timestamp": "00:11:59,752", "timestamp_s": 719.0}, {"text": "is the word new, which again is just what", "timestamp": "00:12:03,710", "timestamp_s": 723.0}, {"text": "we would observe from Chad GPT so the additional thing the hypothesis is saying", "timestamp": "00:12:06,808", "timestamp_s": 726.0}, {"text": "is that, or is asking is that, is there a component that", "timestamp": "00:12:10,636", "timestamp_s": 730.0}, {"text": "encodes the operation that it must do or the function it", "timestamp": "00:12:14,796", "timestamp_s": 734.0}, {"text": "must do to arrive at new, perhaps before it came", "timestamp": "00:12:18,444", "timestamp_s": 738.0}, {"text": "to the conclusion of the next is new, is there a component", "timestamp": "00:12:22,112", "timestamp_s": 742.0}, {"text": "that says I am to do or am to call the function opposite", "timestamp": "00:12:25,718", "timestamp_s": 745.0}, {"text": "on. Surely there must be of some sort, because how", "timestamp": "00:12:29,894", "timestamp_s": 749.0}, {"text": "else would it know to come up with new.", "timestamp": "00:12:33,588", "timestamp_s": 753.0}, {"text": "But the question is if there is linearity to this representation", "timestamp": "00:12:36,530", "timestamp_s": 756.0}, {"text": "by linearity is just what allows us to do things like", "timestamp": "00:12:40,850", "timestamp_s": 760.0}, {"text": "this? Literally take a thing, add it average,", "timestamp": "00:12:44,132", "timestamp_s": 764.0}, {"text": "and add it somewhere else and have it do things right. This assumes a lot", "timestamp": "00:12:47,870", "timestamp_s": 767.0}, {"text": "of linear behavior. So this is kind of the underlying", "timestamp": "00:12:51,208", "timestamp_s": 771.0}, {"text": "implicit assumption that is guiding this hypothesis.", "timestamp": "00:12:54,942", "timestamp_s": 774.0}, {"text": "To start with many of the different", "timestamp": "00:12:58,578", "timestamp_s": 778.0}, {"text": "research inquiries leads to very interesting result.", "timestamp": "00:13:02,524", "timestamp_s": 782.0}, {"text": "Often start with this assumption of can we assume there\u0027s lowlinearity? And again,", "timestamp": "00:13:06,170", "timestamp_s": 786.0}, {"text": "due to details of the architecture of most", "timestamp": "00:13:09,536", "timestamp_s": 789.0}, {"text": "transformer neural networks,", "timestamp": "00:13:13,552", "timestamp_s": 793.0}, {"text": "there are reasons to expect there to be low linearity. But just to see it", "timestamp": "00:13:17,150", "timestamp_s": 797.0}, {"text": "happen for real is always interesting. And I think this is the first time we\u0027re", "timestamp": "00:13:20,960", "timestamp_s": 800.0}, {"text": "seeing it in the context of operations as again, just representations,", "timestamp": "00:13:24,538", "timestamp_s": 804.0}, {"text": "which I think other research has demonstrated before,", "timestamp": "00:13:28,266", "timestamp_s": 808.0}, {"text": "such as for example, the relationship between the word", "timestamp": "00:13:31,570", "timestamp_s": 811.0}, {"text": "car and cars, that is, the relationship between a word", "timestamp": "00:13:34,868", "timestamp_s": 814.0}, {"text": "and its plural. There\u0027s been some regularities observed", "timestamp": "00:13:38,520", "timestamp_s": 818.0}, {"text": "in that regard. But this, however, is trying to take it a step further to", "timestamp": "00:13:42,366", "timestamp_s": 822.0}, {"text": "say, okay, are there encodings also for functions?", "timestamp": "00:13:45,848", "timestamp_s": 825.0}, {"text": "Okay, so we have a rough idea of what it means", "timestamp": "00:13:49,106", "timestamp_s": 829.0}, {"text": "for what this h is. It\u0027s simply just some vector", "timestamp": "00:13:52,780", "timestamp_s": 832.0}, {"text": "that at the very end of the network, right before it", "timestamp": "00:13:56,730", "timestamp_s": 836.0}, {"text": "goes into the penultimate layer, or at the penultimate", "timestamp": "00:14:00,412", "timestamp_s": 840.0}, {"text": "layer, we could run our model three different times", "timestamp": "00:14:03,842", "timestamp_s": 843.0}, {"text": "and snatch that vector across, cross all of them, look at exactly what", "timestamp": "00:14:08,044", "timestamp_s": 848.0}, {"text": "read literally what that vector is saying. Because again,", "timestamp": "00:14:11,684", "timestamp_s": 851.0}, {"text": "the information on what is to come next is embedded", "timestamp": "00:14:14,996", "timestamp_s": 854.0}, {"text": "in the colon token, right? It\u0027s the thing that is saying,", "timestamp": "00:14:18,650", "timestamp_s": 858.0}, {"text": "okay, dark. So all the information", "timestamp": "00:14:22,036", "timestamp_s": 862.0}, {"text": "for what is after dot, dot, dot is in colon. Cool. So we take", "timestamp": "00:14:25,620", "timestamp_s": 865.0}, {"text": "that for different runs and we average it out and try to add.", "timestamp": "00:14:29,032", "timestamp_s": 869.0}, {"text": "So that gives you an idea of just to draw", "timestamp": "00:14:33,190", "timestamp_s": 873.0}, {"text": "a bit of a picture to it.", "timestamp": "00:14:36,728", "timestamp_s": 876.0}, {"text": "And of course, this is just restating the same thing now that we have an", "timestamp": "00:14:39,930", "timestamp_s": 879.0}, {"text": "idea of what h means and what that vector is.", "timestamp": "00:14:43,004", "timestamp_s": 883.0}, {"text": "So for each of the different runs in", "timestamp": "00:14:46,684", "timestamp_s": 886.0}, {"text": "a series of prompts that are basically doing the same task,", "timestamp": "00:14:50,928", "timestamp_s": 890.0}, {"text": "if we literally took all the values of the vectors,", "timestamp": "00:14:54,886", "timestamp_s": 894.0}, {"text": "averaged them in position, added, divided by this", "timestamp": "00:14:58,422", "timestamp_s": 898.0}, {"text": "unified, averaged out mean vector,", "timestamp": "00:15:02,576", "timestamp_s": 902.0}, {"text": "and we took it into a different environment, into a different context,", "timestamp": "00:15:05,510", "timestamp_s": 905.0}, {"text": "and we literally just added it to something else.", "timestamp": "00:15:08,602", "timestamp_s": 908.0}, {"text": "The question is, will we be able to get effects", "timestamp": "00:15:12,690", "timestamp_s": 912.0}, {"text": "like seen below? That is, if we took. So I think here in the example", "timestamp": "00:15:15,834", "timestamp_s": 915.0}, {"text": "you see the representation for encode, again, so encode", "timestamp": "00:15:19,496", "timestamp_s": 919.0}, {"text": "column. So there you can see", "timestamp": "00:15:23,342", "timestamp_s": 923.0}, {"text": "how we can presume that without this intervention at the end,", "timestamp": "00:15:26,732", "timestamp_s": 926.0}, {"text": "after this token goes through the", "timestamp": "00:15:30,380", "timestamp_s": 930.0}, {"text": "entire model, it might say something like the", "timestamp": "00:15:33,788", "timestamp_s": 933.0}, {"text": "thing to come after the column is base 64, I guess, because maybe", "timestamp": "00:15:36,908", "timestamp_s": 936.0}, {"text": "encoding and base 64 is something that shows up", "timestamp": "00:15:40,912", "timestamp_s": 940.0}, {"text": "often, right? Remember, the base function", "timestamp": "00:15:44,096", "timestamp_s": 944.0}, {"text": "of a large language model is just to predict", "timestamp": "00:15:48,032", "timestamp_s": 948.0}, {"text": "the next most likely thing in human generated", "timestamp": "00:15:51,338", "timestamp_s": 951.0}, {"text": "text. However, with the addition of our", "timestamp": "00:15:55,242", "timestamp_s": 955.0}, {"text": "supposed, our hypothetical average out opposite", "timestamp": "00:15:59,684", "timestamp_s": 959.0}, {"text": "function, would we be able to steer", "timestamp": "00:16:03,630", "timestamp_s": 963.0}, {"text": "it towards saying something like, actually, instead of saying encode base 64,", "timestamp": "00:16:07,822", "timestamp_s": 967.0}, {"text": "I all of a sudden feel the urge to say the opposite of", "timestamp": "00:16:12,790", "timestamp_s": 972.0}, {"text": "encode and dev, say encode column decode. This is", "timestamp": "00:16:16,732", "timestamp_s": 976.0}, {"text": "the hypothesis. So again, it would be super interesting and kind", "timestamp": "00:16:20,188", "timestamp_s": 980.0}, {"text": "of weird if we can indeed prove", "timestamp": "00:16:23,868", "timestamp_s": 983.0}, {"text": "this representation. For example, that just has 123456.", "timestamp": "00:16:27,490", "timestamp_s": 987.0}, {"text": "Again, our vector just has about six different dimensions.", "timestamp": "00:16:32,672", "timestamp_s": 992.0}, {"text": "Encoding it. Of course, as we know, actual large language models", "timestamp": "00:16:37,446", "timestamp_s": 997.0}, {"text": "can be much bigger than this in", "timestamp": "00:16:41,370", "timestamp_s": 1001.0}, {"text": "the billions and billions of parameters. So how", "timestamp": "00:16:44,948", "timestamp_s": 1004.0}, {"text": "exactly do we plan to do this? Multiple runs", "timestamp": "00:16:49,076", "timestamp_s": 1009.0}, {"text": "and extracting values and averaging them and intervening and adding", "timestamp": "00:16:52,778", "timestamp_s": 1012.0}, {"text": "them in the real world, not for a toy model.", "timestamp": "00:16:55,998", "timestamp_s": 1015.0}, {"text": "And that brings us to our trusted interpretability libraries", "timestamp": "00:16:59,830", "timestamp_s": 1019.0}, {"text": "and packages. These are packages that are designed solely for", "timestamp": "00:17:03,918", "timestamp_s": 1023.0}, {"text": "this purpose of staring very deep", "timestamp": "00:17:07,628", "timestamp_s": 1027.0}, {"text": "into what large", "timestamp": "00:17:11,042", "timestamp_s": 1031.0}, {"text": "language models of different sizes are up to in", "timestamp": "00:17:14,236", "timestamp_s": 1034.0}, {"text": "a way that is practical to enable this kind of research.", "timestamp": "00:17:18,268", "timestamp_s": 1038.0}, {"text": "So we have an insight which is particularly popular", "timestamp": "00:17:21,390", "timestamp_s": 1041.0}, {"text": "for working with models on the larger side, and I\u0027ll discuss", "timestamp": "00:17:25,526", "timestamp_s": 1045.0}, {"text": "the details of its architecture that afford this", "timestamp": "00:17:30,030", "timestamp_s": 1050.0}, {"text": "kind of behavior. Then we have transformer lens,", "timestamp": "00:17:33,332", "timestamp_s": 1053.0}, {"text": "which is also a very great open source", "timestamp": "00:17:37,010", "timestamp_s": 1057.0}, {"text": "library for doing this. But for today\u0027s work, we\u0027re going to focus on insights.", "timestamp": "00:17:40,218", "timestamp_s": 1060.0}, {"text": "So what is it about insights that", "timestamp": "00:17:45,110", "timestamp_s": 1065.0}, {"text": "makes it work? What is the contact on insight? Where did it come from?", "timestamp": "00:17:48,248", "timestamp_s": 1068.0}, {"text": "From my understanding, the insights package came", "timestamp": "00:17:53,030", "timestamp_s": 1073.0}, {"text": "along with an effort called the", "timestamp": "00:17:57,068", "timestamp_s": 1077.0}, {"text": "NDIF initiative, which is", "timestamp": "00:18:01,180", "timestamp_s": 1081.0}, {"text": "basically a national deep inference facility.", "timestamp": "00:18:05,052", "timestamp_s": 1085.0}, {"text": "This is basically a compute cluster that is available to researchers for", "timestamp": "00:18:07,770", "timestamp_s": 1087.0}, {"text": "doing work that cannot afford the financial burden", "timestamp": "00:18:11,808", "timestamp_s": 1091.0}, {"text": "of actually running these very large models because they\u0027re very costly.", "timestamp": "00:18:15,686", "timestamp_s": 1095.0}, {"text": "Forget just training, even just running inference on them is quite expensive. So basically you", "timestamp": "00:18:19,510", "timestamp_s": 1099.0}, {"text": "have this remote cluster of", "timestamp": "00:18:23,428", "timestamp_s": 1103.0}, {"text": "compute that has been made available to researchers,", "timestamp": "00:18:27,252", "timestamp_s": 1107.0}, {"text": "and the insights package was basically made as", "timestamp": "00:18:31,002", "timestamp_s": 1111.0}, {"text": "a point, as an interface", "timestamp": "00:18:36,550", "timestamp_s": 1116.0}, {"text": "to this compute cluster. So the typical workflow, as is seen here", "timestamp": "00:18:40,238", "timestamp_s": 1120.0}, {"text": "in this schematic, is that you have the", "timestamp": "00:18:43,768", "timestamp_s": 1123.0}, {"text": "researcher working locally, basically writing interventions", "timestamp": "00:18:48,956", "timestamp_s": 1128.0}, {"text": "for how they want to run their experiments and intervene with networks,", "timestamp": "00:18:53,170", "timestamp_s": 1133.0}, {"text": "which we are going to see. And this is basically change into", "timestamp": "00:18:56,818", "timestamp_s": 1136.0}, {"text": "a compute graph, or more specifically an intervention graph, as like,", "timestamp": "00:19:00,432", "timestamp_s": 1140.0}, {"text": "this is how I want the running of this very large model", "timestamp": "00:19:03,888", "timestamp_s": 1143.0}, {"text": "to be tweaked.", "timestamp": "00:19:07,584", "timestamp_s": 1147.0}, {"text": "And this is then sent over the network into this cluster", "timestamp": "00:19:11,570", "timestamp_s": 1151.0}, {"text": "to say, okay, cool, please run this 70 billion model,", "timestamp": "00:19:15,514", "timestamp_s": 1155.0}, {"text": "70 billion parameter model that I definitely cannot run on", "timestamp": "00:19:21,090", "timestamp_s": 1161.0}, {"text": "my M one MacBook, but run it with", "timestamp": "00:19:24,228", "timestamp_s": 1164.0}, {"text": "these different interventions that make it look like that, make it no different from if", "timestamp": "00:19:27,752", "timestamp_s": 1167.0}, {"text": "I could actually run this locally. And as you", "timestamp": "00:19:31,352", "timestamp_s": 1171.0}, {"text": "can see, the thing between this boundary of the local environment", "timestamp": "00:19:35,404", "timestamp_s": 1175.0}, {"text": "to the NDIF infrastructure", "timestamp": "00:19:38,706", "timestamp_s": 1178.0}, {"text": "is simply this compute graph, and this compute graph is the output", "timestamp": "00:19:43,202", "timestamp_s": 1183.0}, {"text": "of the Nnsite library, and we\u0027ll", "timestamp": "00:19:46,882", "timestamp_s": 1186.0}, {"text": "see how it does. Cool. So that is the motivational", "timestamp": "00:19:50,454", "timestamp_s": 1190.0}, {"text": "setup for why an insight exists. It\u0027s basically a", "timestamp": "00:19:53,926", "timestamp_s": 1193.0}, {"text": "counterpart to the NDIF project, which is super interesting,", "timestamp": "00:19:57,104", "timestamp_s": 1197.0}, {"text": "by the way. Again, I think they just released their paper last November announcing", "timestamp": "00:20:00,576", "timestamp_s": 1200.0}, {"text": "the launch of the NDIF facility. It is live right now, I believe so,", "timestamp": "00:20:05,786", "timestamp_s": 1205.0}, {"text": "yeah. Really exciting project. I encourage anyone that\u0027s looking for", "timestamp": "00:20:09,668", "timestamp_s": 1209.0}, {"text": "computer resources, for inference in particular.", "timestamp": "00:20:13,428", "timestamp_s": 1213.0}, {"text": "And again, this has nothing to do with training. It\u0027s just like if you want", "timestamp": "00:20:17,090", "timestamp_s": 1217.0}, {"text": "to run a big model several times and do different", "timestamp": "00:20:19,928", "timestamp_s": 1219.0}, {"text": "interventions or read stuff from it to learn more, as we do", "timestamp": "00:20:23,336", "timestamp_s": 1223.0}, {"text": "with our hypothesis in question, then it", "timestamp": "00:20:27,052", "timestamp_s": 1227.0}, {"text": "works great. But of course, the library also offers", "timestamp": "00:20:31,692", "timestamp_s": 1231.0}, {"text": "the option of just running the if you happen to have", "timestamp": "00:20:35,458", "timestamp_s": 1235.0}, {"text": "several gigabytes of ram to spare.", "timestamp": "00:20:39,790", "timestamp_s": 1239.0}, {"text": "Okay, so let\u0027s jump into the code. What does", "timestamp": "00:20:43,550", "timestamp_s": 1243.0}, {"text": "it look like to do an intervention? By intervention, we just simply means", "timestamp": "00:20:46,992", "timestamp_s": 1246.0}, {"text": "anything that either writes or reads execution", "timestamp": "00:20:51,220", "timestamp_s": 1251.0}, {"text": "state of our model. That is,", "timestamp": "00:20:56,458", "timestamp_s": 1256.0}, {"text": "again, you have a model we put in a token sequence, and then", "timestamp": "00:21:00,372", "timestamp_s": 1260.0}, {"text": "stage after stage, the output of one stage is passed to", "timestamp": "00:21:04,516", "timestamp_s": 1264.0}, {"text": "the next, and that is added to the residual stream, which is just, again,", "timestamp": "00:21:08,008", "timestamp_s": 1268.0}, {"text": "think of it as like this ever accumulating output", "timestamp": "00:21:10,872", "timestamp_s": 1270.0}, {"text": "of each component in the model that eventually leads", "timestamp": "00:21:15,134", "timestamp_s": 1275.0}, {"text": "to a probability distribution or output that we", "timestamp": "00:21:18,322", "timestamp_s": 1278.0}, {"text": "observe. So if we ever want to poke into it either like", "timestamp": "00:21:22,108", "timestamp_s": 1282.0}, {"text": "use our binoculars or microscope to", "timestamp": "00:21:26,812", "timestamp_s": 1286.0}, {"text": "look in, that is one type of intervention, as you can see here on line", "timestamp": "00:21:30,508", "timestamp_s": 1290.0}, {"text": "five. Again, you can ignore the stuff above, I will explain that", "timestamp": "00:21:34,016", "timestamp_s": 1294.0}, {"text": "later. But just to dive straight into what exactly the interventions", "timestamp": "00:21:37,344", "timestamp_s": 1297.0}, {"text": "are, again, what are the things that make up these arrows of this", "timestamp": "00:21:40,982", "timestamp_s": 1300.0}, {"text": "intervention graph that is being sent over, which is the whole point of this package?", "timestamp": "00:21:45,090", "timestamp_s": 1305.0}, {"text": "Line five, we have something that is reading.", "timestamp": "00:21:49,490", "timestamp_s": 1309.0}, {"text": "So you see model layers, input is", "timestamp": "00:21:53,178", "timestamp_s": 1313.0}, {"text": "equal to something and we save it again, I\u0027ll explain why we\u0027re saving", "timestamp": "00:21:56,888", "timestamp_s": 1316.0}, {"text": "that. This giant model is not running on", "timestamp": "00:22:00,456", "timestamp_s": 1320.0}, {"text": "my colab notebook or my", "timestamp": "00:22:04,200", "timestamp_s": 1324.0}, {"text": "local environment, right? So it is interesting", "timestamp": "00:22:07,948", "timestamp_s": 1327.0}, {"text": "that I can indeed read what is happening on it.", "timestamp": "00:22:11,708", "timestamp_s": 1331.0}, {"text": "And on line six we have the opposite,", "timestamp": "00:22:14,970", "timestamp_s": 1334.0}, {"text": "which is me changing something in some other component", "timestamp": "00:22:18,482", "timestamp_s": 1338.0}, {"text": "of my model. So model the layers. So on layer eleven,", "timestamp": "00:22:22,006", "timestamp_s": 1342.0}, {"text": "on a component called the MLP, I want to change its output to", "timestamp": "00:22:26,030", "timestamp_s": 1346.0}, {"text": "become zero. And again, just to remind us what", "timestamp": "00:22:29,888", "timestamp_s": 1349.0}, {"text": "all this is for, how all this relates to our hypothesis,", "timestamp": "00:22:33,136", "timestamp_s": 1353.0}, {"text": "first we want to get the average of a bunch of runs for the task", "timestamp": "00:22:37,570", "timestamp_s": 1357.0}, {"text": "in question, which is the opposite. And then we want to append that", "timestamp": "00:22:41,178", "timestamp_s": 1361.0}, {"text": "average out value to some other examples", "timestamp": "00:22:44,916", "timestamp_s": 1364.0}, {"text": "that are in a different context called like the one shot or the zero shot", "timestamp": "00:22:48,718", "timestamp_s": 1368.0}, {"text": "examples. That is, we\u0027ve not giving the model any idea what we\u0027re trying to achieve.", "timestamp": "00:22:52,158", "timestamp_s": 1372.0}, {"text": "We just wanted to feel the urge to do the thing", "timestamp": "00:22:55,870", "timestamp_s": 1375.0}, {"text": "we want it to do because we have added the vector. So literally the first", "timestamp": "00:22:59,448", "timestamp_s": 1379.0}, {"text": "thing of mean is literally just a read output that we want to run several", "timestamp": "00:23:02,412", "timestamp_s": 1382.0}, {"text": "times, read the average value of this vector, or read the value of this vector", "timestamp": "00:23:06,258", "timestamp_s": 1386.0}, {"text": "and then average out. And then six shows us changing", "timestamp": "00:23:10,326", "timestamp_s": 1390.0}, {"text": "the value of some component. Then we want to", "timestamp": "00:23:14,038", "timestamp_s": 1394.0}, {"text": "add this average value to the running of the", "timestamp": "00:23:17,392", "timestamp_s": 1397.0}, {"text": "different context and see what happens. Okay, cool. So this basically is", "timestamp": "00:23:21,236", "timestamp_s": 1401.0}, {"text": "the scaffolding for what we need for", "timestamp": "00:23:24,708", "timestamp_s": 1404.0}, {"text": "our project. But before we go into the code", "timestamp": "00:23:29,252", "timestamp_s": 1409.0}, {"text": "for our experiment and research", "timestamp": "00:23:32,676", "timestamp_s": 1412.0}, {"text": "in question, just to decode a bit,", "timestamp": "00:23:36,616", "timestamp_s": 1416.0}, {"text": "what exactly is happening here? So one thing is that", "timestamp": "00:23:40,200", "timestamp_s": 1420.0}, {"text": "the insights library loves Python contexts, which is one", "timestamp": "00:23:43,544", "timestamp_s": 1423.0}, {"text": "of the reasons why I guess Python", "timestamp": "00:23:47,080", "timestamp_s": 1427.0}, {"text": "might be a language of choice. But context managers are great in", "timestamp": "00:23:50,434", "timestamp_s": 1430.0}, {"text": "python, as we know, and they do take great advantage of", "timestamp": "00:23:54,124", "timestamp_s": 1434.0}, {"text": "them. And the general structure of it is", "timestamp": "00:23:57,548", "timestamp_s": 1437.0}, {"text": "that as we know, basically the code", "timestamp": "00:24:01,872", "timestamp_s": 1441.0}, {"text": "might look like the model is running locally. When I do things like save,", "timestamp": "00:24:05,616", "timestamp_s": 1445.0}, {"text": "I do edits and I do reads. But the whole point is that", "timestamp": "00:24:09,040", "timestamp_s": 1449.0}, {"text": "none of this, the model actually isn\u0027t running right", "timestamp": "00:24:12,772", "timestamp_s": 1452.0}, {"text": "now. But when the context closes, that\u0027s like when the code execution", "timestamp": "00:24:16,532", "timestamp_s": 1456.0}, {"text": "reaches the point where we exit the uppermost context, which is here", "timestamp": "00:24:20,282", "timestamp_s": 1460.0}, {"text": "is line three. Runner. All the edits we\u0027ve made", "timestamp": "00:24:23,912", "timestamp_s": 1463.0}, {"text": "are, or in the course of running, of being", "timestamp": "00:24:27,528", "timestamp_s": 1467.0}, {"text": "intervention graph is updated with all the I o. That is,", "timestamp": "00:24:31,750", "timestamp_s": 1471.0}, {"text": "all the reading. And the writing we\u0027re doing to the model is basically", "timestamp": "00:24:35,480", "timestamp_s": 1475.0}, {"text": "just planned while in the context. And when the context", "timestamp": "00:24:39,180", "timestamp_s": 1479.0}, {"text": "is exited, this is then sent over,", "timestamp": "00:24:42,802", "timestamp_s": 1482.0}, {"text": "right? So the model does not run until the context of the highest level,", "timestamp": "00:24:46,716", "timestamp_s": 1486.0}, {"text": "which in case is the runner, which is a runner. Context is closed,", "timestamp": "00:24:50,912", "timestamp_s": 1490.0}, {"text": "and for context the", "timestamp": "00:24:56,190", "timestamp_s": 1496.0}, {"text": "invoker. Again, I would encourage anyone to read the documentation, but invoker is", "timestamp": "00:24:59,616", "timestamp_s": 1499.0}, {"text": "basically what", "timestamp": "00:25:04,036", "timestamp_s": 1504.0}, {"text": "does the writing for the graph right between", "timestamp": "00:25:07,876", "timestamp_s": 1507.0}, {"text": "the invoker and the runner. They are both coordinating. I think", "timestamp": "00:25:10,996", "timestamp_s": 1510.0}, {"text": "the runner definitely do some high level management, but one", "timestamp": "00:25:14,424", "timestamp_s": 1514.0}, {"text": "of the initialization inputs to", "timestamp": "00:25:17,992", "timestamp_s": 1517.0}, {"text": "the invoker implicitly is something called a tracer. And again", "timestamp": "00:25:21,768", "timestamp_s": 1521.0}, {"text": "you can think of the tracer as just being a new graph", "timestamp": "00:25:24,952", "timestamp_s": 1524.0}, {"text": "in question. As we\u0027re going to see. You can actually have construct multiple", "timestamp": "00:25:28,414", "timestamp_s": 1528.0}, {"text": "graphs inside of one runner,", "timestamp": "00:25:32,242", "timestamp_s": 1532.0}, {"text": "which we are going to see shortly. That is you can say like okay,", "timestamp": "00:25:35,602", "timestamp_s": 1535.0}, {"text": "I want to plan different experiments. And again, this fits perfectly for our", "timestamp": "00:25:38,732", "timestamp_s": 1538.0}, {"text": "use case, since first we want to run one set of operations that", "timestamp": "00:25:42,048", "timestamp_s": 1542.0}, {"text": "runs our task inputs and takes", "timestamp": "00:25:46,350", "timestamp_s": 1546.0}, {"text": "the average, and another set of operations that", "timestamp": "00:25:50,272", "timestamp_s": 1550.0}, {"text": "take that average and adds it to the state of", "timestamp": "00:25:53,652", "timestamp_s": 1553.0}, {"text": "the different context. Examples, again, that should have no", "timestamp": "00:25:57,604", "timestamp_s": 1557.0}, {"text": "idea about the task, and then see what happens when this average", "timestamp": "00:26:01,268", "timestamp_s": 1561.0}, {"text": "vector is added to it.", "timestamp": "00:26:05,118", "timestamp_s": 1565.0}, {"text": "So the runner is the high level context manager, and then each", "timestamp": "00:26:08,310", "timestamp_s": 1568.0}, {"text": "basic subgraph experiment that we want", "timestamp": "00:26:12,136", "timestamp_s": 1572.0}, {"text": "to run is contained in the", "timestamp": "00:26:15,548", "timestamp_s": 1575.0}, {"text": "invoker context. Interventions, every read", "timestamp": "00:26:18,812", "timestamp_s": 1578.0}, {"text": "and write intervention, all the iOS are what are the nodes in", "timestamp": "00:26:22,092", "timestamp_s": 1582.0}, {"text": "there of type tracing node, which again are", "timestamp": "00:26:26,604", "timestamp_s": 1586.0}, {"text": "what inform what our entire graph is made of to start with.", "timestamp": "00:26:30,972", "timestamp_s": 1590.0}, {"text": "And I said I was going to speak on why we need save. So again,", "timestamp": "00:26:34,944", "timestamp_s": 1594.0}, {"text": "remember that because this isn\u0027t running locally,", "timestamp": "00:26:37,696", "timestamp_s": 1597.0}, {"text": "we have to explicitly tell the model to save any", "timestamp": "00:26:42,006", "timestamp_s": 1602.0}, {"text": "value that we want to read outside of the context, because the standard", "timestamp": "00:26:45,636", "timestamp_s": 1605.0}, {"text": "behavior is when the context is exited, the model actually runs", "timestamp": "00:26:49,620", "timestamp_s": 1609.0}, {"text": "with all our interventions, but because these values are so", "timestamp": "00:26:53,258", "timestamp_s": 1613.0}, {"text": "large, we actually have to explicitly tell okay, please, I would like you to return", "timestamp": "00:26:57,108", "timestamp_s": 1617.0}, {"text": "several hundreds of thousands of vector values", "timestamp": "00:27:00,744", "timestamp_s": 1620.0}, {"text": "to me, because that is important. So that is the only reason why we can", "timestamp": "00:27:04,574", "timestamp_s": 1624.0}, {"text": "access hidden state outside context, otherwise we wouldn\u0027t need to.", "timestamp": "00:27:07,528", "timestamp_s": 1627.0}, {"text": "So perhaps this was just a temporary variable that", "timestamp": "00:27:11,052", "timestamp_s": 1631.0}, {"text": "we needed to use for our computation, which is fine", "timestamp": "00:27:14,668", "timestamp_s": 1634.0}, {"text": "if we have no intention to access it after the", "timestamp": "00:27:18,810", "timestamp_s": 1638.0}, {"text": "contact closes, we wouldn\u0027t put save. So this is only because we want to hold", "timestamp": "00:27:22,668", "timestamp_s": 1642.0}, {"text": "on to the value. So this is just one of the examples where we have", "timestamp": "00:27:26,032", "timestamp_s": 1646.0}, {"text": "to remind ourselves of the difference between running the model locally", "timestamp": "00:27:29,008", "timestamp_s": 1649.0}, {"text": "and just simply using, building an intervention", "timestamp": "00:27:32,970", "timestamp_s": 1652.0}, {"text": "graph for a remote resource that is going to run", "timestamp": "00:27:36,842", "timestamp_s": 1656.0}, {"text": "immediately we leave the context. And again,", "timestamp": "00:27:41,090", "timestamp_s": 1661.0}, {"text": "this is from documentation, basically showing how", "timestamp": "00:27:44,616", "timestamp_s": 1664.0}, {"text": "each. So here you can see that each line of intervention.", "timestamp": "00:27:48,760", "timestamp_s": 1668.0}, {"text": "So the first green arrow on the left blue", "timestamp": "00:27:52,222", "timestamp_s": 1672.0}, {"text": "box is a right. That is, we\u0027re setting some layers", "timestamp": "00:27:55,598", "timestamp_s": 1675.0}, {"text": "output to zero and the next is a read and", "timestamp": "00:27:59,426", "timestamp_s": 1679.0}, {"text": "the third is also a read. But you", "timestamp": "00:28:03,372", "timestamp_s": 1683.0}, {"text": "see here, we use the dot save because we do want this", "timestamp": "00:28:06,668", "timestamp_s": 1686.0}, {"text": "value to be sent over the network when the model isn\u0027t running. And you see", "timestamp": "00:28:11,212", "timestamp_s": 1691.0}, {"text": "the output of this is this intervention graph in the middle.", "timestamp": "00:28:14,688", "timestamp_s": 1694.0}, {"text": "And this is basically what is what is sent over the network in one direction.", "timestamp": "00:28:18,480", "timestamp_s": 1698.0}, {"text": "And then the result for things that we ask", "timestamp": "00:28:22,438", "timestamp_s": 1702.0}, {"text": "the model that we ask the graph to save", "timestamp": "00:28:26,148", "timestamp_s": 1706.0}, {"text": "are then sent back in the other direction when the execution is done.", "timestamp": "00:28:29,652", "timestamp_s": 1709.0}, {"text": "Again, just to remind ourselves on what we\u0027re trying to do now, we have an", "timestamp": "00:28:33,730", "timestamp_s": 1713.0}, {"text": "idea of what our library looks like and", "timestamp": "00:28:36,968", "timestamp_s": 1716.0}, {"text": "how we use it is that we want to pass", "timestamp": "00:28:40,792", "timestamp_s": 1720.0}, {"text": "in some context, we want to run it.", "timestamp": "00:28:44,616", "timestamp_s": 1724.0}, {"text": "Remember, we\u0027re only interested in what happens by the column, so we", "timestamp": "00:28:47,770", "timestamp_s": 1727.0}, {"text": "will be indexing to get only the", "timestamp": "00:28:51,644", "timestamp_s": 1731.0}, {"text": "vector at the very extreme end. Because the", "timestamp": "00:28:55,708", "timestamp_s": 1735.0}, {"text": "idea is that is the token that will", "timestamp": "00:28:58,908", "timestamp_s": 1738.0}, {"text": "contain information on what is to come next. Right? Again,", "timestamp": "00:29:02,704", "timestamp_s": 1742.0}, {"text": "just as a result of how transform architectures work is, the next", "timestamp": "00:29:05,856", "timestamp_s": 1745.0}, {"text": "prediction is containing the last token. To what end do we want to", "timestamp": "00:29:09,872", "timestamp_s": 1749.0}, {"text": "do that to this end? So we could do two sets of runs.", "timestamp": "00:29:13,364", "timestamp_s": 1753.0}, {"text": "The first run is to pass a bunch", "timestamp": "00:29:16,458", "timestamp_s": 1756.0}, {"text": "of examples doing the task we want. Again,", "timestamp": "00:29:20,074", "timestamp_s": 1760.0}, {"text": "this is exactly like how you would tell Chat GPT something", "timestamp": "00:29:23,892", "timestamp_s": 1763.0}, {"text": "like, I want you to give me words. And opposite, like this", "timestamp": "00:29:27,832", "timestamp_s": 1767.0}, {"text": "example, old, young, separated by Colin. Then it", "timestamp": "00:29:31,192", "timestamp_s": 1771.0}, {"text": "does this thing, right? So this is basically just like prompting", "timestamp": "00:29:35,624", "timestamp_s": 1775.0}, {"text": "it with the, this is similar", "timestamp": "00:29:39,218", "timestamp_s": 1779.0}, {"text": "to prompting with the format of code you want. But in this case we\u0027re actually", "timestamp": "00:29:43,116", "timestamp_s": 1783.0}, {"text": "just going to look at the very last token", "timestamp": "00:29:46,332", "timestamp_s": 1786.0}, {"text": "and then right before, when it\u0027s right on the verge", "timestamp": "00:29:50,450", "timestamp_s": 1790.0}, {"text": "of predicting, we just take a first, don\u0027t know the computation", "timestamp": "00:29:53,718", "timestamp_s": 1793.0}, {"text": "to know that, okay, this is a word and opposite game we\u0027re playing, and I", "timestamp": "00:29:56,902", "timestamp_s": 1796.0}, {"text": "am to predict the opposite of the last thing I saw before this token.", "timestamp": "00:30:00,288", "timestamp_s": 1800.0}, {"text": "So, right when it\u0027s supposedly figured all that out, we want", "timestamp": "00:30:03,818", "timestamp_s": 1803.0}, {"text": "to just snatch that vector and average a bunch of them", "timestamp": "00:30:07,268", "timestamp_s": 1807.0}, {"text": "out to get, hopefully a vector that represents in", "timestamp": "00:30:10,468", "timestamp_s": 1810.0}, {"text": "some pure form the very essence of the task that it", "timestamp": "00:30:13,832", "timestamp_s": 1813.0}, {"text": "has figured out, which is opposite function of previous", "timestamp": "00:30:17,528", "timestamp_s": 1817.0}, {"text": "experiment. That\u0027s the first part of the experiment. Then the second part of the experiment", "timestamp": "00:30:22,950", "timestamp_s": 1822.0}, {"text": "is to take this pure vector and then add it to a different context,", "timestamp": "00:30:26,142", "timestamp_s": 1826.0}, {"text": "a different series of examples that supposedly should have no idea", "timestamp": "00:30:30,018", "timestamp_s": 1830.0}, {"text": "what is going on, right? Because again, if you just told chat GBT encode column,", "timestamp": "00:30:33,596", "timestamp_s": 1833.0}, {"text": "it has no idea what you want is it can\u0027t read your mind", "timestamp": "00:30:38,270", "timestamp_s": 1838.0}, {"text": "yet. So this is", "timestamp": "00:30:42,112", "timestamp_s": 1842.0}, {"text": "called the zero shot intervention, which basically just means zero", "timestamp": "00:30:45,392", "timestamp_s": 1845.0}, {"text": "shot means zero examples of what you\u0027re looking", "timestamp": "00:30:49,456", "timestamp_s": 1849.0}, {"text": "for. Except now we\u0027re going to add this", "timestamp": "00:30:52,916", "timestamp_s": 1852.0}, {"text": "hopefully averaged out function has", "timestamp": "00:30:57,108", "timestamp_s": 1857.0}, {"text": "it just again feel compelled to do the thing that that", "timestamp": "00:31:01,252", "timestamp_s": 1861.0}, {"text": "vector was obtained from. Right, so how do", "timestamp": "00:31:04,692", "timestamp_s": 1864.0}, {"text": "we do the first part? That is the part where we just run a", "timestamp": "00:31:08,024", "timestamp_s": 1868.0}, {"text": "bunch of stuff and we extract the value", "timestamp": "00:31:11,064", "timestamp_s": 1871.0}, {"text": "at the very last column for all of them and average out.", "timestamp": "00:31:14,584", "timestamp_s": 1874.0}, {"text": "Cool. So again we have our trusted layout.", "timestamp": "00:31:18,890", "timestamp_s": 1878.0}, {"text": "Of course, first of all we have to determine what component we", "timestamp": "00:31:23,282", "timestamp_s": 1883.0}, {"text": "want to look at. Remember Mac, interp is all about having", "timestamp": "00:31:27,228", "timestamp_s": 1887.0}, {"text": "an observed and interesting observed behavior and trying to", "timestamp": "00:31:32,640", "timestamp_s": 1892.0}, {"text": "find the contribution of some discrete component, right? So in this case we", "timestamp": "00:31:36,272", "timestamp_s": 1896.0}, {"text": "narrow down by saying, okay, we want to look at layer eight again.", "timestamp": "00:31:40,320", "timestamp_s": 1900.0}, {"text": "In the actual experiment we run this for all the different layers, for all the", "timestamp": "00:31:44,370", "timestamp_s": 1904.0}, {"text": "different components of the model, and then we have like a plot of", "timestamp": "00:31:48,244", "timestamp_s": 1908.0}, {"text": "which of them happen to be most interesting. And then we drill down.", "timestamp": "00:31:52,644", "timestamp_s": 1912.0}, {"text": "But this is just showing an example of suppose we wanted to see what layer", "timestamp": "00:31:56,408", "timestamp_s": 1916.0}, {"text": "eight was doing as far as the task is concerned.", "timestamp": "00:32:00,078", "timestamp_s": 1920.0}, {"text": "Cool. So just imagine this done for a bunch of tens of", "timestamp": "00:32:04,390", "timestamp_s": 1924.0}, {"text": "other components. Cool. So we have a runner in Booger.", "timestamp": "00:32:07,852", "timestamp_s": 1927.0}, {"text": "Then here on line six we simply just", "timestamp": "00:32:11,634", "timestamp_s": 1931.0}, {"text": "do our trusted, as we would", "timestamp": "00:32:15,530", "timestamp_s": 1935.0}, {"text": "like to notice here that I don\u0027t do save because this", "timestamp": "00:32:18,988", "timestamp_s": 1938.0}, {"text": "value of hidden states, this variable is only needed", "timestamp": "00:32:23,072", "timestamp_s": 1943.0}, {"text": "for computation inside of my", "timestamp": "00:32:26,736", "timestamp_s": 1946.0}, {"text": "context, right? So I do not need to export", "timestamp": "00:32:30,208", "timestamp_s": 1950.0}, {"text": "this at this very point in time. I just simply need to take", "timestamp": "00:32:33,978", "timestamp_s": 1953.0}, {"text": "the variable, hold on to it, use it for other computation on len", "timestamp": "00:32:38,788", "timestamp_s": 1958.0}, {"text": "ten. And as you can see, line six", "timestamp": "00:32:42,058", "timestamp_s": 1962.0}, {"text": "is simply just the rightmost column. That is, sorry,", "timestamp": "00:32:45,352", "timestamp_s": 1965.0}, {"text": "beg your pardon, that is the", "timestamp": "00:32:49,352", "timestamp_s": 1969.0}, {"text": "on. Right. So between line six and seven we", "timestamp": "00:32:53,528", "timestamp_s": 1973.0}, {"text": "simply just take an example, we choose a layer and", "timestamp": "00:32:57,048", "timestamp_s": 1977.0}, {"text": "on line eight we say the sequence position should be, I think on", "timestamp": "00:33:00,764", "timestamp_s": 1980.0}, {"text": "line one, you see, we define that as minus one. So we just simply want", "timestamp": "00:33:04,508", "timestamp_s": 1984.0}, {"text": "to take the very last value, which is token. So again, all the dark gray", "timestamp": "00:33:07,484", "timestamp_s": 1987.0}, {"text": "bars is what line eight is holding", "timestamp": "00:33:10,918", "timestamp_s": 1990.0}, {"text": "onto. Then on line ten we simply just do the average.", "timestamp": "00:33:14,358", "timestamp_s": 1994.0}, {"text": "So we take that variable and we do the dot mean on the batch", "timestamp": "00:33:18,310", "timestamp_s": 1998.0}, {"text": "dimension, that\u0027s the 0th dimension. Again, that\u0027s the dimension of all the stacked examples", "timestamp": "00:33:21,862", "timestamp_s": 2001.0}, {"text": "on the right there. I think I just put like a clip out to show", "timestamp": "00:33:26,298", "timestamp_s": 2006.0}, {"text": "you what the vectors and", "timestamp": "00:33:28,788", "timestamp_s": 2008.0}, {"text": "matrixes will look like. So each of those stacks is just a batch dimension.", "timestamp": "00:33:32,788", "timestamp_s": 2012.0}, {"text": "So each of the examples of old, young,", "timestamp": "00:33:35,902", "timestamp_s": 2015.0}, {"text": "awake, asleep is represented by one of these slices.", "timestamp": "00:33:38,936", "timestamp_s": 2018.0}, {"text": "And we simply just want to average across that to get some hopefully", "timestamp": "00:33:42,550", "timestamp_s": 2022.0}, {"text": "pure vector that encodes the essence of opposites", "timestamp": "00:33:46,622", "timestamp_s": 2026.0}, {"text": "and that we want to save because that we want to send back. So it\u0027s", "timestamp": "00:33:51,370", "timestamp_s": 2031.0}, {"text": "kind of meant to be an efficient thing such that we don\u0027t want to send", "timestamp": "00:33:54,514", "timestamp_s": 2034.0}, {"text": "everything over the network, we don\u0027t want to send both the full,", "timestamp": "00:33:57,564", "timestamp_s": 2037.0}, {"text": "all the matrices. Thankfully we could decide to save everything and", "timestamp": "00:34:00,908", "timestamp_s": 2040.0}, {"text": "then compute locally. So again, this is just some of the considerations", "timestamp": "00:34:04,608", "timestamp_s": 2044.0}, {"text": "that you make when you remind yourself that actually there is", "timestamp": "00:34:08,022", "timestamp_s": 2048.0}, {"text": "some throughput cost and efficiency cost.", "timestamp": "00:34:12,770", "timestamp_s": 2052.0}, {"text": "So let\u0027s just do as much as we can on this", "timestamp": "00:34:15,556", "timestamp_s": 2055.0}, {"text": "environment and then just send down the most condensed", "timestamp": "00:34:19,300", "timestamp_s": 2059.0}, {"text": "version we want. Again, this should", "timestamp": "00:34:22,570", "timestamp_s": 2062.0}, {"text": "be similar to running using any remote resource or", "timestamp": "00:34:26,472", "timestamp_s": 2066.0}, {"text": "when you have trade offs between remote and local resources to contend", "timestamp": "00:34:29,928", "timestamp_s": 2069.0}, {"text": "with. Cool. So that is how we", "timestamp": "00:34:33,342", "timestamp_s": 2073.0}, {"text": "do the first part, that\u0027s how we get the averages. Literally this is all to", "timestamp": "00:34:36,648", "timestamp_s": 2076.0}, {"text": "do averages for one layer. And just imagine putting this in a for loop if", "timestamp": "00:34:39,308", "timestamp_s": 2079.0}, {"text": "you want to iterate over several layers. And for the", "timestamp": "00:34:42,604", "timestamp_s": 2082.0}, {"text": "second part, having possessed this average", "timestamp": "00:34:45,868", "timestamp_s": 2085.0}, {"text": "pure vector, which we called h, we want", "timestamp": "00:34:51,286", "timestamp_s": 2091.0}, {"text": "to then put h into our zero", "timestamp": "00:34:54,912", "timestamp_s": 2094.0}, {"text": "shot examples. Again, these are the examples that have no context on the task.", "timestamp": "00:34:59,216", "timestamp_s": 2099.0}, {"text": "They\u0027re just doing their own thing. They supposedly", "timestamp": "00:35:03,050", "timestamp_s": 2103.0}, {"text": "oblivious to the task we find interesting of opposites,", "timestamp": "00:35:06,538", "timestamp_s": 2106.0}, {"text": "but from nowhere they would just feel the urge to now just do opposites.", "timestamp": "00:35:11,090", "timestamp_s": 2111.0}, {"text": "Hopefully if we add this average vector", "timestamp": "00:35:14,574", "timestamp_s": 2114.0}, {"text": "state to them. And here is the example", "timestamp": "00:35:18,606", "timestamp_s": 2118.0}, {"text": "I mentioned where we\u0027re running two invoker", "timestamp": "00:35:21,976", "timestamp_s": 2121.0}, {"text": "contexts inside of the runner context. So basically the", "timestamp": "00:35:26,322", "timestamp_s": 2126.0}, {"text": "first is, again, we\u0027re trying to, as with", "timestamp": "00:35:30,268", "timestamp_s": 2130.0}, {"text": "any experiment, we have to have our control example", "timestamp": "00:35:34,028", "timestamp_s": 2134.0}, {"text": "or our reference or our baseline to say that,", "timestamp": "00:35:37,340", "timestamp_s": 2137.0}, {"text": "cool. Without adding this average vector, what does", "timestamp": "00:35:40,016", "timestamp_s": 2140.0}, {"text": "the model feel compelled to predict? So for simple,", "timestamp": "00:35:43,712", "timestamp_s": 2143.0}, {"text": "does it feel compelled to predict simple?", "timestamp": "00:35:46,960", "timestamp_s": 2146.0}, {"text": "Simpler? Maybe it just says cool. Simpler should be something to follow simple", "timestamp": "00:35:51,232", "timestamp_s": 2151.0}, {"text": "or given encode, does it feel compelled to predict base", "timestamp": "00:35:55,332", "timestamp_s": 2155.0}, {"text": "64? Or perhaps it does feel naturally compelled to decode,", "timestamp": "00:36:00,292", "timestamp_s": 2160.0}, {"text": "who knows? So the first run on line four and", "timestamp": "00:36:03,838", "timestamp_s": 2163.0}, {"text": "five is just again simply running the model and", "timestamp": "00:36:07,288", "timestamp_s": 2167.0}, {"text": "saving the output for the very last token.", "timestamp": "00:36:11,144", "timestamp_s": 2171.0}, {"text": "And the second is where we do the interesting stuff of running the", "timestamp": "00:36:15,510", "timestamp_s": 2175.0}, {"text": "model and basically intervening. So on line eleven we literally", "timestamp": "00:36:19,228", "timestamp_s": 2179.0}, {"text": "just plus equals two, which is identical to how", "timestamp": "00:36:23,042", "timestamp_s": 2183.0}, {"text": "we were taking the mean before. But again for this context we just do a", "timestamp": "00:36:26,588", "timestamp_s": 2186.0}, {"text": "plus equals two, add this value to the existing.", "timestamp": "00:36:29,628", "timestamp_s": 2189.0}, {"text": "And on line 13 we again just", "timestamp": "00:36:34,030", "timestamp_s": 2194.0}, {"text": "like line five save to see. Okay, cool. Now let\u0027s see what", "timestamp": "00:36:37,488", "timestamp_s": 2197.0}, {"text": "the predictions are and how similar they are, or to", "timestamp": "00:36:40,868", "timestamp_s": 2200.0}, {"text": "what extent this vector has changed", "timestamp": "00:36:44,532", "timestamp_s": 2204.0}, {"text": "the opinions of this model", "timestamp": "00:36:47,978", "timestamp_s": 2207.0}, {"text": "results. I mean,", "timestamp": "00:36:51,730", "timestamp_s": 2211.0}, {"text": "depending on your standards for impressive", "timestamp": "00:36:54,772", "timestamp_s": 2214.0}, {"text": "or not, this is what it looks like. This is what run one looks like.", "timestamp": "00:36:58,878", "timestamp_s": 2218.0}, {"text": "Just by doing that, we can see that. Indeed, on the third", "timestamp": "00:37:02,392", "timestamp_s": 2222.0}, {"text": "column here, adding that h vector does", "timestamp": "00:37:06,076", "timestamp_s": 2226.0}, {"text": "move the needle a bit does", "timestamp": "00:37:09,820", "timestamp_s": 2229.0}, {"text": "have the effect of the opposite function, right? So on", "timestamp": "00:37:14,572", "timestamp_s": 2234.0}, {"text": "the second column, we just see that the thing the model tries to do,", "timestamp": "00:37:18,268", "timestamp_s": 2238.0}, {"text": "if you tell the model, if you tell the model minimum column, it just repeats", "timestamp": "00:37:21,708", "timestamp_s": 2241.0}, {"text": "a lot of stuff, right? So it just says minimum is minimum, arrogant is", "timestamp": "00:37:25,846", "timestamp_s": 2245.0}, {"text": "arrogant, inside is inside. Although sometimes it does interesting things, like the", "timestamp": "00:37:29,168", "timestamp_s": 2249.0}, {"text": "fifth example from the bottom. If you say on, it says I.", "timestamp": "00:37:34,756", "timestamp_s": 2254.0}, {"text": "If you say answer, it says yes. Again,", "timestamp": "00:37:38,292", "timestamp_s": 2258.0}, {"text": "this is what the model feels compelled to just say if it has no context.", "timestamp": "00:37:41,572", "timestamp_s": 2261.0}, {"text": "But on the third column we see that in some examples", "timestamp": "00:37:45,910", "timestamp_s": 2265.0}, {"text": "we do manage to tilt its final judgment", "timestamp": "00:37:50,270", "timestamp_s": 2270.0}, {"text": "in a different direction. Now, I will mention though, that this", "timestamp": "00:37:54,302", "timestamp_s": 2274.0}, {"text": "is technically not where the paper stops.", "timestamp": "00:37:58,412", "timestamp_s": 2278.0}, {"text": "The paper decides to say beyond just averaging h.", "timestamp": "00:38:01,522", "timestamp_s": 2281.0}, {"text": "Remember, this is taking the value of", "timestamp": "00:38:05,468", "timestamp_s": 2285.0}, {"text": "the output of the entire layer. Remember, we just see layer eight.", "timestamp": "00:38:09,930", "timestamp_s": 2289.0}, {"text": "The paper takes it further by saying, okay, instead of just looking at layer eight,", "timestamp": "00:38:14,110", "timestamp_s": 2294.0}, {"text": "can we drill specifically into what component in layer", "timestamp": "00:38:17,408", "timestamp_s": 2297.0}, {"text": "eight is contributing? So, back to our reference architecture,", "timestamp": "00:38:20,778", "timestamp_s": 2300.0}, {"text": "neural network transformer", "timestamp": "00:38:25,010", "timestamp_s": 2305.0}, {"text": "component transformer block has different things. Our attention", "timestamp": "00:38:28,362", "timestamp_s": 2308.0}, {"text": "block, rather, excuse me, has the mass self attention,", "timestamp": "00:38:32,042", "timestamp_s": 2312.0}, {"text": "the feed forward, it has different things, layer norm,", "timestamp": "00:38:35,770", "timestamp_s": 2315.0}, {"text": "and they decide to drill into the contributions of", "timestamp": "00:38:39,124", "timestamp_s": 2319.0}, {"text": "the attention head. Again, the distinction isn\u0027t that important, but the experimental", "timestamp": "00:38:43,048", "timestamp_s": 2323.0}, {"text": "method is precisely the same. So they just find a way to drill into studying", "timestamp": "00:38:47,298", "timestamp_s": 2327.0}, {"text": "the contribution of the top x", "timestamp": "00:38:51,378", "timestamp_s": 2331.0}, {"text": "attention heads. And instead of averaging looking for the average of", "timestamp": "00:38:56,650", "timestamp_s": 2336.0}, {"text": "just all the components contributions, which again supposedly", "timestamp": "00:39:00,640", "timestamp_s": 2340.0}, {"text": "will have more noise, they basically tried to denoise by just narrowing", "timestamp": "00:39:04,422", "timestamp_s": 2344.0}, {"text": "in on a few, and with that the effect is way more obvious.", "timestamp": "00:39:07,878", "timestamp_s": 2347.0}, {"text": "But I don\u0027t include that for the purpose of this talk.", "timestamp": "00:39:11,840", "timestamp_s": 2351.0}, {"text": "And that was the talk. So if you are interested in", "timestamp": "00:39:15,410", "timestamp_s": 2355.0}, {"text": "looking at the NDIF project and Nn Insight Library", "timestamp": "00:39:18,708", "timestamp_s": 2358.0}, {"text": "as well, which is a companion, please view that site.", "timestamp": "00:39:22,298", "timestamp_s": 2362.0}, {"text": "And if you\u0027re interested in learning more on Mechinterp,", "timestamp": "00:39:26,244", "timestamp_s": 2366.0}, {"text": "many of the code snippets and", "timestamp": "00:39:29,570", "timestamp_s": 2369.0}, {"text": "basically concepts introduce you. They were introduced to myself", "timestamp": "00:39:33,284", "timestamp_s": 2373.0}, {"text": "on the platform called arena education. It is an awesome program.", "timestamp": "00:39:36,892", "timestamp_s": 2376.0}, {"text": "You should check it out. If you\u0027re interested in learning more on doing", "timestamp": "00:39:41,276", "timestamp_s": 2381.0}, {"text": "mechanistic interpretability. I hope you\u0027ve had as much fun", "timestamp": "00:39:45,228", "timestamp_s": 2385.0}, {"text": "going through this as I have. And do enjoy the rest of", "timestamp": "00:39:48,412", "timestamp_s": 2388.0}, {"text": "the conference.", "timestamp": "00:39:51,532", "timestamp_s": 2391.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'uv8CnN9lcpc',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Use of Python for Cutting edge Language Model research
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Join me as I share insights - from Financial Technology at Bloomberg to leading projects at Palantir Tech. Explore the pivot to LLMs, with a focus on Mechanistic Interpretability. Learn how Python, with its versatility, is the key to unraveling the potential of AI in shaping the future of humanity.</p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Bolu will talk about using Python for large language models research. Specifically, he'll be speaking on the insights Python library that enables mechanistic interpretability research. Any effort to understand how these models work will definitely continue to be increasingly important in the future.

              </li>
              
              <li>
                 mechanistic interpretability is a field of research that tackles this problem starting at a very granular level of the models. For today's talk, we're going to be picking one item out of the mechinterp toolkit, which is that of causal interventions.

              </li>
              
              <li>
                Recent research uses this kind of intervention to try to understand how a model achieves some outcomes. The question is, is it possible to have some functional components of large language models? The topic of interest today is that of function vectors.

              </li>
              
              <li>
                The insights package came along with an effort called the NDIF initiative, which is basically a national deep inference facility. This is a compute cluster that is available to researchers for doing work that cannot afford the financial burden of actually running these large models. For today's work, we're going to focus on insights.

              </li>
              
              <li>
                We have to explicitly tell the model to save any value that we want to read outside of the context. This is just one of the examples where we have to remind ourselves of the difference between running the model locally and just simply using, building an intervention graph for a remote resource.

              </li>
              
              <li>
                The paper decides to say beyond just averaging h. Can we drill specifically into what component in layer eight is contributing? If you're interested in learning more on doing mechanistic interpretability, I hope you've had as much fun going through this as I have.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/uv8CnN9lcpc.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:20,730'); seek(20.0)">
              Hi everyone, I'm Bolu and I am
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:23,892'); seek(23.0)">
              an independent AI researcher. And today I'm going
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:27,468'); seek(27.0)">
              to talk about using Python for large language models research.
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:32,490'); seek(32.0)">
              Specifically, I'll be speaking on the insights Python library
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:36,810'); seek(36.0)">
              that enables mechanistic interpretability research.
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:40,650'); seek(40.0)">
              So a bit of a primer on this field
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:43,964'); seek(43.0)">
              of inquiry. What is mechanistic interpretability?
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:48,570'); seek(48.0)">
              Some things we can hopefully all agree on. First is that neural networks
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:52,506'); seek(52.0)">
              solve an increase in number of important tasks.
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:55,890'); seek(55.0)">
              And the second is that it would be at least interesting and probably important
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:01:00,324'); seek(60.0)">
              to understand how they do that is interesting in the sense
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:03,956'); seek(63.0)">
              of, if you feel any sense of curiosity, to basically
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:07,384'); seek(67.0)">
              look inside this whole world that is currently black box to most people
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:11,510'); seek(71.0)">
              just out of. Because these models arrive at solutions
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:15,278'); seek(75.0)">
              that no person could write a program for. So, out of curiosity,
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:18,722'); seek(78.0)">
              it'd be interesting to know what are the algorithms being implemented and
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:22,652'); seek(82.0)">
              hopefully describing them in a human understandable way,
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:26,970'); seek(86.0)">
              and important in a sense that any sufficiently powerful
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:30,950'); seek(90.0)">
              system that is being put in strategic places
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:35,334'); seek(95.0)">
              of great importance in society has
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:39,152'); seek(99.0)">
              to have a certain level of transparency and understanding before we
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:42,292'); seek(102.0)">
              as a society can trust it to be deployed. Any effort to
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:45,812'); seek(105.0)">
              understand how these models work will definitely continue
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:49,476'); seek(109.0)">
              to be increasingly important in the future. Now,
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:52,292'); seek(112.0)">
              mechanistic interpretability, or mechinterp, as I'll call it going
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:55,988'); seek(115.0)">
              forward, because as you can imagine, it's a bit of a mouthful, is a
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:59,704'); seek(119.0)">
              field of research that tackles this problem starting at a very granular
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:03,246'); seek(123.0)">
              level of the models. And what does that mean by
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:06,588'); seek(126.0)">
              granular level? The typical mechanistic interpretability
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:10,578'); seek(130.0)">
              result provides a mechanistic model that basically means
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:14,396'); seek(134.0)">
              a causal model describing how different discrete components
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:18,306'); seek(138.0)">
              in a very large model arrive at some
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:22,012'); seek(142.0)">
              observed behavior. So we have some observed behavior, and the question is, can you,
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:25,792'); seek(145.0)">
              through experimental processes, arrive at
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:29,232'); seek(149.0)">
              an explanation for how these observations come to
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:32,788'); seek(152.0)">
              be? That is the mechanistic approach to it. Again, this is identifying
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:37,130'); seek(157.0)">
              mechinterp in a much larger field of interpretability,
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:40,794'); seek(160.0)">
              which can have different flavors to it. But mechanistic
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:44,398'); seek(164.0)">
              interpretability is unique in taking this granular causal
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:47,998'); seek(167.0)">
              model of trying to drill as deep as possible and hoping
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:51,454'); seek(171.0)">
              to build on larger and larger abstractions,
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:54,862'); seek(174.0)">
              but starting from the very granular level. And for
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:58,028'); seek(178.0)">
              today's talk, we're going to be picking one item out of the mechinterp
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:01,698'); seek(181.0)">
              toolkit, which is that of causal interventions.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:05,586'); seek(185.0)">
              So basically the idea is if we abstract the entire network
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:10,190'); seek(190.0)">
              to be a computational graph, that is, again, we forget that
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:13,808'); seek(193.0)">
              this is machine learning. Just imagine this has been any abstract computational
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:17,846'); seek(197.0)">
              graph, and the current state of
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:21,890'); seek(201.0)">
              not understanding simply means we don't know
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:25,428'); seek(205.0)">
              what computation each of the nodes are running and how they interact with each other.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:29,252'); seek(209.0)">
              So from that perspective, if we're curious about knowing how
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:33,236'); seek(213.0)">
              one component that is, either it is an attention head,
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:36,552'); seek(216.0)">
              or an MLP
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:40,142'); seek(220.0)">
              layer, or a linear layer or an embedding unit, again, you don't have to worry
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:43,810'); seek(223.0)">
              about any of these means. You can just abstract these as being any node in
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:46,952'); seek(226.0)">
              some compute graph. But if you do, it'll help to paint a picture better.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:51,100'); seek(231.0)">
              So if we're curious to know what any of these nodes contribute to
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:55,852'); seek(235.0)">
              start with, even knowing if they contribute anything to start with,
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:59,310'); seek(239.0)">
              one way of doing that is simply taking the node and
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:04,190'); seek(244.0)">
              observing some behavior that we find interesting
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:07,872'); seek(247.0)">
              and then changing the impute to that node to see if the downstream
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:11,466'); seek(251.0)">
              impact for the observed behavior is noteworthy.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:15,370'); seek(255.0)">
              That is, if this node, in this example, the node d,
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:20,210'); seek(260.0)">
              is very vital to some observed behavior downstream.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:23,578'); seek(263.0)">
              If we mess with it a bit, that's if we perturb the node,
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:26,878'); seek(266.0)">
              the observed result should change. That means, okay, this node
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:30,638'); seek(270.0)">
              is on the critical path from input to output for this observed
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:34,158'); seek(274.0)">
              behavior. Of course, we expect some part
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:37,576'); seek(277.0)">
              of the model to change if you mess with anything. But the whole point
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:41,068'); seek(281.0)">
              of this is that we have to first of all settle on some observed behavior,
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:44,322'); seek(284.0)">
              and then we tweak the value of some node of interest and
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:47,804'); seek(287.0)">
              then we observe downstream. If, however, it doesn't have any impact,
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:51,618'); seek(291.0)">
              then that means this node is not that important and then we
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:54,864'); seek(294.0)">
              can ignore it. But if it is, then we know that we can drill deeper.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:58,710'); seek(298.0)">
              So I think in the rest of the course, I'm going to speak on a
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:01,748'); seek(301.0)">
              practical example in very recent research that
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:05,012'); seek(305.0)">
              uses this kind of intervention to try to understand how
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:09,252'); seek(309.0)">
              a model achieves some outcomes. The topic
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:13,146'); seek(313.0)">
              of interest today is that of function vectors. So this is a very
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:16,872'); seek(316.0)">
              recent paper, I think just published last year,
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:19,416'); seek(319.0)">
              October, from a group from Northeastern University,
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:24,710'); seek(324.0)">
              from Corey College of Computer Science.
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:27,670'); seek(327.0)">
              Basically, it is a mechanistic interoperability research effort
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:31,770'); seek(331.0)">
              that tries to observe some behavior in large language
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:35,378'); seek(335.0)">
              models, and that behavior is
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:38,880'); seek(338.0)">
              described thus. So the question is the hypothesis,
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:42,910'); seek(342.0)">
              is it possible to have some
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:46,336'); seek(346.0)">
              functional components of large language models?
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:50,430'); seek(350.0)">
              That is, we can all agree that if I gave,
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:53,972'); seek(353.0)">
              looking at the top left section here of a string
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:57,738'); seek(357.0)">
              of input, that is arrive column, depart small column,
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:01,162'); seek(361.0)">
              big common column, if I
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:04,484'); seek(364.0)">
              gave this input to something like, say, chad GPT, I think we can
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:07,768'); seek(367.0)">
              all agree that it will figure out, okay, this is a simple word and opposite
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:11,118'); seek(371.0)">
              game. That is the first example at the top.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:14,008'); seek(374.0)">
              And the second example, I believe is converting to Spanish.
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:17,930'); seek(377.0)">
              I think we can all agree that something like chat, GBT and
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:21,292'); seek(381.0)">
              similar large language models are able to do such a thing.
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:25,290'); seek(385.0)">
              Is it possible for me to take some kernel
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:29,014'); seek(389.0)">
              of this function of opposite, say again, taking the first example,
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:32,720'); seek(392.0)">
              and transfer it to a completely different context and
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:36,432'); seek(396.0)">
              have that same behavior operate on
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:40,256'); seek(400.0)">
              a token in this new context? What that means is on
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:43,828'); seek(403.0)">
              the right you see the direction of the arrow. The example, the counterpart on the
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:47,364'); seek(407.0)">
              right, simply says the word fast means.
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:51,490'); seek(411.0)">
              Now, under the normal operation of a large language module
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:55,674'); seek(415.0)">
              model, trying to predict the next token, you can say something like the word fast
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:59,524'); seek(419.0)">
              means quick, or it means going quickly or any reasonable
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:03,498'); seek(423.0)">
              thing to follow. However, if this hypothesis
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:08,162'); seek(428.0)">
              of portability of functions, we should
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:11,388'); seek(431.0)">
              be able to move something from this context on the left
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:15,452'); seek(435.0)">
              that clearly is about word and opposite into a completely new
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:18,928'); seek(438.0)">
              context that has no conception of word and opposite
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:22,742'); seek(442.0)">
              as an objective and achieve the result of
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:26,608'); seek(446.0)">
              flipping the word fast into slow. I know
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:30,592'); seek(450.0)">
              it seems very almost crazy to
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:34,068'); seek(454.0)">
              expect this is true, but let's just assume this is the leading hypothesis.
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:37,802'); seek(457.0)">
              And of course we're going to discuss what exactly this thing will be exporting is.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:42,790'); seek(462.0)">
              We see there the letter a average layer activation.
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:46,862'); seek(466.0)">
              What the hypothesis says is this thing in quote that we
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:50,968'); seek(470.0)">
              plan to port over is simply the
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:54,872'); seek(474.0)">
              average activation over a series of
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:58,152'); seek(478.0)">
              projects for a given task. Again, I'm going to break that down a bit.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:01,532'); seek(481.0)">
              So again, let's say our task is simple or an opposite. So we have three
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:04,876'); seek(484.0)">
              different examples. Old young, vanish,
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:07,938'); seek(487.0)">
              appear, dark. Colin and I
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:11,248'); seek(491.0)">
              guess something like bright or dark and light will follow. And the
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:14,848'); seek(494.0)">
              second example, the same thing. Awake, sleep, future, past,
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:18,512'); seek(498.0)">
              joy. Colin at the very
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:21,716'); seek(501.0)">
              end of all these contexts, these like query
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:25,354'); seek(505.0)">
              inputs, the neural network is right on the
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:28,868'); seek(508.0)">
              verge of doing the thing called flip opposite the
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:32,884'); seek(512.0)">
              last thing I saw before my column. So the hypothesis is
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:37,112'); seek(517.0)">
              if we can take that activation state and in
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:41,512'); seek(521.0)">
              the section b you see there, simply add it
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:44,632'); seek(524.0)">
              to a completely unrelated context,
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:47,534'); seek(527.0)">
              would it be possible to observe the same behavior? Because again on
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:50,988'); seek(530.0)">
              the right we see fair simple. In the absence of this
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:54,572'); seek(534.0)">
              intervention, we have no reason to expect the model will
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:58,272'); seek(538.0)">
              say anything other than simple. Then something
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:02,256'); seek(542.0)">
              like simple, easy, or whatever the model finds appropriate to
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:06,272'); seek(546.0)">
              follow simple. But if indeed our intervention is important,
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:09,840'); seek(549.0)">
              we expect to observe something like simple colon
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:13,178'); seek(553.0)">
              complex or at the bottom there encode becomes decode
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:16,554'); seek(556.0)">
              just magically by intervening with this average
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:21,010'); seek(561.0)">
              activation state. Again, I would explain what we mean by activation state in the
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:25,044'); seek(565.0)">
              following line, but I hope you just get the general thesis of what this is
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:27,864'); seek(567.0)">
              meant to be. That is the question is, is there a portable
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:31,246'); seek(571.0)">
              component of operations
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:35,182'); seek(575.0)">
              and functions inside of neural networks
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:38,594'); seek(578.0)">
              and more specifically large language models? All right, so I
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:42,044'); seek(582.0)">
              guess to give a bit of shape what I mean
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:45,916'); seek(585.0)">
              by activation vector and what is being ported left and right.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:49,152'); seek(589.0)">
              So here, this is just like a typical one layer example
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:53,632'); seek(593.0)">
              of an LLM decoder, only what
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:57,328'); seek(597.0)">
              we have here is at the very bottom, we input a token,
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:01,210'); seek(601.0)">
              a sequence of tokens, right? That is like the on colon,
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:04,586'); seek(604.0)">
              off wet column, dry old colon.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:08,290'); seek(608.0)">
              And as we see, the expectation is as this
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:12,036'); seek(612.0)">
              input passes through subsequent layers in,
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:16,550'); seek(616.0)">
              there's one single set of vectors that are going to keep being updated and
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:20,312'); seek(620.0)">
              changed and added on. And again, due to some specifics
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:23,438'); seek(623.0)">
              to the neural network architectures, more specifically the
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:26,712'); seek(626.0)">
              skip connections, which I won't get too much into right now, each subsequent
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:30,482'); seek(630.0)">
              layer adds additional context that is literally just added
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:34,082'); seek(634.0)">
              on top the last. But in any way that's not really important for now.
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:36,876'); seek(636.0)">
              So let's just think of it. For example, again,
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:39,790'); seek(639.0)">
              looking at the journey of the column, the very last column,
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:44,590'); seek(644.0)">
              when it goes to the embedding layer, it has some vector
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:48,422'); seek(648.0)">
              that represents okay, cool. This is how the
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:51,892'); seek(651.0)">
              neural network's embedding layer represents the token of
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:55,572'); seek(655.0)">
              a column. And again, so we can kind of anthropomorphize,
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:00,370'); seek(660.0)">
              pretend it's like self aware almost to say
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:04,052'); seek(664.0)">
              I am a column. Because technically, if you took that embedding vector
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:08,958'); seek(668.0)">
              and you put it through the unembedding vector on the other
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:12,232'); seek(672.0)">
              side, it would come out as column is really likely to come,
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:15,656'); seek(675.0)">
              right? So we might as well just see this as the model
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:18,940'); seek(678.0)">
              being what information the model has for that position in
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:22,844'); seek(682.0)">
              the sequence. So somewhere between starting from I am
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:26,588'); seek(686.0)">
              a column in the beginning to the very end
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:29,948'); seek(689.0)">
              of the thing that follows me is the word new.
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:34,400'); seek(694.0)">
              The model has learned some interesting things, right? By definition, like how
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:38,352'); seek(698.0)">
              else would it know? Again, because it's still
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:41,572'); seek(701.0)">
              that same column vector that has been
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:45,412'); seek(705.0)">
              updated for the sequence position of the token column. So the
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:49,028'); seek(709.0)">
              conjecture here for the hypothesis of portable functions is
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:52,308'); seek(712.0)">
              that somewhere in between or containing that vector is information
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:56,148'); seek(716.0)">
              on amicolum, of course, which it had before. And it
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:59,752'); seek(719.0)">
              also has my next is new. That is, my next token
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:03,710'); seek(723.0)">
              is the word new, which again is just what
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:06,808'); seek(726.0)">
              we would observe from Chad GPT so the additional thing the hypothesis is saying
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:10,636'); seek(730.0)">
              is that, or is asking is that, is there a component that
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:14,796'); seek(734.0)">
              encodes the operation that it must do or the function it
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:18,444'); seek(738.0)">
              must do to arrive at new, perhaps before it came
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:22,112'); seek(742.0)">
              to the conclusion of the next is new, is there a component
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:25,718'); seek(745.0)">
              that says I am to do or am to call the function opposite
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:29,894'); seek(749.0)">
              on. Surely there must be of some sort, because how
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:33,588'); seek(753.0)">
              else would it know to come up with new.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:36,530'); seek(756.0)">
              But the question is if there is linearity to this representation
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:40,850'); seek(760.0)">
              by linearity is just what allows us to do things like
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:44,132'); seek(764.0)">
              this? Literally take a thing, add it average,
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:47,870'); seek(767.0)">
              and add it somewhere else and have it do things right. This assumes a lot
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:51,208'); seek(771.0)">
              of linear behavior. So this is kind of the underlying
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:54,942'); seek(774.0)">
              implicit assumption that is guiding this hypothesis.
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:58,578'); seek(778.0)">
              To start with many of the different
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:02,524'); seek(782.0)">
              research inquiries leads to very interesting result.
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:06,170'); seek(786.0)">
              Often start with this assumption of can we assume there's lowlinearity? And again,
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:09,536'); seek(789.0)">
              due to details of the architecture of most
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:13,552'); seek(793.0)">
              transformer neural networks,
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:17,150'); seek(797.0)">
              there are reasons to expect there to be low linearity. But just to see it
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:20,960'); seek(800.0)">
              happen for real is always interesting. And I think this is the first time we're
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:24,538'); seek(804.0)">
              seeing it in the context of operations as again, just representations,
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:28,266'); seek(808.0)">
              which I think other research has demonstrated before,
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:31,570'); seek(811.0)">
              such as for example, the relationship between the word
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:34,868'); seek(814.0)">
              car and cars, that is, the relationship between a word
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:38,520'); seek(818.0)">
              and its plural. There's been some regularities observed
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:42,366'); seek(822.0)">
              in that regard. But this, however, is trying to take it a step further to
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:45,848'); seek(825.0)">
              say, okay, are there encodings also for functions?
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:49,106'); seek(829.0)">
              Okay, so we have a rough idea of what it means
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:52,780'); seek(832.0)">
              for what this h is. It's simply just some vector
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:56,730'); seek(836.0)">
              that at the very end of the network, right before it
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:00,412'); seek(840.0)">
              goes into the penultimate layer, or at the penultimate
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:03,842'); seek(843.0)">
              layer, we could run our model three different times
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:08,044'); seek(848.0)">
              and snatch that vector across, cross all of them, look at exactly what
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:11,684'); seek(851.0)">
              read literally what that vector is saying. Because again,
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:14,996'); seek(854.0)">
              the information on what is to come next is embedded
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:18,650'); seek(858.0)">
              in the colon token, right? It's the thing that is saying,
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:22,036'); seek(862.0)">
              okay, dark. So all the information
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:25,620'); seek(865.0)">
              for what is after dot, dot, dot is in colon. Cool. So we take
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:29,032'); seek(869.0)">
              that for different runs and we average it out and try to add.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:33,190'); seek(873.0)">
              So that gives you an idea of just to draw
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:36,728'); seek(876.0)">
              a bit of a picture to it.
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:39,930'); seek(879.0)">
              And of course, this is just restating the same thing now that we have an
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:43,004'); seek(883.0)">
              idea of what h means and what that vector is.
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:46,684'); seek(886.0)">
              So for each of the different runs in
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:50,928'); seek(890.0)">
              a series of prompts that are basically doing the same task,
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:54,886'); seek(894.0)">
              if we literally took all the values of the vectors,
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:14:58,422'); seek(898.0)">
              averaged them in position, added, divided by this
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:02,576'); seek(902.0)">
              unified, averaged out mean vector,
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:05,510'); seek(905.0)">
              and we took it into a different environment, into a different context,
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:08,602'); seek(908.0)">
              and we literally just added it to something else.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:12,690'); seek(912.0)">
              The question is, will we be able to get effects
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:15,834'); seek(915.0)">
              like seen below? That is, if we took. So I think here in the example
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:19,496'); seek(919.0)">
              you see the representation for encode, again, so encode
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:23,342'); seek(923.0)">
              column. So there you can see
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:26,732'); seek(926.0)">
              how we can presume that without this intervention at the end,
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:30,380'); seek(930.0)">
              after this token goes through the
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:33,788'); seek(933.0)">
              entire model, it might say something like the
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:36,908'); seek(936.0)">
              thing to come after the column is base 64, I guess, because maybe
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:40,912'); seek(940.0)">
              encoding and base 64 is something that shows up
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:44,096'); seek(944.0)">
              often, right? Remember, the base function
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:48,032'); seek(948.0)">
              of a large language model is just to predict
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:51,338'); seek(951.0)">
              the next most likely thing in human generated
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:55,242'); seek(955.0)">
              text. However, with the addition of our
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:15:59,684'); seek(959.0)">
              supposed, our hypothetical average out opposite
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:03,630'); seek(963.0)">
              function, would we be able to steer
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:07,822'); seek(967.0)">
              it towards saying something like, actually, instead of saying encode base 64,
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:12,790'); seek(972.0)">
              I all of a sudden feel the urge to say the opposite of
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:16,732'); seek(976.0)">
              encode and dev, say encode column decode. This is
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:20,188'); seek(980.0)">
              the hypothesis. So again, it would be super interesting and kind
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:23,868'); seek(983.0)">
              of weird if we can indeed prove
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:27,490'); seek(987.0)">
              this representation. For example, that just has 123456.
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:32,672'); seek(992.0)">
              Again, our vector just has about six different dimensions.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:37,446'); seek(997.0)">
              Encoding it. Of course, as we know, actual large language models
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:41,370'); seek(1001.0)">
              can be much bigger than this in
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:44,948'); seek(1004.0)">
              the billions and billions of parameters. So how
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:49,076'); seek(1009.0)">
              exactly do we plan to do this? Multiple runs
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:52,778'); seek(1012.0)">
              and extracting values and averaging them and intervening and adding
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:55,998'); seek(1015.0)">
              them in the real world, not for a toy model.
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:16:59,830'); seek(1019.0)">
              And that brings us to our trusted interpretability libraries
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:03,918'); seek(1023.0)">
              and packages. These are packages that are designed solely for
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:07,628'); seek(1027.0)">
              this purpose of staring very deep
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:11,042'); seek(1031.0)">
              into what large
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:14,236'); seek(1034.0)">
              language models of different sizes are up to in
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:18,268'); seek(1038.0)">
              a way that is practical to enable this kind of research.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:21,390'); seek(1041.0)">
              So we have an insight which is particularly popular
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:25,526'); seek(1045.0)">
              for working with models on the larger side, and I'll discuss
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:30,030'); seek(1050.0)">
              the details of its architecture that afford this
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:33,332'); seek(1053.0)">
              kind of behavior. Then we have transformer lens,
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:37,010'); seek(1057.0)">
              which is also a very great open source
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:40,218'); seek(1060.0)">
              library for doing this. But for today's work, we're going to focus on insights.
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:45,110'); seek(1065.0)">
              So what is it about insights that
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:48,248'); seek(1068.0)">
              makes it work? What is the contact on insight? Where did it come from?
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:17:53,030'); seek(1073.0)">
              From my understanding, the insights package came
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:17:57,068'); seek(1077.0)">
              along with an effort called the
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:01,180'); seek(1081.0)">
              NDIF initiative, which is
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:05,052'); seek(1085.0)">
              basically a national deep inference facility.
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:07,770'); seek(1087.0)">
              This is basically a compute cluster that is available to researchers for
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:11,808'); seek(1091.0)">
              doing work that cannot afford the financial burden
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:15,686'); seek(1095.0)">
              of actually running these very large models because they're very costly.
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:19,510'); seek(1099.0)">
              Forget just training, even just running inference on them is quite expensive. So basically you
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:23,428'); seek(1103.0)">
              have this remote cluster of
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:27,252'); seek(1107.0)">
              compute that has been made available to researchers,
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:31,002'); seek(1111.0)">
              and the insights package was basically made as
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:36,550'); seek(1116.0)">
              a point, as an interface
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:40,238'); seek(1120.0)">
              to this compute cluster. So the typical workflow, as is seen here
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:43,768'); seek(1123.0)">
              in this schematic, is that you have the
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:48,956'); seek(1128.0)">
              researcher working locally, basically writing interventions
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:53,170'); seek(1133.0)">
              for how they want to run their experiments and intervene with networks,
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:56,818'); seek(1136.0)">
              which we are going to see. And this is basically change into
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:00,432'); seek(1140.0)">
              a compute graph, or more specifically an intervention graph, as like,
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:03,888'); seek(1143.0)">
              this is how I want the running of this very large model
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:07,584'); seek(1147.0)">
              to be tweaked.
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:11,570'); seek(1151.0)">
              And this is then sent over the network into this cluster
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:15,514'); seek(1155.0)">
              to say, okay, cool, please run this 70 billion model,
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:21,090'); seek(1161.0)">
              70 billion parameter model that I definitely cannot run on
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:24,228'); seek(1164.0)">
              my M one MacBook, but run it with
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:27,752'); seek(1167.0)">
              these different interventions that make it look like that, make it no different from if
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:31,352'); seek(1171.0)">
              I could actually run this locally. And as you
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:35,404'); seek(1175.0)">
              can see, the thing between this boundary of the local environment
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:38,706'); seek(1178.0)">
              to the NDIF infrastructure
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:43,202'); seek(1183.0)">
              is simply this compute graph, and this compute graph is the output
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:46,882'); seek(1186.0)">
              of the Nnsite library, and we'll
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:50,454'); seek(1190.0)">
              see how it does. Cool. So that is the motivational
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:53,926'); seek(1193.0)">
              setup for why an insight exists. It's basically a
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:57,104'); seek(1197.0)">
              counterpart to the NDIF project, which is super interesting,
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:00,576'); seek(1200.0)">
              by the way. Again, I think they just released their paper last November announcing
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:05,786'); seek(1205.0)">
              the launch of the NDIF facility. It is live right now, I believe so,
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:09,668'); seek(1209.0)">
              yeah. Really exciting project. I encourage anyone that's looking for
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:13,428'); seek(1213.0)">
              computer resources, for inference in particular.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:17,090'); seek(1217.0)">
              And again, this has nothing to do with training. It's just like if you want
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:19,928'); seek(1219.0)">
              to run a big model several times and do different
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:23,336'); seek(1223.0)">
              interventions or read stuff from it to learn more, as we do
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:27,052'); seek(1227.0)">
              with our hypothesis in question, then it
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:31,692'); seek(1231.0)">
              works great. But of course, the library also offers
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:35,458'); seek(1235.0)">
              the option of just running the if you happen to have
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:39,790'); seek(1239.0)">
              several gigabytes of ram to spare.
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:43,550'); seek(1243.0)">
              Okay, so let's jump into the code. What does
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:46,992'); seek(1246.0)">
              it look like to do an intervention? By intervention, we just simply means
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:51,220'); seek(1251.0)">
              anything that either writes or reads execution
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:56,458'); seek(1256.0)">
              state of our model. That is,
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:21:00,372'); seek(1260.0)">
              again, you have a model we put in a token sequence, and then
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:21:04,516'); seek(1264.0)">
              stage after stage, the output of one stage is passed to
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:08,008'); seek(1268.0)">
              the next, and that is added to the residual stream, which is just, again,
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:10,872'); seek(1270.0)">
              think of it as like this ever accumulating output
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:15,134'); seek(1275.0)">
              of each component in the model that eventually leads
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:18,322'); seek(1278.0)">
              to a probability distribution or output that we
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:22,108'); seek(1282.0)">
              observe. So if we ever want to poke into it either like
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:26,812'); seek(1286.0)">
              use our binoculars or microscope to
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:30,508'); seek(1290.0)">
              look in, that is one type of intervention, as you can see here on line
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:34,016'); seek(1294.0)">
              five. Again, you can ignore the stuff above, I will explain that
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:37,344'); seek(1297.0)">
              later. But just to dive straight into what exactly the interventions
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:40,982'); seek(1300.0)">
              are, again, what are the things that make up these arrows of this
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:45,090'); seek(1305.0)">
              intervention graph that is being sent over, which is the whole point of this package?
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:49,490'); seek(1309.0)">
              Line five, we have something that is reading.
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:53,178'); seek(1313.0)">
              So you see model layers, input is
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:56,888'); seek(1316.0)">
              equal to something and we save it again, I'll explain why we're saving
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:22:00,456'); seek(1320.0)">
              that. This giant model is not running on
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:22:04,200'); seek(1324.0)">
              my colab notebook or my
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:07,948'); seek(1327.0)">
              local environment, right? So it is interesting
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:11,708'); seek(1331.0)">
              that I can indeed read what is happening on it.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:14,970'); seek(1334.0)">
              And on line six we have the opposite,
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:18,482'); seek(1338.0)">
              which is me changing something in some other component
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:22,006'); seek(1342.0)">
              of my model. So model the layers. So on layer eleven,
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:26,030'); seek(1346.0)">
              on a component called the MLP, I want to change its output to
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:29,888'); seek(1349.0)">
              become zero. And again, just to remind us what
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:33,136'); seek(1353.0)">
              all this is for, how all this relates to our hypothesis,
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:37,570'); seek(1357.0)">
              first we want to get the average of a bunch of runs for the task
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:41,178'); seek(1361.0)">
              in question, which is the opposite. And then we want to append that
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:44,916'); seek(1364.0)">
              average out value to some other examples
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:22:48,718'); seek(1368.0)">
              that are in a different context called like the one shot or the zero shot
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:52,158'); seek(1372.0)">
              examples. That is, we've not giving the model any idea what we're trying to achieve.
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:22:55,870'); seek(1375.0)">
              We just wanted to feel the urge to do the thing
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:59,448'); seek(1379.0)">
              we want it to do because we have added the vector. So literally the first
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:02,412'); seek(1382.0)">
              thing of mean is literally just a read output that we want to run several
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:06,258'); seek(1386.0)">
              times, read the average value of this vector, or read the value of this vector
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:10,326'); seek(1390.0)">
              and then average out. And then six shows us changing
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:14,038'); seek(1394.0)">
              the value of some component. Then we want to
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:17,392'); seek(1397.0)">
              add this average value to the running of the
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:21,236'); seek(1401.0)">
              different context and see what happens. Okay, cool. So this basically is
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:24,708'); seek(1404.0)">
              the scaffolding for what we need for
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:29,252'); seek(1409.0)">
              our project. But before we go into the code
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:32,676'); seek(1412.0)">
              for our experiment and research
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:36,616'); seek(1416.0)">
              in question, just to decode a bit,
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:40,200'); seek(1420.0)">
              what exactly is happening here? So one thing is that
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:23:43,544'); seek(1423.0)">
              the insights library loves Python contexts, which is one
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:47,080'); seek(1427.0)">
              of the reasons why I guess Python
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:50,434'); seek(1430.0)">
              might be a language of choice. But context managers are great in
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:23:54,124'); seek(1434.0)">
              python, as we know, and they do take great advantage of
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:23:57,548'); seek(1437.0)">
              them. And the general structure of it is
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:01,872'); seek(1441.0)">
              that as we know, basically the code
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:24:05,616'); seek(1445.0)">
              might look like the model is running locally. When I do things like save,
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:09,040'); seek(1449.0)">
              I do edits and I do reads. But the whole point is that
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:12,772'); seek(1452.0)">
              none of this, the model actually isn't running right
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:16,532'); seek(1456.0)">
              now. But when the context closes, that's like when the code execution
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:20,282'); seek(1460.0)">
              reaches the point where we exit the uppermost context, which is here
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:23,912'); seek(1463.0)">
              is line three. Runner. All the edits we've made
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:27,528'); seek(1467.0)">
              are, or in the course of running, of being
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:31,750'); seek(1471.0)">
              intervention graph is updated with all the I o. That is,
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:35,480'); seek(1475.0)">
              all the reading. And the writing we're doing to the model is basically
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:24:39,180'); seek(1479.0)">
              just planned while in the context. And when the context
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:24:42,802'); seek(1482.0)">
              is exited, this is then sent over,
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:24:46,716'); seek(1486.0)">
              right? So the model does not run until the context of the highest level,
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:24:50,912'); seek(1490.0)">
              which in case is the runner, which is a runner. Context is closed,
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:24:56,190'); seek(1496.0)">
              and for context the
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:24:59,616'); seek(1499.0)">
              invoker. Again, I would encourage anyone to read the documentation, but invoker is
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:25:04,036'); seek(1504.0)">
              basically what
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:25:07,876'); seek(1507.0)">
              does the writing for the graph right between
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:25:10,996'); seek(1510.0)">
              the invoker and the runner. They are both coordinating. I think
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:25:14,424'); seek(1514.0)">
              the runner definitely do some high level management, but one
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:25:17,992'); seek(1517.0)">
              of the initialization inputs to
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:25:21,768'); seek(1521.0)">
              the invoker implicitly is something called a tracer. And again
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:24,952'); seek(1524.0)">
              you can think of the tracer as just being a new graph
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:28,414'); seek(1528.0)">
              in question. As we're going to see. You can actually have construct multiple
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:32,242'); seek(1532.0)">
              graphs inside of one runner,
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:25:35,602'); seek(1535.0)">
              which we are going to see shortly. That is you can say like okay,
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:25:38,732'); seek(1538.0)">
              I want to plan different experiments. And again, this fits perfectly for our
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:25:42,048'); seek(1542.0)">
              use case, since first we want to run one set of operations that
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:25:46,350'); seek(1546.0)">
              runs our task inputs and takes
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:25:50,272'); seek(1550.0)">
              the average, and another set of operations that
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:25:53,652'); seek(1553.0)">
              take that average and adds it to the state of
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:25:57,604'); seek(1557.0)">
              the different context. Examples, again, that should have no
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:26:01,268'); seek(1561.0)">
              idea about the task, and then see what happens when this average
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:26:05,118'); seek(1565.0)">
              vector is added to it.
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:26:08,310'); seek(1568.0)">
              So the runner is the high level context manager, and then each
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:26:12,136'); seek(1572.0)">
              basic subgraph experiment that we want
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:26:15,548'); seek(1575.0)">
              to run is contained in the
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:26:18,812'); seek(1578.0)">
              invoker context. Interventions, every read
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:22,092'); seek(1582.0)">
              and write intervention, all the iOS are what are the nodes in
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:26,604'); seek(1586.0)">
              there of type tracing node, which again are
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:30,972'); seek(1590.0)">
              what inform what our entire graph is made of to start with.
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:26:34,944'); seek(1594.0)">
              And I said I was going to speak on why we need save. So again,
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:26:37,696'); seek(1597.0)">
              remember that because this isn't running locally,
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:26:42,006'); seek(1602.0)">
              we have to explicitly tell the model to save any
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:26:45,636'); seek(1605.0)">
              value that we want to read outside of the context, because the standard
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:26:49,620'); seek(1609.0)">
              behavior is when the context is exited, the model actually runs
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:26:53,258'); seek(1613.0)">
              with all our interventions, but because these values are so
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:26:57,108'); seek(1617.0)">
              large, we actually have to explicitly tell okay, please, I would like you to return
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:27:00,744'); seek(1620.0)">
              several hundreds of thousands of vector values
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:27:04,574'); seek(1624.0)">
              to me, because that is important. So that is the only reason why we can
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:27:07,528'); seek(1627.0)">
              access hidden state outside context, otherwise we wouldn't need to.
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:27:11,052'); seek(1631.0)">
              So perhaps this was just a temporary variable that
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:27:14,668'); seek(1634.0)">
              we needed to use for our computation, which is fine
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:27:18,810'); seek(1638.0)">
              if we have no intention to access it after the
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:27:22,668'); seek(1642.0)">
              contact closes, we wouldn't put save. So this is only because we want to hold
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:26,032'); seek(1646.0)">
              on to the value. So this is just one of the examples where we have
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:27:29,008'); seek(1649.0)">
              to remind ourselves of the difference between running the model locally
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:27:32,970'); seek(1652.0)">
              and just simply using, building an intervention
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:27:36,842'); seek(1656.0)">
              graph for a remote resource that is going to run
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:27:41,090'); seek(1661.0)">
              immediately we leave the context. And again,
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:27:44,616'); seek(1664.0)">
              this is from documentation, basically showing how
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:27:48,760'); seek(1668.0)">
              each. So here you can see that each line of intervention.
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:27:52,222'); seek(1672.0)">
              So the first green arrow on the left blue
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:27:55,598'); seek(1675.0)">
              box is a right. That is, we're setting some layers
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:27:59,426'); seek(1679.0)">
              output to zero and the next is a read and
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:28:03,372'); seek(1683.0)">
              the third is also a read. But you
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:28:06,668'); seek(1686.0)">
              see here, we use the dot save because we do want this
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:28:11,212'); seek(1691.0)">
              value to be sent over the network when the model isn't running. And you see
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:28:14,688'); seek(1694.0)">
              the output of this is this intervention graph in the middle.
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:28:18,480'); seek(1698.0)">
              And this is basically what is what is sent over the network in one direction.
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:28:22,438'); seek(1702.0)">
              And then the result for things that we ask
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:26,148'); seek(1706.0)">
              the model that we ask the graph to save
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:28:29,652'); seek(1709.0)">
              are then sent back in the other direction when the execution is done.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:28:33,730'); seek(1713.0)">
              Again, just to remind ourselves on what we're trying to do now, we have an
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:28:36,968'); seek(1716.0)">
              idea of what our library looks like and
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:28:40,792'); seek(1720.0)">
              how we use it is that we want to pass
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:28:44,616'); seek(1724.0)">
              in some context, we want to run it.
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:28:47,770'); seek(1727.0)">
              Remember, we're only interested in what happens by the column, so we
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:28:51,644'); seek(1731.0)">
              will be indexing to get only the
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:28:55,708'); seek(1735.0)">
              vector at the very extreme end. Because the
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:28:58,908'); seek(1738.0)">
              idea is that is the token that will
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:29:02,704'); seek(1742.0)">
              contain information on what is to come next. Right? Again,
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:29:05,856'); seek(1745.0)">
              just as a result of how transform architectures work is, the next
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:29:09,872'); seek(1749.0)">
              prediction is containing the last token. To what end do we want to
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:29:13,364'); seek(1753.0)">
              do that to this end? So we could do two sets of runs.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:29:16,458'); seek(1756.0)">
              The first run is to pass a bunch
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:29:20,074'); seek(1760.0)">
              of examples doing the task we want. Again,
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:23,892'); seek(1763.0)">
              this is exactly like how you would tell Chat GPT something
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:29:27,832'); seek(1767.0)">
              like, I want you to give me words. And opposite, like this
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:29:31,192'); seek(1771.0)">
              example, old, young, separated by Colin. Then it
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:29:35,624'); seek(1775.0)">
              does this thing, right? So this is basically just like prompting
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:29:39,218'); seek(1779.0)">
              it with the, this is similar
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:29:43,116'); seek(1783.0)">
              to prompting with the format of code you want. But in this case we're actually
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:29:46,332'); seek(1786.0)">
              just going to look at the very last token
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:29:50,450'); seek(1790.0)">
              and then right before, when it's right on the verge
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:29:53,718'); seek(1793.0)">
              of predicting, we just take a first, don't know the computation
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:29:56,902'); seek(1796.0)">
              to know that, okay, this is a word and opposite game we're playing, and I
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:30:00,288'); seek(1800.0)">
              am to predict the opposite of the last thing I saw before this token.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:30:03,818'); seek(1803.0)">
              So, right when it's supposedly figured all that out, we want
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:30:07,268'); seek(1807.0)">
              to just snatch that vector and average a bunch of them
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:30:10,468'); seek(1810.0)">
              out to get, hopefully a vector that represents in
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:30:13,832'); seek(1813.0)">
              some pure form the very essence of the task that it
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:30:17,528'); seek(1817.0)">
              has figured out, which is opposite function of previous
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:22,950'); seek(1822.0)">
              experiment. That's the first part of the experiment. Then the second part of the experiment
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:30:26,142'); seek(1826.0)">
              is to take this pure vector and then add it to a different context,
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:30:30,018'); seek(1830.0)">
              a different series of examples that supposedly should have no idea
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:30:33,596'); seek(1833.0)">
              what is going on, right? Because again, if you just told chat GBT encode column,
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:30:38,270'); seek(1838.0)">
              it has no idea what you want is it can't read your mind
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:30:42,112'); seek(1842.0)">
              yet. So this is
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:30:45,392'); seek(1845.0)">
              called the zero shot intervention, which basically just means zero
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:30:49,456'); seek(1849.0)">
              shot means zero examples of what you're looking
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:30:52,916'); seek(1852.0)">
              for. Except now we're going to add this
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:30:57,108'); seek(1857.0)">
              hopefully averaged out function has
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:31:01,252'); seek(1861.0)">
              it just again feel compelled to do the thing that that
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:31:04,692'); seek(1864.0)">
              vector was obtained from. Right, so how do
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:31:08,024'); seek(1868.0)">
              we do the first part? That is the part where we just run a
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:31:11,064'); seek(1871.0)">
              bunch of stuff and we extract the value
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:31:14,584'); seek(1874.0)">
              at the very last column for all of them and average out.
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:31:18,890'); seek(1878.0)">
              Cool. So again we have our trusted layout.
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:31:23,282'); seek(1883.0)">
              Of course, first of all we have to determine what component we
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:31:27,228'); seek(1887.0)">
              want to look at. Remember Mac, interp is all about having
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:31:32,640'); seek(1892.0)">
              an observed and interesting observed behavior and trying to
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:31:36,272'); seek(1896.0)">
              find the contribution of some discrete component, right? So in this case we
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:31:40,320'); seek(1900.0)">
              narrow down by saying, okay, we want to look at layer eight again.
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:31:44,370'); seek(1904.0)">
              In the actual experiment we run this for all the different layers, for all the
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:31:48,244'); seek(1908.0)">
              different components of the model, and then we have like a plot of
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:31:52,644'); seek(1912.0)">
              which of them happen to be most interesting. And then we drill down.
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:31:56,408'); seek(1916.0)">
              But this is just showing an example of suppose we wanted to see what layer
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:32:00,078'); seek(1920.0)">
              eight was doing as far as the task is concerned.
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:32:04,390'); seek(1924.0)">
              Cool. So just imagine this done for a bunch of tens of
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:32:07,852'); seek(1927.0)">
              other components. Cool. So we have a runner in Booger.
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:32:11,634'); seek(1931.0)">
              Then here on line six we simply just
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:32:15,530'); seek(1935.0)">
              do our trusted, as we would
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:32:18,988'); seek(1938.0)">
              like to notice here that I don't do save because this
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:32:23,072'); seek(1943.0)">
              value of hidden states, this variable is only needed
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:32:26,736'); seek(1946.0)">
              for computation inside of my
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:32:30,208'); seek(1950.0)">
              context, right? So I do not need to export
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:32:33,978'); seek(1953.0)">
              this at this very point in time. I just simply need to take
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:32:38,788'); seek(1958.0)">
              the variable, hold on to it, use it for other computation on len
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:32:42,058'); seek(1962.0)">
              ten. And as you can see, line six
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:32:45,352'); seek(1965.0)">
              is simply just the rightmost column. That is, sorry,
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:32:49,352'); seek(1969.0)">
              beg your pardon, that is the
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:32:53,528'); seek(1973.0)">
              on. Right. So between line six and seven we
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:32:57,048'); seek(1977.0)">
              simply just take an example, we choose a layer and
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:33:00,764'); seek(1980.0)">
              on line eight we say the sequence position should be, I think on
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:33:04,508'); seek(1984.0)">
              line one, you see, we define that as minus one. So we just simply want
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:33:07,484'); seek(1987.0)">
              to take the very last value, which is token. So again, all the dark gray
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:33:10,918'); seek(1990.0)">
              bars is what line eight is holding
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:33:14,358'); seek(1994.0)">
              onto. Then on line ten we simply just do the average.
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:33:18,310'); seek(1998.0)">
              So we take that variable and we do the dot mean on the batch
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:33:21,862'); seek(2001.0)">
              dimension, that's the 0th dimension. Again, that's the dimension of all the stacked examples
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:33:26,298'); seek(2006.0)">
              on the right there. I think I just put like a clip out to show
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:33:28,788'); seek(2008.0)">
              you what the vectors and
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:33:32,788'); seek(2012.0)">
              matrixes will look like. So each of those stacks is just a batch dimension.
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:33:35,902'); seek(2015.0)">
              So each of the examples of old, young,
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:33:38,936'); seek(2018.0)">
              awake, asleep is represented by one of these slices.
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:33:42,550'); seek(2022.0)">
              And we simply just want to average across that to get some hopefully
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:33:46,622'); seek(2026.0)">
              pure vector that encodes the essence of opposites
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:33:51,370'); seek(2031.0)">
              and that we want to save because that we want to send back. So it's
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:33:54,514'); seek(2034.0)">
              kind of meant to be an efficient thing such that we don't want to send
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:33:57,564'); seek(2037.0)">
              everything over the network, we don't want to send both the full,
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:34:00,908'); seek(2040.0)">
              all the matrices. Thankfully we could decide to save everything and
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:34:04,608'); seek(2044.0)">
              then compute locally. So again, this is just some of the considerations
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:34:08,022'); seek(2048.0)">
              that you make when you remind yourself that actually there is
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:34:12,770'); seek(2052.0)">
              some throughput cost and efficiency cost.
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:34:15,556'); seek(2055.0)">
              So let's just do as much as we can on this
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:34:19,300'); seek(2059.0)">
              environment and then just send down the most condensed
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:34:22,570'); seek(2062.0)">
              version we want. Again, this should
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:34:26,472'); seek(2066.0)">
              be similar to running using any remote resource or
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:34:29,928'); seek(2069.0)">
              when you have trade offs between remote and local resources to contend
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:34:33,342'); seek(2073.0)">
              with. Cool. So that is how we
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:34:36,648'); seek(2076.0)">
              do the first part, that's how we get the averages. Literally this is all to
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:34:39,308'); seek(2079.0)">
              do averages for one layer. And just imagine putting this in a for loop if
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:34:42,604'); seek(2082.0)">
              you want to iterate over several layers. And for the
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:34:45,868'); seek(2085.0)">
              second part, having possessed this average
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:34:51,286'); seek(2091.0)">
              pure vector, which we called h, we want
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:34:54,912'); seek(2094.0)">
              to then put h into our zero
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:34:59,216'); seek(2099.0)">
              shot examples. Again, these are the examples that have no context on the task.
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:35:03,050'); seek(2103.0)">
              They're just doing their own thing. They supposedly
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:35:06,538'); seek(2106.0)">
              oblivious to the task we find interesting of opposites,
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:35:11,090'); seek(2111.0)">
              but from nowhere they would just feel the urge to now just do opposites.
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:35:14,574'); seek(2114.0)">
              Hopefully if we add this average vector
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:35:18,606'); seek(2118.0)">
              state to them. And here is the example
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:35:21,976'); seek(2121.0)">
              I mentioned where we're running two invoker
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:35:26,322'); seek(2126.0)">
              contexts inside of the runner context. So basically the
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:35:30,268'); seek(2130.0)">
              first is, again, we're trying to, as with
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:35:34,028'); seek(2134.0)">
              any experiment, we have to have our control example
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:35:37,340'); seek(2137.0)">
              or our reference or our baseline to say that,
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:35:40,016'); seek(2140.0)">
              cool. Without adding this average vector, what does
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:35:43,712'); seek(2143.0)">
              the model feel compelled to predict? So for simple,
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:35:46,960'); seek(2146.0)">
              does it feel compelled to predict simple?
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:35:51,232'); seek(2151.0)">
              Simpler? Maybe it just says cool. Simpler should be something to follow simple
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:35:55,332'); seek(2155.0)">
              or given encode, does it feel compelled to predict base
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:36:00,292'); seek(2160.0)">
              64? Or perhaps it does feel naturally compelled to decode,
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:36:03,838'); seek(2163.0)">
              who knows? So the first run on line four and
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:36:07,288'); seek(2167.0)">
              five is just again simply running the model and
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:36:11,144'); seek(2171.0)">
              saving the output for the very last token.
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:36:15,510'); seek(2175.0)">
              And the second is where we do the interesting stuff of running the
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:36:19,228'); seek(2179.0)">
              model and basically intervening. So on line eleven we literally
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:36:23,042'); seek(2183.0)">
              just plus equals two, which is identical to how
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:36:26,588'); seek(2186.0)">
              we were taking the mean before. But again for this context we just do a
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:36:29,628'); seek(2189.0)">
              plus equals two, add this value to the existing.
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:36:34,030'); seek(2194.0)">
              And on line 13 we again just
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:36:37,488'); seek(2197.0)">
              like line five save to see. Okay, cool. Now let's see what
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:36:40,868'); seek(2200.0)">
              the predictions are and how similar they are, or to
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:36:44,532'); seek(2204.0)">
              what extent this vector has changed
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:36:47,978'); seek(2207.0)">
              the opinions of this model
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:36:51,730'); seek(2211.0)">
              results. I mean,
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:36:54,772'); seek(2214.0)">
              depending on your standards for impressive
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:36:58,878'); seek(2218.0)">
              or not, this is what it looks like. This is what run one looks like.
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:37:02,392'); seek(2222.0)">
              Just by doing that, we can see that. Indeed, on the third
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:37:06,076'); seek(2226.0)">
              column here, adding that h vector does
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:37:09,820'); seek(2229.0)">
              move the needle a bit does
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:37:14,572'); seek(2234.0)">
              have the effect of the opposite function, right? So on
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:37:18,268'); seek(2238.0)">
              the second column, we just see that the thing the model tries to do,
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:37:21,708'); seek(2241.0)">
              if you tell the model, if you tell the model minimum column, it just repeats
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:37:25,846'); seek(2245.0)">
              a lot of stuff, right? So it just says minimum is minimum, arrogant is
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:37:29,168'); seek(2249.0)">
              arrogant, inside is inside. Although sometimes it does interesting things, like the
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:37:34,756'); seek(2254.0)">
              fifth example from the bottom. If you say on, it says I.
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:37:38,292'); seek(2258.0)">
              If you say answer, it says yes. Again,
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:37:41,572'); seek(2261.0)">
              this is what the model feels compelled to just say if it has no context.
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:37:45,910'); seek(2265.0)">
              But on the third column we see that in some examples
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:37:50,270'); seek(2270.0)">
              we do manage to tilt its final judgment
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:37:54,302'); seek(2274.0)">
              in a different direction. Now, I will mention though, that this
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:37:58,412'); seek(2278.0)">
              is technically not where the paper stops.
            </span>
            
            <span id="chunk-605" class="transcript-chunks" onclick="console.log('00:38:01,522'); seek(2281.0)">
              The paper decides to say beyond just averaging h.
            </span>
            
            <span id="chunk-606" class="transcript-chunks" onclick="console.log('00:38:05,468'); seek(2285.0)">
              Remember, this is taking the value of
            </span>
            
            <span id="chunk-607" class="transcript-chunks" onclick="console.log('00:38:09,930'); seek(2289.0)">
              the output of the entire layer. Remember, we just see layer eight.
            </span>
            
            <span id="chunk-608" class="transcript-chunks" onclick="console.log('00:38:14,110'); seek(2294.0)">
              The paper takes it further by saying, okay, instead of just looking at layer eight,
            </span>
            
            <span id="chunk-609" class="transcript-chunks" onclick="console.log('00:38:17,408'); seek(2297.0)">
              can we drill specifically into what component in layer
            </span>
            
            <span id="chunk-610" class="transcript-chunks" onclick="console.log('00:38:20,778'); seek(2300.0)">
              eight is contributing? So, back to our reference architecture,
            </span>
            
            <span id="chunk-611" class="transcript-chunks" onclick="console.log('00:38:25,010'); seek(2305.0)">
              neural network transformer
            </span>
            
            <span id="chunk-612" class="transcript-chunks" onclick="console.log('00:38:28,362'); seek(2308.0)">
              component transformer block has different things. Our attention
            </span>
            
            <span id="chunk-613" class="transcript-chunks" onclick="console.log('00:38:32,042'); seek(2312.0)">
              block, rather, excuse me, has the mass self attention,
            </span>
            
            <span id="chunk-614" class="transcript-chunks" onclick="console.log('00:38:35,770'); seek(2315.0)">
              the feed forward, it has different things, layer norm,
            </span>
            
            <span id="chunk-615" class="transcript-chunks" onclick="console.log('00:38:39,124'); seek(2319.0)">
              and they decide to drill into the contributions of
            </span>
            
            <span id="chunk-616" class="transcript-chunks" onclick="console.log('00:38:43,048'); seek(2323.0)">
              the attention head. Again, the distinction isn't that important, but the experimental
            </span>
            
            <span id="chunk-617" class="transcript-chunks" onclick="console.log('00:38:47,298'); seek(2327.0)">
              method is precisely the same. So they just find a way to drill into studying
            </span>
            
            <span id="chunk-618" class="transcript-chunks" onclick="console.log('00:38:51,378'); seek(2331.0)">
              the contribution of the top x
            </span>
            
            <span id="chunk-619" class="transcript-chunks" onclick="console.log('00:38:56,650'); seek(2336.0)">
              attention heads. And instead of averaging looking for the average of
            </span>
            
            <span id="chunk-620" class="transcript-chunks" onclick="console.log('00:39:00,640'); seek(2340.0)">
              just all the components contributions, which again supposedly
            </span>
            
            <span id="chunk-621" class="transcript-chunks" onclick="console.log('00:39:04,422'); seek(2344.0)">
              will have more noise, they basically tried to denoise by just narrowing
            </span>
            
            <span id="chunk-622" class="transcript-chunks" onclick="console.log('00:39:07,878'); seek(2347.0)">
              in on a few, and with that the effect is way more obvious.
            </span>
            
            <span id="chunk-623" class="transcript-chunks" onclick="console.log('00:39:11,840'); seek(2351.0)">
              But I don't include that for the purpose of this talk.
            </span>
            
            <span id="chunk-624" class="transcript-chunks" onclick="console.log('00:39:15,410'); seek(2355.0)">
              And that was the talk. So if you are interested in
            </span>
            
            <span id="chunk-625" class="transcript-chunks" onclick="console.log('00:39:18,708'); seek(2358.0)">
              looking at the NDIF project and Nn Insight Library
            </span>
            
            <span id="chunk-626" class="transcript-chunks" onclick="console.log('00:39:22,298'); seek(2362.0)">
              as well, which is a companion, please view that site.
            </span>
            
            <span id="chunk-627" class="transcript-chunks" onclick="console.log('00:39:26,244'); seek(2366.0)">
              And if you're interested in learning more on Mechinterp,
            </span>
            
            <span id="chunk-628" class="transcript-chunks" onclick="console.log('00:39:29,570'); seek(2369.0)">
              many of the code snippets and
            </span>
            
            <span id="chunk-629" class="transcript-chunks" onclick="console.log('00:39:33,284'); seek(2373.0)">
              basically concepts introduce you. They were introduced to myself
            </span>
            
            <span id="chunk-630" class="transcript-chunks" onclick="console.log('00:39:36,892'); seek(2376.0)">
              on the platform called arena education. It is an awesome program.
            </span>
            
            <span id="chunk-631" class="transcript-chunks" onclick="console.log('00:39:41,276'); seek(2381.0)">
              You should check it out. If you're interested in learning more on doing
            </span>
            
            <span id="chunk-632" class="transcript-chunks" onclick="console.log('00:39:45,228'); seek(2385.0)">
              mechanistic interpretability. I hope you've had as much fun
            </span>
            
            <span id="chunk-633" class="transcript-chunks" onclick="console.log('00:39:48,412'); seek(2388.0)">
              going through this as I have. And do enjoy the rest of
            </span>
            
            <span id="chunk-634" class="transcript-chunks" onclick="console.log('00:39:51,532'); seek(2391.0)">
              the conference.
            </span>
            
            </div>
          </div>
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Boluwatife%20Ben-Adeola%20-%20Conf42%20Python%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Boluwatife%20Ben-Adeola%20-%20Conf42%20Python%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #69811f;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/python2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #69811f;">
                <i class="fe fe-grid me-2"></i>
                See all 32 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Boluwatife%20Ben-Adeola_python.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Boluwatife Ben-Adeola
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Independent AI Researcher 
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/ben-adeola/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Boluwatife Ben-Adeola's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Boluwatife Ben-Adeola"
                  data-url="https://www.conf42.com/python2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/python2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Python"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>