<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: LLMs in AWS: Observability Maturity from Foundation to AIOps</title>
    <meta name="description" content="One model, extra large, please!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Indika%20Wimalasuriya_llm.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="LLMs in AWS: Observability Maturity from Foundation to AIOps | Conf42"/>
    <meta property="og:description" content="Unleash Large Language Models in AWS! Join "AWS LLMs Observability: Foundation to AIOps" Boost LLM performance, troubleshoot precisely, stay ahead. Dive into a comprehensive maturity model for immediate impact, creating value for GenAI applications. Innovate now! Elevate your insights."/>
    <meta property="og:url" content="https://conf42.com/Large_Language_Models_LLMs_2024_Indika_Wimalasuriya_aws_observability_maturity"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/ML2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Machine Learning 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-05-30
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/ml2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday San Francisco
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday London
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday Amsterdam
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/DnyHgrC7jC" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #CCB87B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Large Language Models (LLMs) 2024 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2024-04-11">April 11 2024</time>
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- One model, extra large, please!
 -->
              <script>
                const event_date = new Date("2024-04-11T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-04-11T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "TeiLhP54hfU"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "TQwxk0c4sh0"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBjR6ZR0g0LRq9Fp8c_4HrI" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Welcome to LLM 2024 organized by", "timestamp": "00:00:21,200", "timestamp_s": 21.0}, {"text": "conferred tour. My name is Indika Vimalasurier", "timestamp": "00:00:24,750", "timestamp_s": 24.0}, {"text": "and I\u0027ll walk you through about how you can leverage observability", "timestamp": "00:00:28,186", "timestamp_s": 28.0}, {"text": "maturity model improve end", "timestamp": "00:00:32,466", "timestamp_s": 32.0}, {"text": "user experience of the apps you are going to develop", "timestamp": "00:00:36,098", "timestamp_s": 36.0}, {"text": "using LL lens. So we will touch about how to start", "timestamp": "00:00:39,498", "timestamp_s": 39.0}, {"text": "which is the foundation, and then probably take it up to", "timestamp": "00:00:43,442", "timestamp_s": 43.0}, {"text": "around using AI to", "timestamp": "00:00:47,514", "timestamp_s": 47.0}, {"text": "support your operations. As you might aware,", "timestamp": "00:00:51,082", "timestamp_s": 51.0}, {"text": "by around 2022,", "timestamp": "00:00:55,234", "timestamp_s": 55.0}, {"text": "the hype started with Chatgbt. ChatgBT was", "timestamp": "00:00:58,674", "timestamp_s": 58.0}, {"text": "a hit, it was mainstream and it resulted", "timestamp": "00:01:02,714", "timestamp_s": 62.0}, {"text": "in lot of people who are not into AI start creating", "timestamp": "00:01:06,562", "timestamp_s": 66.0}, {"text": "generative AI apps. So now it\u0027s already has taken over", "timestamp": "00:01:10,978", "timestamp_s": 70.0}, {"text": "the world. The world is looking at what are the use cases", "timestamp": "00:01:15,114", "timestamp_s": 75.0}, {"text": "which we can use and", "timestamp": "00:01:18,874", "timestamp_s": 78.0}, {"text": "leverage. It\u0027s already mainstream.", "timestamp": "00:01:23,092", "timestamp_s": 83.0}, {"text": "There\u0027s lot of developers who are building apps", "timestamp": "00:01:26,380", "timestamp_s": 86.0}, {"text": "connecting llms. So there\u0027s a need.", "timestamp": "00:01:30,580", "timestamp_s": 90.0}, {"text": "Apps which we are going to develop has a capability of", "timestamp": "00:01:34,644", "timestamp_s": 94.0}, {"text": "providing full end user experience because", "timestamp": "00:01:37,700", "timestamp_s": 97.0}, {"text": "we all know how it can end. While generative", "timestamp": "00:01:42,028", "timestamp_s": 102.0}, {"text": "AI is which is opening creating", "timestamp": "00:01:45,496", "timestamp_s": 105.0}, {"text": "lot of new opportunities. We also want to ensure that the apps which", "timestamp": "00:01:48,736", "timestamp_s": 108.0}, {"text": "is being developed, deployed properly", "timestamp": "00:01:52,720", "timestamp_s": 112.0}, {"text": "in production environments and are being served to end users", "timestamp": "00:01:55,840", "timestamp_s": 115.0}, {"text": "as per the expectation and we don\u0027t want to make it", "timestamp": "00:01:59,960", "timestamp_s": 119.0}, {"text": "ops problem. So we want to ensure we build a solid", "timestamp": "00:02:03,440", "timestamp_s": 123.0}, {"text": "observability into our llms as well.", "timestamp": "00:02:07,144", "timestamp_s": 127.0}, {"text": "So as part of today\u0027s presentation, I\u0027ll provide you a quick intro,", "timestamp": "00:02:10,673", "timestamp_s": 130.0}, {"text": "what is observability? And we\u0027ll discuss about what", "timestamp": "00:02:14,689", "timestamp_s": 134.0}, {"text": "is observability mean for llms. So there are two kind", "timestamp": "00:02:18,433", "timestamp_s": 138.0}, {"text": "of observability which we can discuss,", "timestamp": "00:02:21,769", "timestamp_s": 141.0}, {"text": "so which we are going to discuss, so which is about a direct", "timestamp": "00:02:25,225", "timestamp_s": 145.0}, {"text": "observability, and second one is about indirect observability.", "timestamp": "00:02:28,465", "timestamp_s": 148.0}, {"text": "So I\u0027ll be focusing more on indirect observability", "timestamp": "00:02:32,081", "timestamp_s": 152.0}, {"text": "when discussing about the maturity model, which I\u0027m going to", "timestamp": "00:02:35,569", "timestamp_s": 155.0}, {"text": "walk you through. Then I\u0027ll walk you through about some of the pillars,", "timestamp": "00:02:39,522", "timestamp_s": 159.0}, {"text": "give a quick intro about what is the LLM", "timestamp": "00:02:43,210", "timestamp_s": 163.0}, {"text": "look like, and then we\u0027ll jump into my main focus,", "timestamp": "00:02:46,714", "timestamp_s": 166.0}, {"text": "a maturity model for LLM. So then", "timestamp": "00:02:50,690", "timestamp_s": 170.0}, {"text": "we\u0027ll look at some of the implementation", "timestamp": "00:02:54,202", "timestamp_s": 174.0}, {"text": "guidelines, the services which we can leverage, and then of course,", "timestamp": "00:02:58,130", "timestamp_s": 178.0}, {"text": "like every other maturity model, this should", "timestamp": "00:03:02,522", "timestamp_s": 182.0}, {"text": "not be just a maturity model where people will just follow blindly,", "timestamp": "00:03:06,490", "timestamp_s": 186.0}, {"text": "but we want to ensure we tack into business outcomes", "timestamp": "00:03:11,106", "timestamp_s": 191.0}, {"text": "so we have an ability to measure the", "timestamp": "00:03:15,010", "timestamp_s": 195.0}, {"text": "progress. And then we\u0027ll wrap this up with some of the", "timestamp": "00:03:18,242", "timestamp_s": 198.0}, {"text": "best practices and some of the pitfalls I think you should avoid.", "timestamp": "00:03:21,370", "timestamp_s": 201.0}, {"text": "Before we start, a quick intro about myself.", "timestamp": "00:03:26,114", "timestamp_s": 206.0}, {"text": "My name is Indigo Emilasuri. I\u0027m based out of Colombo.", "timestamp": "00:03:29,774", "timestamp_s": 209.0}, {"text": "I\u0027m a serious reliability", "timestamp": "00:03:33,334", "timestamp_s": 213.0}, {"text": "engineering advocate and a practitioner as", "timestamp": "00:03:37,870", "timestamp_s": 217.0}, {"text": "well. I\u0027m a solution architect with specialize in SRE observability,", "timestamp": "00:03:40,982", "timestamp_s": 220.0}, {"text": "AI ops and generative AI working", "timestamp": "00:03:45,550", "timestamp_s": 225.0}, {"text": "at Vergisa as a senior systems engineering manager,", "timestamp": "00:03:49,126", "timestamp_s": 229.0}, {"text": "I\u0027m a passionate technical trainer. I have trained hundreds", "timestamp": "00:03:52,622", "timestamp_s": 232.0}, {"text": "of people when it comes to SRE observability", "timestamp": "00:03:56,318", "timestamp_s": 236.0}, {"text": "aiops and I\u0027m an energetic technical blogger", "timestamp": "00:04:00,286", "timestamp_s": 240.0}, {"text": "and I\u0027m very proud AWS community builder", "timestamp": "00:04:04,454", "timestamp_s": 244.0}, {"text": "under cloud operations and also a very proud ambassador", "timestamp": "00:04:07,702", "timestamp_s": 247.0}, {"text": "at DevOps Institute which is also known as PC", "timestamp": "00:04:11,742", "timestamp_s": 251.0}, {"text": "CERT because they have acquired it. So that\u0027s about me.", "timestamp": "00:04:15,182", "timestamp_s": 255.0}, {"text": "So I am very passionate about this topic, observability. So when", "timestamp": "00:04:18,838", "timestamp_s": 258.0}, {"text": "it comes to the distributed systems and then llms,", "timestamp": "00:04:22,740", "timestamp_s": 262.0}, {"text": "the end of the day I look at things from a customer experience", "timestamp": "00:04:26,084", "timestamp_s": 266.0}, {"text": "and how we can provide better customer experience to our end users", "timestamp": "00:04:30,156", "timestamp_s": 270.0}, {"text": "and then how we can make a better business", "timestamp": "00:04:34,076", "timestamp_s": 274.0}, {"text": "outcomes part of the presentation I\u0027m", "timestamp": "00:04:37,572", "timestamp_s": 277.0}, {"text": "mainly focused on AWS. So I\u0027m looking at", "timestamp": "00:04:41,508", "timestamp_s": 281.0}, {"text": "llms especially deployed in and", "timestamp": "00:04:44,620", "timestamp_s": 284.0}, {"text": "been accessed through AWS. So one", "timestamp": "00:04:47,952", "timestamp_s": 287.0}, {"text": "of the fantastic service AWS has offered is Amazon", "timestamp": "00:04:51,768", "timestamp_s": 291.0}, {"text": "Bedrock, which is a managed", "timestamp": "00:04:55,744", "timestamp_s": 295.0}, {"text": "service where you are able to use APIs to", "timestamp": "00:04:59,776", "timestamp_s": 299.0}, {"text": "access the foundational models. So it\u0027s", "timestamp": "00:05:03,592", "timestamp_s": 303.0}, {"text": "really fast, it\u0027s really quick, you just have to ensure that", "timestamp": "00:05:06,968", "timestamp_s": 306.0}, {"text": "you have the ability of connecting. So the key features", "timestamp": "00:05:10,352", "timestamp_s": 310.0}, {"text": "are it\u0027s giving access to the foundation models and", "timestamp": "00:05:14,152", "timestamp_s": 314.0}, {"text": "the use cases such as text generation, image generation and", "timestamp": "00:05:18,394", "timestamp_s": 318.0}, {"text": "the use cases around those. So it\u0027s also providing", "timestamp": "00:05:23,234", "timestamp_s": 323.0}, {"text": "this private customization with own data with the", "timestamp": "00:05:26,562", "timestamp_s": 326.0}, {"text": "techniques like the retrieval augmented", "timestamp": "00:05:30,402", "timestamp_s": 330.0}, {"text": "generation. We call it rack. And it\u0027s also providing the", "timestamp": "00:05:33,826", "timestamp_s": 333.0}, {"text": "ability of building agents and executed tasks using", "timestamp": "00:05:37,962", "timestamp_s": 337.0}, {"text": "the system, enterprise systems and other data sources.", "timestamp": "00:05:42,546", "timestamp_s": 342.0}, {"text": "Obviously one good thing is that there\u0027s no infrastructure, so you", "timestamp": "00:05:46,944", "timestamp_s": 346.0}, {"text": "don\u0027t have to worry about infrastructure. So AWS is taking", "timestamp": "00:05:50,408", "timestamp_s": 350.0}, {"text": "care of the infrastructure. So that, that\u0027s why we call it fully managed.", "timestamp": "00:05:53,968", "timestamp_s": 353.0}, {"text": "So it\u0027s a very secure and it\u0027s a, you know, it\u0027s a", "timestamp": "00:05:58,040", "timestamp_s": 358.0}, {"text": "go to tool if you want to develop generative AI apps.", "timestamp": "00:06:01,376", "timestamp_s": 361.0}, {"text": "It\u0027s already consist of, you know, some of the most", "timestamp": "00:06:05,424", "timestamp_s": 365.0}, {"text": "widely used foundation models provided by a 21", "timestamp": "00:06:10,014", "timestamp_s": 370.0}, {"text": "labs anthropic cohere, meta and stability", "timestamp": "00:06:14,206", "timestamp_s": 374.0}, {"text": "AI, Amazon as well. So there are a lot of models and they are", "timestamp": "00:06:18,022", "timestamp_s": 378.0}, {"text": "also continuously adding these models into their.", "timestamp": "00:06:21,462", "timestamp_s": 381.0}, {"text": "So with that, our observability", "timestamp": "00:06:26,454", "timestamp_s": 386.0}, {"text": "maturity model or the approach is mainly focused on application,", "timestamp": "00:06:31,334", "timestamp_s": 391.0}, {"text": "which has been developed using Amazon bedrock.", "timestamp": "00:06:35,550", "timestamp_s": 395.0}, {"text": "So moving on. I just want to give a quick idea like you know,", "timestamp": "00:06:38,644", "timestamp_s": 398.0}, {"text": "so when we say generate AI apps, so what is kind of the", "timestamp": "00:06:42,060", "timestamp_s": 402.0}, {"text": "use case? The use cases, a typical user", "timestamp": "00:06:45,460", "timestamp_s": 405.0}, {"text": "can kind of like enter query. So it will", "timestamp": "00:06:49,676", "timestamp_s": 409.0}, {"text": "come into our, the query interface like we can take it from", "timestamp": "00:06:53,244", "timestamp_s": 413.0}, {"text": "my API or user interface. And then the,", "timestamp": "00:06:56,740", "timestamp_s": 416.0}, {"text": "we will process, start processing this user query and", "timestamp": "00:07:00,028", "timestamp_s": 420.0}, {"text": "then we will try to connect it with the vector encoding.", "timestamp": "00:07:03,700", "timestamp_s": 423.0}, {"text": "So it\u0027s trying to find similar queries,", "timestamp": "00:07:07,252", "timestamp_s": 427.0}, {"text": "similar patterns using in our vector database.", "timestamp": "00:07:11,908", "timestamp_s": 431.0}, {"text": "And then we will kind of like looking at retrieving the top k", "timestamp": "00:07:15,172", "timestamp_s": 435.0}, {"text": "most relevant context from the vector", "timestamp": "00:07:18,636", "timestamp_s": 438.0}, {"text": "database and then we\u0027ll make it as input in,", "timestamp": "00:07:22,460", "timestamp_s": 442.0}, {"text": "combine it with input when we are providing into llab.", "timestamp": "00:07:25,932", "timestamp_s": 445.0}, {"text": "So why? So the key thing to notice that we generally combine", "timestamp": "00:07:29,476", "timestamp_s": 449.0}, {"text": "the user input as well as the retrievals", "timestamp": "00:07:33,500", "timestamp_s": 453.0}, {"text": "we receive from the vector database. With that we", "timestamp": "00:07:38,170", "timestamp_s": 458.0}, {"text": "will start inferencing with the LLM, we will send", "timestamp": "00:07:42,010", "timestamp_s": 462.0}, {"text": "the LLM the request input and", "timestamp": "00:07:45,242", "timestamp_s": 465.0}, {"text": "then we will start updating the output as well,", "timestamp": "00:07:48,850", "timestamp_s": 468.0}, {"text": "which we can combine with our rack integration and then finally", "timestamp": "00:07:53,234", "timestamp_s": 473.0}, {"text": "we can send it to after customization to end user.", "timestamp": "00:07:57,450", "timestamp_s": 477.0}, {"text": "So this is typically a workflow of", "timestamp": "00:08:01,410", "timestamp_s": 481.0}, {"text": "generative AI and this is the way we want to kind of like", "timestamp": "00:08:04,982", "timestamp_s": 484.0}, {"text": "enable observability. What is observability?", "timestamp": "00:08:08,654", "timestamp_s": 488.0}, {"text": "I\u0027m sure most of you are aware, but just to ensure", "timestamp": "00:08:12,430", "timestamp_s": 492.0}, {"text": "that we are kind of in the same page, I just spend a quick", "timestamp": "00:08:15,878", "timestamp_s": 495.0}, {"text": "short amount of time to give my perspective", "timestamp": "00:08:20,078", "timestamp_s": 500.0}, {"text": "of observability. So observability is nothing but ability", "timestamp": "00:08:23,614", "timestamp_s": 503.0}, {"text": "to intercept or understand the system\u0027s internal", "timestamp": "00:08:28,002", "timestamp_s": 508.0}, {"text": "state by looking at its external output.", "timestamp": "00:08:31,818", "timestamp_s": 511.0}, {"text": "So what are the external outputs? We are typically looking at locks,", "timestamp": "00:08:34,914", "timestamp_s": 514.0}, {"text": "metrics and traces. So I like to think, you know,", "timestamp": "00:08:38,530", "timestamp_s": 518.0}, {"text": "observability is like, you know, looking at the big picture entire this mountain,", "timestamp": "00:08:41,634", "timestamp_s": 521.0}, {"text": "not only focusing on what\u0027s, you know, outside the water.", "timestamp": "00:08:45,506", "timestamp_s": 525.0}, {"text": "So what we are trying to look at it, trying to ask the questions like", "timestamp": "00:08:49,178", "timestamp_s": 529.0}, {"text": "what is happening in my system right now, how the systems are performing", "timestamp": "00:08:52,858", "timestamp_s": 532.0}, {"text": "and what anomalies they are in my system, what are the different", "timestamp": "00:08:57,304", "timestamp_s": 537.0}, {"text": "components interacting with each other, what causes", "timestamp": "00:09:01,440", "timestamp_s": 541.0}, {"text": "a particular issue or failure? So when it comes", "timestamp": "00:09:04,728", "timestamp_s": 544.0}, {"text": "to monitoring observability, obviously there are a", "timestamp": "00:09:08,072", "timestamp_s": 548.0}, {"text": "lot of good things when it comes to observability,", "timestamp": "00:09:11,176", "timestamp_s": 551.0}, {"text": "because observability is more of a proactive", "timestamp": "00:09:13,864", "timestamp_s": 553.0}, {"text": "approach, it\u0027s active approach instead of a passive,", "timestamp": "00:09:17,776", "timestamp_s": 557.0}, {"text": "and it\u0027s looking at the big picture and looking at more of a qualitative", "timestamp": "00:09:21,172", "timestamp_s": 561.0}, {"text": "and quantitative data. We want to make a quick", "timestamp": "00:09:25,228", "timestamp_s": 565.0}, {"text": "discussion and agree on something. And we want to agree when", "timestamp": "00:09:29,412", "timestamp_s": 569.0}, {"text": "we say observability and llms, what that means. So when", "timestamp": "00:09:33,460", "timestamp_s": 573.0}, {"text": "it comes to observability in llms or the", "timestamp": "00:09:37,380", "timestamp_s": 577.0}, {"text": "apps being developed using llms, we can divide it into", "timestamp": "00:09:41,228", "timestamp_s": 581.0}, {"text": "two parts. One is something we call direct LLM", "timestamp": "00:09:44,524", "timestamp_s": 584.0}, {"text": "observability or observability of LLM itself.", "timestamp": "00:09:47,894", "timestamp_s": 587.0}, {"text": "So what that means is that we in this scenario we will", "timestamp": "00:09:51,574", "timestamp_s": 591.0}, {"text": "start monitor, evaluate and look at the large language", "timestamp": "00:09:55,294", "timestamp_s": 595.0}, {"text": "model directly. So this is all about observability into", "timestamp": "00:09:59,246", "timestamp_s": 599.0}, {"text": "large language model. But then there are other aspects like indirect", "timestamp": "00:10:02,758", "timestamp_s": 602.0}, {"text": "LLM observability or observability of applications of", "timestamp": "00:10:07,094", "timestamp_s": 607.0}, {"text": "the systems using LLM. Here we are not looking at", "timestamp": "00:10:10,830", "timestamp_s": 610.0}, {"text": "the LLM directly, but we are looking at the applications", "timestamp": "00:10:14,268", "timestamp_s": 614.0}, {"text": "or the systems connecting utilizing LLM.", "timestamp": "00:10:18,764", "timestamp_s": 618.0}, {"text": "So this is just to ensure that, you know, we are able to both ways,", "timestamp": "00:10:22,684", "timestamp_s": 622.0}, {"text": "we are able to provide some really good benefits to the end users.", "timestamp": "00:10:26,444", "timestamp_s": 626.0}, {"text": "So both have its and", "timestamp": "00:10:30,196", "timestamp_s": 630.0}, {"text": "the techniques we will use is pretty much the similar standard way.", "timestamp": "00:10:33,612", "timestamp_s": 633.0}, {"text": "When it comes to observability, we will look at, you know, how we can look,", "timestamp": "00:10:37,188", "timestamp_s": 637.0}, {"text": "leverage the logs, the metrics, the traces and other things.", "timestamp": "00:10:40,848", "timestamp_s": 640.0}, {"text": "So now if we kind of quickly look at, you know, what,", "timestamp": "00:10:45,224", "timestamp_s": 645.0}, {"text": "when we mean direct LLM observability, what that means.", "timestamp": "00:10:48,432", "timestamp_s": 648.0}, {"text": "So here we will integrate observability capabilities during", "timestamp": "00:10:51,848", "timestamp_s": 651.0}, {"text": "the training, the deployments of LLM and while it\u0027s been", "timestamp": "00:10:55,832", "timestamp_s": 655.0}, {"text": "used, so it\u0027s at LLM itself.", "timestamp": "00:10:59,432", "timestamp_s": 659.0}, {"text": "So the main objective is we want to gain insight", "timestamp": "00:11:02,384", "timestamp_s": 662.0}, {"text": "into how the LLM is functioning, identify anomalies and other", "timestamp": "00:11:05,920", "timestamp_s": 665.0}, {"text": "issues directly related to LLM, understand the decision making", "timestamp": "00:11:09,676", "timestamp_s": 669.0}, {"text": "process of LLM here, how we approach this, we will activate", "timestamp": "00:11:13,540", "timestamp_s": 673.0}, {"text": "logging and we will look at things like the attention,", "timestamp": "00:11:17,756", "timestamp_s": 677.0}, {"text": "weights and other internal states of the LLMs when it\u0027s doing,", "timestamp": "00:11:21,324", "timestamp_s": 681.0}, {"text": "when we are doing inferences, we will implement probes or", "timestamp": "00:11:24,980", "timestamp_s": 684.0}, {"text": "instrumentation with the model architecture. So the observability", "timestamp": "00:11:28,852", "timestamp_s": 688.0}, {"text": "is being implemented at LLM level and we", "timestamp": "00:11:32,740", "timestamp_s": 692.0}, {"text": "will start tracking performance metrics such as latency and the", "timestamp": "00:11:36,168", "timestamp_s": 696.0}, {"text": "memory usage, and also things like external techniques like", "timestamp": "00:11:39,400", "timestamp_s": 699.0}, {"text": "attention visualization. So as I said, this is more of", "timestamp": "00:11:43,408", "timestamp_s": 703.0}, {"text": "LLM level. So this is about fully fledged looking", "timestamp": "00:11:48,504", "timestamp_s": 708.0}, {"text": "at how the LLM is performing. So when", "timestamp": "00:11:52,416", "timestamp_s": 712.0}, {"text": "it comes to indirect LLM observability, we are mainly looking", "timestamp": "00:11:55,768", "timestamp_s": 715.0}, {"text": "at the applications or the systems", "timestamp": "00:11:59,800", "timestamp_s": 719.0}, {"text": "which we have developed connecting with LLM. So here", "timestamp": "00:12:03,680", "timestamp_s": 723.0}, {"text": "we are not looking at LLM isolately,", "timestamp": "00:12:07,252", "timestamp_s": 727.0}, {"text": "but we are fully focused on the application side. So this is", "timestamp": "00:12:10,708", "timestamp_s": 730.0}, {"text": "to understand when it comes to our application, how is our application", "timestamp": "00:12:14,740", "timestamp_s": 734.0}, {"text": "is behaving, what observability things which we can enable", "timestamp": "00:12:18,588", "timestamp_s": 738.0}, {"text": "and how we can interpret the internal state.", "timestamp": "00:12:22,588", "timestamp_s": 742.0}, {"text": "This makes sense because just like any other application", "timestamp": "00:12:26,084", "timestamp_s": 746.0}, {"text": "for Genai also we want to understand how is our", "timestamp": "00:12:30,156", "timestamp_s": 750.0}, {"text": "application performed like because there can be any number of issues coming in.", "timestamp": "00:12:33,584", "timestamp_s": 753.0}, {"text": "And here again, you know, it\u0027s end of end user customer experience, it\u0027s the users", "timestamp": "00:12:37,656", "timestamp_s": 757.0}, {"text": "who are using our solution here what", "timestamp": "00:12:41,760", "timestamp_s": 761.0}, {"text": "we are looking at is again we will look at the logging,", "timestamp": "00:12:45,392", "timestamp_s": 765.0}, {"text": "the other inputs and outputs related to LLM.", "timestamp": "00:12:48,312", "timestamp_s": 768.0}, {"text": "We will looking at the monitoring metrics,", "timestamp": "00:12:51,680", "timestamp_s": 771.0}, {"text": "we will look at enabling anomaly detections", "timestamp": "00:12:54,864", "timestamp_s": 774.0}, {"text": "on some of the LLM outputs. Obviously we need", "timestamp": "00:12:57,992", "timestamp_s": 777.0}, {"text": "the human feedback loops as well. And then you know,", "timestamp": "00:13:01,502", "timestamp_s": 781.0}, {"text": "we will look at lot of metrics such as error rate, latency.", "timestamp": "00:13:05,190", "timestamp_s": 785.0}, {"text": "And the key objective as you would have already guessed is to understand", "timestamp": "00:13:08,998", "timestamp_s": 788.0}, {"text": "how is our application is behaving and how is our application", "timestamp": "00:13:13,022", "timestamp_s": 793.0}, {"text": "is leveraging LLM and how good kind", "timestamp": "00:13:16,942", "timestamp_s": 796.0}, {"text": "of output we are providing into our end users.", "timestamp": "00:13:20,174", "timestamp_s": 800.0}, {"text": "So when it comes to the LLM observability in", "timestamp": "00:13:24,094", "timestamp_s": 804.0}, {"text": "this presentation, when I say LLM observability, I am looking at indirect", "timestamp": "00:13:28,702", "timestamp_s": 808.0}, {"text": "LLM observability. So I am looking at coming up with", "timestamp": "00:13:32,630", "timestamp_s": 812.0}, {"text": "the maturity model which is catering", "timestamp": "00:13:36,270", "timestamp_s": 816.0}, {"text": "to applications develop using", "timestamp": "00:13:40,542", "timestamp_s": 820.0}, {"text": "application develops connecting to AWS bedrock because AWS is", "timestamp": "00:13:44,214", "timestamp_s": 824.0}, {"text": "what I am focusing on and the other aspects of AWS", "timestamp": "00:13:47,902", "timestamp_s": 827.0}, {"text": "is bedrock. So we are trying to see how we can integrate", "timestamp": "00:13:51,910", "timestamp_s": 831.0}, {"text": "observability practice into generative AI applications.", "timestamp": "00:13:55,558", "timestamp_s": 835.0}, {"text": "So we are looking at, you know, how we can identify", "timestamp": "00:13:58,966", "timestamp_s": 838.0}, {"text": "these applications internally. States also focus on some", "timestamp": "00:14:02,222", "timestamp_s": 842.0}, {"text": "aspects of LLM and the prompt engineering.", "timestamp": "00:14:05,606", "timestamp_s": 845.0}, {"text": "So we will look at the indirect oversight of LLM functionalities", "timestamp": "00:14:08,974", "timestamp_s": 848.0}, {"text": "and we try to make sure that the generative AI applications are", "timestamp": "00:14:13,182", "timestamp_s": 853.0}, {"text": "reliable and they are providing", "timestamp": "00:14:16,342", "timestamp_s": 856.0}, {"text": "what is it\u0027s been designed and the end users are happy with", "timestamp": "00:14:19,910", "timestamp_s": 859.0}, {"text": "the performance. So we want to answer this question,", "timestamp": "00:14:23,690", "timestamp_s": 863.0}, {"text": "why observe build for llms. So just like any other application", "timestamp": "00:14:27,754", "timestamp_s": 867.0}, {"text": "llms also that generative apps being developed", "timestamp": "00:14:31,826", "timestamp_s": 871.0}, {"text": "using llms also require observability because we need", "timestamp": "00:14:35,650", "timestamp_s": 875.0}, {"text": "observability, you know, when it comes to ensuring we kind", "timestamp": "00:14:39,266", "timestamp_s": 879.0}, {"text": "of like make sure that our generative applications", "timestamp": "00:14:42,922", "timestamp_s": 882.0}, {"text": "are correct, it\u0027s provide the correctness,", "timestamp": "00:14:46,402", "timestamp_s": 886.0}, {"text": "the accuracy and it\u0027s about the, the performance,", "timestamp": "00:14:49,450", "timestamp_s": 889.0}, {"text": "it\u0027s about providing great customer experience. But when it", "timestamp": "00:14:53,530", "timestamp_s": 893.0}, {"text": "comes to llms, llms have its own challenge. It\u0027s sometimes", "timestamp": "00:14:56,978", "timestamp_s": 896.0}, {"text": "it\u0027s complex. We might have to look at,", "timestamp": "00:15:00,690", "timestamp_s": 900.0}, {"text": "you know, what kind of anomalies, you know, or the model bias it\u0027s having", "timestamp": "00:15:03,818", "timestamp_s": 903.0}, {"text": "or the model drift. So when we say model", "timestamp": "00:15:07,922", "timestamp_s": 907.0}, {"text": "drift is the model can be working fine when", "timestamp": "00:15:11,154", "timestamp_s": 911.0}, {"text": "we are doing testing for considerable period of time but it", "timestamp": "00:15:15,340", "timestamp_s": 915.0}, {"text": "started, you know, it start failing. So this can have a adverse", "timestamp": "00:15:18,804", "timestamp_s": 918.0}, {"text": "impact on our end user performance.", "timestamp": "00:15:22,964", "timestamp_s": 922.0}, {"text": "And sometimes the models can create some biasness,", "timestamp": "00:15:26,284", "timestamp_s": 926.0}, {"text": "you know, which is again, you know, bad, which can create some bad customer experiences.", "timestamp": "00:15:30,180", "timestamp_s": 930.0}, {"text": "Then we will look at the pretty much the other standard things like debugging,", "timestamp": "00:15:34,636", "timestamp_s": 934.0}, {"text": "troubleshooting, how best we are using our resources", "timestamp": "00:15:38,052", "timestamp_s": 938.0}, {"text": "and the ethics, the data privacy, security.", "timestamp": "00:15:42,024", "timestamp_s": 942.0}, {"text": "So implementing kind of like looking at these things", "timestamp": "00:15:45,048", "timestamp_s": 945.0}, {"text": "again, observability for LLM is very important because", "timestamp": "00:15:48,200", "timestamp_s": 948.0}, {"text": "that is again allowing us to provide and", "timestamp": "00:15:52,224", "timestamp_s": 952.0}, {"text": "you know, kind of like give generate", "timestamp": "00:15:55,720", "timestamp_s": 955.0}, {"text": "great customer end user experiences.", "timestamp": "00:15:59,224", "timestamp_s": 959.0}, {"text": "So now we\u0027ll focus on trying to understand what are the pillar shaping", "timestamp": "00:16:01,984", "timestamp_s": 961.0}, {"text": "llms. What are the pillar shaping or", "timestamp": "00:16:06,088", "timestamp_s": 966.0}, {"text": "LLM observability. So one of the key things is", "timestamp": "00:16:09,706", "timestamp_s": 969.0}, {"text": "I\u0027d like to split into few parts. One is that LLM", "timestamp": "00:16:13,570", "timestamp_s": 973.0}, {"text": "specific metrics. So one is that LLM inference", "timestamp": "00:16:17,634", "timestamp_s": 977.0}, {"text": "latency. Here we track the LL latency of llms,", "timestamp": "00:16:21,234", "timestamp_s": 981.0}, {"text": "the request, you know, which is coming to bedrock application.", "timestamp": "00:16:24,954", "timestamp_s": 984.0}, {"text": "We will start monitoring the latency at different stages", "timestamp": "00:16:28,594", "timestamp_s": 988.0}, {"text": "of the request, such as like when they coming from the", "timestamp": "00:16:31,682", "timestamp_s": 991.0}, {"text": "API gateways and lambda functions, LLM itself.", "timestamp": "00:16:35,768", "timestamp_s": 995.0}, {"text": "So where however we have defined, we\u0027ll try to look at the", "timestamp": "00:16:38,976", "timestamp_s": 998.0}, {"text": "potential bottlenecks and how we can improve or optimize the performance.", "timestamp": "00:16:42,880", "timestamp_s": 1002.0}, {"text": "And then we will look at LLM inference success rate. So we will start", "timestamp": "00:16:47,584", "timestamp_s": 1007.0}, {"text": "monitoring the success rate of, you know, the request going and coming from LLM.", "timestamp": "00:16:51,304", "timestamp_s": 1011.0}, {"text": "And then we will start, you know, looking at what are the errors", "timestamp": "00:16:55,792", "timestamp_s": 1015.0}, {"text": "and whether there\u0027s increase in errors, what is the reason for errors,", "timestamp": "00:16:59,276", "timestamp_s": 1019.0}, {"text": "all the troubleshooting aspects as well. And we have", "timestamp": "00:17:02,564", "timestamp_s": 1022.0}, {"text": "this LLM quality, output quality where, you know,", "timestamp": "00:17:06,252", "timestamp_s": 1026.0}, {"text": "we will like trying to understand the quality", "timestamp": "00:17:09,884", "timestamp_s": 1029.0}, {"text": "of the LLM outputs. So which is again important.", "timestamp": "00:17:13,852", "timestamp_s": 1033.0}, {"text": "So that kind of gives us the ability to kind of like, you know,", "timestamp": "00:17:17,068", "timestamp_s": 1037.0}, {"text": "improving those areas. And one other important thing is LLM prompt", "timestamp": "00:17:20,596", "timestamp_s": 1040.0}, {"text": "effectiveness. So it tracks the effectiveness of the prompts", "timestamp": "00:17:24,484", "timestamp_s": 1044.0}, {"text": "which we are kind of like sending to LLM. So this again,", "timestamp": "00:17:27,979", "timestamp_s": 1047.0}, {"text": "you know, we will start monitoring the quality of LLM outputs", "timestamp": "00:17:31,587", "timestamp_s": 1051.0}, {"text": "based on those prompts and based on various different kind of prompts and", "timestamp": "00:17:35,483", "timestamp_s": 1055.0}, {"text": "how these are getting deviated and then start continuously", "timestamp": "00:17:39,451", "timestamp_s": 1059.0}, {"text": "refining this and moving on. Some of the other things", "timestamp": "00:17:43,171", "timestamp_s": 1063.0}, {"text": "are, you know, about LLM model drift. So we will", "timestamp": "00:17:46,883", "timestamp_s": 1066.0}, {"text": "start monitor the distribution of, you know, LLM outputs with the application,", "timestamp": "00:17:50,555", "timestamp_s": 1070.0}, {"text": "understand over period of time whether there\u0027s any significant", "timestamp": "00:17:54,739", "timestamp_s": 1074.0}, {"text": "output distributions. And then we\u0027ll start tracking the performance.", "timestamp": "00:17:59,434", "timestamp_s": 1079.0}, {"text": "And of course we will have to start looking at the cost and then when", "timestamp": "00:18:03,354", "timestamp_s": 1083.0}, {"text": "we are integrating with llms, whether there are integration issues,", "timestamp": "00:18:07,090", "timestamp_s": 1087.0}, {"text": "especially because, you know, we are integrating with the", "timestamp": "00:18:10,738", "timestamp_s": 1090.0}, {"text": "AWS, the bedrock, and then we will look at", "timestamp": "00:18:15,058", "timestamp_s": 1095.0}, {"text": "some of the ethical consideration as well. So we will start monitor llms output", "timestamp": "00:18:18,250", "timestamp_s": 1098.0}, {"text": "with the bedrock itself for potential", "timestamp": "00:18:22,562", "timestamp_s": 1102.0}, {"text": "ethical things, violations and other things. So we\u0027ll have to ensure", "timestamp": "00:18:25,858", "timestamp_s": 1105.0}, {"text": "that, you know, our generative AI apps which we have developed", "timestamp": "00:18:29,306", "timestamp_s": 1109.0}, {"text": "are 100% safe, there\u0027s no harm, illegal or discriminatory", "timestamp": "00:18:33,290", "timestamp_s": 1113.0}, {"text": "content, and llms are, and the", "timestamp": "00:18:37,426", "timestamp_s": 1117.0}, {"text": "generative AI apps are safe to use. So with", "timestamp": "00:18:40,618", "timestamp_s": 1120.0}, {"text": "that, you know, we are looking, we kind of like, those are the key things,", "timestamp": "00:18:44,210", "timestamp_s": 1124.0}, {"text": "you know, when it comes to the LLM specific metrics.", "timestamp": "00:18:47,314", "timestamp_s": 1127.0}, {"text": "And then when it comes to the prompt engineering properties, we will look at the", "timestamp": "00:18:50,874", "timestamp_s": 1130.0}, {"text": "temperature, we will start, you know, see how we can control randomness in", "timestamp": "00:18:54,410", "timestamp_s": 1134.0}, {"text": "the model, because the more higher the temperature,", "timestamp": "00:18:58,458", "timestamp_s": 1138.0}, {"text": "the diverse the outputs are. And you know,", "timestamp": "00:19:02,834", "timestamp_s": 1142.0}, {"text": "if you can lower the temperature, the more focused the outputs are. And then", "timestamp": "00:19:05,858", "timestamp_s": 1145.0}, {"text": "we will look at the top P sampling so that we know we can control", "timestamp": "00:19:09,490", "timestamp_s": 1149.0}, {"text": "the output diversity. And then we will look at the top k", "timestamp": "00:19:12,970", "timestamp_s": 1152.0}, {"text": "sampling and things like Max token", "timestamp": "00:19:16,626", "timestamp_s": 1156.0}, {"text": "and the stop tokens, you know, which is about signals to model to step", "timestamp": "00:19:19,830", "timestamp_s": 1159.0}, {"text": "generating text when, you know, this encountered.", "timestamp": "00:19:23,678", "timestamp_s": 1163.0}, {"text": "We will look at the repetition penalties, present penalties, batch sizes as well.", "timestamp": "00:19:26,638", "timestamp_s": 1166.0}, {"text": "So all of these things, you know, we can extract via logs and then send", "timestamp": "00:19:30,974", "timestamp_s": 1170.0}, {"text": "it to cloud lots, the cloudbot. And then, you know, we can", "timestamp": "00:19:34,310", "timestamp_s": 1174.0}, {"text": "create custom metrics and then start visualizing.", "timestamp": "00:19:37,878", "timestamp_s": 1177.0}, {"text": "And then two other thing is we can look at the, you know, in the", "timestamp": "00:19:41,494", "timestamp_s": 1181.0}, {"text": "inference latency, we can check whether the time taken for", "timestamp": "00:19:44,814", "timestamp_s": 1184.0}, {"text": "model to generate output for the given inputs.", "timestamp": "00:19:48,342", "timestamp_s": 1188.0}, {"text": "And then we look at the model accuracy and the matrix as well.", "timestamp": "00:19:51,486", "timestamp_s": 1191.0}, {"text": "So these things, you know, probably we are using AWS X ray and", "timestamp": "00:19:55,470", "timestamp_s": 1195.0}, {"text": "then, you know, start publishing these things into cloud work and", "timestamp": "00:19:59,222", "timestamp_s": 1199.0}, {"text": "then we can bring and create the alarms and wrappers around", "timestamp": "00:20:02,710", "timestamp_s": 1202.0}, {"text": "that. And few other things are other specific", "timestamp": "00:20:06,758", "timestamp_s": 1206.0}, {"text": "things. One thing we have to look at it that, you know, when it comes", "timestamp": "00:20:10,830", "timestamp_s": 1210.0}, {"text": "to the rag models, so what are the key things?", "timestamp": "00:20:14,014", "timestamp_s": 1214.0}, {"text": "So when it comes to rags, you know, we again have metrics like query latency.", "timestamp": "00:20:17,448", "timestamp_s": 1217.0}, {"text": "We want to understand the time it takes for the rack models", "timestamp": "00:20:21,704", "timestamp_s": 1221.0}, {"text": "to process a query and generate the response. And then we will look", "timestamp": "00:20:24,976", "timestamp_s": 1224.0}, {"text": "at the success rate, how successful are these queries and how", "timestamp": "00:20:28,360", "timestamp_s": 1228.0}, {"text": "often it\u0027s getting failed. We will look at the resource utilization and", "timestamp": "00:20:31,752", "timestamp_s": 1231.0}, {"text": "you know, in case if you are using caching, we look, we can look at", "timestamp": "00:20:35,720", "timestamp_s": 1235.0}, {"text": "the cache, it\u0027s as well. And when it comes to logs,", "timestamp": "00:20:39,000", "timestamp_s": 1239.0}, {"text": "we look at the query logs, error logs and the audit logs, you know,", "timestamp": "00:20:42,172", "timestamp_s": 1242.0}, {"text": "which will probably, probably give us a comprehensive way of, you know,", "timestamp": "00:20:45,300", "timestamp_s": 1245.0}, {"text": "auditing, troubleshooting. And then we\u0027ll try to enable traces,", "timestamp": "00:20:48,972", "timestamp_s": 1248.0}, {"text": "x ray, you know, which will provide us the end to end tracing so that", "timestamp": "00:20:52,820", "timestamp_s": 1252.0}, {"text": "way that we can have a complete", "timestamp": "00:20:56,220", "timestamp_s": 1256.0}, {"text": "observability into the data store or data retriever", "timestamp": "00:20:59,908", "timestamp_s": 1259.0}, {"text": "and other pillars are the tracing. So we have, we will use x ray,", "timestamp": "00:21:05,084", "timestamp_s": 1265.0}, {"text": "you know, so that will enable us to get integrate the traces and", "timestamp": "00:21:08,428", "timestamp_s": 1268.0}, {"text": "we will look at, you know, other integration AWS services as well.", "timestamp": "00:21:12,128", "timestamp_s": 1272.0}, {"text": "And then we will use Cloudwatch as a visualization tool. We can", "timestamp": "00:21:15,576", "timestamp_s": 1275.0}, {"text": "also use the Grafana, the AWS managed Grafana", "timestamp": "00:21:19,104", "timestamp_s": 1279.0}, {"text": "or any other things as well.", "timestamp": "00:21:23,048", "timestamp_s": 1283.0}, {"text": "So one other key thing is be mindful about alerting and incident", "timestamp": "00:21:26,224", "timestamp_s": 1286.0}, {"text": "management. So we can use the cloud virtual arms and we", "timestamp": "00:21:29,968", "timestamp_s": 1289.0}, {"text": "can leverage AWS system manager as well.", "timestamp": "00:21:33,528", "timestamp_s": 1293.0}, {"text": "So one important thing is the security. So we will leverage AWS cloud", "timestamp": "00:21:36,824", "timestamp_s": 1296.0}, {"text": "trail to audit and monitor the API calls and", "timestamp": "00:21:40,936", "timestamp_s": 1300.0}, {"text": "we\u0027ll ensure that the compliance with security and regulatory requirements", "timestamp": "00:21:44,560", "timestamp_s": 1304.0}, {"text": "are being tracked. I know we can integrate crowd logs with cloud", "timestamp": "00:21:48,128", "timestamp_s": 1308.0}, {"text": "work logs for centralization and then we will use", "timestamp": "00:21:52,184", "timestamp_s": 1312.0}, {"text": "AWS config so that we can continuously monitor and", "timestamp": "00:21:55,808", "timestamp_s": 1315.0}, {"text": "assess the configuration of our systems, AWS resources and", "timestamp": "00:21:59,592", "timestamp_s": 1319.0}, {"text": "we can ensure that we have compliance and best practices with", "timestamp": "00:22:03,520", "timestamp_s": 1323.0}, {"text": "the compliance standard with that.", "timestamp": "00:22:07,152", "timestamp_s": 1327.0}, {"text": "One key aspect is cost as well. So the more we are using our", "timestamp": "00:22:10,782", "timestamp_s": 1330.0}, {"text": "llms, you know, the more the cost factor comes in. So we", "timestamp": "00:22:14,334", "timestamp_s": 1334.0}, {"text": "can leverage AWs cost explorer and AWS budgets.", "timestamp": "00:22:17,470", "timestamp_s": 1337.0}, {"text": "And finally, one other important thing is that, you know, AI upscale building.", "timestamp": "00:22:21,894", "timestamp_s": 1341.0}, {"text": "So we will have to ensure that all the metrics, you know, whether it\u0027s", "timestamp": "00:22:25,982", "timestamp_s": 1345.0}, {"text": "the LLM specific, application specific or the RaG is specific,", "timestamp": "00:22:29,606", "timestamp_s": 1349.0}, {"text": "we will kind of like enable anomaly detection. And then for", "timestamp": "00:22:33,878", "timestamp_s": 1353.0}, {"text": "all the logs which we are putting into cloud work, we are", "timestamp": "00:22:37,754", "timestamp_s": 1357.0}, {"text": "able to enable the log anomaly detection as well. So we can also use", "timestamp": "00:22:41,162", "timestamp_s": 1361.0}, {"text": "Aws, the DevOps guru. So it\u0027s a machine learning", "timestamp": "00:22:45,362", "timestamp_s": 1365.0}, {"text": "service provided by AWS. So it,", "timestamp": "00:22:49,498", "timestamp_s": 1369.0}, {"text": "the DevOps guru will help us to detect and resolve issues", "timestamp": "00:22:52,970", "timestamp_s": 1372.0}, {"text": "in our system, especially identifying anomalies and other", "timestamp": "00:22:56,898", "timestamp_s": 1376.0}, {"text": "issues which probably we might not be able to uncover manually.", "timestamp": "00:23:00,386", "timestamp_s": 1380.0}, {"text": "And then we will look at leveraging AWS code guru as", "timestamp": "00:23:04,954", "timestamp_s": 1384.0}, {"text": "well because this allow us to integrate with the application so that", "timestamp": "00:23:08,418", "timestamp_s": 1388.0}, {"text": "we can do profiling and we can do the understand the", "timestamp": "00:23:12,362", "timestamp_s": 1392.0}, {"text": "resource utilizations usage based on our applications.", "timestamp": "00:23:16,826", "timestamp_s": 1396.0}, {"text": "Another very important thing is use AWS forecasting.", "timestamp": "00:23:21,010", "timestamp_s": 1401.0}, {"text": "So all the metrics and other things which we are bringing into the", "timestamp": "00:23:24,018", "timestamp_s": 1404.0}, {"text": "table, we can use the forecasting that will able to", "timestamp": "00:23:27,610", "timestamp_s": 1407.0}, {"text": "understand things in advance so that we can make better decisions", "timestamp": "00:23:31,314", "timestamp_s": 1411.0}, {"text": "and we can plan things ahead with that.", "timestamp": "00:23:34,882", "timestamp_s": 1414.0}, {"text": "Probably you can ask the question why we need a maturity model. So I", "timestamp": "00:23:37,762", "timestamp_s": 1417.0}, {"text": "am a big fan of maturity model because I think maturity models act as", "timestamp": "00:23:41,346", "timestamp_s": 1421.0}, {"text": "a north star. So we all want to start someplace and then take", "timestamp": "00:23:45,250", "timestamp_s": 1425.0}, {"text": "our systems into observability journey. So if you do", "timestamp": "00:23:49,698", "timestamp_s": 1429.0}, {"text": "that without kind of a maturity model or framework, then it\u0027s are,", "timestamp": "00:23:52,842", "timestamp_s": 1432.0}, {"text": "you know, you, you may ended up with any place, but by", "timestamp": "00:23:56,624", "timestamp_s": 1436.0}, {"text": "using a maturity model you can guarantee that, you know, you start with the basic", "timestamp": "00:24:00,256", "timestamp_s": 1440.0}, {"text": "steps and then you can finish with it some of the advanced things", "timestamp": "00:24:03,952", "timestamp_s": 1443.0}, {"text": "and you have better control of how you go there.", "timestamp": "00:24:07,880", "timestamp_s": 1447.0}, {"text": "So the LLM, the indirect", "timestamp": "00:24:12,024", "timestamp_s": 1452.0}, {"text": "observability maturity model, I have three pillars. One is,", "timestamp": "00:24:15,824", "timestamp_s": 1455.0}, {"text": "I call it level one, which is about foundational observability.", "timestamp": "00:24:19,456", "timestamp_s": 1459.0}, {"text": "And level two is the proactive observability. At level three", "timestamp": "00:24:23,220", "timestamp_s": 1463.0}, {"text": "we are looking at advanced LLM observability with AI Ops.", "timestamp": "00:24:26,660", "timestamp_s": 1466.0}, {"text": "So in the level one we will start, you know, capturing some of the basic", "timestamp": "00:24:30,364", "timestamp_s": 1470.0}, {"text": "LLM metrics. We will start getting the logs and start", "timestamp": "00:24:33,612", "timestamp_s": 1473.0}, {"text": "monitor the basic from properties, and we will implement basic", "timestamp": "00:24:37,156", "timestamp_s": 1477.0}, {"text": "logging and other distributed tracing. And then we will put up the visualization", "timestamp": "00:24:41,268", "timestamp_s": 1481.0}, {"text": "and other basic alerts as well. So this", "timestamp": "00:24:45,292", "timestamp_s": 1485.0}, {"text": "will kind of give you a foundational observability into your generative AI", "timestamp": "00:24:48,604", "timestamp_s": 1488.0}, {"text": "application. The next step is, you know, taking system more", "timestamp": "00:24:52,524", "timestamp_s": 1492.0}, {"text": "proactive, like be proactive. So here we will start,", "timestamp": "00:24:56,004", "timestamp_s": 1496.0}, {"text": "you know, capture and analyze the advanced LLM metrics and you know,", "timestamp": "00:24:59,332", "timestamp_s": 1499.0}, {"text": "start, you know, start leveraging the logs, then the", "timestamp": "00:25:02,876", "timestamp_s": 1502.0}, {"text": "other advanced prompt properties. And then we", "timestamp": "00:25:06,572", "timestamp_s": 1506.0}, {"text": "will enhance alerts and other incident management workflow so that", "timestamp": "00:25:09,820", "timestamp_s": 1509.0}, {"text": "we can identify things much faster and you know,", "timestamp": "00:25:13,340", "timestamp_s": 1513.0}, {"text": "resolve things much faster as well. So we will bring in the security", "timestamp": "00:25:16,292", "timestamp_s": 1516.0}, {"text": "aspect, the security compliance. We will start generating,", "timestamp": "00:25:19,868", "timestamp_s": 1519.0}, {"text": "leveraging, AWS forecasting so that we can start focusing", "timestamp": "00:25:23,340", "timestamp_s": 1523.0}, {"text": "about some of the LlMe specific matrix, matrix and the", "timestamp": "00:25:27,004", "timestamp_s": 1527.0}, {"text": "prompt properties as well. And for the logs", "timestamp": "00:25:32,004", "timestamp_s": 1532.0}, {"text": "we can also set up log anomaly detection.", "timestamp": "00:25:35,372", "timestamp_s": 1535.0}, {"text": "And when it comes to level three, which is kind of like the advanced level,", "timestamp": "00:25:38,684", "timestamp_s": 1538.0}, {"text": "which is the kind of place where you all want to be in, but you", "timestamp": "00:25:41,876", "timestamp_s": 1541.0}, {"text": "have to be mindful that it\u0027s a journey. Like you will have to start with", "timestamp": "00:25:45,758", "timestamp_s": 1545.0}, {"text": "level one, go to level two, and then we can be into level three.", "timestamp": "00:25:48,582", "timestamp_s": 1548.0}, {"text": "So at level three we start with integrating with DevOps guru", "timestamp": "00:25:52,278", "timestamp_s": 1552.0}, {"text": "and the code guru, so that with DevOps Guru will provide the AI", "timestamp": "00:25:56,214", "timestamp_s": 1556.0}, {"text": "and ML capabilities code guru will provide our quality", "timestamp": "00:25:59,862", "timestamp_s": 1559.0}, {"text": "of the code and then we will start implementing AIOps", "timestamp": "00:26:03,422", "timestamp_s": 1563.0}, {"text": "capabilities like other things like the noise reduction,", "timestamp": "00:26:07,302", "timestamp_s": 1567.0}, {"text": "smart intelligent root causes and then kind", "timestamp": "00:26:11,644", "timestamp_s": 1571.0}, {"text": "of like business impact assessments. So the forecasting feature", "timestamp": "00:26:15,332", "timestamp_s": 1575.0}, {"text": "will kind of like allow us to understand, if at all, if the", "timestamp": "00:26:18,668", "timestamp_s": 1578.0}, {"text": "models can drift, when that can happen, if at all,", "timestamp": "00:26:22,180", "timestamp_s": 1582.0}, {"text": "the models can start having a bias, when that can happen,", "timestamp": "00:26:25,868", "timestamp_s": 1585.0}, {"text": "the response time predictions and all those things. So the", "timestamp": "00:26:29,428", "timestamp_s": 1589.0}, {"text": "AI kind of thing is, can give you a full control of, you know,", "timestamp": "00:26:32,836", "timestamp_s": 1592.0}, {"text": "predictability of your generative AI application.", "timestamp": "00:26:35,804", "timestamp_s": 1595.0}, {"text": "So now I am kind of like look at more focus on", "timestamp": "00:26:39,234", "timestamp_s": 1599.0}, {"text": "implementation angle. So in the foundation model, like, you know, we can", "timestamp": "00:26:42,874", "timestamp_s": 1602.0}, {"text": "use cloud work metrics, like so that we can capture the", "timestamp": "00:26:46,642", "timestamp_s": 1606.0}, {"text": "basic LLM metrics, like, you know, the inference time, model size,", "timestamp": "00:26:50,074", "timestamp_s": 1610.0}, {"text": "prompt length and those things, the prompt properties. Again, we can,", "timestamp": "00:26:53,834", "timestamp_s": 1613.0}, {"text": "you know, leverage the sender, those logs into cloud work logs", "timestamp": "00:26:57,570", "timestamp_s": 1617.0}, {"text": "so that, you know, we can start monitoring basic properties like from", "timestamp": "00:27:01,082", "timestamp_s": 1621.0}, {"text": "content prompt sources and those things, any other logs,", "timestamp": "00:27:04,418", "timestamp_s": 1624.0}, {"text": "you know, we will be shipping into cloud work so that we can start,", "timestamp": "00:27:07,880", "timestamp_s": 1627.0}, {"text": "you know, getting the basic, the detail.", "timestamp": "00:27:10,656", "timestamp_s": 1630.0}, {"text": "And then we will integrate AWS x ray based on the technology", "timestamp": "00:27:14,344", "timestamp_s": 1634.0}, {"text": "we are using to develop our LLM to generate", "timestamp": "00:27:17,976", "timestamp_s": 1637.0}, {"text": "a app so that we can have ability to start looking", "timestamp": "00:27:21,624", "timestamp_s": 1641.0}, {"text": "at the traces and then visualization the dashboards.", "timestamp": "00:27:25,344", "timestamp_s": 1645.0}, {"text": "We can use AWS Cloudword dashboards and if required, you know,", "timestamp": "00:27:28,656", "timestamp_s": 1648.0}, {"text": "we can go into AWS managed Grafana dashboards", "timestamp": "00:27:32,142", "timestamp_s": 1652.0}, {"text": "as well. Alert and incident management. We are leveraging Cloudwatch", "timestamp": "00:27:35,526", "timestamp_s": 1655.0}, {"text": "and that will help us to understand some of the", "timestamp": "00:27:39,582", "timestamp_s": 1659.0}, {"text": "more the basic to a medium complex", "timestamp": "00:27:44,534", "timestamp_s": 1664.0}, {"text": "some of these monitors so that we can have a good control of", "timestamp": "00:27:49,198", "timestamp_s": 1669.0}, {"text": "our, how the llms are behaving and like how,", "timestamp": "00:27:52,390", "timestamp_s": 1672.0}, {"text": "how is our, the prompt is successful", "timestamp": "00:27:55,622", "timestamp_s": 1675.0}, {"text": "and overall how is our generative application is behaving and probably", "timestamp": "00:27:59,382", "timestamp_s": 1679.0}, {"text": "not probably, but how our end users are feeling about it.", "timestamp": "00:28:03,902", "timestamp_s": 1683.0}, {"text": "And then we will wrap this up with the cost like we using AWS Explorer.", "timestamp": "00:28:07,382", "timestamp_s": 1687.0}, {"text": "Because llms are sometimes costly, we\u0027ll have to ensure the usage and", "timestamp": "00:28:12,046", "timestamp_s": 1692.0}, {"text": "we start monitoring that as well. So level", "timestamp": "00:28:15,470", "timestamp_s": 1695.0}, {"text": "two, like, you know, we will go a little bit advanced for the metrics.", "timestamp": "00:28:18,878", "timestamp_s": 1698.0}, {"text": "We will start, you know, looking at advanced metrics like model performance and", "timestamp": "00:28:21,942", "timestamp_s": 1701.0}, {"text": "output quality. And again, prompt the properties.", "timestamp": "00:28:26,070", "timestamp_s": 1706.0}, {"text": "We will look at the advanced properties like the prompt performance,", "timestamp": "00:28:29,466", "timestamp_s": 1709.0}, {"text": "prompt versioning. And then, you know, we will start advancing,", "timestamp": "00:28:33,058", "timestamp_s": 1713.0}, {"text": "improving the incident workflows. We will look at the", "timestamp": "00:28:36,922", "timestamp_s": 1716.0}, {"text": "security compliance, we will look at more into the uplifting", "timestamp": "00:28:40,618", "timestamp_s": 1720.0}, {"text": "and like improving the cost factor as well. And one", "timestamp": "00:28:44,234", "timestamp_s": 1724.0}, {"text": "of the key thing, you know, here we will bring in is AWS forecasting.", "timestamp": "00:28:47,978", "timestamp_s": 1727.0}, {"text": "So using forecasting we want to ensure that we have the ability of", "timestamp": "00:28:52,090", "timestamp_s": 1732.0}, {"text": "forecasting all the key, the complex or even every", "timestamp": "00:28:55,522", "timestamp_s": 1735.0}, {"text": "metric related to LLM performance, LLM the", "timestamp": "00:29:00,066", "timestamp_s": 1740.0}, {"text": "inference, the accuracy and the prompt properties related", "timestamp": "00:29:04,306", "timestamp_s": 1744.0}, {"text": "things as well. So and then we will also look at", "timestamp": "00:29:07,722", "timestamp_s": 1747.0}, {"text": "enabling metric anomalies, log anomalies so that, you know, we start", "timestamp": "00:29:11,762", "timestamp_s": 1751.0}, {"text": "using some of the capabilities of anomalies this and", "timestamp": "00:29:15,330", "timestamp_s": 1755.0}, {"text": "finally we will bringing in AWS DevOps guru,", "timestamp": "00:29:19,762", "timestamp_s": 1759.0}, {"text": "so and the code guru and that will allow us to bringing in", "timestamp": "00:29:23,850", "timestamp_s": 1763.0}, {"text": "the AI capabilities into here the AI ML capability so", "timestamp": "00:29:27,874", "timestamp_s": 1767.0}, {"text": "that we can look at things from holistic ways. DevOps guru is a", "timestamp": "00:29:31,898", "timestamp_s": 1771.0}, {"text": "perfect tool. And then we will, you know, bringing in AI of practices", "timestamp": "00:29:35,730", "timestamp_s": 1775.0}, {"text": "and then kind of like, you know, bring ensuring that our incident workflow", "timestamp": "00:29:39,826", "timestamp_s": 1779.0}, {"text": "are more into self healing and there are a", "timestamp": "00:29:43,586", "timestamp_s": 1783.0}, {"text": "lot of other improvements and AI kind of", "timestamp": "00:29:47,230", "timestamp_s": 1787.0}, {"text": "things which we can bring. So what", "timestamp": "00:29:51,230", "timestamp_s": 1791.0}, {"text": "are the other things like, you know, so we will look at bringing in while", "timestamp": "00:29:55,670", "timestamp_s": 1795.0}, {"text": "we do this, we want to ensure that we measure the progress.", "timestamp": "00:30:00,134", "timestamp_s": 1800.0}, {"text": "So once we enable observability. So we want to ensure", "timestamp": "00:30:03,702", "timestamp_s": 1803.0}, {"text": "that we look at LLM. So, so how the LLM output quality", "timestamp": "00:30:07,406", "timestamp_s": 1807.0}, {"text": "is getting improved, how we are improving, optimizing our", "timestamp": "00:30:11,012", "timestamp_s": 1811.0}, {"text": "LLM prompt engineering area and then like", "timestamp": "00:30:14,284", "timestamp_s": 1814.0}, {"text": "ensuring that, you know, we can able to detect our model drifts in advance", "timestamp": "00:30:17,972", "timestamp_s": 1817.0}, {"text": "and then we can take necessary actions. We look at, you know, what are the", "timestamp": "00:30:21,700", "timestamp_s": 1821.0}, {"text": "ethical things, you know, our models based", "timestamp": "00:30:24,988", "timestamp_s": 1824.0}, {"text": "on that, how our models are behaving and then, you know, we look at", "timestamp": "00:30:28,604", "timestamp_s": 1828.0}, {"text": "the interpredictability, extendability and start", "timestamp": "00:30:31,860", "timestamp_s": 1831.0}, {"text": "keep a close eye on those things and generally like,", "timestamp": "00:30:35,756", "timestamp_s": 1835.0}, {"text": "you know, we will start kind of looking at end user experience", "timestamp": "00:30:39,272", "timestamp_s": 1839.0}, {"text": "as well. We will clearly define some end user specific", "timestamp": "00:30:42,528", "timestamp_s": 1842.0}, {"text": "service level objectives. We will start, you know, track the", "timestamp": "00:30:46,352", "timestamp_s": 1846.0}, {"text": "metrics and the improvements and we will start looking at", "timestamp": "00:30:50,312", "timestamp_s": 1850.0}, {"text": "the customer experience, ensure that, you know, whatever we", "timestamp": "00:30:53,960", "timestamp_s": 1853.0}, {"text": "do is align and correlate with customer", "timestamp": "00:30:57,392", "timestamp_s": 1857.0}, {"text": "experience. We see increasing customer experience as well.", "timestamp": "00:31:00,592", "timestamp_s": 1860.0}, {"text": "So and like overall that,", "timestamp": "00:31:04,352", "timestamp_s": 1864.0}, {"text": "you know, we develop and provide a better world class services into for our", "timestamp": "00:31:07,808", "timestamp_s": 1867.0}, {"text": "end users. And then some of the best practices is", "timestamp": "00:31:11,712", "timestamp_s": 1871.0}, {"text": "like, you know, so we will have to use a structured log and you know,", "timestamp": "00:31:15,304", "timestamp_s": 1875.0}, {"text": "if case you are heavily using lambda, probably go to power tools,", "timestamp": "00:31:18,776", "timestamp_s": 1878.0}, {"text": "you\u0027ll have to instrument the code, ensure that, you know, you get all the,", "timestamp": "00:31:22,208", "timestamp_s": 1882.0}, {"text": "the critical, the LLM specific metrics.", "timestamp": "00:31:25,592", "timestamp_s": 1885.0}, {"text": "Then you obviously use x ray to enable the traces as well.", "timestamp": "00:31:29,416", "timestamp_s": 1889.0}, {"text": "So the metrics which we are extracting, it has to be meaningful", "timestamp": "00:31:33,102", "timestamp_s": 1893.0}, {"text": "it has to add value. So it should be aligned with our business objectives", "timestamp": "00:31:36,974", "timestamp_s": 1896.0}, {"text": "as well. And to wrap up like, you know, some of the", "timestamp": "00:31:40,710", "timestamp_s": 1900.0}, {"text": "pitfall is that, you know, ensure that, you know, we kind of have a security", "timestamp": "00:31:44,646", "timestamp_s": 1904.0}, {"text": "we plan in advance and the compliance as well", "timestamp": "00:31:49,174", "timestamp_s": 1909.0}, {"text": "because that\u0027s again a key thing, you know, modern day when we are", "timestamp": "00:31:52,974", "timestamp_s": 1912.0}, {"text": "using generative applications and clearly define", "timestamp": "00:31:56,518", "timestamp_s": 1916.0}, {"text": "the roles like whatever objectives, you know, we are going", "timestamp": "00:31:59,886", "timestamp_s": 1919.0}, {"text": "to achieve with this. And probably you can have some numbers,", "timestamp": "00:32:03,750", "timestamp_s": 1923.0}, {"text": "you can have some measurable things so that you can start, you know,", "timestamp": "00:32:06,854", "timestamp_s": 1926.0}, {"text": "performing and kind of like try to get the benefit.", "timestamp": "00:32:10,294", "timestamp_s": 1930.0}, {"text": "So with this like, you know I\u0027m, we are at the close so", "timestamp": "00:32:13,974", "timestamp_s": 1933.0}, {"text": "thank you very much. So here I have taken AWS as", "timestamp": "00:32:17,254", "timestamp_s": 1937.0}, {"text": "example, especially AWS bedrock. From here we have look", "timestamp": "00:32:21,014", "timestamp_s": 1941.0}, {"text": "at what is a general architecture of workflow of", "timestamp": "00:32:24,802", "timestamp_s": 1944.0}, {"text": "generator application and what are the key pillars of", "timestamp": "00:32:28,826", "timestamp_s": 1948.0}, {"text": "the observability LLM related observability pillars", "timestamp": "00:32:33,154", "timestamp_s": 1953.0}, {"text": "which we have to enable and then we will look at the three,", "timestamp": "00:32:36,826", "timestamp_s": 1956.0}, {"text": "the levels, the foundation, the proactive observability", "timestamp": "00:32:40,714", "timestamp_s": 1960.0}, {"text": "and advanced observability is aiops. And then we", "timestamp": "00:32:44,682", "timestamp_s": 1964.0}, {"text": "have look at some of the best practices and the pitfalls and", "timestamp": "00:32:48,170", "timestamp_s": 1968.0}, {"text": "more importantly how we can look at this from ROI perspective.", "timestamp": "00:32:52,232", "timestamp_s": 1972.0}, {"text": "So with this, thank you very much for taking time. I hope, you know,", "timestamp": "00:32:56,456", "timestamp_s": 1976.0}, {"text": "you kind of like enjoy this and then you have understood", "timestamp": "00:32:59,928", "timestamp_s": 1979.0}, {"text": "or you have taken few things which you can take into your", "timestamp": "00:33:04,288", "timestamp_s": 1984.0}, {"text": "generative application and make it observable and leverage into to", "timestamp": "00:33:08,264", "timestamp_s": 1988.0}, {"text": "deliver great customer experiences.", "timestamp": "00:33:11,952", "timestamp_s": 1991.0}, {"text": "So with this, thank you very much.", "timestamp": "00:33:14,944", "timestamp_s": 1994.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'TeiLhP54hfU',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              LLMs in AWS: Observability Maturity from Foundation to AIOps
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Unleash Large Language Models in AWS! Join &ldquo;AWS LLMs Observability: Foundation to AIOps&rdquo; Boost LLM performance, troubleshoot precisely, stay ahead. Dive into a comprehensive maturity model for immediate impact, creating value for GenAI applications. Innovate now! Elevate your insights.</p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Welcome to LLM 2024 organized by conferred tour. Indika Vimalasurier will discuss how you can leverage observability maturity model to improve end user experience of the apps you are going to develop using LL lens. And then we'll wrap this up with some of the best practices and the pitfalls I think you should avoid.

              </li>
              
              <li>
                 Indigo Emilasuri is a solution architect with specialize in SRE observability, AI ops and generative AI working at Vergisa. He is mainly focused on llms especially deployed in and been accessed through AWS.

              </li>
              
              <li>
                We combine user input as well as retrievals we receive from the vector database. This is typically a workflow of generative AI and this is the way we want to kind of like enable observability. Finally we can send it to after customization to end user.

              </li>
              
              <li>
                What is observability? So observability is nothing but ability to intercept or understand the system's internal state by looking at its external output. In indirect LLM observability, we are mainly looking at the applications or the systems which have developed connecting with LLM.

              </li>
              
              <li>
                Generative apps being developed using llms also require observability because we need observability. Sometimes the models can create some biasness, which is again, you know, bad. Can create some bad customer experiences. So implementing observability for LLM is very important.

              </li>
              
              <li>
                So one of the key things is I'd like to split into few parts. One is that LLM specific metrics. Here we track the LLM inference latency. And one other important thing is LLM prompt effectiveness. We will have to ensure that our generative AI apps are 100% safe.

              </li>
              
              <li>
                At level three we are looking at advanced LLM observability with AI Ops. The next step is, you know, taking system more proactive, like be proactive. So the AI kind of thing is, can give you a full control of predictability of your generative AI application.

              </li>
              
              <li>
                So here I have taken AWS as example, especially AWS bedrock. From here we have look at what is a general architecture of workflow of generator application. And then we have looked at some of the best practices and the pitfalls and more importantly how we can look at this from ROI perspective. With this, thank you very much.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/assemblyai/TeiLhP54hfU.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:21,200'); seek(21.0)">
              Welcome to LLM 2024 organized by
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:24,750'); seek(24.0)">
              conferred tour. My name is Indika Vimalasurier
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:28,186'); seek(28.0)">
              and I'll walk you through about how you can leverage observability
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:32,466'); seek(32.0)">
              maturity model improve end
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:36,098'); seek(36.0)">
              user experience of the apps you are going to develop
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:39,498'); seek(39.0)">
              using LL lens. So we will touch about how to start
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:43,442'); seek(43.0)">
              which is the foundation, and then probably take it up to
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:47,514'); seek(47.0)">
              around using AI to
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:51,082'); seek(51.0)">
              support your operations. As you might aware,
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:55,234'); seek(55.0)">
              by around 2022,
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:58,674'); seek(58.0)">
              the hype started with Chatgbt. ChatgBT was
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:02,714'); seek(62.0)">
              a hit, it was mainstream and it resulted
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:06,562'); seek(66.0)">
              in lot of people who are not into AI start creating
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:10,978'); seek(70.0)">
              generative AI apps. So now it's already has taken over
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:15,114'); seek(75.0)">
              the world. The world is looking at what are the use cases
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:18,874'); seek(78.0)">
              which we can use and
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:23,092'); seek(83.0)">
              leverage. It's already mainstream.
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:26,380'); seek(86.0)">
              There's lot of developers who are building apps
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:30,580'); seek(90.0)">
              connecting llms. So there's a need.
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:34,644'); seek(94.0)">
              Apps which we are going to develop has a capability of
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:37,700'); seek(97.0)">
              providing full end user experience because
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:42,028'); seek(102.0)">
              we all know how it can end. While generative
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:45,496'); seek(105.0)">
              AI is which is opening creating
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:48,736'); seek(108.0)">
              lot of new opportunities. We also want to ensure that the apps which
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:52,720'); seek(112.0)">
              is being developed, deployed properly
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:55,840'); seek(115.0)">
              in production environments and are being served to end users
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:59,960'); seek(119.0)">
              as per the expectation and we don't want to make it
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:03,440'); seek(123.0)">
              ops problem. So we want to ensure we build a solid
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:07,144'); seek(127.0)">
              observability into our llms as well.
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:10,673'); seek(130.0)">
              So as part of today's presentation, I'll provide you a quick intro,
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:14,689'); seek(134.0)">
              what is observability? And we'll discuss about what
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:18,433'); seek(138.0)">
              is observability mean for llms. So there are two kind
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:21,769'); seek(141.0)">
              of observability which we can discuss,
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:25,225'); seek(145.0)">
              so which we are going to discuss, so which is about a direct
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:28,465'); seek(148.0)">
              observability, and second one is about indirect observability.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:32,081'); seek(152.0)">
              So I'll be focusing more on indirect observability
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:35,569'); seek(155.0)">
              when discussing about the maturity model, which I'm going to
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:39,522'); seek(159.0)">
              walk you through. Then I'll walk you through about some of the pillars,
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:43,210'); seek(163.0)">
              give a quick intro about what is the LLM
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:46,714'); seek(166.0)">
              look like, and then we'll jump into my main focus,
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:50,690'); seek(170.0)">
              a maturity model for LLM. So then
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:54,202'); seek(174.0)">
              we'll look at some of the implementation
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:58,130'); seek(178.0)">
              guidelines, the services which we can leverage, and then of course,
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:02,522'); seek(182.0)">
              like every other maturity model, this should
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:06,490'); seek(186.0)">
              not be just a maturity model where people will just follow blindly,
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:11,106'); seek(191.0)">
              but we want to ensure we tack into business outcomes
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:15,010'); seek(195.0)">
              so we have an ability to measure the
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:18,242'); seek(198.0)">
              progress. And then we'll wrap this up with some of the
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:21,370'); seek(201.0)">
              best practices and some of the pitfalls I think you should avoid.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:26,114'); seek(206.0)">
              Before we start, a quick intro about myself.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:29,774'); seek(209.0)">
              My name is Indigo Emilasuri. I'm based out of Colombo.
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:33,334'); seek(213.0)">
              I'm a serious reliability
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:37,870'); seek(217.0)">
              engineering advocate and a practitioner as
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:40,982'); seek(220.0)">
              well. I'm a solution architect with specialize in SRE observability,
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:45,550'); seek(225.0)">
              AI ops and generative AI working
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:49,126'); seek(229.0)">
              at Vergisa as a senior systems engineering manager,
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:52,622'); seek(232.0)">
              I'm a passionate technical trainer. I have trained hundreds
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:56,318'); seek(236.0)">
              of people when it comes to SRE observability
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:00,286'); seek(240.0)">
              aiops and I'm an energetic technical blogger
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:04,454'); seek(244.0)">
              and I'm very proud AWS community builder
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:07,702'); seek(247.0)">
              under cloud operations and also a very proud ambassador
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:11,742'); seek(251.0)">
              at DevOps Institute which is also known as PC
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:15,182'); seek(255.0)">
              CERT because they have acquired it. So that's about me.
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:18,838'); seek(258.0)">
              So I am very passionate about this topic, observability. So when
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:22,740'); seek(262.0)">
              it comes to the distributed systems and then llms,
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:26,084'); seek(266.0)">
              the end of the day I look at things from a customer experience
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:30,156'); seek(270.0)">
              and how we can provide better customer experience to our end users
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:34,076'); seek(274.0)">
              and then how we can make a better business
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:37,572'); seek(277.0)">
              outcomes part of the presentation I'm
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:41,508'); seek(281.0)">
              mainly focused on AWS. So I'm looking at
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:44,620'); seek(284.0)">
              llms especially deployed in and
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:47,952'); seek(287.0)">
              been accessed through AWS. So one
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:51,768'); seek(291.0)">
              of the fantastic service AWS has offered is Amazon
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:55,744'); seek(295.0)">
              Bedrock, which is a managed
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:59,776'); seek(299.0)">
              service where you are able to use APIs to
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:03,592'); seek(303.0)">
              access the foundational models. So it's
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:06,968'); seek(306.0)">
              really fast, it's really quick, you just have to ensure that
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:10,352'); seek(310.0)">
              you have the ability of connecting. So the key features
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:14,152'); seek(314.0)">
              are it's giving access to the foundation models and
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:18,394'); seek(318.0)">
              the use cases such as text generation, image generation and
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:23,234'); seek(323.0)">
              the use cases around those. So it's also providing
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:26,562'); seek(326.0)">
              this private customization with own data with the
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:30,402'); seek(330.0)">
              techniques like the retrieval augmented
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:33,826'); seek(333.0)">
              generation. We call it rack. And it's also providing the
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:37,962'); seek(337.0)">
              ability of building agents and executed tasks using
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:42,546'); seek(342.0)">
              the system, enterprise systems and other data sources.
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:46,944'); seek(346.0)">
              Obviously one good thing is that there's no infrastructure, so you
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:50,408'); seek(350.0)">
              don't have to worry about infrastructure. So AWS is taking
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:53,968'); seek(353.0)">
              care of the infrastructure. So that, that's why we call it fully managed.
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:58,040'); seek(358.0)">
              So it's a very secure and it's a, you know, it's a
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:01,376'); seek(361.0)">
              go to tool if you want to develop generative AI apps.
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:05,424'); seek(365.0)">
              It's already consist of, you know, some of the most
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:10,014'); seek(370.0)">
              widely used foundation models provided by a 21
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:14,206'); seek(374.0)">
              labs anthropic cohere, meta and stability
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:18,022'); seek(378.0)">
              AI, Amazon as well. So there are a lot of models and they are
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:21,462'); seek(381.0)">
              also continuously adding these models into their.
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:26,454'); seek(386.0)">
              So with that, our observability
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:31,334'); seek(391.0)">
              maturity model or the approach is mainly focused on application,
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:35,550'); seek(395.0)">
              which has been developed using Amazon bedrock.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:38,644'); seek(398.0)">
              So moving on. I just want to give a quick idea like you know,
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:42,060'); seek(402.0)">
              so when we say generate AI apps, so what is kind of the
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:45,460'); seek(405.0)">
              use case? The use cases, a typical user
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:49,676'); seek(409.0)">
              can kind of like enter query. So it will
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:53,244'); seek(413.0)">
              come into our, the query interface like we can take it from
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:56,740'); seek(416.0)">
              my API or user interface. And then the,
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:00,028'); seek(420.0)">
              we will process, start processing this user query and
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:03,700'); seek(423.0)">
              then we will try to connect it with the vector encoding.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:07,252'); seek(427.0)">
              So it's trying to find similar queries,
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:11,908'); seek(431.0)">
              similar patterns using in our vector database.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:15,172'); seek(435.0)">
              And then we will kind of like looking at retrieving the top k
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:18,636'); seek(438.0)">
              most relevant context from the vector
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:22,460'); seek(442.0)">
              database and then we'll make it as input in,
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:25,932'); seek(445.0)">
              combine it with input when we are providing into llab.
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:29,476'); seek(449.0)">
              So why? So the key thing to notice that we generally combine
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:33,500'); seek(453.0)">
              the user input as well as the retrievals
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:38,170'); seek(458.0)">
              we receive from the vector database. With that we
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:42,010'); seek(462.0)">
              will start inferencing with the LLM, we will send
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:45,242'); seek(465.0)">
              the LLM the request input and
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:48,850'); seek(468.0)">
              then we will start updating the output as well,
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:53,234'); seek(473.0)">
              which we can combine with our rack integration and then finally
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:57,450'); seek(477.0)">
              we can send it to after customization to end user.
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:01,410'); seek(481.0)">
              So this is typically a workflow of
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:04,982'); seek(484.0)">
              generative AI and this is the way we want to kind of like
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:08,654'); seek(488.0)">
              enable observability. What is observability?
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:12,430'); seek(492.0)">
              I'm sure most of you are aware, but just to ensure
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:15,878'); seek(495.0)">
              that we are kind of in the same page, I just spend a quick
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:20,078'); seek(500.0)">
              short amount of time to give my perspective
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:23,614'); seek(503.0)">
              of observability. So observability is nothing but ability
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:28,002'); seek(508.0)">
              to intercept or understand the system's internal
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:31,818'); seek(511.0)">
              state by looking at its external output.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:34,914'); seek(514.0)">
              So what are the external outputs? We are typically looking at locks,
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:38,530'); seek(518.0)">
              metrics and traces. So I like to think, you know,
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:41,634'); seek(521.0)">
              observability is like, you know, looking at the big picture entire this mountain,
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:45,506'); seek(525.0)">
              not only focusing on what's, you know, outside the water.
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:49,178'); seek(529.0)">
              So what we are trying to look at it, trying to ask the questions like
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:52,858'); seek(532.0)">
              what is happening in my system right now, how the systems are performing
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:57,304'); seek(537.0)">
              and what anomalies they are in my system, what are the different
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:01,440'); seek(541.0)">
              components interacting with each other, what causes
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:04,728'); seek(544.0)">
              a particular issue or failure? So when it comes
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:08,072'); seek(548.0)">
              to monitoring observability, obviously there are a
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:11,176'); seek(551.0)">
              lot of good things when it comes to observability,
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:13,864'); seek(553.0)">
              because observability is more of a proactive
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:17,776'); seek(557.0)">
              approach, it's active approach instead of a passive,
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:21,172'); seek(561.0)">
              and it's looking at the big picture and looking at more of a qualitative
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:25,228'); seek(565.0)">
              and quantitative data. We want to make a quick
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:29,412'); seek(569.0)">
              discussion and agree on something. And we want to agree when
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:33,460'); seek(573.0)">
              we say observability and llms, what that means. So when
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:37,380'); seek(577.0)">
              it comes to observability in llms or the
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:41,228'); seek(581.0)">
              apps being developed using llms, we can divide it into
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:44,524'); seek(584.0)">
              two parts. One is something we call direct LLM
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:47,894'); seek(587.0)">
              observability or observability of LLM itself.
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:51,574'); seek(591.0)">
              So what that means is that we in this scenario we will
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:55,294'); seek(595.0)">
              start monitor, evaluate and look at the large language
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:59,246'); seek(599.0)">
              model directly. So this is all about observability into
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:02,758'); seek(602.0)">
              large language model. But then there are other aspects like indirect
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:07,094'); seek(607.0)">
              LLM observability or observability of applications of
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:10,830'); seek(610.0)">
              the systems using LLM. Here we are not looking at
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:14,268'); seek(614.0)">
              the LLM directly, but we are looking at the applications
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:18,764'); seek(618.0)">
              or the systems connecting utilizing LLM.
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:22,684'); seek(622.0)">
              So this is just to ensure that, you know, we are able to both ways,
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:26,444'); seek(626.0)">
              we are able to provide some really good benefits to the end users.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:30,196'); seek(630.0)">
              So both have its and
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:33,612'); seek(633.0)">
              the techniques we will use is pretty much the similar standard way.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:37,188'); seek(637.0)">
              When it comes to observability, we will look at, you know, how we can look,
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:40,848'); seek(640.0)">
              leverage the logs, the metrics, the traces and other things.
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:45,224'); seek(645.0)">
              So now if we kind of quickly look at, you know, what,
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:48,432'); seek(648.0)">
              when we mean direct LLM observability, what that means.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:51,848'); seek(651.0)">
              So here we will integrate observability capabilities during
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:55,832'); seek(655.0)">
              the training, the deployments of LLM and while it's been
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:59,432'); seek(659.0)">
              used, so it's at LLM itself.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:02,384'); seek(662.0)">
              So the main objective is we want to gain insight
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:05,920'); seek(665.0)">
              into how the LLM is functioning, identify anomalies and other
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:09,676'); seek(669.0)">
              issues directly related to LLM, understand the decision making
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:13,540'); seek(673.0)">
              process of LLM here, how we approach this, we will activate
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:17,756'); seek(677.0)">
              logging and we will look at things like the attention,
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:21,324'); seek(681.0)">
              weights and other internal states of the LLMs when it's doing,
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:24,980'); seek(684.0)">
              when we are doing inferences, we will implement probes or
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:28,852'); seek(688.0)">
              instrumentation with the model architecture. So the observability
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:32,740'); seek(692.0)">
              is being implemented at LLM level and we
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:36,168'); seek(696.0)">
              will start tracking performance metrics such as latency and the
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:39,400'); seek(699.0)">
              memory usage, and also things like external techniques like
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:43,408'); seek(703.0)">
              attention visualization. So as I said, this is more of
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:48,504'); seek(708.0)">
              LLM level. So this is about fully fledged looking
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:52,416'); seek(712.0)">
              at how the LLM is performing. So when
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:55,768'); seek(715.0)">
              it comes to indirect LLM observability, we are mainly looking
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:59,800'); seek(719.0)">
              at the applications or the systems
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:03,680'); seek(723.0)">
              which we have developed connecting with LLM. So here
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:07,252'); seek(727.0)">
              we are not looking at LLM isolately,
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:10,708'); seek(730.0)">
              but we are fully focused on the application side. So this is
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:14,740'); seek(734.0)">
              to understand when it comes to our application, how is our application
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:18,588'); seek(738.0)">
              is behaving, what observability things which we can enable
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:22,588'); seek(742.0)">
              and how we can interpret the internal state.
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:26,084'); seek(746.0)">
              This makes sense because just like any other application
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:30,156'); seek(750.0)">
              for Genai also we want to understand how is our
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:33,584'); seek(753.0)">
              application performed like because there can be any number of issues coming in.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:37,656'); seek(757.0)">
              And here again, you know, it's end of end user customer experience, it's the users
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:41,760'); seek(761.0)">
              who are using our solution here what
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:45,392'); seek(765.0)">
              we are looking at is again we will look at the logging,
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:48,312'); seek(768.0)">
              the other inputs and outputs related to LLM.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:51,680'); seek(771.0)">
              We will looking at the monitoring metrics,
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:54,864'); seek(774.0)">
              we will look at enabling anomaly detections
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:57,992'); seek(777.0)">
              on some of the LLM outputs. Obviously we need
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:01,502'); seek(781.0)">
              the human feedback loops as well. And then you know,
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:05,190'); seek(785.0)">
              we will look at lot of metrics such as error rate, latency.
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:08,998'); seek(788.0)">
              And the key objective as you would have already guessed is to understand
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:13,022'); seek(793.0)">
              how is our application is behaving and how is our application
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:16,942'); seek(796.0)">
              is leveraging LLM and how good kind
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:20,174'); seek(800.0)">
              of output we are providing into our end users.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:24,094'); seek(804.0)">
              So when it comes to the LLM observability in
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:28,702'); seek(808.0)">
              this presentation, when I say LLM observability, I am looking at indirect
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:32,630'); seek(812.0)">
              LLM observability. So I am looking at coming up with
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:36,270'); seek(816.0)">
              the maturity model which is catering
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:40,542'); seek(820.0)">
              to applications develop using
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:44,214'); seek(824.0)">
              application develops connecting to AWS bedrock because AWS is
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:47,902'); seek(827.0)">
              what I am focusing on and the other aspects of AWS
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:51,910'); seek(831.0)">
              is bedrock. So we are trying to see how we can integrate
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:55,558'); seek(835.0)">
              observability practice into generative AI applications.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:58,966'); seek(838.0)">
              So we are looking at, you know, how we can identify
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:02,222'); seek(842.0)">
              these applications internally. States also focus on some
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:05,606'); seek(845.0)">
              aspects of LLM and the prompt engineering.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:08,974'); seek(848.0)">
              So we will look at the indirect oversight of LLM functionalities
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:13,182'); seek(853.0)">
              and we try to make sure that the generative AI applications are
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:16,342'); seek(856.0)">
              reliable and they are providing
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:19,910'); seek(859.0)">
              what is it's been designed and the end users are happy with
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:23,690'); seek(863.0)">
              the performance. So we want to answer this question,
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:27,754'); seek(867.0)">
              why observe build for llms. So just like any other application
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:31,826'); seek(871.0)">
              llms also that generative apps being developed
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:35,650'); seek(875.0)">
              using llms also require observability because we need
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:39,266'); seek(879.0)">
              observability, you know, when it comes to ensuring we kind
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:42,922'); seek(882.0)">
              of like make sure that our generative applications
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:46,402'); seek(886.0)">
              are correct, it's provide the correctness,
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:49,450'); seek(889.0)">
              the accuracy and it's about the, the performance,
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:53,530'); seek(893.0)">
              it's about providing great customer experience. But when it
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:56,978'); seek(896.0)">
              comes to llms, llms have its own challenge. It's sometimes
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:00,690'); seek(900.0)">
              it's complex. We might have to look at,
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:03,818'); seek(903.0)">
              you know, what kind of anomalies, you know, or the model bias it's having
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:07,922'); seek(907.0)">
              or the model drift. So when we say model
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:11,154'); seek(911.0)">
              drift is the model can be working fine when
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:15,340'); seek(915.0)">
              we are doing testing for considerable period of time but it
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:18,804'); seek(918.0)">
              started, you know, it start failing. So this can have a adverse
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:22,964'); seek(922.0)">
              impact on our end user performance.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:26,284'); seek(926.0)">
              And sometimes the models can create some biasness,
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:30,180'); seek(930.0)">
              you know, which is again, you know, bad, which can create some bad customer experiences.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:34,636'); seek(934.0)">
              Then we will look at the pretty much the other standard things like debugging,
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:38,052'); seek(938.0)">
              troubleshooting, how best we are using our resources
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:42,024'); seek(942.0)">
              and the ethics, the data privacy, security.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:45,048'); seek(945.0)">
              So implementing kind of like looking at these things
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:48,200'); seek(948.0)">
              again, observability for LLM is very important because
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:52,224'); seek(952.0)">
              that is again allowing us to provide and
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:55,720'); seek(955.0)">
              you know, kind of like give generate
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:59,224'); seek(959.0)">
              great customer end user experiences.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:01,984'); seek(961.0)">
              So now we'll focus on trying to understand what are the pillar shaping
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:06,088'); seek(966.0)">
              llms. What are the pillar shaping or
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:09,706'); seek(969.0)">
              LLM observability. So one of the key things is
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:13,570'); seek(973.0)">
              I'd like to split into few parts. One is that LLM
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:17,634'); seek(977.0)">
              specific metrics. So one is that LLM inference
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:21,234'); seek(981.0)">
              latency. Here we track the LL latency of llms,
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:24,954'); seek(984.0)">
              the request, you know, which is coming to bedrock application.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:28,594'); seek(988.0)">
              We will start monitoring the latency at different stages
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:31,682'); seek(991.0)">
              of the request, such as like when they coming from the
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:35,768'); seek(995.0)">
              API gateways and lambda functions, LLM itself.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:38,976'); seek(998.0)">
              So where however we have defined, we'll try to look at the
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:42,880'); seek(1002.0)">
              potential bottlenecks and how we can improve or optimize the performance.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:47,584'); seek(1007.0)">
              And then we will look at LLM inference success rate. So we will start
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:51,304'); seek(1011.0)">
              monitoring the success rate of, you know, the request going and coming from LLM.
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:55,792'); seek(1015.0)">
              And then we will start, you know, looking at what are the errors
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:59,276'); seek(1019.0)">
              and whether there's increase in errors, what is the reason for errors,
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:02,564'); seek(1022.0)">
              all the troubleshooting aspects as well. And we have
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:06,252'); seek(1026.0)">
              this LLM quality, output quality where, you know,
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:09,884'); seek(1029.0)">
              we will like trying to understand the quality
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:13,852'); seek(1033.0)">
              of the LLM outputs. So which is again important.
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:17,068'); seek(1037.0)">
              So that kind of gives us the ability to kind of like, you know,
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:20,596'); seek(1040.0)">
              improving those areas. And one other important thing is LLM prompt
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:24,484'); seek(1044.0)">
              effectiveness. So it tracks the effectiveness of the prompts
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:27,979'); seek(1047.0)">
              which we are kind of like sending to LLM. So this again,
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:31,587'); seek(1051.0)">
              you know, we will start monitoring the quality of LLM outputs
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:35,483'); seek(1055.0)">
              based on those prompts and based on various different kind of prompts and
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:39,451'); seek(1059.0)">
              how these are getting deviated and then start continuously
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:43,171'); seek(1063.0)">
              refining this and moving on. Some of the other things
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:46,883'); seek(1066.0)">
              are, you know, about LLM model drift. So we will
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:50,555'); seek(1070.0)">
              start monitor the distribution of, you know, LLM outputs with the application,
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:54,739'); seek(1074.0)">
              understand over period of time whether there's any significant
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:59,434'); seek(1079.0)">
              output distributions. And then we'll start tracking the performance.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:03,354'); seek(1083.0)">
              And of course we will have to start looking at the cost and then when
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:07,090'); seek(1087.0)">
              we are integrating with llms, whether there are integration issues,
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:10,738'); seek(1090.0)">
              especially because, you know, we are integrating with the
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:15,058'); seek(1095.0)">
              AWS, the bedrock, and then we will look at
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:18,250'); seek(1098.0)">
              some of the ethical consideration as well. So we will start monitor llms output
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:22,562'); seek(1102.0)">
              with the bedrock itself for potential
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:25,858'); seek(1105.0)">
              ethical things, violations and other things. So we'll have to ensure
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:29,306'); seek(1109.0)">
              that, you know, our generative AI apps which we have developed
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:33,290'); seek(1113.0)">
              are 100% safe, there's no harm, illegal or discriminatory
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:37,426'); seek(1117.0)">
              content, and llms are, and the
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:40,618'); seek(1120.0)">
              generative AI apps are safe to use. So with
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:44,210'); seek(1124.0)">
              that, you know, we are looking, we kind of like, those are the key things,
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:47,314'); seek(1127.0)">
              you know, when it comes to the LLM specific metrics.
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:50,874'); seek(1130.0)">
              And then when it comes to the prompt engineering properties, we will look at the
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:54,410'); seek(1134.0)">
              temperature, we will start, you know, see how we can control randomness in
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:58,458'); seek(1138.0)">
              the model, because the more higher the temperature,
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:02,834'); seek(1142.0)">
              the diverse the outputs are. And you know,
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:05,858'); seek(1145.0)">
              if you can lower the temperature, the more focused the outputs are. And then
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:09,490'); seek(1149.0)">
              we will look at the top P sampling so that we know we can control
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:12,970'); seek(1152.0)">
              the output diversity. And then we will look at the top k
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:16,626'); seek(1156.0)">
              sampling and things like Max token
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:19,830'); seek(1159.0)">
              and the stop tokens, you know, which is about signals to model to step
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:23,678'); seek(1163.0)">
              generating text when, you know, this encountered.
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:26,638'); seek(1166.0)">
              We will look at the repetition penalties, present penalties, batch sizes as well.
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:30,974'); seek(1170.0)">
              So all of these things, you know, we can extract via logs and then send
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:34,310'); seek(1174.0)">
              it to cloud lots, the cloudbot. And then, you know, we can
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:37,878'); seek(1177.0)">
              create custom metrics and then start visualizing.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:41,494'); seek(1181.0)">
              And then two other thing is we can look at the, you know, in the
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:44,814'); seek(1184.0)">
              inference latency, we can check whether the time taken for
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:48,342'); seek(1188.0)">
              model to generate output for the given inputs.
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:51,486'); seek(1191.0)">
              And then we look at the model accuracy and the matrix as well.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:55,470'); seek(1195.0)">
              So these things, you know, probably we are using AWS X ray and
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:59,222'); seek(1199.0)">
              then, you know, start publishing these things into cloud work and
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:02,710'); seek(1202.0)">
              then we can bring and create the alarms and wrappers around
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:06,758'); seek(1206.0)">
              that. And few other things are other specific
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:10,830'); seek(1210.0)">
              things. One thing we have to look at it that, you know, when it comes
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:14,014'); seek(1214.0)">
              to the rag models, so what are the key things?
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:17,448'); seek(1217.0)">
              So when it comes to rags, you know, we again have metrics like query latency.
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:21,704'); seek(1221.0)">
              We want to understand the time it takes for the rack models
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:24,976'); seek(1224.0)">
              to process a query and generate the response. And then we will look
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:28,360'); seek(1228.0)">
              at the success rate, how successful are these queries and how
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:31,752'); seek(1231.0)">
              often it's getting failed. We will look at the resource utilization and
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:35,720'); seek(1235.0)">
              you know, in case if you are using caching, we look, we can look at
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:39,000'); seek(1239.0)">
              the cache, it's as well. And when it comes to logs,
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:42,172'); seek(1242.0)">
              we look at the query logs, error logs and the audit logs, you know,
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:45,300'); seek(1245.0)">
              which will probably, probably give us a comprehensive way of, you know,
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:48,972'); seek(1248.0)">
              auditing, troubleshooting. And then we'll try to enable traces,
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:52,820'); seek(1252.0)">
              x ray, you know, which will provide us the end to end tracing so that
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:20:56,220'); seek(1256.0)">
              way that we can have a complete
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:20:59,908'); seek(1259.0)">
              observability into the data store or data retriever
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:05,084'); seek(1265.0)">
              and other pillars are the tracing. So we have, we will use x ray,
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:08,428'); seek(1268.0)">
              you know, so that will enable us to get integrate the traces and
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:12,128'); seek(1272.0)">
              we will look at, you know, other integration AWS services as well.
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:15,576'); seek(1275.0)">
              And then we will use Cloudwatch as a visualization tool. We can
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:19,104'); seek(1279.0)">
              also use the Grafana, the AWS managed Grafana
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:23,048'); seek(1283.0)">
              or any other things as well.
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:26,224'); seek(1286.0)">
              So one other key thing is be mindful about alerting and incident
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:29,968'); seek(1289.0)">
              management. So we can use the cloud virtual arms and we
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:33,528'); seek(1293.0)">
              can leverage AWS system manager as well.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:36,824'); seek(1296.0)">
              So one important thing is the security. So we will leverage AWS cloud
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:40,936'); seek(1300.0)">
              trail to audit and monitor the API calls and
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:44,560'); seek(1304.0)">
              we'll ensure that the compliance with security and regulatory requirements
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:48,128'); seek(1308.0)">
              are being tracked. I know we can integrate crowd logs with cloud
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:52,184'); seek(1312.0)">
              work logs for centralization and then we will use
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:21:55,808'); seek(1315.0)">
              AWS config so that we can continuously monitor and
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:21:59,592'); seek(1319.0)">
              assess the configuration of our systems, AWS resources and
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:03,520'); seek(1323.0)">
              we can ensure that we have compliance and best practices with
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:07,152'); seek(1327.0)">
              the compliance standard with that.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:10,782'); seek(1330.0)">
              One key aspect is cost as well. So the more we are using our
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:14,334'); seek(1334.0)">
              llms, you know, the more the cost factor comes in. So we
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:17,470'); seek(1337.0)">
              can leverage AWs cost explorer and AWS budgets.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:21,894'); seek(1341.0)">
              And finally, one other important thing is that, you know, AI upscale building.
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:25,982'); seek(1345.0)">
              So we will have to ensure that all the metrics, you know, whether it's
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:29,606'); seek(1349.0)">
              the LLM specific, application specific or the RaG is specific,
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:33,878'); seek(1353.0)">
              we will kind of like enable anomaly detection. And then for
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:37,754'); seek(1357.0)">
              all the logs which we are putting into cloud work, we are
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:41,162'); seek(1361.0)">
              able to enable the log anomaly detection as well. So we can also use
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:22:45,362'); seek(1365.0)">
              Aws, the DevOps guru. So it's a machine learning
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:49,498'); seek(1369.0)">
              service provided by AWS. So it,
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:22:52,970'); seek(1372.0)">
              the DevOps guru will help us to detect and resolve issues
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:56,898'); seek(1376.0)">
              in our system, especially identifying anomalies and other
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:00,386'); seek(1380.0)">
              issues which probably we might not be able to uncover manually.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:04,954'); seek(1384.0)">
              And then we will look at leveraging AWS code guru as
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:08,418'); seek(1388.0)">
              well because this allow us to integrate with the application so that
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:12,362'); seek(1392.0)">
              we can do profiling and we can do the understand the
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:16,826'); seek(1396.0)">
              resource utilizations usage based on our applications.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:21,010'); seek(1401.0)">
              Another very important thing is use AWS forecasting.
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:24,018'); seek(1404.0)">
              So all the metrics and other things which we are bringing into the
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:27,610'); seek(1407.0)">
              table, we can use the forecasting that will able to
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:31,314'); seek(1411.0)">
              understand things in advance so that we can make better decisions
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:34,882'); seek(1414.0)">
              and we can plan things ahead with that.
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:37,762'); seek(1417.0)">
              Probably you can ask the question why we need a maturity model. So I
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:23:41,346'); seek(1421.0)">
              am a big fan of maturity model because I think maturity models act as
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:45,250'); seek(1425.0)">
              a north star. So we all want to start someplace and then take
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:49,698'); seek(1429.0)">
              our systems into observability journey. So if you do
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:23:52,842'); seek(1432.0)">
              that without kind of a maturity model or framework, then it's are,
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:23:56,624'); seek(1436.0)">
              you know, you, you may ended up with any place, but by
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:00,256'); seek(1440.0)">
              using a maturity model you can guarantee that, you know, you start with the basic
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:24:03,952'); seek(1443.0)">
              steps and then you can finish with it some of the advanced things
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:07,880'); seek(1447.0)">
              and you have better control of how you go there.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:12,024'); seek(1452.0)">
              So the LLM, the indirect
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:15,824'); seek(1455.0)">
              observability maturity model, I have three pillars. One is,
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:19,456'); seek(1459.0)">
              I call it level one, which is about foundational observability.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:23,220'); seek(1463.0)">
              And level two is the proactive observability. At level three
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:26,660'); seek(1466.0)">
              we are looking at advanced LLM observability with AI Ops.
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:30,364'); seek(1470.0)">
              So in the level one we will start, you know, capturing some of the basic
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:33,612'); seek(1473.0)">
              LLM metrics. We will start getting the logs and start
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:24:37,156'); seek(1477.0)">
              monitor the basic from properties, and we will implement basic
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:24:41,268'); seek(1481.0)">
              logging and other distributed tracing. And then we will put up the visualization
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:24:45,292'); seek(1485.0)">
              and other basic alerts as well. So this
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:24:48,604'); seek(1488.0)">
              will kind of give you a foundational observability into your generative AI
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:24:52,524'); seek(1492.0)">
              application. The next step is, you know, taking system more
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:24:56,004'); seek(1496.0)">
              proactive, like be proactive. So here we will start,
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:24:59,332'); seek(1499.0)">
              you know, capture and analyze the advanced LLM metrics and you know,
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:25:02,876'); seek(1502.0)">
              start, you know, start leveraging the logs, then the
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:25:06,572'); seek(1506.0)">
              other advanced prompt properties. And then we
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:25:09,820'); seek(1509.0)">
              will enhance alerts and other incident management workflow so that
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:25:13,340'); seek(1513.0)">
              we can identify things much faster and you know,
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:25:16,292'); seek(1516.0)">
              resolve things much faster as well. So we will bring in the security
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:19,868'); seek(1519.0)">
              aspect, the security compliance. We will start generating,
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:23,340'); seek(1523.0)">
              leveraging, AWS forecasting so that we can start focusing
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:27,004'); seek(1527.0)">
              about some of the LlMe specific matrix, matrix and the
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:25:32,004'); seek(1532.0)">
              prompt properties as well. And for the logs
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:25:35,372'); seek(1535.0)">
              we can also set up log anomaly detection.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:25:38,684'); seek(1538.0)">
              And when it comes to level three, which is kind of like the advanced level,
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:25:41,876'); seek(1541.0)">
              which is the kind of place where you all want to be in, but you
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:25:45,758'); seek(1545.0)">
              have to be mindful that it's a journey. Like you will have to start with
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:25:48,582'); seek(1548.0)">
              level one, go to level two, and then we can be into level three.
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:25:52,278'); seek(1552.0)">
              So at level three we start with integrating with DevOps guru
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:25:56,214'); seek(1556.0)">
              and the code guru, so that with DevOps Guru will provide the AI
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:25:59,862'); seek(1559.0)">
              and ML capabilities code guru will provide our quality
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:26:03,422'); seek(1563.0)">
              of the code and then we will start implementing AIOps
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:26:07,302'); seek(1567.0)">
              capabilities like other things like the noise reduction,
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:26:11,644'); seek(1571.0)">
              smart intelligent root causes and then kind
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:26:15,332'); seek(1575.0)">
              of like business impact assessments. So the forecasting feature
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:18,668'); seek(1578.0)">
              will kind of like allow us to understand, if at all, if the
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:22,180'); seek(1582.0)">
              models can drift, when that can happen, if at all,
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:25,868'); seek(1585.0)">
              the models can start having a bias, when that can happen,
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:26:29,428'); seek(1589.0)">
              the response time predictions and all those things. So the
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:26:32,836'); seek(1592.0)">
              AI kind of thing is, can give you a full control of, you know,
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:26:35,804'); seek(1595.0)">
              predictability of your generative AI application.
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:26:39,234'); seek(1599.0)">
              So now I am kind of like look at more focus on
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:26:42,874'); seek(1602.0)">
              implementation angle. So in the foundation model, like, you know, we can
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:26:46,642'); seek(1606.0)">
              use cloud work metrics, like so that we can capture the
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:26:50,074'); seek(1610.0)">
              basic LLM metrics, like, you know, the inference time, model size,
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:26:53,834'); seek(1613.0)">
              prompt length and those things, the prompt properties. Again, we can,
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:26:57,570'); seek(1617.0)">
              you know, leverage the sender, those logs into cloud work logs
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:27:01,082'); seek(1621.0)">
              so that, you know, we can start monitoring basic properties like from
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:27:04,418'); seek(1624.0)">
              content prompt sources and those things, any other logs,
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:27:07,880'); seek(1627.0)">
              you know, we will be shipping into cloud work so that we can start,
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:27:10,656'); seek(1630.0)">
              you know, getting the basic, the detail.
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:27:14,344'); seek(1634.0)">
              And then we will integrate AWS x ray based on the technology
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:17,976'); seek(1637.0)">
              we are using to develop our LLM to generate
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:27:21,624'); seek(1641.0)">
              a app so that we can have ability to start looking
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:27:25,344'); seek(1645.0)">
              at the traces and then visualization the dashboards.
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:27:28,656'); seek(1648.0)">
              We can use AWS Cloudword dashboards and if required, you know,
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:27:32,142'); seek(1652.0)">
              we can go into AWS managed Grafana dashboards
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:27:35,526'); seek(1655.0)">
              as well. Alert and incident management. We are leveraging Cloudwatch
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:27:39,582'); seek(1659.0)">
              and that will help us to understand some of the
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:27:44,534'); seek(1664.0)">
              more the basic to a medium complex
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:27:49,198'); seek(1669.0)">
              some of these monitors so that we can have a good control of
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:27:52,390'); seek(1672.0)">
              our, how the llms are behaving and like how,
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:27:55,622'); seek(1675.0)">
              how is our, the prompt is successful
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:27:59,382'); seek(1679.0)">
              and overall how is our generative application is behaving and probably
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:28:03,902'); seek(1683.0)">
              not probably, but how our end users are feeling about it.
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:28:07,382'); seek(1687.0)">
              And then we will wrap this up with the cost like we using AWS Explorer.
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:28:12,046'); seek(1692.0)">
              Because llms are sometimes costly, we'll have to ensure the usage and
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:28:15,470'); seek(1695.0)">
              we start monitoring that as well. So level
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:18,878'); seek(1698.0)">
              two, like, you know, we will go a little bit advanced for the metrics.
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:28:21,942'); seek(1701.0)">
              We will start, you know, looking at advanced metrics like model performance and
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:28:26,070'); seek(1706.0)">
              output quality. And again, prompt the properties.
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:28:29,466'); seek(1709.0)">
              We will look at the advanced properties like the prompt performance,
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:28:33,058'); seek(1713.0)">
              prompt versioning. And then, you know, we will start advancing,
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:28:36,922'); seek(1716.0)">
              improving the incident workflows. We will look at the
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:28:40,618'); seek(1720.0)">
              security compliance, we will look at more into the uplifting
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:28:44,234'); seek(1724.0)">
              and like improving the cost factor as well. And one
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:28:47,978'); seek(1727.0)">
              of the key thing, you know, here we will bring in is AWS forecasting.
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:28:52,090'); seek(1732.0)">
              So using forecasting we want to ensure that we have the ability of
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:28:55,522'); seek(1735.0)">
              forecasting all the key, the complex or even every
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:29:00,066'); seek(1740.0)">
              metric related to LLM performance, LLM the
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:29:04,306'); seek(1744.0)">
              inference, the accuracy and the prompt properties related
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:29:07,722'); seek(1747.0)">
              things as well. So and then we will also look at
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:29:11,762'); seek(1751.0)">
              enabling metric anomalies, log anomalies so that, you know, we start
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:29:15,330'); seek(1755.0)">
              using some of the capabilities of anomalies this and
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:19,762'); seek(1759.0)">
              finally we will bringing in AWS DevOps guru,
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:29:23,850'); seek(1763.0)">
              so and the code guru and that will allow us to bringing in
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:29:27,874'); seek(1767.0)">
              the AI capabilities into here the AI ML capability so
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:29:31,898'); seek(1771.0)">
              that we can look at things from holistic ways. DevOps guru is a
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:29:35,730'); seek(1775.0)">
              perfect tool. And then we will, you know, bringing in AI of practices
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:29:39,826'); seek(1779.0)">
              and then kind of like, you know, bring ensuring that our incident workflow
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:29:43,586'); seek(1783.0)">
              are more into self healing and there are a
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:29:47,230'); seek(1787.0)">
              lot of other improvements and AI kind of
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:29:51,230'); seek(1791.0)">
              things which we can bring. So what
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:29:55,670'); seek(1795.0)">
              are the other things like, you know, so we will look at bringing in while
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:30:00,134'); seek(1800.0)">
              we do this, we want to ensure that we measure the progress.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:30:03,702'); seek(1803.0)">
              So once we enable observability. So we want to ensure
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:30:07,406'); seek(1807.0)">
              that we look at LLM. So, so how the LLM output quality
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:30:11,012'); seek(1811.0)">
              is getting improved, how we are improving, optimizing our
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:30:14,284'); seek(1814.0)">
              LLM prompt engineering area and then like
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:30:17,972'); seek(1817.0)">
              ensuring that, you know, we can able to detect our model drifts in advance
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:21,700'); seek(1821.0)">
              and then we can take necessary actions. We look at, you know, what are the
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:30:24,988'); seek(1824.0)">
              ethical things, you know, our models based
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:30:28,604'); seek(1828.0)">
              on that, how our models are behaving and then, you know, we look at
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:30:31,860'); seek(1831.0)">
              the interpredictability, extendability and start
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:30:35,756'); seek(1835.0)">
              keep a close eye on those things and generally like,
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:30:39,272'); seek(1839.0)">
              you know, we will start kind of looking at end user experience
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:30:42,528'); seek(1842.0)">
              as well. We will clearly define some end user specific
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:30:46,352'); seek(1846.0)">
              service level objectives. We will start, you know, track the
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:30:50,312'); seek(1850.0)">
              metrics and the improvements and we will start looking at
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:30:53,960'); seek(1853.0)">
              the customer experience, ensure that, you know, whatever we
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:30:57,392'); seek(1857.0)">
              do is align and correlate with customer
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:31:00,592'); seek(1860.0)">
              experience. We see increasing customer experience as well.
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:31:04,352'); seek(1864.0)">
              So and like overall that,
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:31:07,808'); seek(1867.0)">
              you know, we develop and provide a better world class services into for our
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:31:11,712'); seek(1871.0)">
              end users. And then some of the best practices is
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:31:15,304'); seek(1875.0)">
              like, you know, so we will have to use a structured log and you know,
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:31:18,776'); seek(1878.0)">
              if case you are heavily using lambda, probably go to power tools,
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:31:22,208'); seek(1882.0)">
              you'll have to instrument the code, ensure that, you know, you get all the,
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:31:25,592'); seek(1885.0)">
              the critical, the LLM specific metrics.
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:31:29,416'); seek(1889.0)">
              Then you obviously use x ray to enable the traces as well.
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:31:33,102'); seek(1893.0)">
              So the metrics which we are extracting, it has to be meaningful
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:31:36,974'); seek(1896.0)">
              it has to add value. So it should be aligned with our business objectives
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:31:40,710'); seek(1900.0)">
              as well. And to wrap up like, you know, some of the
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:31:44,646'); seek(1904.0)">
              pitfall is that, you know, ensure that, you know, we kind of have a security
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:31:49,174'); seek(1909.0)">
              we plan in advance and the compliance as well
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:31:52,974'); seek(1912.0)">
              because that's again a key thing, you know, modern day when we are
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:31:56,518'); seek(1916.0)">
              using generative applications and clearly define
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:31:59,886'); seek(1919.0)">
              the roles like whatever objectives, you know, we are going
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:32:03,750'); seek(1923.0)">
              to achieve with this. And probably you can have some numbers,
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:32:06,854'); seek(1926.0)">
              you can have some measurable things so that you can start, you know,
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:32:10,294'); seek(1930.0)">
              performing and kind of like try to get the benefit.
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:32:13,974'); seek(1933.0)">
              So with this like, you know I'm, we are at the close so
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:32:17,254'); seek(1937.0)">
              thank you very much. So here I have taken AWS as
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:32:21,014'); seek(1941.0)">
              example, especially AWS bedrock. From here we have look
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:32:24,802'); seek(1944.0)">
              at what is a general architecture of workflow of
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:32:28,826'); seek(1948.0)">
              generator application and what are the key pillars of
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:32:33,154'); seek(1953.0)">
              the observability LLM related observability pillars
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:32:36,826'); seek(1956.0)">
              which we have to enable and then we will look at the three,
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:32:40,714'); seek(1960.0)">
              the levels, the foundation, the proactive observability
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:32:44,682'); seek(1964.0)">
              and advanced observability is aiops. And then we
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:32:48,170'); seek(1968.0)">
              have look at some of the best practices and the pitfalls and
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:32:52,232'); seek(1972.0)">
              more importantly how we can look at this from ROI perspective.
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:32:56,456'); seek(1976.0)">
              So with this, thank you very much for taking time. I hope, you know,
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:32:59,928'); seek(1979.0)">
              you kind of like enjoy this and then you have understood
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:33:04,288'); seek(1984.0)">
              or you have taken few things which you can take into your
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:33:08,264'); seek(1988.0)">
              generative application and make it observable and leverage into to
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:33:11,952'); seek(1991.0)">
              deliver great customer experiences.
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:33:14,944'); seek(1994.0)">
              So with this, thank you very much.
            </span>
            
            </div>
          </div>
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Indika%20Wimalasuriya%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Indika%20Wimalasuriya%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #CCB87B;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/llms2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #CCB87B;">
                <i class="fe fe-grid me-2"></i>
                See all 28 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Indika%20Wimalasuriya_llm.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Indika Wimalasuriya
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Senior Systems Engineering Manager @ Virtusa
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/indika-wimalasuriya/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Indika Wimalasuriya's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Indika Wimalasuriya"
                  data-url="https://www.conf42.com/llms2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/llms2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Large Language Models"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday San Francisco 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday London 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday Amsterdam 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

  </body>
</html>