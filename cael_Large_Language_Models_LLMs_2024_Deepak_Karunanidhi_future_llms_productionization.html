<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Future of LLM's and Machine learning Productionization</title>
    <meta name="description" content="One model, extra large, please!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Deepak%20Karunanidhi_llm.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Future of LLM's and Machine learning Productionization | Conf42"/>
    <meta property="og:description" content="Unlock the future of LLMs and Machine Learning Productionization in my talk! Dive into the cutting-edge trends and transformative potential that will redefine how we use LLMs and implement machine learning at scale. Don't miss the chance to navigate the next frontier of AI innovation."/>
    <meta property="og:url" content="https://conf42.com/Large_Language_Models_LLMs_2024_Deepak_Karunanidhi_future_llms_productionization"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/OBS2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Observability 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-06-05
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/obs2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #CCB87B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Large Language Models (LLMs) 2024 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- One model, extra large, please!
 -->
              <script>
                const event_date = new Date("2024-04-11T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-04-11T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "mCh6qtqNBes"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "TQwxk0c4sh0"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBjR6ZR0g0LRq9Fp8c_4HrI" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Thank you all for joining the session. Today, I am going to talk about", "timestamp": "00:00:20,880", "timestamp_s": 20.0}, {"text": "the large language models and the future of large language", "timestamp": "00:00:24,790", "timestamp_s": 24.0}, {"text": "models and productionization of the large language models.", "timestamp": "00:00:28,864", "timestamp_s": 28.0}, {"text": "Myself, Deepak. I am working as an associate director for data science and", "timestamp": "00:00:32,744", "timestamp_s": 32.0}, {"text": "machine learning projects. I also have more", "timestamp": "00:00:36,600", "timestamp_s": 36.0}, {"text": "than 15 years of experience in data science and", "timestamp": "00:00:39,680", "timestamp_s": 39.0}, {"text": "machine learning, dominantly working in generative AI", "timestamp": "00:00:42,760", "timestamp_s": 42.0}, {"text": "for the past three years. All right,", "timestamp": "00:00:46,296", "timestamp_s": 46.0}, {"text": "now take you to the next slide.", "timestamp": "00:00:49,288", "timestamp_s": 49.0}, {"text": "So, before I\u0027m going to talk about the large language model productionization or", "timestamp": "00:00:52,864", "timestamp_s": 52.0}, {"text": "deploying in cloud, let\u0027s understand the traditional", "timestamp": "00:00:56,840", "timestamp_s": 56.0}, {"text": "AI model development and deployment, followed by the challenges", "timestamp": "00:01:00,432", "timestamp_s": 60.0}, {"text": "we have in deploying or", "timestamp": "00:01:04,272", "timestamp_s": 64.0}, {"text": "productionizing the traditional AI models. Then I\u0027ll", "timestamp": "00:01:07,464", "timestamp_s": 67.0}, {"text": "walk you through on the large language models like GPT four,", "timestamp": "00:01:10,768", "timestamp_s": 70.0}, {"text": "and I can explain you the architecture of the large language models", "timestamp": "00:01:14,336", "timestamp_s": 74.0}, {"text": "or the generated AI model. Then I can take you through the", "timestamp": "00:01:17,776", "timestamp_s": 77.0}, {"text": "concept of lancing framework and how the applications can", "timestamp": "00:01:21,182", "timestamp_s": 81.0}, {"text": "be developed with lancing, followed by a demo.", "timestamp": "00:01:24,782", "timestamp_s": 84.0}, {"text": "Moving to the next slide, let\u0027s talk about the traditional", "timestamp": "00:01:28,734", "timestamp_s": 88.0}, {"text": "AI models. When I say traditional AI models,", "timestamp": "00:01:32,342", "timestamp_s": 92.0}, {"text": "we began with linear regression, logistic regression, random forest", "timestamp": "00:01:35,734", "timestamp_s": 95.0}, {"text": "decision trees, boosting adaboost, exaboost,", "timestamp": "00:01:39,662", "timestamp_s": 99.0}, {"text": "neural network, and the evolution started from neural", "timestamp": "00:01:43,398", "timestamp_s": 103.0}, {"text": "network to transformers in 2019 or 2017.", "timestamp": "00:01:46,818", "timestamp_s": 106.0}, {"text": "I\u0027m sorry. So that is how the industry has a breakthrough,", "timestamp": "00:01:49,842", "timestamp_s": 109.0}, {"text": "by coming up with a model called Bert, which is a bidirectional encoder", "timestamp": "00:01:53,562", "timestamp_s": 113.0}, {"text": "representation for transformers. I think that has", "timestamp": "00:01:57,474", "timestamp_s": 117.0}, {"text": "significantly performed well", "timestamp": "00:02:02,130", "timestamp_s": 122.0}, {"text": "in most of the natural language processing tasks. When I", "timestamp": "00:02:05,418", "timestamp_s": 125.0}, {"text": "talk about traditional AI models, let me not talk about starting", "timestamp": "00:02:09,090", "timestamp_s": 129.0}, {"text": "from linear or logistic regressions or random forest.", "timestamp": "00:02:12,712", "timestamp_s": 132.0}, {"text": "Let\u0027s begin with a small large language model", "timestamp": "00:02:16,152", "timestamp_s": 136.0}, {"text": "which has been known, which I, which I call that as invert", "timestamp": "00:02:19,632", "timestamp_s": 139.0}, {"text": "the process involved in model training", "timestamp": "00:02:23,264", "timestamp_s": 143.0}, {"text": "or model fine tuning has a requires huge amount", "timestamp": "00:02:26,984", "timestamp_s": 146.0}, {"text": "of data set to train the model. So once", "timestamp": "00:02:30,432", "timestamp_s": 150.0}, {"text": "we train the model or fine tune the model, we have to fine tune the", "timestamp": "00:02:33,696", "timestamp_s": 153.0}, {"text": "model for a specific task or for a specific domain.", "timestamp": "00:02:36,848", "timestamp_s": 156.0}, {"text": "Typically, it needs a GPU machine to do the model fine tuning", "timestamp": "00:02:40,404", "timestamp_s": 160.0}, {"text": "process. Once we perform the model fine", "timestamp": "00:02:43,820", "timestamp_s": 163.0}, {"text": "tuning, we have to do the hyper parameter tuning like learning rate", "timestamp": "00:02:47,060", "timestamp_s": 167.0}, {"text": "epoch with multiple additional parameters to come", "timestamp": "00:02:50,964", "timestamp_s": 170.0}, {"text": "up with the right ways for the model to classify or question answering", "timestamp": "00:02:54,900", "timestamp_s": 174.0}, {"text": "or any of the tasks which it can perform.", "timestamp": "00:02:59,268", "timestamp_s": 179.0}, {"text": "So then once we do the fine tuning and optimization of", "timestamp": "00:03:02,484", "timestamp_s": 182.0}, {"text": "the model, we have to deploy the model in a cloud", "timestamp": "00:03:06,020", "timestamp_s": 186.0}, {"text": "environment. It could be AWS or Azure,", "timestamp": "00:03:09,426", "timestamp_s": 189.0}, {"text": "even Google Cloud platform.", "timestamp": "00:03:13,754", "timestamp_s": 193.0}, {"text": "But before that, when you are going to deploy the", "timestamp": "00:03:16,514", "timestamp_s": 196.0}, {"text": "model, the model has to be serialized, meaning the model", "timestamp": "00:03:19,730", "timestamp_s": 199.0}, {"text": "as to when we deploy the model in production, it should have the scalability", "timestamp": "00:03:23,562", "timestamp_s": 203.0}, {"text": "and reliability and durability. Considering that", "timestamp": "00:03:27,706", "timestamp_s": 207.0}, {"text": "when we move the model to production, we have to serialize the model by having", "timestamp": "00:03:32,374", "timestamp_s": 212.0}, {"text": "a pytorch or a tensorflow saved model", "timestamp": "00:03:36,022", "timestamp_s": 216.0}, {"text": "format to serve the model. Introduction so,", "timestamp": "00:03:39,334", "timestamp_s": 219.0}, {"text": "model serving, as I talked about as a framework called Pytorch serving,", "timestamp": "00:03:42,862", "timestamp_s": 222.0}, {"text": "so which is a framework can have a scalability on", "timestamp": "00:03:46,742", "timestamp_s": 226.0}, {"text": "performing the inference. This framework provides an API", "timestamp": "00:03:50,254", "timestamp_s": 230.0}, {"text": "where the application, once we build a real world application,", "timestamp": "00:03:54,494", "timestamp_s": 234.0}, {"text": "the application can invoke the inference or the prediction", "timestamp": "00:03:58,126", "timestamp_s": 238.0}, {"text": "by invoking the Pytorch serving part.", "timestamp": "00:04:01,690", "timestamp_s": 241.0}, {"text": "Also, we provide that as an API that helps us to", "timestamp": "00:04:04,594", "timestamp_s": 244.0}, {"text": "come up with an API design and based on the API design we can start", "timestamp": "00:04:08,018", "timestamp_s": 248.0}, {"text": "invoking the model. The model can be a single model or multiple", "timestamp": "00:04:11,890", "timestamp_s": 251.0}, {"text": "models can be deployed in production. I will talk about the Pytorch serving", "timestamp": "00:04:15,554", "timestamp_s": 255.0}, {"text": "architecture in a minute. Before that I will tell how the scalability", "timestamp": "00:04:19,474", "timestamp_s": 259.0}, {"text": "and load balancing will be performed in the cloud environment.", "timestamp": "00:04:23,546", "timestamp_s": 263.0}, {"text": "When we deploy these models in production, it can be deployed in elastic", "timestamp": "00:04:27,054", "timestamp_s": 267.0}, {"text": "container service, in AWS or in Azure", "timestamp": "00:04:30,782", "timestamp_s": 270.0}, {"text": "container app. When we deploy the models, we have a load balancer", "timestamp": "00:04:34,254", "timestamp_s": 274.0}, {"text": "has to be created and we have to create the cloud formation template to create", "timestamp": "00:04:38,566", "timestamp_s": 278.0}, {"text": "a container and we have to deploy this model as a docker image and", "timestamp": "00:04:42,102", "timestamp_s": 282.0}, {"text": "internally it has a Pytorch serving framework.", "timestamp": "00:04:45,894", "timestamp_s": 285.0}, {"text": "Once we deploy the model, we have to have a auditability", "timestamp": "00:04:48,734", "timestamp_s": 288.0}, {"text": "which is nothing but monitoring and logging. So there most", "timestamp": "00:04:52,094", "timestamp_s": 292.0}, {"text": "of the model would be logged along with the number of invocation has been", "timestamp": "00:04:55,492", "timestamp_s": 295.0}, {"text": "made to the model along with the throughput and error rates.", "timestamp": "00:04:58,964", "timestamp_s": 298.0}, {"text": "So the model should be highly secured where it cannot be having unauthorized", "timestamp": "00:05:03,444", "timestamp_s": 303.0}, {"text": "access and attacks. So also once we build", "timestamp": "00:05:07,220", "timestamp_s": 307.0}, {"text": "a model, it should have an security along with the", "timestamp": "00:05:10,764", "timestamp_s": 310.0}, {"text": "CI CD pipeline for the reinforcement learning whenever the", "timestamp": "00:05:14,428", "timestamp_s": 314.0}, {"text": "model trains and fine tune and deployed in production,", "timestamp": "00:05:18,060", "timestamp_s": 318.0}, {"text": "if the model has any variation from the data", "timestamp": "00:05:21,924", "timestamp_s": 321.0}, {"text": "which it has been trained, then the model has to be", "timestamp": "00:05:25,138", "timestamp_s": 325.0}, {"text": "when in the production the model has a deviation", "timestamp": "00:05:28,506", "timestamp_s": 328.0}, {"text": "in the data, then it cannot identify the data accurately.", "timestamp": "00:05:31,882", "timestamp_s": 331.0}, {"text": "So we have a CACD pipeline to have a reinforcement.", "timestamp": "00:05:35,410", "timestamp_s": 335.0}, {"text": "Human learning to ensure if there isn\u0027t any deviation,", "timestamp": "00:05:39,218", "timestamp_s": 339.0}, {"text": "model has to automatically, automatically, after a certain time it", "timestamp": "00:05:42,442", "timestamp_s": 342.0}, {"text": "has to train and fine tune and again it has to be deployed.", "timestamp": "00:05:46,530", "timestamp_s": 346.0}, {"text": "That variant is called a b testing or multiple variants", "timestamp": "00:05:50,260", "timestamp_s": 350.0}, {"text": "of models will be deployed in production that comes under versioning", "timestamp": "00:05:53,564", "timestamp_s": 353.0}, {"text": "and rollback. So we are all talking about the traditional AI.", "timestamp": "00:05:57,228", "timestamp_s": 357.0}, {"text": "So this comes under the concept of mlops.", "timestamp": "00:06:01,812", "timestamp_s": 361.0}, {"text": "So we design the model, we develop the model", "timestamp": "00:06:04,620", "timestamp_s": 364.0}, {"text": "and operationalize the model so in case of design,", "timestamp": "00:06:08,020", "timestamp_s": 368.0}, {"text": "we identify the data set, we identify the", "timestamp": "00:06:11,548", "timestamp_s": 371.0}, {"text": "model. Then once we have done the identification,", "timestamp": "00:06:15,002", "timestamp_s": 375.0}, {"text": "we understand what the model task is. It could be in classification or", "timestamp": "00:06:18,514", "timestamp_s": 378.0}, {"text": "summarization, abstraction or like", "timestamp": "00:06:22,442", "timestamp_s": 382.0}, {"text": "a question and answering or next sentence for prediction. There could be multiple", "timestamp": "00:06:25,818", "timestamp_s": 385.0}, {"text": "kind of tasks the model can perform. So as part of", "timestamp": "00:06:30,314", "timestamp_s": 390.0}, {"text": "the recurrent gathering or use case prioritization that has to be identified", "timestamp": "00:06:33,546", "timestamp_s": 393.0}, {"text": "along with the data availability to train or fine tune the model.", "timestamp": "00:06:37,586", "timestamp_s": 397.0}, {"text": "I think fine tune is the right word, followed by model engineering", "timestamp": "00:06:40,946", "timestamp_s": 400.0}, {"text": "which has a technique to identify the model, then perform", "timestamp": "00:06:44,896", "timestamp_s": 404.0}, {"text": "Eiffel parameter tuning and fine tune the model and deploy the", "timestamp": "00:06:48,504", "timestamp_s": 408.0}, {"text": "model. In operations. That deployment process would", "timestamp": "00:06:52,200", "timestamp_s": 412.0}, {"text": "be in a cloud environment by having a CACD pipeline like Azure DevOps,", "timestamp": "00:06:55,728", "timestamp_s": 415.0}, {"text": "or then we can monitor via", "timestamp": "00:07:00,448", "timestamp_s": 420.0}, {"text": "Amazon Cloudwatch or Azure monitoring logs.", "timestamp": "00:07:04,464", "timestamp_s": 424.0}, {"text": "So this traditional AI model development involves", "timestamp": "00:07:09,084", "timestamp_s": 429.0}, {"text": "a huge number, there\u0027s a certain amount of process has to be followed,", "timestamp": "00:07:12,644", "timestamp_s": 432.0}, {"text": "right? So before getting into the large language models,", "timestamp": "00:07:17,124", "timestamp_s": 437.0}, {"text": "I would like to touch base on the Pytorch serving. So Pytorch serving is", "timestamp": "00:07:20,748", "timestamp_s": 440.0}, {"text": "nothing but a framework where large, sorry, where Bert models", "timestamp": "00:07:24,932", "timestamp_s": 444.0}, {"text": "or large language models like Bert", "timestamp": "00:07:28,356", "timestamp_s": 448.0}, {"text": "can be deployed. So it is a framework which comes up with", "timestamp": "00:07:31,692", "timestamp_s": 451.0}, {"text": "an inference and management API where multiple models can", "timestamp": "00:07:34,908", "timestamp_s": 454.0}, {"text": "be deployed inside the container. So again,", "timestamp": "00:07:38,300", "timestamp_s": 458.0}, {"text": "this container, when you mean this pyth serving, has to be", "timestamp": "00:07:42,248", "timestamp_s": 462.0}, {"text": "built as a docker and it has to be deployed inside a container. It could", "timestamp": "00:07:45,448", "timestamp_s": 465.0}, {"text": "be an Amazon elastic container instance or Azure", "timestamp": "00:07:48,728", "timestamp_s": 468.0}, {"text": "where we can deploy multiple machine learning models by", "timestamp": "00:07:53,464", "timestamp_s": 473.0}, {"text": "using model store. Under the model store we can start", "timestamp": "00:07:56,888", "timestamp_s": 476.0}, {"text": "using EBS or elastic storage mechanism,", "timestamp": "00:08:01,384", "timestamp_s": 481.0}, {"text": "we can start to save the models", "timestamp": "00:08:05,980", "timestamp_s": 485.0}, {"text": "by use by running an API and we can serve the model by", "timestamp": "00:08:09,292", "timestamp_s": 489.0}, {"text": "an HTTP endpoint. I think this is the holistic process.", "timestamp": "00:08:12,700", "timestamp_s": 492.0}, {"text": "Now you understand the amount of efforts or time we spent in", "timestamp": "00:08:16,156", "timestamp_s": 496.0}, {"text": "the whole machine learning or traditional machine learning model development productionization.", "timestamp": "00:08:20,068", "timestamp_s": 500.0}, {"text": "So now the offer which we are going to make", "timestamp": "00:08:25,084", "timestamp_s": 505.0}, {"text": "is lang chain. But before that", "timestamp": "00:08:28,828", "timestamp_s": 508.0}, {"text": "I will give you a few touch base on large language models. See large language", "timestamp": "00:08:32,368", "timestamp_s": 512.0}, {"text": "models like GPT-3 or GPT four which has been", "timestamp": "00:08:36,408", "timestamp_s": 516.0}, {"text": "trained more than 175 billion parameters. We have other models like Lama", "timestamp": "00:08:39,664", "timestamp_s": 519.0}, {"text": "two or Mistral or cloud which comes up with 7", "timestamp": "00:08:43,888", "timestamp_s": 523.0}, {"text": "billion or 70 billion parameters. Of the amount of", "timestamp": "00:08:47,368", "timestamp_s": 527.0}, {"text": "data has been trained. When it comes to charge DpT,", "timestamp": "00:08:50,672", "timestamp_s": 530.0}, {"text": "chart, GPT, we all know it\u0027s from OpenAI. It\u0027s more like", "timestamp": "00:08:54,520", "timestamp_s": 534.0}, {"text": "a very large language model. It is a", "timestamp": "00:08:57,914", "timestamp_s": 537.0}, {"text": "foundational model. It has a capability to answer any questions", "timestamp": "00:09:01,178", "timestamp_s": 541.0}, {"text": "or any task it can perform without any fine tuning.", "timestamp": "00:09:04,786", "timestamp_s": 544.0}, {"text": "So the whole process without fine tuning can be", "timestamp": "00:09:08,602", "timestamp_s": 548.0}, {"text": "achieved by providing in context learning to the model. So where the", "timestamp": "00:09:11,842", "timestamp_s": 551.0}, {"text": "in context learning would be providing", "timestamp": "00:09:15,786", "timestamp_s": 555.0}, {"text": "the model by giving some context.", "timestamp": "00:09:19,810", "timestamp_s": 559.0}, {"text": "In context learning means as part of the problem techniques,", "timestamp": "00:09:23,254", "timestamp_s": 563.0}, {"text": "instruction can be specified to the GPT four model", "timestamp": "00:09:27,510", "timestamp_s": 567.0}, {"text": "to perform a specific task. So when I say", "timestamp": "00:09:31,390", "timestamp_s": 571.0}, {"text": "performing a specific task, we can use multiple prompt engineering techniques.", "timestamp": "00:09:36,094", "timestamp_s": 576.0}, {"text": "So before the tradition was writing a programming language", "timestamp": "00:09:40,310", "timestamp_s": 580.0}, {"text": "in Java or in Python to perform a task for a programming language,", "timestamp": "00:09:44,198", "timestamp_s": 584.0}, {"text": "but now natural language process is", "timestamp": "00:09:48,398", "timestamp_s": 588.0}, {"text": "a programming language, nothing but, it\u0027s an English. So where we", "timestamp": "00:09:52,378", "timestamp_s": 592.0}, {"text": "can specify an instruction to the model which is nothing but a prompt along", "timestamp": "00:09:56,138", "timestamp_s": 596.0}, {"text": "with the input, and we say if it performs a summarization", "timestamp": "00:10:00,154", "timestamp_s": 600.0}, {"text": "or translation task, we specify the task information", "timestamp": "00:10:04,338", "timestamp_s": 604.0}, {"text": "by providing in context learning via prompt to along with", "timestamp": "00:10:08,146", "timestamp_s": 608.0}, {"text": "an input, we get the relevant answers from the GPT four.", "timestamp": "00:10:11,898", "timestamp_s": 611.0}, {"text": "So that\u0027s the evolution of large language models. So large language models", "timestamp": "00:10:15,626", "timestamp_s": 615.0}, {"text": "are not necessarily need to be fine tuned, which saves the significant", "timestamp": "00:10:19,460", "timestamp_s": 619.0}, {"text": "amount of resources like infrastructure and time.", "timestamp": "00:10:23,356", "timestamp_s": 623.0}, {"text": "And you know, to have a safer and cleaner environment,", "timestamp": "00:10:26,548", "timestamp_s": 626.0}, {"text": "not to fine tune or train the algorithm every time.", "timestamp": "00:10:29,644", "timestamp_s": 629.0}, {"text": "Now we know about large language models.", "timestamp": "00:10:33,484", "timestamp_s": 633.0}, {"text": "Now we know how we can utilize the large language models", "timestamp": "00:10:36,644", "timestamp_s": 636.0}, {"text": "to perform a specific task. But it all", "timestamp": "00:10:40,132", "timestamp_s": 640.0}, {"text": "looks good when you are doing some kind of a prototype.", "timestamp": "00:10:43,234", "timestamp_s": 643.0}, {"text": "So where you can specify a prompt and", "timestamp": "00:10:46,610", "timestamp_s": 646.0}, {"text": "you can give an input and you can get an output on the prompt.", "timestamp": "00:10:49,810", "timestamp_s": 649.0}, {"text": "So how do you productionize the large language models? That is an interesting area", "timestamp": "00:10:52,826", "timestamp_s": 652.0}, {"text": "to focus on. Okay, that\u0027s how we offer lang", "timestamp": "00:10:56,826", "timestamp_s": 656.0}, {"text": "chain. But again, before getting into lang chain, let\u0027s look into the architecture", "timestamp": "00:11:00,058", "timestamp_s": 660.0}, {"text": "of large language models. So before let\u0027s", "timestamp": "00:11:04,410", "timestamp_s": 664.0}, {"text": "have a small comparison between traditional model and generate the algorithm,", "timestamp": "00:11:07,570", "timestamp_s": 667.0}, {"text": "it\u0027s nothing but large language model. In traditional model", "timestamp": "00:11:11,850", "timestamp_s": 671.0}, {"text": "we have a data pre processing, then we identify the features required", "timestamp": "00:11:15,084", "timestamp_s": 675.0}, {"text": "for training or fine tuning", "timestamp": "00:11:19,060", "timestamp_s": 679.0}, {"text": "the model. After identifying the features, then we perform a fine tuning", "timestamp": "00:11:22,740", "timestamp_s": 682.0}, {"text": "job by having the data. Then once the data has been trained,", "timestamp": "00:11:26,644", "timestamp_s": 686.0}, {"text": "then we deploy in production in cloud environment. So typically the model", "timestamp": "00:11:30,124", "timestamp_s": 690.0}, {"text": "uses a framework like Tensorflow, Pytorch,", "timestamp": "00:11:34,276", "timestamp_s": 694.0}, {"text": "keras, then underlying it could be an IBM", "timestamp": "00:11:37,396", "timestamp_s": 697.0}, {"text": "Watson API, or it could be an Pytorch serving which", "timestamp": "00:11:40,804", "timestamp_s": 700.0}, {"text": "I was mentioning. Similarly, we would have used multiple databases like no", "timestamp": "00:11:44,820", "timestamp_s": 704.0}, {"text": "SQL or SQL database and mlops, avant Docker and Jenkins.", "timestamp": "00:11:48,700", "timestamp_s": 708.0}, {"text": "Right now the shift, as in paradigm shift, the reason", "timestamp": "00:11:53,044", "timestamp_s": 713.0}, {"text": "we are in the era of more interesting things happening", "timestamp": "00:11:56,964", "timestamp_s": 716.0}, {"text": "every day or every week to identify which", "timestamp": "00:12:00,692", "timestamp_s": 720.0}, {"text": "is realistic and which can be productionized is a key challenge. So that", "timestamp": "00:12:04,522", "timestamp_s": 724.0}, {"text": "would be addressed as part of this demo or as part of this", "timestamp": "00:12:08,370", "timestamp_s": 728.0}, {"text": "conversation which we are having now. Even after the conversation you can", "timestamp": "00:12:11,642", "timestamp_s": 731.0}, {"text": "reach out to me and have a discussion. Now the", "timestamp": "00:12:15,122", "timestamp_s": 735.0}, {"text": "whole process has been converted into prompt tuning or prompt", "timestamp": "00:12:18,770", "timestamp_s": 738.0}, {"text": "engineering on neat basis. We can go for fine tuning,", "timestamp": "00:12:22,186", "timestamp_s": 742.0}, {"text": "but it\u0027s not necessary. But even prompt engineering significantly", "timestamp": "00:12:25,538", "timestamp_s": 745.0}, {"text": "performs well on the tasks. Data pre processing it\u0027s", "timestamp": "00:12:29,274", "timestamp_s": 749.0}, {"text": "all about the input. Data has to be cleansed and given as an input", "timestamp": "00:12:32,468", "timestamp_s": 752.0}, {"text": "along with the prompt. Then it has an underlying foundational model like", "timestamp": "00:12:35,988", "timestamp_s": 755.0}, {"text": "GPT four or Claude or Mistral. Any of the model can be used.", "timestamp": "00:12:39,660", "timestamp_s": 759.0}, {"text": "Then we deploy the model by using orchestration platform", "timestamp": "00:12:43,420", "timestamp_s": 763.0}, {"text": "like LangChain or Lama index. So today the offer is about", "timestamp": "00:12:47,300", "timestamp_s": 767.0}, {"text": "LangChain. It\u0027s not only about developing machine", "timestamp": "00:12:51,036", "timestamp_s": 771.0}, {"text": "learning models. Deploying a machine learning model and invoking", "timestamp": "00:12:54,612", "timestamp_s": 774.0}, {"text": "a machine learning model is become much more easier than what we have", "timestamp": "00:12:57,980", "timestamp_s": 777.0}, {"text": "done earlier. If you have any questions,", "timestamp": "00:13:01,452", "timestamp_s": 781.0}, {"text": "I\u0027ll move to the next slide. LangChain so", "timestamp": "00:13:04,594", "timestamp_s": 784.0}, {"text": "LangChain is a framework to develop the large language", "timestamp": "00:13:08,402", "timestamp_s": 788.0}, {"text": "models. It facilitates the creation of applications", "timestamp": "00:13:12,186", "timestamp_s": 792.0}, {"text": "that are contextual, aware and capable of reasoning, thereby enhancing", "timestamp": "00:13:15,690", "timestamp_s": 795.0}, {"text": "the practical utility of llms in various scenarios.", "timestamp": "00:13:19,994", "timestamp_s": 799.0}, {"text": "LangChain has split the job into sequential", "timestamp": "00:13:23,954", "timestamp_s": 803.0}, {"text": "steps where the preprocessing could be an independent step and model", "timestamp": "00:13:28,138", "timestamp_s": 808.0}, {"text": "invocation would be an independent step. There are", "timestamp": "00:13:32,884", "timestamp_s": 812.0}, {"text": "like Azure offers a prompt flow where the model", "timestamp": "00:13:36,324", "timestamp_s": 816.0}, {"text": "sequence can be split into multiple steps where if there isn\u0027t any", "timestamp": "00:13:39,964", "timestamp_s": 819.0}, {"text": "change happens, even each layer could be a plug and play.", "timestamp": "00:13:43,668", "timestamp_s": 823.0}, {"text": "So the amount of time it takes from prototype to production", "timestamp": "00:13:47,604", "timestamp_s": 827.0}, {"text": "by having a suite of tools like lang chain,", "timestamp": "00:13:51,612", "timestamp_s": 831.0}, {"text": "makes the productionization more secure and scalable.", "timestamp": "00:13:54,348", "timestamp_s": 834.0}, {"text": "So as I said, LangChain is a framework", "timestamp": "00:13:58,804", "timestamp_s": 838.0}, {"text": "to develop machine learning model and by using an", "timestamp": "00:14:02,564", "timestamp_s": 842.0}, {"text": "API the models can be invoked.", "timestamp": "00:14:06,124", "timestamp_s": 846.0}, {"text": "About the lancing framework, which I said in the previous slide.", "timestamp": "00:14:09,444", "timestamp_s": 849.0}, {"text": "Lancing can be developed in Python as well JavaScript.", "timestamp": "00:14:13,052", "timestamp_s": 853.0}, {"text": "This offers multiple interfaces and integration with pandas", "timestamp": "00:14:17,164", "timestamp_s": 857.0}, {"text": "or numpy or scikit learn. It doesn\u0027t offer to integrate", "timestamp": "00:14:21,324", "timestamp_s": 861.0}, {"text": "with multiple other panda Python libraries.", "timestamp": "00:14:24,976", "timestamp_s": 864.0}, {"text": "Also, these are not having a chain agent. But what do you mean by chain?", "timestamp": "00:14:29,104", "timestamp_s": 869.0}, {"text": "Multiple sequential steps can be integrated together", "timestamp": "00:14:33,256", "timestamp_s": 873.0}, {"text": "like pre processing invocation, model invocation", "timestamp": "00:14:37,008", "timestamp_s": 877.0}, {"text": "and post processing that can be performed by chain agents.", "timestamp": "00:14:40,256", "timestamp_s": 880.0}, {"text": "Here are nothing but where collection of activities or multiple", "timestamp": "00:14:44,464", "timestamp_s": 884.0}, {"text": "events can be performed without having much trouble in the", "timestamp": "00:14:48,040", "timestamp_s": 888.0}, {"text": "execution. And they are ready made chain and they are very good", "timestamp": "00:14:51,332", "timestamp_s": 891.0}, {"text": "in agent implementation. Also lang chain", "timestamp": "00:14:55,060", "timestamp_s": 895.0}, {"text": "as a Langsmith and the templates and Langserve", "timestamp": "00:14:58,556", "timestamp_s": 898.0}, {"text": "is used for serving the model. Introduction by and rest API", "timestamp": "00:15:02,164", "timestamp_s": 902.0}, {"text": "Langsmith is for debugging and evaluating and monitoring the chains", "timestamp": "00:15:06,244", "timestamp_s": 906.0}, {"text": "within the LLM framework. So this", "timestamp": "00:15:10,260", "timestamp_s": 910.0}, {"text": "is all comes as part of the package of lang chain framework.", "timestamp": "00:15:13,540", "timestamp_s": 913.0}, {"text": "Lang chain is a sequential chain", "timestamp": "00:15:17,538", "timestamp_s": 917.0}, {"text": "where multiple models can be invoked simultaneously, or it can have a sequential", "timestamp": "00:15:21,018", "timestamp_s": 921.0}, {"text": "model invocation, or it can also have a parallel model invocation.", "timestamp": "00:15:25,058", "timestamp_s": 925.0}, {"text": "So as part of this model lang chain framework, they also offer lang chain", "timestamp": "00:15:28,554", "timestamp_s": 928.0}, {"text": "compression language, so where the", "timestamp": "00:15:32,514", "timestamp_s": 932.0}, {"text": "amount of code which we write in python could be drastically reduced", "timestamp": "00:15:35,778", "timestamp_s": 935.0}, {"text": "by using expression language of lang chain.", "timestamp": "00:15:39,850", "timestamp_s": 939.0}, {"text": "So interesting. So after that, let\u0027s see how", "timestamp": "00:15:43,704", "timestamp_s": 943.0}, {"text": "the generated way application can be developed with LangChain.", "timestamp": "00:15:47,664", "timestamp_s": 947.0}, {"text": "So whenever we start with a generative way application, we have to", "timestamp": "00:15:51,128", "timestamp_s": 951.0}, {"text": "identify the objective of what is a task we are going to perform.", "timestamp": "00:15:54,400", "timestamp_s": 954.0}, {"text": "It could be an prototype to identify", "timestamp": "00:15:58,144", "timestamp_s": 958.0}, {"text": "or perform an image classification, or even it can be a natural language processing", "timestamp": "00:16:01,968", "timestamp_s": 961.0}, {"text": "task like translation. So where we", "timestamp": "00:16:06,296", "timestamp_s": 966.0}, {"text": "have to provide the context to the generative AI model, then we", "timestamp": "00:16:09,598", "timestamp_s": 969.0}, {"text": "have to offer a support to have an integrate with multiple platforms.", "timestamp": "00:16:13,198", "timestamp_s": 973.0}, {"text": "Then the code which we write should be in a mode to productionize and", "timestamp": "00:16:17,030", "timestamp_s": 977.0}, {"text": "we should have a collaborative environment like Azure Notebook or Amazon", "timestamp": "00:16:20,990", "timestamp_s": 980.0}, {"text": "Sagemaker. There are many things where there is a platform to develop", "timestamp": "00:16:25,374", "timestamp_s": 985.0}, {"text": "the machine learning models. Then after the diversified model", "timestamp": "00:16:29,134", "timestamp_s": 989.0}, {"text": "application, it can suit for various range of applications from", "timestamp": "00:16:32,806", "timestamp_s": 992.0}, {"text": "chatbot to document summarization or analyzation.", "timestamp": "00:16:36,094", "timestamp_s": 996.0}, {"text": "Now the development takes into product, product or productionization.", "timestamp": "00:16:39,664", "timestamp_s": 999.0}, {"text": "So whenever we talk about productionization, scalability is a very important", "timestamp": "00:16:43,472", "timestamp_s": 1003.0}, {"text": "feature where when the model should serve multiple requests", "timestamp": "00:16:47,384", "timestamp_s": 1007.0}, {"text": "in parallel or in concurrent fashion. Also it have", "timestamp": "00:16:50,960", "timestamp_s": 1010.0}, {"text": "a framework should have supported testing, we should", "timestamp": "00:16:54,560", "timestamp_s": 1014.0}, {"text": "have monitoring tools to check how the model is performing in", "timestamp": "00:16:58,008", "timestamp_s": 1018.0}, {"text": "production. And the deployment should be ease by having an API", "timestamp": "00:17:01,360", "timestamp_s": 1021.0}, {"text": "as an invocation the model. So also that will be a continuous", "timestamp": "00:17:04,992", "timestamp_s": 1024.0}, {"text": "improvement for the model by having a prompt versioning where", "timestamp": "00:17:08,966", "timestamp_s": 1028.0}, {"text": "multiple prompts can be identified and fine tuned", "timestamp": "00:17:12,574", "timestamp_s": 1032.0}, {"text": "on the prompt and the prompt will go through an evaluation phase and", "timestamp": "00:17:16,454", "timestamp_s": 1036.0}, {"text": "the prompt will be further fine tuned to deploy into production.", "timestamp": "00:17:20,118", "timestamp_s": 1040.0}, {"text": "Again, the most interesting thing is deployment,", "timestamp": "00:17:23,934", "timestamp_s": 1043.0}, {"text": "where LaNC serve can be used to deploy the lank chain.", "timestamp": "00:17:27,262", "timestamp_s": 1047.0}, {"text": "So lank serve is nothing but like a fast API. It\u0027s like a", "timestamp": "00:17:30,998", "timestamp_s": 1050.0}, {"text": "server, which on top of lank chain where", "timestamp": "00:17:34,750", "timestamp_s": 1054.0}, {"text": "it acts as a server and it communicates and provides the rest API to", "timestamp": "00:17:38,244", "timestamp_s": 1058.0}, {"text": "invoke the Lang chain or the agents inside the lancing.", "timestamp": "00:17:42,212", "timestamp_s": 1062.0}, {"text": "All right, now when we go into lancing, there are multiple deployment", "timestamp": "00:17:46,164", "timestamp_s": 1066.0}, {"text": "templates which is readily available to consume and where", "timestamp": "00:17:50,404", "timestamp_s": 1070.0}, {"text": "each and every time we can have a plug and play features", "timestamp": "00:17:54,404", "timestamp_s": 1074.0}, {"text": "like providing templates in model invocation and scalability", "timestamp": "00:17:58,044", "timestamp_s": 1078.0}, {"text": "and these of integration and production grades. The production grade support.", "timestamp": "00:18:02,004", "timestamp_s": 1082.0}, {"text": "I think these are all the features supported by lanching to offer.", "timestamp": "00:18:05,962", "timestamp_s": 1085.0}, {"text": "Introduction let\u0027s see the", "timestamp": "00:18:09,490", "timestamp_s": 1089.0}, {"text": "difference between the prompting and fine tuning wise and alternatives,", "timestamp": "00:18:13,690", "timestamp_s": 1093.0}, {"text": "right? In the case of prompting, which you could see,", "timestamp": "00:18:16,962", "timestamp_s": 1096.0}, {"text": "we specify you are an unbiased professor and your input", "timestamp": "00:18:20,930", "timestamp_s": 1100.0}, {"text": "score should be zero to ten, and then we pass it to the foundation model", "timestamp": "00:18:24,946", "timestamp_s": 1104.0}, {"text": "along with an input. Then we can get an output. So where as part of", "timestamp": "00:18:28,994", "timestamp_s": 1108.0}, {"text": "the problem we are specifying the instruction to the model. In the case", "timestamp": "00:18:31,856", "timestamp_s": 1111.0}, {"text": "of fine tuning, which we are talking all the time,", "timestamp": "00:18:35,632", "timestamp_s": 1115.0}, {"text": "where we need the data set and to take the foundational model, and then", "timestamp": "00:18:38,736", "timestamp_s": 1118.0}, {"text": "we fine tune the model and then we deploy the model in production. That is", "timestamp": "00:18:42,440", "timestamp_s": 1122.0}, {"text": "all the LLM engineering or the prompting wise fine tuning works.", "timestamp": "00:18:45,928", "timestamp_s": 1125.0}, {"text": "Still, I am not saying that we should go only for prompt engineering.", "timestamp": "00:18:49,696", "timestamp_s": 1129.0}, {"text": "That could be a domain specific task where you may require", "timestamp": "00:18:53,312", "timestamp_s": 1133.0}, {"text": "fine tuning, but typically most of the problem can", "timestamp": "00:18:57,194", "timestamp_s": 1137.0}, {"text": "be solved well enough by using the right amount of prompting technique", "timestamp": "00:19:00,482", "timestamp_s": 1140.0}, {"text": "like chain of thought or self consistency tree of thoughts,", "timestamp": "00:19:05,002", "timestamp_s": 1145.0}, {"text": "the multiple prompt engineering technique can be tried out.", "timestamp": "00:19:08,634", "timestamp_s": 1148.0}, {"text": "Now let\u0027s move on to the Lang chain demo,", "timestamp": "00:19:12,034", "timestamp_s": 1152.0}, {"text": "and I\u0027ll show you how easily a prototype", "timestamp": "00:19:15,434", "timestamp_s": 1155.0}, {"text": "and productionization of the model can be performed.", "timestamp": "00:19:19,514", "timestamp_s": 1159.0}, {"text": "As usual, for any libraries to be installed", "timestamp": "00:19:22,694", "timestamp_s": 1162.0}, {"text": "in Python, it has to follow via pip install or puda install.", "timestamp": "00:19:26,230", "timestamp_s": 1166.0}, {"text": "But once we install the Lang chain and the LangChain API", "timestamp": "00:19:30,614", "timestamp_s": 1170.0}, {"text": "installed, you have to procure the open API key,", "timestamp": "00:19:35,614", "timestamp_s": 1175.0}, {"text": "followed by installing the Lang chain, OpenAI and long chain libraries", "timestamp": "00:19:38,990", "timestamp_s": 1178.0}, {"text": "in a Python environment, followed by we", "timestamp": "00:19:44,654", "timestamp_s": 1184.0}, {"text": "have to import the lounging OpenAI and import", "timestamp": "00:19:48,618", "timestamp_s": 1188.0}, {"text": "the chat OpenAI and create call the function and specify", "timestamp": "00:19:52,146", "timestamp_s": 1192.0}, {"text": "the instruction via LLM invoke.", "timestamp": "00:19:56,482", "timestamp_s": 1196.0}, {"text": "So that is the power of three lines of code can effectively", "timestamp": "00:19:59,874", "timestamp_s": 1199.0}, {"text": "perform a prototyping for you. And when you wanted", "timestamp": "00:20:04,026", "timestamp_s": 1204.0}, {"text": "to print. How can Langsmith help with testing? You can get an", "timestamp": "00:20:07,418", "timestamp_s": 1207.0}, {"text": "output saying from the chart GPT model. Okay, this is how", "timestamp": "00:20:10,938", "timestamp_s": 1210.0}, {"text": "steps required Langsmith can help with the testing.", "timestamp": "00:20:14,850", "timestamp_s": 1214.0}, {"text": "Now we without a prompt, we have given an input, but by", "timestamp": "00:20:18,514", "timestamp_s": 1218.0}, {"text": "adding a prompt, we are specifying an instruction saying that what kind of", "timestamp": "00:20:22,474", "timestamp_s": 1222.0}, {"text": "task the input would be given. So in this", "timestamp": "00:20:26,306", "timestamp_s": 1226.0}, {"text": "case we are saying in the prompt that you", "timestamp": "00:20:29,794", "timestamp_s": 1229.0}, {"text": "are a world class technical documentation writer. By providing an input,", "timestamp": "00:20:32,858", "timestamp_s": 1232.0}, {"text": "it writes the document in a more efficient", "timestamp": "00:20:38,194", "timestamp_s": 1238.0}, {"text": "way manner like how the technical documentation writer would write.", "timestamp": "00:20:41,458", "timestamp_s": 1241.0}, {"text": "So that\u0027s the power of prompt by specifying the prompt.", "timestamp": "00:20:45,410", "timestamp_s": 1245.0}, {"text": "So you can see here we are, okay, the same thing. We are importing the", "timestamp": "00:20:48,714", "timestamp_s": 1248.0}, {"text": "packages libraries and we are invoking a chat open a", "timestamp": "00:20:51,882", "timestamp_s": 1251.0}, {"text": "function. Typically you have to for a security reason.", "timestamp": "00:20:55,202", "timestamp_s": 1255.0}, {"text": "I have written all the API key where you have to provide the API keys", "timestamp": "00:20:59,106", "timestamp_s": 1259.0}, {"text": "in the function, followed by the prompt template where you give the", "timestamp": "00:21:02,442", "timestamp_s": 1262.0}, {"text": "template as an instruction, as a prompt, followed by the user", "timestamp": "00:21:05,874", "timestamp_s": 1265.0}, {"text": "input. Now once I give like this your world", "timestamp": "00:21:09,212", "timestamp_s": 1269.0}, {"text": "class technical documentation writer the system prompt followed by the user", "timestamp": "00:21:12,732", "timestamp_s": 1272.0}, {"text": "input, user input and specifying the chain dot in put", "timestamp": "00:21:16,140", "timestamp_s": 1276.0}, {"text": "I can specify very good amount of output,", "timestamp": "00:21:20,308", "timestamp_s": 1280.0}, {"text": "like how a technical document writer would write it. So most", "timestamp": "00:21:24,380", "timestamp_s": 1284.0}, {"text": "of the things are very similar. On top of that we can have an output", "timestamp": "00:21:28,292", "timestamp_s": 1288.0}, {"text": "parser where we can define. The output parser could", "timestamp": "00:21:31,692", "timestamp_s": 1291.0}, {"text": "be in a JSON format, or it could be an excel, or how do we", "timestamp": "00:21:34,948", "timestamp_s": 1294.0}, {"text": "define the format for for a large language model,", "timestamp": "00:21:37,860", "timestamp_s": 1297.0}, {"text": "the output has to be performed. So by using this", "timestamp": "00:21:41,376", "timestamp_s": 1301.0}, {"text": "input and a prompt template and output parser,", "timestamp": "00:21:45,584", "timestamp_s": 1305.0}, {"text": "you\u0027re all set to get an output from the large language model like GPT", "timestamp": "00:21:49,160", "timestamp_s": 1309.0}, {"text": "four. If you have any questions, I\u0027m more happier to talk", "timestamp": "00:21:52,600", "timestamp_s": 1312.0}, {"text": "after the session. Once again, thank you all for", "timestamp": "00:21:56,208", "timestamp_s": 1316.0}, {"text": "this, for your time and listening to the session. If you", "timestamp": "00:21:59,824", "timestamp_s": 1319.0}, {"text": "have any doubts, you can reach out to me at any point of time.", "timestamp": "00:22:03,176", "timestamp_s": 1323.0}, {"text": "Thank you all. Have a nice evening, have a good day and rest", "timestamp": "00:22:06,008", "timestamp_s": 1326.0}, {"text": "of the week.", "timestamp": "00:22:10,290", "timestamp_s": 1330.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'mCh6qtqNBes',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Future of LLM's and Machine learning Productionization
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Unlock the future of LLMs and Machine Learning Productionization in my talk! Dive into the cutting-edge trends and transformative potential that will redefine how we use LLMs and implement machine learning at scale. Don&rsquo;t miss the chance to navigate the next frontier of AI innovation.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Deepak has more than 15 years of experience in data science and machine learning. He will talk about the future of large language models and productionization. First, he will explain the traditional AI model development and deployment. Then he will walk you through on the large language model like GPT four.

              </li>
              
              <li>
                Traditional AI models began with linear regression, logistic regression, random forest decision trees. The evolution started from neural network to transformers in 2019 or 2017. When we deploy these models in production, it can be deployed in elastic container service. I will talk about the Pytorch serving architecture in a minute.

              </li>
              
              <li>
                Pytorch serving is a framework where large, sorry, where Bert models or large language models like Bert can be deployed inside a container. Now the offer which we are going to make is lang chain.

              </li>
              
              <li>
                Large language models like GPT-3 or GPT four have been trained more than 175 billion parameters. The whole process without fine tuning can be achieved by providing in context learning to the model. How do you productionize the large language models?

              </li>
              
              <li>
                LangChain is a framework to develop the large language models. It facilitates the creation of applications that are contextual, aware and capable of reasoning. By using an API the models can be invoked. Making the productionization more secure and scalable.

              </li>
              
              <li>
                Lancing can be developed in Python as well JavaScript. Offers multiple interfaces and integration with pandas or numpy or scikit learn. Multiple sequential steps can be integrated together. Can suit for various range of applications from chatbot to document summarization. Now the development takes into product, product or productionization.

              </li>
              
              <li>
                The difference between prompting and fine tuning wise and alternatives. Most of the problem can be solved well enough by using the right amount of prompting technique. Still, I am not saying that we should go only for prompt engineering.

              </li>
              
              <li>
                Langsmith can be used to create a prototype and productionization of the model. Using a prompt template and an output parser, you can get an output from a large language model. Once again, thank you all for your time and listening to the session.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/mCh6qtqNBes.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:20,880'); seek(20.0)">
              Thank you all for joining the session. Today, I am going to talk about
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:24,790'); seek(24.0)">
              the large language models and the future of large language
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:28,864'); seek(28.0)">
              models and productionization of the large language models.
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:32,744'); seek(32.0)">
              Myself, Deepak. I am working as an associate director for data science and
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:36,600'); seek(36.0)">
              machine learning projects. I also have more
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:39,680'); seek(39.0)">
              than 15 years of experience in data science and
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:42,760'); seek(42.0)">
              machine learning, dominantly working in generative AI
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:46,296'); seek(46.0)">
              for the past three years. All right,
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:49,288'); seek(49.0)">
              now take you to the next slide.
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:52,864'); seek(52.0)">
              So, before I'm going to talk about the large language model productionization or
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:56,840'); seek(56.0)">
              deploying in cloud, let's understand the traditional
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:00,432'); seek(60.0)">
              AI model development and deployment, followed by the challenges
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:04,272'); seek(64.0)">
              we have in deploying or
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:07,464'); seek(67.0)">
              productionizing the traditional AI models. Then I'll
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:10,768'); seek(70.0)">
              walk you through on the large language models like GPT four,
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:14,336'); seek(74.0)">
              and I can explain you the architecture of the large language models
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:17,776'); seek(77.0)">
              or the generated AI model. Then I can take you through the
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:21,182'); seek(81.0)">
              concept of lancing framework and how the applications can
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:24,782'); seek(84.0)">
              be developed with lancing, followed by a demo.
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:28,734'); seek(88.0)">
              Moving to the next slide, let's talk about the traditional
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:32,342'); seek(92.0)">
              AI models. When I say traditional AI models,
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:35,734'); seek(95.0)">
              we began with linear regression, logistic regression, random forest
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:39,662'); seek(99.0)">
              decision trees, boosting adaboost, exaboost,
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:43,398'); seek(103.0)">
              neural network, and the evolution started from neural
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:46,818'); seek(106.0)">
              network to transformers in 2019 or 2017.
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:49,842'); seek(109.0)">
              I'm sorry. So that is how the industry has a breakthrough,
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:53,562'); seek(113.0)">
              by coming up with a model called Bert, which is a bidirectional encoder
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:57,474'); seek(117.0)">
              representation for transformers. I think that has
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:02,130'); seek(122.0)">
              significantly performed well
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:05,418'); seek(125.0)">
              in most of the natural language processing tasks. When I
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:09,090'); seek(129.0)">
              talk about traditional AI models, let me not talk about starting
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:12,712'); seek(132.0)">
              from linear or logistic regressions or random forest.
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:16,152'); seek(136.0)">
              Let's begin with a small large language model
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:19,632'); seek(139.0)">
              which has been known, which I, which I call that as invert
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:23,264'); seek(143.0)">
              the process involved in model training
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:26,984'); seek(146.0)">
              or model fine tuning has a requires huge amount
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:30,432'); seek(150.0)">
              of data set to train the model. So once
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:33,696'); seek(153.0)">
              we train the model or fine tune the model, we have to fine tune the
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:36,848'); seek(156.0)">
              model for a specific task or for a specific domain.
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:40,404'); seek(160.0)">
              Typically, it needs a GPU machine to do the model fine tuning
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:43,820'); seek(163.0)">
              process. Once we perform the model fine
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:47,060'); seek(167.0)">
              tuning, we have to do the hyper parameter tuning like learning rate
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:50,964'); seek(170.0)">
              epoch with multiple additional parameters to come
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:54,900'); seek(174.0)">
              up with the right ways for the model to classify or question answering
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:02:59,268'); seek(179.0)">
              or any of the tasks which it can perform.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:02,484'); seek(182.0)">
              So then once we do the fine tuning and optimization of
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:06,020'); seek(186.0)">
              the model, we have to deploy the model in a cloud
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:09,426'); seek(189.0)">
              environment. It could be AWS or Azure,
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:13,754'); seek(193.0)">
              even Google Cloud platform.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:16,514'); seek(196.0)">
              But before that, when you are going to deploy the
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:19,730'); seek(199.0)">
              model, the model has to be serialized, meaning the model
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:23,562'); seek(203.0)">
              as to when we deploy the model in production, it should have the scalability
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:27,706'); seek(207.0)">
              and reliability and durability. Considering that
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:32,374'); seek(212.0)">
              when we move the model to production, we have to serialize the model by having
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:36,022'); seek(216.0)">
              a pytorch or a tensorflow saved model
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:39,334'); seek(219.0)">
              format to serve the model. Introduction so,
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:42,862'); seek(222.0)">
              model serving, as I talked about as a framework called Pytorch serving,
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:46,742'); seek(226.0)">
              so which is a framework can have a scalability on
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:50,254'); seek(230.0)">
              performing the inference. This framework provides an API
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:54,494'); seek(234.0)">
              where the application, once we build a real world application,
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:03:58,126'); seek(238.0)">
              the application can invoke the inference or the prediction
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:01,690'); seek(241.0)">
              by invoking the Pytorch serving part.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:04,594'); seek(244.0)">
              Also, we provide that as an API that helps us to
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:08,018'); seek(248.0)">
              come up with an API design and based on the API design we can start
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:11,890'); seek(251.0)">
              invoking the model. The model can be a single model or multiple
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:15,554'); seek(255.0)">
              models can be deployed in production. I will talk about the Pytorch serving
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:19,474'); seek(259.0)">
              architecture in a minute. Before that I will tell how the scalability
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:23,546'); seek(263.0)">
              and load balancing will be performed in the cloud environment.
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:27,054'); seek(267.0)">
              When we deploy these models in production, it can be deployed in elastic
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:30,782'); seek(270.0)">
              container service, in AWS or in Azure
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:34,254'); seek(274.0)">
              container app. When we deploy the models, we have a load balancer
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:38,566'); seek(278.0)">
              has to be created and we have to create the cloud formation template to create
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:42,102'); seek(282.0)">
              a container and we have to deploy this model as a docker image and
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:45,894'); seek(285.0)">
              internally it has a Pytorch serving framework.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:48,734'); seek(288.0)">
              Once we deploy the model, we have to have a auditability
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:52,094'); seek(292.0)">
              which is nothing but monitoring and logging. So there most
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:04:55,492'); seek(295.0)">
              of the model would be logged along with the number of invocation has been
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:04:58,964'); seek(298.0)">
              made to the model along with the throughput and error rates.
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:03,444'); seek(303.0)">
              So the model should be highly secured where it cannot be having unauthorized
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:07,220'); seek(307.0)">
              access and attacks. So also once we build
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:10,764'); seek(310.0)">
              a model, it should have an security along with the
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:14,428'); seek(314.0)">
              CI CD pipeline for the reinforcement learning whenever the
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:18,060'); seek(318.0)">
              model trains and fine tune and deployed in production,
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:21,924'); seek(321.0)">
              if the model has any variation from the data
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:25,138'); seek(325.0)">
              which it has been trained, then the model has to be
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:28,506'); seek(328.0)">
              when in the production the model has a deviation
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:31,882'); seek(331.0)">
              in the data, then it cannot identify the data accurately.
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:35,410'); seek(335.0)">
              So we have a CACD pipeline to have a reinforcement.
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:39,218'); seek(339.0)">
              Human learning to ensure if there isn't any deviation,
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:42,442'); seek(342.0)">
              model has to automatically, automatically, after a certain time it
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:46,530'); seek(346.0)">
              has to train and fine tune and again it has to be deployed.
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:50,260'); seek(350.0)">
              That variant is called a b testing or multiple variants
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:05:53,564'); seek(353.0)">
              of models will be deployed in production that comes under versioning
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:05:57,228'); seek(357.0)">
              and rollback. So we are all talking about the traditional AI.
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:01,812'); seek(361.0)">
              So this comes under the concept of mlops.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:04,620'); seek(364.0)">
              So we design the model, we develop the model
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:08,020'); seek(368.0)">
              and operationalize the model so in case of design,
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:11,548'); seek(371.0)">
              we identify the data set, we identify the
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:15,002'); seek(375.0)">
              model. Then once we have done the identification,
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:18,514'); seek(378.0)">
              we understand what the model task is. It could be in classification or
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:22,442'); seek(382.0)">
              summarization, abstraction or like
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:25,818'); seek(385.0)">
              a question and answering or next sentence for prediction. There could be multiple
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:30,314'); seek(390.0)">
              kind of tasks the model can perform. So as part of
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:33,546'); seek(393.0)">
              the recurrent gathering or use case prioritization that has to be identified
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:37,586'); seek(397.0)">
              along with the data availability to train or fine tune the model.
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:40,946'); seek(400.0)">
              I think fine tune is the right word, followed by model engineering
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:44,896'); seek(404.0)">
              which has a technique to identify the model, then perform
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:48,504'); seek(408.0)">
              Eiffel parameter tuning and fine tune the model and deploy the
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:06:52,200'); seek(412.0)">
              model. In operations. That deployment process would
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:06:55,728'); seek(415.0)">
              be in a cloud environment by having a CACD pipeline like Azure DevOps,
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:00,448'); seek(420.0)">
              or then we can monitor via
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:04,464'); seek(424.0)">
              Amazon Cloudwatch or Azure monitoring logs.
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:09,084'); seek(429.0)">
              So this traditional AI model development involves
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:12,644'); seek(432.0)">
              a huge number, there's a certain amount of process has to be followed,
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:17,124'); seek(437.0)">
              right? So before getting into the large language models,
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:20,748'); seek(440.0)">
              I would like to touch base on the Pytorch serving. So Pytorch serving is
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:24,932'); seek(444.0)">
              nothing but a framework where large, sorry, where Bert models
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:28,356'); seek(448.0)">
              or large language models like Bert
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:31,692'); seek(451.0)">
              can be deployed. So it is a framework which comes up with
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:34,908'); seek(454.0)">
              an inference and management API where multiple models can
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:38,300'); seek(458.0)">
              be deployed inside the container. So again,
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:42,248'); seek(462.0)">
              this container, when you mean this pyth serving, has to be
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:45,448'); seek(465.0)">
              built as a docker and it has to be deployed inside a container. It could
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:48,728'); seek(468.0)">
              be an Amazon elastic container instance or Azure
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:07:53,464'); seek(473.0)">
              where we can deploy multiple machine learning models by
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:07:56,888'); seek(476.0)">
              using model store. Under the model store we can start
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:01,384'); seek(481.0)">
              using EBS or elastic storage mechanism,
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:05,980'); seek(485.0)">
              we can start to save the models
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:09,292'); seek(489.0)">
              by use by running an API and we can serve the model by
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:12,700'); seek(492.0)">
              an HTTP endpoint. I think this is the holistic process.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:16,156'); seek(496.0)">
              Now you understand the amount of efforts or time we spent in
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:20,068'); seek(500.0)">
              the whole machine learning or traditional machine learning model development productionization.
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:25,084'); seek(505.0)">
              So now the offer which we are going to make
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:28,828'); seek(508.0)">
              is lang chain. But before that
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:32,368'); seek(512.0)">
              I will give you a few touch base on large language models. See large language
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:36,408'); seek(516.0)">
              models like GPT-3 or GPT four which has been
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:39,664'); seek(519.0)">
              trained more than 175 billion parameters. We have other models like Lama
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:43,888'); seek(523.0)">
              two or Mistral or cloud which comes up with 7
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:47,368'); seek(527.0)">
              billion or 70 billion parameters. Of the amount of
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:50,672'); seek(530.0)">
              data has been trained. When it comes to charge DpT,
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:08:54,520'); seek(534.0)">
              chart, GPT, we all know it's from OpenAI. It's more like
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:08:57,914'); seek(537.0)">
              a very large language model. It is a
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:01,178'); seek(541.0)">
              foundational model. It has a capability to answer any questions
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:04,786'); seek(544.0)">
              or any task it can perform without any fine tuning.
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:08,602'); seek(548.0)">
              So the whole process without fine tuning can be
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:11,842'); seek(551.0)">
              achieved by providing in context learning to the model. So where the
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:15,786'); seek(555.0)">
              in context learning would be providing
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:19,810'); seek(559.0)">
              the model by giving some context.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:23,254'); seek(563.0)">
              In context learning means as part of the problem techniques,
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:27,510'); seek(567.0)">
              instruction can be specified to the GPT four model
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:31,390'); seek(571.0)">
              to perform a specific task. So when I say
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:36,094'); seek(576.0)">
              performing a specific task, we can use multiple prompt engineering techniques.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:40,310'); seek(580.0)">
              So before the tradition was writing a programming language
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:44,198'); seek(584.0)">
              in Java or in Python to perform a task for a programming language,
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:48,398'); seek(588.0)">
              but now natural language process is
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:52,378'); seek(592.0)">
              a programming language, nothing but, it's an English. So where we
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:09:56,138'); seek(596.0)">
              can specify an instruction to the model which is nothing but a prompt along
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:00,154'); seek(600.0)">
              with the input, and we say if it performs a summarization
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:04,338'); seek(604.0)">
              or translation task, we specify the task information
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:08,146'); seek(608.0)">
              by providing in context learning via prompt to along with
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:11,898'); seek(611.0)">
              an input, we get the relevant answers from the GPT four.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:15,626'); seek(615.0)">
              So that's the evolution of large language models. So large language models
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:19,460'); seek(619.0)">
              are not necessarily need to be fine tuned, which saves the significant
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:23,356'); seek(623.0)">
              amount of resources like infrastructure and time.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:26,548'); seek(626.0)">
              And you know, to have a safer and cleaner environment,
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:29,644'); seek(629.0)">
              not to fine tune or train the algorithm every time.
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:33,484'); seek(633.0)">
              Now we know about large language models.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:36,644'); seek(636.0)">
              Now we know how we can utilize the large language models
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:40,132'); seek(640.0)">
              to perform a specific task. But it all
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:43,234'); seek(643.0)">
              looks good when you are doing some kind of a prototype.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:46,610'); seek(646.0)">
              So where you can specify a prompt and
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:49,810'); seek(649.0)">
              you can give an input and you can get an output on the prompt.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:10:52,826'); seek(652.0)">
              So how do you productionize the large language models? That is an interesting area
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:10:56,826'); seek(656.0)">
              to focus on. Okay, that's how we offer lang
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:00,058'); seek(660.0)">
              chain. But again, before getting into lang chain, let's look into the architecture
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:04,410'); seek(664.0)">
              of large language models. So before let's
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:07,570'); seek(667.0)">
              have a small comparison between traditional model and generate the algorithm,
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:11,850'); seek(671.0)">
              it's nothing but large language model. In traditional model
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:15,084'); seek(675.0)">
              we have a data pre processing, then we identify the features required
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:19,060'); seek(679.0)">
              for training or fine tuning
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:22,740'); seek(682.0)">
              the model. After identifying the features, then we perform a fine tuning
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:26,644'); seek(686.0)">
              job by having the data. Then once the data has been trained,
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:30,124'); seek(690.0)">
              then we deploy in production in cloud environment. So typically the model
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:34,276'); seek(694.0)">
              uses a framework like Tensorflow, Pytorch,
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:37,396'); seek(697.0)">
              keras, then underlying it could be an IBM
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:40,804'); seek(700.0)">
              Watson API, or it could be an Pytorch serving which
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:44,820'); seek(704.0)">
              I was mentioning. Similarly, we would have used multiple databases like no
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:48,700'); seek(708.0)">
              SQL or SQL database and mlops, avant Docker and Jenkins.
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:53,044'); seek(713.0)">
              Right now the shift, as in paradigm shift, the reason
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:11:56,964'); seek(716.0)">
              we are in the era of more interesting things happening
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:00,692'); seek(720.0)">
              every day or every week to identify which
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:04,522'); seek(724.0)">
              is realistic and which can be productionized is a key challenge. So that
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:08,370'); seek(728.0)">
              would be addressed as part of this demo or as part of this
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:11,642'); seek(731.0)">
              conversation which we are having now. Even after the conversation you can
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:15,122'); seek(735.0)">
              reach out to me and have a discussion. Now the
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:18,770'); seek(738.0)">
              whole process has been converted into prompt tuning or prompt
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:22,186'); seek(742.0)">
              engineering on neat basis. We can go for fine tuning,
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:25,538'); seek(745.0)">
              but it's not necessary. But even prompt engineering significantly
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:29,274'); seek(749.0)">
              performs well on the tasks. Data pre processing it's
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:32,468'); seek(752.0)">
              all about the input. Data has to be cleansed and given as an input
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:35,988'); seek(755.0)">
              along with the prompt. Then it has an underlying foundational model like
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:39,660'); seek(759.0)">
              GPT four or Claude or Mistral. Any of the model can be used.
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:43,420'); seek(763.0)">
              Then we deploy the model by using orchestration platform
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:47,300'); seek(767.0)">
              like LangChain or Lama index. So today the offer is about
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:51,036'); seek(771.0)">
              LangChain. It's not only about developing machine
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:12:54,612'); seek(774.0)">
              learning models. Deploying a machine learning model and invoking
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:12:57,980'); seek(777.0)">
              a machine learning model is become much more easier than what we have
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:01,452'); seek(781.0)">
              done earlier. If you have any questions,
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:04,594'); seek(784.0)">
              I'll move to the next slide. LangChain so
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:08,402'); seek(788.0)">
              LangChain is a framework to develop the large language
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:12,186'); seek(792.0)">
              models. It facilitates the creation of applications
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:15,690'); seek(795.0)">
              that are contextual, aware and capable of reasoning, thereby enhancing
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:19,994'); seek(799.0)">
              the practical utility of llms in various scenarios.
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:23,954'); seek(803.0)">
              LangChain has split the job into sequential
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:28,138'); seek(808.0)">
              steps where the preprocessing could be an independent step and model
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:32,884'); seek(812.0)">
              invocation would be an independent step. There are
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:36,324'); seek(816.0)">
              like Azure offers a prompt flow where the model
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:39,964'); seek(819.0)">
              sequence can be split into multiple steps where if there isn't any
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:43,668'); seek(823.0)">
              change happens, even each layer could be a plug and play.
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:47,604'); seek(827.0)">
              So the amount of time it takes from prototype to production
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:51,612'); seek(831.0)">
              by having a suite of tools like lang chain,
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:13:54,348'); seek(834.0)">
              makes the productionization more secure and scalable.
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:13:58,804'); seek(838.0)">
              So as I said, LangChain is a framework
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:02,564'); seek(842.0)">
              to develop machine learning model and by using an
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:06,124'); seek(846.0)">
              API the models can be invoked.
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:09,444'); seek(849.0)">
              About the lancing framework, which I said in the previous slide.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:13,052'); seek(853.0)">
              Lancing can be developed in Python as well JavaScript.
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:17,164'); seek(857.0)">
              This offers multiple interfaces and integration with pandas
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:21,324'); seek(861.0)">
              or numpy or scikit learn. It doesn't offer to integrate
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:24,976'); seek(864.0)">
              with multiple other panda Python libraries.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:29,104'); seek(869.0)">
              Also, these are not having a chain agent. But what do you mean by chain?
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:33,256'); seek(873.0)">
              Multiple sequential steps can be integrated together
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:37,008'); seek(877.0)">
              like pre processing invocation, model invocation
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:40,256'); seek(880.0)">
              and post processing that can be performed by chain agents.
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:44,464'); seek(884.0)">
              Here are nothing but where collection of activities or multiple
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:48,040'); seek(888.0)">
              events can be performed without having much trouble in the
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:51,332'); seek(891.0)">
              execution. And they are ready made chain and they are very good
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:14:55,060'); seek(895.0)">
              in agent implementation. Also lang chain
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:14:58,556'); seek(898.0)">
              as a Langsmith and the templates and Langserve
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:02,164'); seek(902.0)">
              is used for serving the model. Introduction by and rest API
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:06,244'); seek(906.0)">
              Langsmith is for debugging and evaluating and monitoring the chains
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:10,260'); seek(910.0)">
              within the LLM framework. So this
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:13,540'); seek(913.0)">
              is all comes as part of the package of lang chain framework.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:17,538'); seek(917.0)">
              Lang chain is a sequential chain
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:21,018'); seek(921.0)">
              where multiple models can be invoked simultaneously, or it can have a sequential
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:25,058'); seek(925.0)">
              model invocation, or it can also have a parallel model invocation.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:28,554'); seek(928.0)">
              So as part of this model lang chain framework, they also offer lang chain
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:32,514'); seek(932.0)">
              compression language, so where the
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:35,778'); seek(935.0)">
              amount of code which we write in python could be drastically reduced
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:39,850'); seek(939.0)">
              by using expression language of lang chain.
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:43,704'); seek(943.0)">
              So interesting. So after that, let's see how
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:47,664'); seek(947.0)">
              the generated way application can be developed with LangChain.
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:51,128'); seek(951.0)">
              So whenever we start with a generative way application, we have to
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:54,400'); seek(954.0)">
              identify the objective of what is a task we are going to perform.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:15:58,144'); seek(958.0)">
              It could be an prototype to identify
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:01,968'); seek(961.0)">
              or perform an image classification, or even it can be a natural language processing
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:06,296'); seek(966.0)">
              task like translation. So where we
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:09,598'); seek(969.0)">
              have to provide the context to the generative AI model, then we
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:13,198'); seek(973.0)">
              have to offer a support to have an integrate with multiple platforms.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:17,030'); seek(977.0)">
              Then the code which we write should be in a mode to productionize and
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:20,990'); seek(980.0)">
              we should have a collaborative environment like Azure Notebook or Amazon
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:25,374'); seek(985.0)">
              Sagemaker. There are many things where there is a platform to develop
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:29,134'); seek(989.0)">
              the machine learning models. Then after the diversified model
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:32,806'); seek(992.0)">
              application, it can suit for various range of applications from
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:36,094'); seek(996.0)">
              chatbot to document summarization or analyzation.
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:39,664'); seek(999.0)">
              Now the development takes into product, product or productionization.
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:43,472'); seek(1003.0)">
              So whenever we talk about productionization, scalability is a very important
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:47,384'); seek(1007.0)">
              feature where when the model should serve multiple requests
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:50,960'); seek(1010.0)">
              in parallel or in concurrent fashion. Also it have
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:16:54,560'); seek(1014.0)">
              a framework should have supported testing, we should
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:16:58,008'); seek(1018.0)">
              have monitoring tools to check how the model is performing in
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:01,360'); seek(1021.0)">
              production. And the deployment should be ease by having an API
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:04,992'); seek(1024.0)">
              as an invocation the model. So also that will be a continuous
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:08,966'); seek(1028.0)">
              improvement for the model by having a prompt versioning where
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:12,574'); seek(1032.0)">
              multiple prompts can be identified and fine tuned
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:16,454'); seek(1036.0)">
              on the prompt and the prompt will go through an evaluation phase and
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:20,118'); seek(1040.0)">
              the prompt will be further fine tuned to deploy into production.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:23,934'); seek(1043.0)">
              Again, the most interesting thing is deployment,
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:27,262'); seek(1047.0)">
              where LaNC serve can be used to deploy the lank chain.
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:30,998'); seek(1050.0)">
              So lank serve is nothing but like a fast API. It's like a
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:34,750'); seek(1054.0)">
              server, which on top of lank chain where
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:38,244'); seek(1058.0)">
              it acts as a server and it communicates and provides the rest API to
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:42,212'); seek(1062.0)">
              invoke the Lang chain or the agents inside the lancing.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:17:46,164'); seek(1066.0)">
              All right, now when we go into lancing, there are multiple deployment
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:17:50,404'); seek(1070.0)">
              templates which is readily available to consume and where
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:17:54,404'); seek(1074.0)">
              each and every time we can have a plug and play features
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:17:58,044'); seek(1078.0)">
              like providing templates in model invocation and scalability
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:02,004'); seek(1082.0)">
              and these of integration and production grades. The production grade support.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:05,962'); seek(1085.0)">
              I think these are all the features supported by lanching to offer.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:09,490'); seek(1089.0)">
              Introduction let's see the
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:13,690'); seek(1093.0)">
              difference between the prompting and fine tuning wise and alternatives,
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:16,962'); seek(1096.0)">
              right? In the case of prompting, which you could see,
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:20,930'); seek(1100.0)">
              we specify you are an unbiased professor and your input
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:24,946'); seek(1104.0)">
              score should be zero to ten, and then we pass it to the foundation model
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:28,994'); seek(1108.0)">
              along with an input. Then we can get an output. So where as part of
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:31,856'); seek(1111.0)">
              the problem we are specifying the instruction to the model. In the case
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:35,632'); seek(1115.0)">
              of fine tuning, which we are talking all the time,
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:38,736'); seek(1118.0)">
              where we need the data set and to take the foundational model, and then
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:42,440'); seek(1122.0)">
              we fine tune the model and then we deploy the model in production. That is
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:45,928'); seek(1125.0)">
              all the LLM engineering or the prompting wise fine tuning works.
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:18:49,696'); seek(1129.0)">
              Still, I am not saying that we should go only for prompt engineering.
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:18:53,312'); seek(1133.0)">
              That could be a domain specific task where you may require
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:18:57,194'); seek(1137.0)">
              fine tuning, but typically most of the problem can
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:00,482'); seek(1140.0)">
              be solved well enough by using the right amount of prompting technique
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:05,002'); seek(1145.0)">
              like chain of thought or self consistency tree of thoughts,
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:08,634'); seek(1148.0)">
              the multiple prompt engineering technique can be tried out.
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:12,034'); seek(1152.0)">
              Now let's move on to the Lang chain demo,
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:15,434'); seek(1155.0)">
              and I'll show you how easily a prototype
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:19,514'); seek(1159.0)">
              and productionization of the model can be performed.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:22,694'); seek(1162.0)">
              As usual, for any libraries to be installed
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:26,230'); seek(1166.0)">
              in Python, it has to follow via pip install or puda install.
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:30,614'); seek(1170.0)">
              But once we install the Lang chain and the LangChain API
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:35,614'); seek(1175.0)">
              installed, you have to procure the open API key,
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:38,990'); seek(1178.0)">
              followed by installing the Lang chain, OpenAI and long chain libraries
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:44,654'); seek(1184.0)">
              in a Python environment, followed by we
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:48,618'); seek(1188.0)">
              have to import the lounging OpenAI and import
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:19:52,146'); seek(1192.0)">
              the chat OpenAI and create call the function and specify
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:19:56,482'); seek(1196.0)">
              the instruction via LLM invoke.
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:19:59,874'); seek(1199.0)">
              So that is the power of three lines of code can effectively
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:04,026'); seek(1204.0)">
              perform a prototyping for you. And when you wanted
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:07,418'); seek(1207.0)">
              to print. How can Langsmith help with testing? You can get an
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:10,938'); seek(1210.0)">
              output saying from the chart GPT model. Okay, this is how
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:14,850'); seek(1214.0)">
              steps required Langsmith can help with the testing.
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:18,514'); seek(1218.0)">
              Now we without a prompt, we have given an input, but by
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:22,474'); seek(1222.0)">
              adding a prompt, we are specifying an instruction saying that what kind of
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:26,306'); seek(1226.0)">
              task the input would be given. So in this
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:29,794'); seek(1229.0)">
              case we are saying in the prompt that you
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:32,858'); seek(1232.0)">
              are a world class technical documentation writer. By providing an input,
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:38,194'); seek(1238.0)">
              it writes the document in a more efficient
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:41,458'); seek(1241.0)">
              way manner like how the technical documentation writer would write.
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:45,410'); seek(1245.0)">
              So that's the power of prompt by specifying the prompt.
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:20:48,714'); seek(1248.0)">
              So you can see here we are, okay, the same thing. We are importing the
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:20:51,882'); seek(1251.0)">
              packages libraries and we are invoking a chat open a
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:20:55,202'); seek(1255.0)">
              function. Typically you have to for a security reason.
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:20:59,106'); seek(1259.0)">
              I have written all the API key where you have to provide the API keys
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:02,442'); seek(1262.0)">
              in the function, followed by the prompt template where you give the
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:05,874'); seek(1265.0)">
              template as an instruction, as a prompt, followed by the user
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:09,212'); seek(1269.0)">
              input. Now once I give like this your world
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:12,732'); seek(1272.0)">
              class technical documentation writer the system prompt followed by the user
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:16,140'); seek(1276.0)">
              input, user input and specifying the chain dot in put
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:20,308'); seek(1280.0)">
              I can specify very good amount of output,
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:24,380'); seek(1284.0)">
              like how a technical document writer would write it. So most
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:28,292'); seek(1288.0)">
              of the things are very similar. On top of that we can have an output
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:31,692'); seek(1291.0)">
              parser where we can define. The output parser could
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:34,948'); seek(1294.0)">
              be in a JSON format, or it could be an excel, or how do we
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:37,860'); seek(1297.0)">
              define the format for for a large language model,
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:41,376'); seek(1301.0)">
              the output has to be performed. So by using this
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:21:45,584'); seek(1305.0)">
              input and a prompt template and output parser,
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:21:49,160'); seek(1309.0)">
              you're all set to get an output from the large language model like GPT
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:21:52,600'); seek(1312.0)">
              four. If you have any questions, I'm more happier to talk
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:21:56,208'); seek(1316.0)">
              after the session. Once again, thank you all for
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:21:59,824'); seek(1319.0)">
              this, for your time and listening to the session. If you
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:03,176'); seek(1323.0)">
              have any doubts, you can reach out to me at any point of time.
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:06,008'); seek(1326.0)">
              Thank you all. Have a nice evening, have a good day and rest
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:10,290'); seek(1330.0)">
              of the week.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Deepak%20Karunanidhi%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Deepak%20Karunanidhi%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #CCB87B;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/llms2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #CCB87B;">
                <i class="fe fe-grid me-2"></i>
                See all 28 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Deepak%20Karunanidhi_llm.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Deepak Karunanidhi
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Associate Director - Data Science & Machine Learning @ Novartis
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/akdeepak85" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Deepak Karunanidhi's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Deepak Karunanidhi"
                  data-url="https://www.conf42.com/llms2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/llms2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Large Language Models"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>