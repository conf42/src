<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Develop and Productionize the AI and ML algorithm in Cloud Environment</title>
    <meta name="description" content="Everything Cloud Native and Cloud Security. It came from the Cloud!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Deepak%20Karunanidhi_cloud.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Develop and Productionize the AI and ML algorithm in Cloud Environment | Conf42"/>
    <meta property="og:description" content="Discover the secrets of creating and deploying AI/ML algorithms in the cloud! Join me in examining the complexities of producing cutting-edge models, leveraging my 15 years of experience. Let's look at the future of AI in the cloud, optimizing development workflows for maximum efficiency and impact."/>
    <meta property="og:url" content="https://conf42.com/Cloud_Native_2024_Deepak_Karunanidhi_development_productionize_cloud"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/PROMPT2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Prompt Engineering 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-11-14
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/prompt2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://sreday.com/2024-amsterdam/">
                            SREday Amsterdam
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #7B2726;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Cloud Native 2024 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Everything Cloud Native and Cloud Security. It came from the Cloud!
 -->
              <script>
                const event_date = new Date("2024-03-21T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-03-21T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "YE1YzpKZXE8"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "cMgNOs-0jzo"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrDqcWhv2eREIUiv2_MqQatx" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello all. Thank you for joining for the presentation.", "timestamp": "00:00:24,650", "timestamp_s": 24.0}, {"text": "So myself, Deepak. I\u0027m working as an associate director for", "timestamp": "00:00:27,954", "timestamp_s": 27.0}, {"text": "data science and machine learning here at Novartis. I\u0027m also responsible", "timestamp": "00:00:31,468", "timestamp_s": 31.0}, {"text": "for generative AI product deliverables", "timestamp": "00:00:35,682", "timestamp_s": 35.0}, {"text": "and building novel machine learning algorithms and deploying", "timestamp": "00:00:39,170", "timestamp_s": 39.0}, {"text": "that into production. Today I\u0027m", "timestamp": "00:00:42,594", "timestamp_s": 42.0}, {"text": "going to talk about development of", "timestamp": "00:00:46,098", "timestamp_s": 46.0}, {"text": "an machine learning model and productionizing the ML algorithm in cloud", "timestamp": "00:00:49,788", "timestamp_s": 49.0}, {"text": "environment. So in the recent era, the most of the", "timestamp": "00:00:53,492", "timestamp_s": 53.0}, {"text": "time the data scientists spend time in creating or developing", "timestamp": "00:00:56,964", "timestamp_s": 56.0}, {"text": "machine learning model, it could be starting from linear regression,", "timestamp": "00:01:00,602", "timestamp_s": 60.0}, {"text": "logistic regression, or nave bayesian or", "timestamp": "00:01:04,026", "timestamp_s": 64.0}, {"text": "dradum forest decision trees. Any algorithms let them", "timestamp": "00:01:07,396", "timestamp_s": 67.0}, {"text": "take. Finally, the ERa has been moved from traditional", "timestamp": "00:01:10,872", "timestamp_s": 70.0}, {"text": "or classical machine learning algorithms to large", "timestamp": "00:01:14,766", "timestamp_s": 74.0}, {"text": "language models. So now deploying the large language models", "timestamp": "00:01:19,276", "timestamp_s": 79.0}, {"text": "or productionizing the large language models in cloud is the biggest challenge we have.", "timestamp": "00:01:23,362", "timestamp_s": 83.0}, {"text": "Not only productionizing and how we scale with high", "timestamp": "00:01:28,060", "timestamp_s": 88.0}, {"text": "inference is another important aspect to consider.", "timestamp": "00:01:31,952", "timestamp_s": 91.0}, {"text": "All right, let\u0027s move to the next slide before", "timestamp": "00:01:36,110", "timestamp_s": 96.0}, {"text": "getting into the development of algorithms. First, let\u0027s understand", "timestamp": "00:01:40,368", "timestamp_s": 100.0}, {"text": "what is the development lifecycle of any machine learning algorithm.", "timestamp": "00:01:43,908", "timestamp_s": 103.0}, {"text": "Now, to choose any problem", "timestamp": "00:01:48,530", "timestamp_s": 108.0}, {"text": "statement, we have to clearly define what is a problem statement we", "timestamp": "00:01:52,244", "timestamp_s": 112.0}, {"text": "are trying to solve. It could be an image classification,", "timestamp": "00:01:55,848", "timestamp_s": 115.0}, {"text": "text classification or language translation", "timestamp": "00:01:59,166", "timestamp_s": 119.0}, {"text": "question and answering or predicting the next sentence or document summarization,", "timestamp": "00:02:02,710", "timestamp_s": 122.0}, {"text": "text summarization or there are many tasks involved", "timestamp": "00:02:06,962", "timestamp_s": 126.0}, {"text": "in the problem statement. So before starting any", "timestamp": "00:02:10,546", "timestamp_s": 130.0}, {"text": "machine learning algorithm development, we define the problem", "timestamp": "00:02:14,572", "timestamp_s": 134.0}, {"text": "statement. Once we define the problem statement,", "timestamp": "00:02:17,936", "timestamp_s": 137.0}, {"text": "then we have to start with the collectioning of data that is", "timestamp": "00:02:21,286", "timestamp_s": 141.0}, {"text": "called a data collection or data acquisition and gathering.", "timestamp": "00:02:25,088", "timestamp_s": 145.0}, {"text": "So typically in a domain based when", "timestamp": "00:02:28,870", "timestamp_s": 148.0}, {"text": "we are working in an organization which is specific to", "timestamp": "00:02:33,572", "timestamp_s": 153.0}, {"text": "healthcare or infrastructure financial investment", "timestamp": "00:02:37,940", "timestamp_s": 157.0}, {"text": "banking, there are many scenarios where we cloud", "timestamp": "00:02:43,642", "timestamp_s": 163.0}, {"text": "get some real time data for model training.", "timestamp": "00:02:46,872", "timestamp_s": 166.0}, {"text": "Excuse me, but in the case of when you are trying to do some research,", "timestamp": "00:02:50,630", "timestamp_s": 170.0}, {"text": "then we go for open source data set. But ideally we have to", "timestamp": "00:02:54,664", "timestamp_s": 174.0}, {"text": "get the data set or we have to collect the data set and understand", "timestamp": "00:02:58,248", "timestamp_s": 178.0}, {"text": "the data. As part of understanding the data, we may", "timestamp": "00:03:01,884", "timestamp_s": 181.0}, {"text": "have to do some amount of data preprocessing techniques which we\u0027ll", "timestamp": "00:03:05,372", "timestamp_s": 185.0}, {"text": "see later. After that we have to", "timestamp": "00:03:09,238", "timestamp_s": 189.0}, {"text": "see what is the performance metrics you are going to define", "timestamp": "00:03:12,832", "timestamp_s": 192.0}, {"text": "to achieve the objective which we have defined.", "timestamp": "00:03:17,710", "timestamp_s": 197.0}, {"text": "Let\u0027s take a text classification problem here.", "timestamp": "00:03:20,842", "timestamp_s": 200.0}, {"text": "The classification result can be measured in the performance", "timestamp": "00:03:24,290", "timestamp_s": 204.0}, {"text": "metrics of recall, precision and", "timestamp": "00:03:28,026", "timestamp_s": 208.0}, {"text": "accuracy. So these are all comes under confusion matrix.", "timestamp": "00:03:31,892", "timestamp_s": 211.0}, {"text": "So we have to define what are all the performance metrics we do before", "timestamp": "00:03:35,998", "timestamp_s": 215.0}, {"text": "performing any model training activity. Now further,", "timestamp": "00:03:41,430", "timestamp_s": 221.0}, {"text": "we have to evaluate the procedures. When we", "timestamp": "00:03:45,362", "timestamp_s": 225.0}, {"text": "take a model training process, we divide the data", "timestamp": "00:03:48,828", "timestamp_s": 228.0}, {"text": "set into training, testing and validation.", "timestamp": "00:03:52,076", "timestamp_s": 232.0}, {"text": "So we have a split of around 80 or 680", "timestamp": "00:03:56,250", "timestamp_s": 236.0}, {"text": "2020 or 7015 in", "timestamp": "00:04:00,390", "timestamp_s": 240.0}, {"text": "this kind. When I say the numbers are in percentage,", "timestamp": "00:04:04,608", "timestamp_s": 244.0}, {"text": "right, we have to split the data in train test and validation,", "timestamp": "00:04:07,846", "timestamp_s": 247.0}, {"text": "and we have to see whether the model can perform well with", "timestamp": "00:04:11,578", "timestamp_s": 251.0}, {"text": "the validation and test data. Again, the metrics we use", "timestamp": "00:04:15,108", "timestamp_s": 255.0}, {"text": "based on confusion metrics.", "timestamp": "00:04:19,092", "timestamp_s": 259.0}, {"text": "Next, coming to data preprocessing and cleaning", "timestamp": "00:04:22,870", "timestamp_s": 262.0}, {"text": "when it comes to data preprocessing, okay, we got the data.", "timestamp": "00:04:27,430", "timestamp_s": 267.0}, {"text": "In case of text, what are all the process we follow?", "timestamp": "00:04:31,272", "timestamp_s": 271.0}, {"text": "In case of natural language processing, we remove ASCII", "timestamp": "00:04:34,744", "timestamp_s": 274.0}, {"text": "and special characters and we do stop words removal followed", "timestamp": "00:04:38,578", "timestamp_s": 278.0}, {"text": "by stemming and lamatization. If required, we may go for parts", "timestamp": "00:04:42,098", "timestamp_s": 282.0}, {"text": "of speech tagging or some kind of an embeddings, which may be required", "timestamp": "00:04:46,466", "timestamp_s": 286.0}, {"text": "before building a model. Now,", "timestamp": "00:04:50,758", "timestamp_s": 290.0}, {"text": "once we come to the construction of a baseline model, so when we are using", "timestamp": "00:04:54,830", "timestamp_s": 294.0}, {"text": "models like random forest or bayesian network or", "timestamp": "00:04:59,184", "timestamp_s": 299.0}, {"text": "generative adverseial networks,", "timestamp": "00:05:03,412", "timestamp_s": 303.0}, {"text": "Ada boosting, exg boosting,", "timestamp": "00:05:06,530", "timestamp_s": 306.0}, {"text": "when you\u0027re using models like this, we may have to think about", "timestamp": "00:05:10,058", "timestamp_s": 310.0}, {"text": "the, we don\u0027t need to think about the baseline model because they are already a", "timestamp": "00:05:14,452", "timestamp_s": 314.0}, {"text": "base models where we use a data set to further", "timestamp": "00:05:17,928", "timestamp_s": 317.0}, {"text": "train and we do the prediction or classification with that", "timestamp": "00:05:21,006", "timestamp_s": 321.0}, {"text": "algorithm. But the approach which we are going to talk about,", "timestamp": "00:05:24,792", "timestamp_s": 324.0}, {"text": "baseline model is bit different. What I\u0027m saying now,", "timestamp": "00:05:28,140", "timestamp_s": 328.0}, {"text": "once I walk you through on the slides, you will understand what", "timestamp": "00:05:32,250", "timestamp_s": 332.0}, {"text": "I mean by the typical base models. Then we have", "timestamp": "00:05:35,628", "timestamp_s": 335.0}, {"text": "to fine tune a model. So the fine tuning a model comes", "timestamp": "00:05:39,008", "timestamp_s": 339.0}, {"text": "up with hyperparameter tuning and that techniques also.", "timestamp": "00:05:42,192", "timestamp_s": 342.0}, {"text": "I\u0027ll talk later here, but ideally we", "timestamp": "00:05:45,712", "timestamp_s": 345.0}, {"text": "have to get a pretrained model and fine tune a model to perform a specific", "timestamp": "00:05:49,488", "timestamp_s": 349.0}, {"text": "task. Then how do we deploy and operationalize", "timestamp": "00:05:53,188", "timestamp_s": 353.0}, {"text": "or industrialize the model in cloud environment?", "timestamp": "00:05:56,922", "timestamp_s": 356.0}, {"text": "All right, so we have discussed", "timestamp": "00:06:01,010", "timestamp_s": 361.0}, {"text": "about the lifecycle of ML algorithm. Let\u0027s move to the next slide.", "timestamp": "00:06:04,638", "timestamp_s": 364.0}, {"text": "Before getting into a model training activity,", "timestamp": "00:06:09,350", "timestamp_s": 369.0}, {"text": "I clearly wanted to define the problem. What we are", "timestamp": "00:06:12,894", "timestamp_s": 372.0}, {"text": "going to solve here is natural language processing tasks. As I", "timestamp": "00:06:16,844", "timestamp_s": 376.0}, {"text": "said, natural language processing could be language", "timestamp": "00:06:20,588", "timestamp_s": 380.0}, {"text": "translation or entity recognition, or it could be a spam", "timestamp": "00:06:24,402", "timestamp_s": 384.0}, {"text": "detection, or we do some amount of part of speech tagging,", "timestamp": "00:06:28,022", "timestamp_s": 388.0}, {"text": "text generation or document summarization.", "timestamp": "00:06:32,262", "timestamp_s": 392.0}, {"text": "Question answering there are many natural language processing tasks involved,", "timestamp": "00:06:35,558", "timestamp_s": 395.0}, {"text": "but ideally for this use case or for this demo, I\u0027m going", "timestamp": "00:06:39,798", "timestamp_s": 399.0}, {"text": "to walk you through or take you through on text classification.", "timestamp": "00:06:43,988", "timestamp_s": 403.0}, {"text": "Right, let\u0027s move on.", "timestamp": "00:06:48,210", "timestamp_s": 408.0}, {"text": "Now let\u0027s come to the natural language processing as a concept", "timestamp": "00:06:51,890", "timestamp_s": 411.0}, {"text": "human understand English as a language or any other", "timestamp": "00:06:56,630", "timestamp_s": 416.0}, {"text": "language which he has been known to from the birth.", "timestamp": "00:07:00,072", "timestamp_s": 420.0}, {"text": "But when it comes to natural language processing,", "timestamp": "00:07:04,270", "timestamp_s": 424.0}, {"text": "machine has to understand the language, right?", "timestamp": "00:07:07,610", "timestamp_s": 427.0}, {"text": "So when I say machine, so whatever how humans interpret the", "timestamp": "00:07:10,828", "timestamp_s": 430.0}, {"text": "language under response. Similar way we are building a machine learning", "timestamp": "00:07:14,828", "timestamp_s": 434.0}, {"text": "or AI platform or AI machine to perform", "timestamp": "00:07:18,560", "timestamp_s": 438.0}, {"text": "a specific job. That is what the natural language understanding", "timestamp": "00:07:22,336", "timestamp_s": 442.0}, {"text": "has been given to the machine. So how human have an", "timestamp": "00:07:26,262", "timestamp_s": 446.0}, {"text": "understanding by reading the text. Similarly by", "timestamp": "00:07:30,128", "timestamp_s": 450.0}, {"text": "having or building a model, but to perform like a human", "timestamp": "00:07:35,428", "timestamp_s": 455.0}, {"text": "to have a natural language understanding based on that,", "timestamp": "00:07:40,210", "timestamp_s": 460.0}, {"text": "it determines the answer. Machine understand. Okay,", "timestamp": "00:07:43,668", "timestamp_s": 463.0}, {"text": "this is the natural language understanding I got. Now what is the response I", "timestamp": "00:07:46,790", "timestamp_s": 466.0}, {"text": "have to make which would be in human readable format? Again, we can make", "timestamp": "00:07:50,424", "timestamp_s": 470.0}, {"text": "an output as a natural language generation, which could be text abstraction,", "timestamp": "00:07:54,696", "timestamp_s": 474.0}, {"text": "text summarization, or we can do any natural language classification job.", "timestamp": "00:07:58,562", "timestamp_s": 478.0}, {"text": "This is what I\u0027m going to walk you through. So right now I put a", "timestamp": "00:08:03,450", "timestamp_s": 483.0}, {"text": "Bert model here. If you could see in the center of the picture. But yes,", "timestamp": "00:08:06,924", "timestamp_s": 486.0}, {"text": "I\u0027ll elaborately talk once I walk through the next couple of slides.", "timestamp": "00:08:10,812", "timestamp_s": 490.0}, {"text": "Ideally we pass an input and we ask the model to", "timestamp": "00:08:15,710", "timestamp_s": 495.0}, {"text": "classify. Then here it could be a spam", "timestamp": "00:08:19,712", "timestamp_s": 499.0}, {"text": "or ham. So based on that it performs it.", "timestamp": "00:08:23,066", "timestamp_s": 503.0}, {"text": "Let\u0027s move on to the next slide. Okay, hugging face", "timestamp": "00:08:27,890", "timestamp_s": 507.0}, {"text": "now you would have heard this is getting very", "timestamp": "00:08:32,470", "timestamp_s": 512.0}, {"text": "popular. Now. Hugging face is a framework or library to solve", "timestamp": "00:08:35,928", "timestamp_s": 515.0}, {"text": "most of the NLP problems. They have built", "timestamp": "00:08:39,662", "timestamp_s": 519.0}, {"text": "40,000 models around. They have built by now as of today", "timestamp": "00:08:44,710", "timestamp_s": 524.0}, {"text": "which are having all as a pretrained model and some amount of instac", "timestamp": "00:08:48,780", "timestamp_s": 528.0}, {"text": "based model or fine tuned model also available.", "timestamp": "00:08:52,706", "timestamp_s": 532.0}, {"text": "Now we are going to use an agingface platform to perform our model", "timestamp": "00:08:56,890", "timestamp_s": 536.0}, {"text": "training. Okay, now as I said, agingfare is", "timestamp": "00:09:00,656", "timestamp_s": 540.0}, {"text": "the most popular framework which has been used by right", "timestamp": "00:09:04,464", "timestamp_s": 544.0}, {"text": "now, sorry. It has around 4000 models or which", "timestamp": "00:09:09,088", "timestamp_s": 549.0}, {"text": "can be deployed in cloud which is based on Pytorch or Tensorflow.", "timestamp": "00:09:14,052", "timestamp_s": 554.0}, {"text": "Even Keras library are supported in hugging phase.", "timestamp": "00:09:18,570", "timestamp_s": 558.0}, {"text": "Ideally we use a transformer based architecture models to", "timestamp": "00:09:21,738", "timestamp_s": 561.0}, {"text": "develop our models. Now when you are", "timestamp": "00:09:25,252", "timestamp_s": 565.0}, {"text": "talking about hugging phase, as I", "timestamp": "00:09:28,728", "timestamp_s": 568.0}, {"text": "said, there are 4000 pretrained model and for each task they", "timestamp": "00:09:31,784", "timestamp_s": 571.0}, {"text": "have a separate model. Let\u0027s say when we want to perform a text classification they", "timestamp": "00:09:35,128", "timestamp_s": 575.0}, {"text": "have Bert, Robota, distal Bert XLM,", "timestamp": "00:09:38,668", "timestamp_s": 578.0}, {"text": "Robota. Similarly for language translation they have Marian,", "timestamp": "00:09:41,618", "timestamp_s": 581.0}, {"text": "Mt. Bard and T, five. For V and chat bots", "timestamp": "00:09:45,122", "timestamp_s": 585.0}, {"text": "they have GPT, GPT-2 and now we", "timestamp": "00:09:49,362", "timestamp_s": 589.0}, {"text": "would have got GPT-3 and four as well.", "timestamp": "00:09:52,528", "timestamp_s": 592.0}, {"text": "When it comes to named entity recognition. Again, we can use the", "timestamp": "00:09:55,312", "timestamp_s": 595.0}, {"text": "Bert model. Ideally, I\u0027m going to talk more about the Bert model.", "timestamp": "00:09:58,848", "timestamp_s": 598.0}, {"text": "The reason why I\u0027ve kept Bert here is Bert is nothing but a", "timestamp": "00:10:02,276", "timestamp_s": 602.0}, {"text": "bi directional encoder representation for transformer.", "timestamp": "00:10:05,988", "timestamp_s": 605.0}, {"text": "It is based on the transform architecture or all the attention", "timestamp": "00:10:09,738", "timestamp_s": 609.0}, {"text": "is you need based on that they have a transform architecture.", "timestamp": "00:10:13,242", "timestamp_s": 613.0}, {"text": "In that way, Bert has been built once", "timestamp": "00:10:17,038", "timestamp_s": 617.0}, {"text": "it came in 2018 or 19. Then it shook the", "timestamp": "00:10:20,632", "timestamp_s": 620.0}, {"text": "industry to think about the whole machine learning development has been", "timestamp": "00:10:24,248", "timestamp_s": 624.0}, {"text": "taken into a next space or next level.", "timestamp": "00:10:28,344", "timestamp_s": 628.0}, {"text": "Okay, now, so we are going to use BERT model and we", "timestamp": "00:10:31,450", "timestamp_s": 631.0}, {"text": "are going to fine tune the BerT model. Bert model has", "timestamp": "00:10:34,764", "timestamp_s": 634.0}, {"text": "comes up with its own strength like it is based", "timestamp": "00:10:38,316", "timestamp_s": 638.0}, {"text": "on masked language modeling and next sentence prediction. So if you want to", "timestamp": "00:10:41,772", "timestamp_s": 641.0}, {"text": "know more about the Bert model, I have a separate video. Please go", "timestamp": "00:10:45,408", "timestamp_s": 645.0}, {"text": "and have a look into that. Now. When coming to", "timestamp": "00:10:48,992", "timestamp_s": 648.0}, {"text": "the Bert now, Bert can perform multiple tasks,", "timestamp": "00:10:52,784", "timestamp_s": 652.0}, {"text": "but as a general model, you can do a downstream", "timestamp": "00:10:56,122", "timestamp_s": 656.0}, {"text": "job to make it specific to a domain or specific to", "timestamp": "00:11:00,250", "timestamp_s": 660.0}, {"text": "a task which has to be performed. So yes, Bird can perform", "timestamp": "00:11:03,528", "timestamp_s": 663.0}, {"text": "text classification or text generation or next", "timestamp": "00:11:07,016", "timestamp_s": 667.0}, {"text": "sentence productionize question and answering.", "timestamp": "00:11:10,712", "timestamp_s": 670.0}, {"text": "Similar like a chatbot, it can also perform. But how do we fine", "timestamp": "00:11:15,510", "timestamp_s": 675.0}, {"text": "tune the model? Right, we have a prechained model, then we fine", "timestamp": "00:11:19,112", "timestamp_s": 679.0}, {"text": "tune a model based on the data set. Then we deploy the model.", "timestamp": "00:11:22,252", "timestamp_s": 682.0}, {"text": "We deploy the fine tuned model in production in a cloud environment.", "timestamp": "00:11:26,604", "timestamp_s": 686.0}, {"text": "All right, let me walk you through the as I said,", "timestamp": "00:11:31,230", "timestamp_s": 691.0}, {"text": "we are going to take the text classification example. Our objective is", "timestamp": "00:11:34,832", "timestamp_s": 694.0}, {"text": "to understand the sentiment. Let\u0027s say this is an amazing", "timestamp": "00:11:38,528", "timestamp_s": 698.0}, {"text": "model. Then we are going to say whether it is positive or negative.", "timestamp": "00:11:42,064", "timestamp_s": 702.0}, {"text": "That\u0027s what the classification job does. Now, I\u0027m taking a binary classification", "timestamp": "00:11:45,770", "timestamp_s": 705.0}, {"text": "here. Going to call that as a positive or negative here.", "timestamp": "00:11:49,834", "timestamp_s": 709.0}, {"text": "So this is an example I\u0027m going to take. Now I\u0027m going to walk you", "timestamp": "00:11:53,156", "timestamp_s": 713.0}, {"text": "through on how we can perform model chaining.", "timestamp": "00:11:56,456", "timestamp_s": 716.0}, {"text": "But before getting into model chaining, I want to tell you", "timestamp": "00:12:00,974", "timestamp_s": 720.0}, {"text": "about ML Flow. What is ML Flow?", "timestamp": "00:12:04,552", "timestamp_s": 724.0}, {"text": "ML Flow is a platform or is", "timestamp": "00:12:08,290", "timestamp_s": 728.0}, {"text": "an API library which can be injected into your model", "timestamp": "00:12:11,708", "timestamp_s": 731.0}, {"text": "development process to perform all the model tracking and", "timestamp": "00:12:15,420", "timestamp_s": 735.0}, {"text": "model experiments. What I mean by that,", "timestamp": "00:12:19,152", "timestamp_s": 739.0}, {"text": "we can build many models and many iteration", "timestamp": "00:12:22,912", "timestamp_s": 742.0}, {"text": "of models reasoning. We have to fine tune the model. We have", "timestamp": "00:12:26,662", "timestamp_s": 746.0}, {"text": "to change the parameters of the model. Once we keep changing", "timestamp": "00:12:29,968", "timestamp_s": 749.0}, {"text": "the parameters of the model every time, model will have a different", "timestamp": "00:12:33,338", "timestamp_s": 753.0}, {"text": "outputs. What could be an output here? It could be an precision", "timestamp": "00:12:36,916", "timestamp_s": 756.0}, {"text": "recall accuracy, f one score. F two score. There are", "timestamp": "00:12:42,234", "timestamp_s": 762.0}, {"text": "many elements we consider as part of model development activity.", "timestamp": "00:12:45,688", "timestamp_s": 765.0}, {"text": "So there could be a scenario if in", "timestamp": "00:12:50,126", "timestamp_s": 770.0}, {"text": "case of text classification, we have seen positive or negative, how much", "timestamp": "00:12:54,568", "timestamp_s": 774.0}, {"text": "I\u0027m more inclined to positive. If I always wants a positive,", "timestamp": "00:12:58,492", "timestamp_s": 778.0}, {"text": "I should not miss any false negative means. If algorithm", "timestamp": "00:13:02,642", "timestamp_s": 782.0}, {"text": "says it is a", "timestamp": "00:13:07,090", "timestamp_s": 787.0}, {"text": "negative, but actually it is a", "timestamp": "00:13:10,590", "timestamp_s": 790.0}, {"text": "positive, I should not miss these kind of scenarios,", "timestamp": "00:13:14,320", "timestamp_s": 794.0}, {"text": "right? So if I should not", "timestamp": "00:13:18,110", "timestamp_s": 798.0}, {"text": "miss any false negative, then I\u0027ll be focusing more on", "timestamp": "00:13:21,284", "timestamp_s": 801.0}, {"text": "recall. Similarly, when the algorithm", "timestamp": "00:13:25,810", "timestamp_s": 805.0}, {"text": "says okay, it is a", "timestamp": "00:13:29,770", "timestamp_s": 809.0}, {"text": "positive and algorithm says it is a negative,", "timestamp": "00:13:33,060", "timestamp_s": 813.0}, {"text": "then again it comes under false positive, right? So it misses the crucial", "timestamp": "00:13:36,590", "timestamp_s": 816.0}, {"text": "element. Right. Now, to handle these", "timestamp": "00:13:40,526", "timestamp_s": 820.0}, {"text": "kind of scenarios, we need a tracking platform which is called ML flow,", "timestamp": "00:13:44,168", "timestamp_s": 824.0}, {"text": "which is used to record and track all the experiments", "timestamp": "00:13:48,386", "timestamp_s": 828.0}, {"text": "along with the results. But I can also show", "timestamp": "00:13:52,226", "timestamp_s": 832.0}, {"text": "you a quick sample on how the code looks like by having an", "timestamp": "00:13:55,548", "timestamp_s": 835.0}, {"text": "ML flow and without an ML flow before", "timestamp": "00:13:59,008", "timestamp_s": 839.0}, {"text": "that, this is how the model experiment looks like.", "timestamp": "00:14:02,816", "timestamp_s": 842.0}, {"text": "When I talk about model experiments,", "timestamp": "00:14:05,760", "timestamp_s": 845.0}, {"text": "let\u0027s say I\u0027m going to train an algorithm and I may", "timestamp": "00:14:08,990", "timestamp_s": 848.0}, {"text": "train n number of times. So I wanted to know based", "timestamp": "00:14:12,756", "timestamp_s": 852.0}, {"text": "on which seed and which parameter my model really performed.", "timestamp": "00:14:16,276", "timestamp_s": 856.0}, {"text": "Well, considering the scenario, I\u0027ll take all", "timestamp": "00:14:19,578", "timestamp_s": 859.0}, {"text": "the historical experiments which I performed that", "timestamp": "00:14:22,932", "timestamp_s": 862.0}, {"text": "would be tracked in ML flow, which you can see each,", "timestamp": "00:14:26,328", "timestamp_s": 866.0}, {"text": "along with the timestamp, you can see the model which I ran, and if", "timestamp": "00:14:30,790", "timestamp_s": 870.0}, {"text": "you go deep and along with that features, what kind of features I configured,", "timestamp": "00:14:34,408", "timestamp_s": 874.0}, {"text": "then I get an accuracy, precision, recall value, whatever. I have that in", "timestamp": "00:14:39,042", "timestamp_s": 879.0}, {"text": "a metrics for confusion matrix. This really helps", "timestamp": "00:14:42,748", "timestamp_s": 882.0}, {"text": "in performing in multiple iteration of experiments and to get the tracking", "timestamp": "00:14:46,562", "timestamp_s": 886.0}, {"text": "of the models. All right, now coming", "timestamp": "00:14:50,838", "timestamp_s": 890.0}, {"text": "to the code. So typically what we do to train and model,", "timestamp": "00:14:54,192", "timestamp_s": 894.0}, {"text": "we load an input data and we extract some of the features.", "timestamp": "00:14:57,504", "timestamp_s": 897.0}, {"text": "And I\u0027m using an Ingrams to extract the features. Then I\u0027m going to", "timestamp": "00:15:01,318", "timestamp_s": 901.0}, {"text": "train and model, and I\u0027m going to compute the accuracy. Now,", "timestamp": "00:15:04,548", "timestamp_s": 904.0}, {"text": "what version of my code was this result from? No idea.", "timestamp": "00:15:08,212", "timestamp_s": 908.0}, {"text": "To perform this, we need an ML flow tracking,", "timestamp": "00:15:12,610", "timestamp_s": 912.0}, {"text": "which is ideally used to track all the experiment", "timestamp": "00:15:15,998", "timestamp_s": 915.0}, {"text": "results. Now let\u0027s see how the code looks like with", "timestamp": "00:15:19,326", "timestamp_s": 919.0}, {"text": "ML flow. So with pythonic way, by having a", "timestamp": "00:15:23,048", "timestamp_s": 923.0}, {"text": "packages import having ML flow and ML Tensorflow,", "timestamp": "00:15:26,760", "timestamp_s": 926.0}, {"text": "then we say ML flow start run as a run.", "timestamp": "00:15:30,962", "timestamp_s": 930.0}, {"text": "Then we start to log the metrics. Then we keep training our", "timestamp": "00:15:34,636", "timestamp_s": 934.0}, {"text": "model along with fine tuning the parameters. In this", "timestamp": "00:15:38,816", "timestamp_s": 938.0}, {"text": "way, everything get stored in a", "timestamp": "00:15:42,192", "timestamp_s": 942.0}, {"text": "database, ideally in cloud environment. We configure with an", "timestamp": "00:15:45,744", "timestamp_s": 945.0}, {"text": "S three bucket of AWS service, Amazon Web service.", "timestamp": "00:15:49,408", "timestamp_s": 949.0}, {"text": "Then once the setup is done, then we", "timestamp": "00:15:53,490", "timestamp_s": 953.0}, {"text": "add an implementation accordingly to make an ML flow start,", "timestamp": "00:15:57,348", "timestamp_s": 957.0}, {"text": "and then iterate the model multiple times", "timestamp": "00:16:00,980", "timestamp_s": 960.0}, {"text": "and keep having experiment results get stored.", "timestamp": "00:16:04,840", "timestamp_s": 964.0}, {"text": "ML flow comes with a default UI where all the model", "timestamp": "00:16:08,150", "timestamp_s": 968.0}, {"text": "experiment can be visualized, which I\u0027ve shown you in the earlier", "timestamp": "00:16:11,496", "timestamp_s": 971.0}, {"text": "slide. Now coming to the model training.", "timestamp": "00:16:15,182", "timestamp_s": 975.0}, {"text": "So now the reason why I kept explaining about the experiment and", "timestamp": "00:16:19,116", "timestamp_s": 979.0}, {"text": "tracking and all. When you start the model training framework,", "timestamp": "00:16:23,132", "timestamp_s": 983.0}, {"text": "you should have all the experiments needs to be tracked somehow.", "timestamp": "00:16:26,498", "timestamp_s": 986.0}, {"text": "Right now I\u0027ve taken a small example code of", "timestamp": "00:16:29,798", "timestamp_s": 989.0}, {"text": "how do we perform the model training activity.", "timestamp": "00:16:33,456", "timestamp_s": 993.0}, {"text": "So ideally we are going to use a BERT model which you can see somewhere,", "timestamp": "00:16:37,150", "timestamp_s": 997.0}, {"text": "which I am using a pre trained BERT model. Then I\u0027m using", "timestamp": "00:16:40,822", "timestamp_s": 1000.0}, {"text": "auto model for sequence classification reasoning. I can", "timestamp": "00:16:44,132", "timestamp_s": 1004.0}, {"text": "put this instead of BeRT model. I can try with robota", "timestamp": "00:16:47,476", "timestamp_s": 1007.0}, {"text": "XLM, robota distal, Bird, Biobird,", "timestamp": "00:16:51,546", "timestamp_s": 1011.0}, {"text": "GPT-2 or GPT-3 any of the pretrained models", "timestamp": "00:16:55,010", "timestamp_s": 1015.0}, {"text": "I can put here. So when I build a framework I", "timestamp": "00:16:58,718", "timestamp_s": 1018.0}, {"text": "have to call the transformer library. Then I use auto tokenizer", "timestamp": "00:17:02,488", "timestamp_s": 1022.0}, {"text": "and auto model for sequence classification and load the pretrained", "timestamp": "00:17:06,398", "timestamp_s": 1026.0}, {"text": "model and in the next line I\u0027ll show the code further.", "timestamp": "00:17:10,098", "timestamp_s": 1030.0}, {"text": "But before that we are using a GPU machine to run all this model", "timestamp": "00:17:13,826", "timestamp_s": 1033.0}, {"text": "training activity because this is a large language model.", "timestamp": "00:17:17,756", "timestamp_s": 1037.0}, {"text": "Then again when we are doing a fine tuning, it recurs a GPU machine", "timestamp": "00:17:21,504", "timestamp_s": 1041.0}, {"text": "with CUDA library to perform this activity,", "timestamp": "00:17:25,030", "timestamp_s": 1045.0}, {"text": "the model two device and the torch device,", "timestamp": "00:17:29,070", "timestamp_s": 1049.0}, {"text": "by using a CUDA library specifies", "timestamp": "00:17:32,586", "timestamp_s": 1052.0}, {"text": "the GPU would be let\u0027s say I\u0027m using an eight GPU, the processor", "timestamp": "00:17:36,698", "timestamp_s": 1056.0}, {"text": "would get split into multiple GPUs and it starts to", "timestamp": "00:17:40,778", "timestamp_s": 1060.0}, {"text": "perform the model training activity. The reason why we\u0027re using", "timestamp": "00:17:44,950", "timestamp_s": 1064.0}, {"text": "auto model so this is a framework which we can build and", "timestamp": "00:17:48,312", "timestamp_s": 1068.0}, {"text": "by passing in the command line prompt the model framework.", "timestamp": "00:17:52,710", "timestamp_s": 1072.0}, {"text": "Based on that we can further train the model.", "timestamp": "00:17:56,546", "timestamp_s": 1076.0}, {"text": "Now coming to the model train", "timestamp": "00:18:00,650", "timestamp_s": 1080.0}, {"text": "which you can see is an abstract class to", "timestamp": "00:18:04,076", "timestamp_s": 1084.0}, {"text": "perform the model training which has been given by transformer", "timestamp": "00:18:07,308", "timestamp_s": 1087.0}, {"text": "architecture. Then I can start training", "timestamp": "00:18:10,838", "timestamp_s": 1090.0}, {"text": "model will get trained. Then I can keep changing my training.", "timestamp": "00:18:15,024", "timestamp_s": 1095.0}, {"text": "Hyperparameters it\u0027s based on learning rate, number of", "timestamp": "00:18:18,416", "timestamp_s": 1098.0}, {"text": "epochs and lock size.", "timestamp": "00:18:21,908", "timestamp_s": 1101.0}, {"text": "And there are additional parameters which we can use which", "timestamp": "00:18:25,892", "timestamp_s": 1105.0}, {"text": "mainly we use learning rate and number of epochs which", "timestamp": "00:18:30,260", "timestamp_s": 1110.0}, {"text": "is used for fine tuning the model parameters.", "timestamp": "00:18:34,392", "timestamp_s": 1114.0}, {"text": "Right? Now, once we train the model,", "timestamp": "00:18:37,774", "timestamp_s": 1117.0}, {"text": "then we use a data loader, then we use a model fit to", "timestamp": "00:18:41,384", "timestamp_s": 1121.0}, {"text": "train the model and along with that hyperparameters tuning,", "timestamp": "00:18:45,672", "timestamp_s": 1125.0}, {"text": "the fine tuning job is nothing but take a pretrained model", "timestamp": "00:18:50,090", "timestamp_s": 1130.0}, {"text": "and fine tune the model to a specific task. In Bert we are", "timestamp": "00:18:54,410", "timestamp_s": 1134.0}, {"text": "going to perform a text classification for that", "timestamp": "00:18:57,808", "timestamp_s": 1137.0}, {"text": "give an input data set and keep training the model", "timestamp": "00:19:01,152", "timestamp_s": 1141.0}, {"text": "until you get the accuracy or recall", "timestamp": "00:19:05,248", "timestamp_s": 1145.0}, {"text": "and precision to a certain benchmark. 85% or 95%.", "timestamp": "00:19:09,158", "timestamp_s": 1149.0}, {"text": "How much would you require based on the problem definition or problem statement", "timestamp": "00:19:12,996", "timestamp_s": 1152.0}, {"text": "which you defined? Now we", "timestamp": "00:19:17,066", "timestamp_s": 1157.0}, {"text": "have done with the model training, then we have to evaluate the model.", "timestamp": "00:19:20,628", "timestamp_s": 1160.0}, {"text": "As I said at the beginning of the conversation,", "timestamp": "00:19:23,652", "timestamp_s": 1163.0}, {"text": "when we get the model we have to do the model evaluation metrics,", "timestamp": "00:19:27,946", "timestamp_s": 1167.0}, {"text": "then split the data into train test and validation.", "timestamp": "00:19:31,246", "timestamp_s": 1171.0}, {"text": "Then once the training has been performed with the", "timestamp": "00:19:34,894", "timestamp_s": 1174.0}, {"text": "train data, we can use evaluation or validation", "timestamp": "00:19:38,216", "timestamp_s": 1178.0}, {"text": "data set to evaluate the model. Then further we can", "timestamp": "00:19:42,162", "timestamp_s": 1182.0}, {"text": "use a prediction logic which could be based on logics", "timestamp": "00:19:45,660", "timestamp_s": 1185.0}, {"text": "or softmax classifier or neural network in the behind. I don\u0027t", "timestamp": "00:19:49,186", "timestamp_s": 1189.0}, {"text": "want to go deeper in that. Our idea is to define a framework,", "timestamp": "00:19:52,614", "timestamp_s": 1192.0}, {"text": "do a model training and productionize the model, or deploy the model", "timestamp": "00:19:56,910", "timestamp_s": 1196.0}, {"text": "in cloud. That\u0027s where our focus is. If anything,", "timestamp": "00:20:00,880", "timestamp_s": 1200.0}, {"text": "please feel free to reach out to me after the presentation or after", "timestamp": "00:20:03,988", "timestamp_s": 1203.0}, {"text": "this live event. Then we can discuss or take the conversation", "timestamp": "00:20:07,668", "timestamp_s": 1207.0}, {"text": "further. Now talked about model training and model evaluation", "timestamp": "00:20:11,626", "timestamp_s": 1211.0}, {"text": "and we can use model accuracy, sorry, model prediction", "timestamp": "00:20:15,838", "timestamp_s": 1215.0}, {"text": "and based on the model prediction we can compute the confusion", "timestamp": "00:20:19,630", "timestamp_s": 1219.0}, {"text": "matrix score that can be used for", "timestamp": "00:20:23,614", "timestamp_s": 1223.0}, {"text": "taking the model to production. Now how do", "timestamp": "00:20:27,356", "timestamp_s": 1227.0}, {"text": "we take the model to production? That is another interesting problem which", "timestamp": "00:20:31,292", "timestamp_s": 1231.0}, {"text": "can be solved by using Pytorch serving,", "timestamp": "00:20:35,116", "timestamp_s": 1235.0}, {"text": "right? So now", "timestamp": "00:20:39,150", "timestamp_s": 1239.0}, {"text": "when we say PyTorch serving of ML models,", "timestamp": "00:20:42,624", "timestamp_s": 1242.0}, {"text": "Pytorch serving is nothing. But we have built a", "timestamp": "00:20:47,070", "timestamp_s": 1247.0}, {"text": "model in Pytorch and how do we serve the model in production to", "timestamp": "00:20:50,368", "timestamp_s": 1250.0}, {"text": "achieve the low latency and scalability problem,", "timestamp": "00:20:54,132", "timestamp_s": 1254.0}, {"text": "right? So as you could see, once you train", "timestamp": "00:20:57,810", "timestamp_s": 1257.0}, {"text": "the model and you evaluate the model, and if you feel the model", "timestamp": "00:21:01,300", "timestamp_s": 1261.0}, {"text": "is good enough to take to higher environments,", "timestamp": "00:21:05,030", "timestamp_s": 1265.0}, {"text": "then you have to convert the model into Mar file.", "timestamp": "00:21:08,230", "timestamp_s": 1268.0}, {"text": "So which is nothing but a torch serve which we\u0027ve been", "timestamp": "00:21:12,078", "timestamp_s": 1272.0}, {"text": "using. And we have a model store where we have to convert the model", "timestamp": "00:21:15,992", "timestamp_s": 1275.0}, {"text": "into Mar file and the model has to be deployed", "timestamp": "00:21:19,980", "timestamp_s": 1279.0}, {"text": "into a pytot serving inference place.", "timestamp": "00:21:23,682", "timestamp_s": 1283.0}, {"text": "So there is a logic which we have to follow. We have to build the", "timestamp": "00:21:27,050", "timestamp_s": 1287.0}, {"text": "docker image. Once we have a model, then we have to deploy", "timestamp": "00:21:30,688", "timestamp_s": 1290.0}, {"text": "that into the ECs that we can take in the next slide.", "timestamp": "00:21:34,246", "timestamp_s": 1294.0}, {"text": "But overall torch serving will help us to", "timestamp": "00:21:37,878", "timestamp_s": 1297.0}, {"text": "deploy the Mar files inside a model store. Then it will have", "timestamp": "00:21:41,344", "timestamp_s": 1301.0}, {"text": "an inference API so that via invoking an API we can", "timestamp": "00:21:44,884", "timestamp_s": 1304.0}, {"text": "call the model prediction results. So internally this has", "timestamp": "00:21:48,548", "timestamp_s": 1308.0}, {"text": "an architecture where you can have multiple model can be served", "timestamp": "00:21:52,228", "timestamp_s": 1312.0}, {"text": "in a single python serving instance.", "timestamp": "00:21:56,122", "timestamp_s": 1316.0}, {"text": "Right. Now, ideally this can be used for an API invocation", "timestamp": "00:21:59,110", "timestamp_s": 1319.0}, {"text": "to call all the models which has been deployed inside the Python serving model", "timestamp": "00:22:02,718", "timestamp_s": 1322.0}, {"text": "again. For more questions please feel free to reach out to me after the", "timestamp": "00:22:06,632", "timestamp_s": 1326.0}, {"text": "event. Now once we have the", "timestamp": "00:22:10,108", "timestamp_s": 1330.0}, {"text": "Pytotch serving, this is the most interesting piece. So we", "timestamp": "00:22:13,164", "timestamp_s": 1333.0}, {"text": "have trained and deploy or sorry productionize the model", "timestamp": "00:22:17,452", "timestamp_s": 1337.0}, {"text": "in cloud right now whenever", "timestamp": "00:22:21,164", "timestamp_s": 1341.0}, {"text": "you talk about the model training activity. Once the Mar file", "timestamp": "00:22:24,482", "timestamp_s": 1344.0}, {"text": "is generated, we have to push inside the S three bucket because all", "timestamp": "00:22:28,454", "timestamp_s": 1348.0}, {"text": "the model can be stored in s three bucket because that", "timestamp": "00:22:31,728", "timestamp_s": 1351.0}, {"text": "is a huge file and is a blob storage or it\u0027s a file storage.", "timestamp": "00:22:34,964", "timestamp_s": 1354.0}, {"text": "S three bucket from Amazon can be used to store the models.", "timestamp": "00:22:37,786", "timestamp_s": 1357.0}, {"text": "Then we can use an ECR elastic container", "timestamp": "00:22:41,898", "timestamp_s": 1361.0}, {"text": "registry to push the model. Or we", "timestamp": "00:22:45,038", "timestamp_s": 1365.0}, {"text": "have to push the image into an ECS or EC two instance.", "timestamp": "00:22:48,328", "timestamp_s": 1368.0}, {"text": "There we could see we build every model as a docker image.", "timestamp": "00:22:51,870", "timestamp_s": 1371.0}, {"text": "Then once we have a docker image, then we have an EBS elastic", "timestamp": "00:22:56,318", "timestamp_s": 1376.0}, {"text": "load balancing server or EBS storage is used in", "timestamp": "00:23:01,530", "timestamp_s": 1381.0}, {"text": "the backend to connect to the ECS and the model can", "timestamp": "00:23:05,004", "timestamp_s": 1385.0}, {"text": "be stored over there in the model store. Then from there via inference", "timestamp": "00:23:08,268", "timestamp_s": 1388.0}, {"text": "API we can pick the model by writing a python function or Python", "timestamp": "00:23:12,038", "timestamp_s": 1392.0}, {"text": "code that will be deployed as a docker image or docker container.", "timestamp": "00:23:15,542", "timestamp_s": 1395.0}, {"text": "Then it can pull the model and it can perform the inference logic or", "timestamp": "00:23:19,238", "timestamp_s": 1399.0}, {"text": "it can do the prediction. So now you can see how the old model", "timestamp": "00:23:22,788", "timestamp_s": 1402.0}, {"text": "is getting developed from the time we", "timestamp": "00:23:26,644", "timestamp_s": 1406.0}, {"text": "start the model development to productionize the algorithms.", "timestamp": "00:23:30,228", "timestamp_s": 1410.0}, {"text": "Now let\u0027s talk more about the AWS cloud environment.", "timestamp": "00:23:35,090", "timestamp_s": 1415.0}, {"text": "So already we have a sage maker, but let\u0027s not use a sage maker", "timestamp": "00:23:38,318", "timestamp_s": 1418.0}, {"text": "or sage maker endpoint. But ideally we are saving a cost literally by", "timestamp": "00:23:42,094", "timestamp_s": 1422.0}, {"text": "having ECs, ECR and S three bucket and perform", "timestamp": "00:23:46,488", "timestamp_s": 1426.0}, {"text": "a model training in a GPU machine. And once the model has been trained,", "timestamp": "00:23:50,236", "timestamp_s": 1430.0}, {"text": "push the file with and we can write", "timestamp": "00:23:55,050", "timestamp_s": 1435.0}, {"text": "some scripts to push the trained model into S", "timestamp": "00:23:58,412", "timestamp_s": 1438.0}, {"text": "three bucket. Then once that is done we can use Jenkins", "timestamp": "00:24:01,868", "timestamp_s": 1441.0}, {"text": "Ci CD pipeline to", "timestamp": "00:24:05,350", "timestamp_s": 1445.0}, {"text": "push the docker image into an ECS container which underneath it", "timestamp": "00:24:08,752", "timestamp_s": 1448.0}, {"text": "uses Fargate or EC two. In my case I refer to EC two,", "timestamp": "00:24:12,768", "timestamp_s": 1452.0}, {"text": "right? As I said, once a model has been built, all the models", "timestamp": "00:24:16,900", "timestamp_s": 1456.0}, {"text": "will get stored under model store. Once the models are", "timestamp": "00:24:20,954", "timestamp_s": 1460.0}, {"text": "stored under model store, there is a management API, an inference API", "timestamp": "00:24:24,808", "timestamp_s": 1464.0}, {"text": "which by using an pytotch serving command where we", "timestamp": "00:24:28,702", "timestamp_s": 1468.0}, {"text": "can provide inference API for the applications", "timestamp": "00:24:33,528", "timestamp_s": 1473.0}, {"text": "to consume. I think this is a holistic step involved in", "timestamp": "00:24:38,658", "timestamp_s": 1478.0}, {"text": "creating the model or developing the machine learning algorithm", "timestamp": "00:24:42,316", "timestamp_s": 1482.0}, {"text": "or deploying the model in cloud environment", "timestamp": "00:24:45,810", "timestamp_s": 1485.0}, {"text": "right. Any further questions? As I said, you can always reach out to me after", "timestamp": "00:24:49,370", "timestamp_s": 1489.0}, {"text": "the event. All right. Thank you for", "timestamp": "00:24:53,052", "timestamp_s": 1493.0}, {"text": "watching my video. Have they had.", "timestamp": "00:24:57,596", "timestamp_s": 1497.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'YE1YzpKZXE8',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Develop and Productionize the AI and ML algorithm in Cloud Environment
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Discover the secrets of creating and deploying AI/ML algorithms in the cloud! Join me in examining the complexities of producing cutting-edge models, leveraging my 15 years of experience. Let&rsquo;s look at the future of AI in the cloud, optimizing development workflows for maximum efficiency and impact.</p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Deepak is an associate director for data science and machine learning at Novartis. Today he will talk about development of an machine learning model and productionizing the ML algorithm in cloud environment.

              </li>
              
              <li>
                Before starting any machine learning algorithm development, we define the problem statement. What we are going to solve here is natural language processing tasks. When we take a model training process, we divide the data set into training, testing and validation. Then how do we deploy and operationalize or industrialize the model in cloud environment?

              </li>
              
              <li>
                When it comes to natural language processing, machine has to understand the language. Similar way we are building a machine learning or AI platform or AI machine to perform a specific job. Now what is the response I have to make which would be in human readable format?

              </li>
              
              <li>
                Hugging face is a framework or library to solve most of the NLP problems. It has around 4000 models or which can be deployed in cloud. Ideally we use a transformer based architecture models to develop our models. How do we fine tune the model?

              </li>
              
              <li>
                ML Flow is a platform or is an API library which can be injected into your model development process to perform all the model tracking and model experiments. This really helps in performing in multiple iteration of experiments and to get the tracking of the models.

              </li>
              
              <li>
                 ML flow tracking is ideally used to track all the experiment results. Everything get stored in a database, ideally in cloud environment. ML flow comes with a default UI where all the model experiment can be visualized.

              </li>
              
              <li>
                Our idea is to define a framework, do a model training and productionize the model, or deploy the model in cloud. Once the training has been performed with the train data, we can use evaluation or validation data set to evaluate the model.

              </li>
              
              <li>
                How do we take the model to production? That is another interesting problem which can be solved by using Pytorch serving. This has an architecture where you can have multiple model can be served in a single python serving instance. For more questions please feel free to reach out to me after the event.

              </li>
              
              <li>
                Once a model has been built, all the models will get stored under model store. From there via inference API we can pick the model. This is a holistic step involved in creating the model or developing the machine learning algorithm or deploying the model in cloud environment.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/YE1YzpKZXE8.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:24,650'); seek(24.0)">
              Hello all. Thank you for joining for the presentation.
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:27,954'); seek(27.0)">
              So myself, Deepak. I'm working as an associate director for
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:31,468'); seek(31.0)">
              data science and machine learning here at Novartis. I'm also responsible
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:35,682'); seek(35.0)">
              for generative AI product deliverables
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:39,170'); seek(39.0)">
              and building novel machine learning algorithms and deploying
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:42,594'); seek(42.0)">
              that into production. Today I'm
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:46,098'); seek(46.0)">
              going to talk about development of
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:49,788'); seek(49.0)">
              an machine learning model and productionizing the ML algorithm in cloud
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:53,492'); seek(53.0)">
              environment. So in the recent era, the most of the
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:56,964'); seek(56.0)">
              time the data scientists spend time in creating or developing
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:01:00,602'); seek(60.0)">
              machine learning model, it could be starting from linear regression,
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:04,026'); seek(64.0)">
              logistic regression, or nave bayesian or
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:07,396'); seek(67.0)">
              dradum forest decision trees. Any algorithms let them
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:10,872'); seek(70.0)">
              take. Finally, the ERa has been moved from traditional
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:14,766'); seek(74.0)">
              or classical machine learning algorithms to large
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:19,276'); seek(79.0)">
              language models. So now deploying the large language models
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:23,362'); seek(83.0)">
              or productionizing the large language models in cloud is the biggest challenge we have.
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:28,060'); seek(88.0)">
              Not only productionizing and how we scale with high
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:31,952'); seek(91.0)">
              inference is another important aspect to consider.
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:36,110'); seek(96.0)">
              All right, let's move to the next slide before
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:40,368'); seek(100.0)">
              getting into the development of algorithms. First, let's understand
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:43,908'); seek(103.0)">
              what is the development lifecycle of any machine learning algorithm.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:48,530'); seek(108.0)">
              Now, to choose any problem
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:52,244'); seek(112.0)">
              statement, we have to clearly define what is a problem statement we
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:55,848'); seek(115.0)">
              are trying to solve. It could be an image classification,
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:59,166'); seek(119.0)">
              text classification or language translation
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:02:02,710'); seek(122.0)">
              question and answering or predicting the next sentence or document summarization,
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:06,962'); seek(126.0)">
              text summarization or there are many tasks involved
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:10,546'); seek(130.0)">
              in the problem statement. So before starting any
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:14,572'); seek(134.0)">
              machine learning algorithm development, we define the problem
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:17,936'); seek(137.0)">
              statement. Once we define the problem statement,
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:21,286'); seek(141.0)">
              then we have to start with the collectioning of data that is
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:25,088'); seek(145.0)">
              called a data collection or data acquisition and gathering.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:28,870'); seek(148.0)">
              So typically in a domain based when
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:33,572'); seek(153.0)">
              we are working in an organization which is specific to
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:37,940'); seek(157.0)">
              healthcare or infrastructure financial investment
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:43,642'); seek(163.0)">
              banking, there are many scenarios where we cloud
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:46,872'); seek(166.0)">
              get some real time data for model training.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:50,630'); seek(170.0)">
              Excuse me, but in the case of when you are trying to do some research,
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:54,664'); seek(174.0)">
              then we go for open source data set. But ideally we have to
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:58,248'); seek(178.0)">
              get the data set or we have to collect the data set and understand
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:03:01,884'); seek(181.0)">
              the data. As part of understanding the data, we may
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:05,372'); seek(185.0)">
              have to do some amount of data preprocessing techniques which we'll
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:09,238'); seek(189.0)">
              see later. After that we have to
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:12,832'); seek(192.0)">
              see what is the performance metrics you are going to define
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:17,710'); seek(197.0)">
              to achieve the objective which we have defined.
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:20,842'); seek(200.0)">
              Let's take a text classification problem here.
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:24,290'); seek(204.0)">
              The classification result can be measured in the performance
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:28,026'); seek(208.0)">
              metrics of recall, precision and
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:31,892'); seek(211.0)">
              accuracy. So these are all comes under confusion matrix.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:35,998'); seek(215.0)">
              So we have to define what are all the performance metrics we do before
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:41,430'); seek(221.0)">
              performing any model training activity. Now further,
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:45,362'); seek(225.0)">
              we have to evaluate the procedures. When we
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:48,828'); seek(228.0)">
              take a model training process, we divide the data
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:52,076'); seek(232.0)">
              set into training, testing and validation.
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:56,250'); seek(236.0)">
              So we have a split of around 80 or 680
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:04:00,390'); seek(240.0)">
              2020 or 7015 in
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:04:04,608'); seek(244.0)">
              this kind. When I say the numbers are in percentage,
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:07,846'); seek(247.0)">
              right, we have to split the data in train test and validation,
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:11,578'); seek(251.0)">
              and we have to see whether the model can perform well with
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:15,108'); seek(255.0)">
              the validation and test data. Again, the metrics we use
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:19,092'); seek(259.0)">
              based on confusion metrics.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:22,870'); seek(262.0)">
              Next, coming to data preprocessing and cleaning
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:27,430'); seek(267.0)">
              when it comes to data preprocessing, okay, we got the data.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:31,272'); seek(271.0)">
              In case of text, what are all the process we follow?
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:34,744'); seek(274.0)">
              In case of natural language processing, we remove ASCII
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:38,578'); seek(278.0)">
              and special characters and we do stop words removal followed
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:42,098'); seek(282.0)">
              by stemming and lamatization. If required, we may go for parts
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:46,466'); seek(286.0)">
              of speech tagging or some kind of an embeddings, which may be required
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:50,758'); seek(290.0)">
              before building a model. Now,
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:54,830'); seek(294.0)">
              once we come to the construction of a baseline model, so when we are using
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:59,184'); seek(299.0)">
              models like random forest or bayesian network or
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:05:03,412'); seek(303.0)">
              generative adverseial networks,
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:05:06,530'); seek(306.0)">
              Ada boosting, exg boosting,
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:10,058'); seek(310.0)">
              when you're using models like this, we may have to think about
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:14,452'); seek(314.0)">
              the, we don't need to think about the baseline model because they are already a
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:17,928'); seek(317.0)">
              base models where we use a data set to further
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:21,006'); seek(321.0)">
              train and we do the prediction or classification with that
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:24,792'); seek(324.0)">
              algorithm. But the approach which we are going to talk about,
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:28,140'); seek(328.0)">
              baseline model is bit different. What I'm saying now,
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:32,250'); seek(332.0)">
              once I walk you through on the slides, you will understand what
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:35,628'); seek(335.0)">
              I mean by the typical base models. Then we have
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:39,008'); seek(339.0)">
              to fine tune a model. So the fine tuning a model comes
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:42,192'); seek(342.0)">
              up with hyperparameter tuning and that techniques also.
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:45,712'); seek(345.0)">
              I'll talk later here, but ideally we
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:49,488'); seek(349.0)">
              have to get a pretrained model and fine tune a model to perform a specific
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:53,188'); seek(353.0)">
              task. Then how do we deploy and operationalize
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:56,922'); seek(356.0)">
              or industrialize the model in cloud environment?
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:06:01,010'); seek(361.0)">
              All right, so we have discussed
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:06:04,638'); seek(364.0)">
              about the lifecycle of ML algorithm. Let's move to the next slide.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:09,350'); seek(369.0)">
              Before getting into a model training activity,
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:12,894'); seek(372.0)">
              I clearly wanted to define the problem. What we are
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:16,844'); seek(376.0)">
              going to solve here is natural language processing tasks. As I
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:20,588'); seek(380.0)">
              said, natural language processing could be language
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:24,402'); seek(384.0)">
              translation or entity recognition, or it could be a spam
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:28,022'); seek(388.0)">
              detection, or we do some amount of part of speech tagging,
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:32,262'); seek(392.0)">
              text generation or document summarization.
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:35,558'); seek(395.0)">
              Question answering there are many natural language processing tasks involved,
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:39,798'); seek(399.0)">
              but ideally for this use case or for this demo, I'm going
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:43,988'); seek(403.0)">
              to walk you through or take you through on text classification.
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:48,210'); seek(408.0)">
              Right, let's move on.
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:51,890'); seek(411.0)">
              Now let's come to the natural language processing as a concept
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:56,630'); seek(416.0)">
              human understand English as a language or any other
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:07:00,072'); seek(420.0)">
              language which he has been known to from the birth.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:07:04,270'); seek(424.0)">
              But when it comes to natural language processing,
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:07,610'); seek(427.0)">
              machine has to understand the language, right?
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:10,828'); seek(430.0)">
              So when I say machine, so whatever how humans interpret the
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:14,828'); seek(434.0)">
              language under response. Similar way we are building a machine learning
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:18,560'); seek(438.0)">
              or AI platform or AI machine to perform
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:22,336'); seek(442.0)">
              a specific job. That is what the natural language understanding
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:26,262'); seek(446.0)">
              has been given to the machine. So how human have an
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:30,128'); seek(450.0)">
              understanding by reading the text. Similarly by
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:35,428'); seek(455.0)">
              having or building a model, but to perform like a human
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:40,210'); seek(460.0)">
              to have a natural language understanding based on that,
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:43,668'); seek(463.0)">
              it determines the answer. Machine understand. Okay,
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:46,790'); seek(466.0)">
              this is the natural language understanding I got. Now what is the response I
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:50,424'); seek(470.0)">
              have to make which would be in human readable format? Again, we can make
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:54,696'); seek(474.0)">
              an output as a natural language generation, which could be text abstraction,
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:58,562'); seek(478.0)">
              text summarization, or we can do any natural language classification job.
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:08:03,450'); seek(483.0)">
              This is what I'm going to walk you through. So right now I put a
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:08:06,924'); seek(486.0)">
              Bert model here. If you could see in the center of the picture. But yes,
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:10,812'); seek(490.0)">
              I'll elaborately talk once I walk through the next couple of slides.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:15,710'); seek(495.0)">
              Ideally we pass an input and we ask the model to
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:19,712'); seek(499.0)">
              classify. Then here it could be a spam
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:23,066'); seek(503.0)">
              or ham. So based on that it performs it.
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:27,890'); seek(507.0)">
              Let's move on to the next slide. Okay, hugging face
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:32,470'); seek(512.0)">
              now you would have heard this is getting very
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:35,928'); seek(515.0)">
              popular. Now. Hugging face is a framework or library to solve
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:39,662'); seek(519.0)">
              most of the NLP problems. They have built
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:44,710'); seek(524.0)">
              40,000 models around. They have built by now as of today
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:48,780'); seek(528.0)">
              which are having all as a pretrained model and some amount of instac
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:52,706'); seek(532.0)">
              based model or fine tuned model also available.
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:56,890'); seek(536.0)">
              Now we are going to use an agingface platform to perform our model
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:09:00,656'); seek(540.0)">
              training. Okay, now as I said, agingfare is
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:09:04,464'); seek(544.0)">
              the most popular framework which has been used by right
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:09:09,088'); seek(549.0)">
              now, sorry. It has around 4000 models or which
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:09:14,052'); seek(554.0)">
              can be deployed in cloud which is based on Pytorch or Tensorflow.
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:18,570'); seek(558.0)">
              Even Keras library are supported in hugging phase.
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:21,738'); seek(561.0)">
              Ideally we use a transformer based architecture models to
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:25,252'); seek(565.0)">
              develop our models. Now when you are
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:28,728'); seek(568.0)">
              talking about hugging phase, as I
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:31,784'); seek(571.0)">
              said, there are 4000 pretrained model and for each task they
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:35,128'); seek(575.0)">
              have a separate model. Let's say when we want to perform a text classification they
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:38,668'); seek(578.0)">
              have Bert, Robota, distal Bert XLM,
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:41,618'); seek(581.0)">
              Robota. Similarly for language translation they have Marian,
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:45,122'); seek(585.0)">
              Mt. Bard and T, five. For V and chat bots
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:49,362'); seek(589.0)">
              they have GPT, GPT-2 and now we
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:52,528'); seek(592.0)">
              would have got GPT-3 and four as well.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:55,312'); seek(595.0)">
              When it comes to named entity recognition. Again, we can use the
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:58,848'); seek(598.0)">
              Bert model. Ideally, I'm going to talk more about the Bert model.
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:10:02,276'); seek(602.0)">
              The reason why I've kept Bert here is Bert is nothing but a
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:10:05,988'); seek(605.0)">
              bi directional encoder representation for transformer.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:10:09,738'); seek(609.0)">
              It is based on the transform architecture or all the attention
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:13,242'); seek(613.0)">
              is you need based on that they have a transform architecture.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:17,038'); seek(617.0)">
              In that way, Bert has been built once
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:20,632'); seek(620.0)">
              it came in 2018 or 19. Then it shook the
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:24,248'); seek(624.0)">
              industry to think about the whole machine learning development has been
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:28,344'); seek(628.0)">
              taken into a next space or next level.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:31,450'); seek(631.0)">
              Okay, now, so we are going to use BERT model and we
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:34,764'); seek(634.0)">
              are going to fine tune the BerT model. Bert model has
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:38,316'); seek(638.0)">
              comes up with its own strength like it is based
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:41,772'); seek(641.0)">
              on masked language modeling and next sentence prediction. So if you want to
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:45,408'); seek(645.0)">
              know more about the Bert model, I have a separate video. Please go
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:48,992'); seek(648.0)">
              and have a look into that. Now. When coming to
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:52,784'); seek(652.0)">
              the Bert now, Bert can perform multiple tasks,
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:56,122'); seek(656.0)">
              but as a general model, you can do a downstream
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:11:00,250'); seek(660.0)">
              job to make it specific to a domain or specific to
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:11:03,528'); seek(663.0)">
              a task which has to be performed. So yes, Bird can perform
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:11:07,016'); seek(667.0)">
              text classification or text generation or next
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:11:10,712'); seek(670.0)">
              sentence productionize question and answering.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:15,510'); seek(675.0)">
              Similar like a chatbot, it can also perform. But how do we fine
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:19,112'); seek(679.0)">
              tune the model? Right, we have a prechained model, then we fine
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:22,252'); seek(682.0)">
              tune a model based on the data set. Then we deploy the model.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:26,604'); seek(686.0)">
              We deploy the fine tuned model in production in a cloud environment.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:31,230'); seek(691.0)">
              All right, let me walk you through the as I said,
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:34,832'); seek(694.0)">
              we are going to take the text classification example. Our objective is
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:38,528'); seek(698.0)">
              to understand the sentiment. Let's say this is an amazing
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:42,064'); seek(702.0)">
              model. Then we are going to say whether it is positive or negative.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:45,770'); seek(705.0)">
              That's what the classification job does. Now, I'm taking a binary classification
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:49,834'); seek(709.0)">
              here. Going to call that as a positive or negative here.
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:53,156'); seek(713.0)">
              So this is an example I'm going to take. Now I'm going to walk you
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:56,456'); seek(716.0)">
              through on how we can perform model chaining.
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:12:00,974'); seek(720.0)">
              But before getting into model chaining, I want to tell you
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:12:04,552'); seek(724.0)">
              about ML Flow. What is ML Flow?
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:12:08,290'); seek(728.0)">
              ML Flow is a platform or is
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:11,708'); seek(731.0)">
              an API library which can be injected into your model
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:15,420'); seek(735.0)">
              development process to perform all the model tracking and
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:19,152'); seek(739.0)">
              model experiments. What I mean by that,
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:22,912'); seek(742.0)">
              we can build many models and many iteration
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:26,662'); seek(746.0)">
              of models reasoning. We have to fine tune the model. We have
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:29,968'); seek(749.0)">
              to change the parameters of the model. Once we keep changing
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:33,338'); seek(753.0)">
              the parameters of the model every time, model will have a different
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:36,916'); seek(756.0)">
              outputs. What could be an output here? It could be an precision
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:42,234'); seek(762.0)">
              recall accuracy, f one score. F two score. There are
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:45,688'); seek(765.0)">
              many elements we consider as part of model development activity.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:50,126'); seek(770.0)">
              So there could be a scenario if in
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:54,568'); seek(774.0)">
              case of text classification, we have seen positive or negative, how much
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:58,492'); seek(778.0)">
              I'm more inclined to positive. If I always wants a positive,
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:13:02,642'); seek(782.0)">
              I should not miss any false negative means. If algorithm
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:13:07,090'); seek(787.0)">
              says it is a
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:13:10,590'); seek(790.0)">
              negative, but actually it is a
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:13:14,320'); seek(794.0)">
              positive, I should not miss these kind of scenarios,
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:18,110'); seek(798.0)">
              right? So if I should not
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:21,284'); seek(801.0)">
              miss any false negative, then I'll be focusing more on
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:25,810'); seek(805.0)">
              recall. Similarly, when the algorithm
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:29,770'); seek(809.0)">
              says okay, it is a
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:33,060'); seek(813.0)">
              positive and algorithm says it is a negative,
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:36,590'); seek(816.0)">
              then again it comes under false positive, right? So it misses the crucial
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:40,526'); seek(820.0)">
              element. Right. Now, to handle these
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:44,168'); seek(824.0)">
              kind of scenarios, we need a tracking platform which is called ML flow,
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:48,386'); seek(828.0)">
              which is used to record and track all the experiments
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:52,226'); seek(832.0)">
              along with the results. But I can also show
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:55,548'); seek(835.0)">
              you a quick sample on how the code looks like by having an
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:59,008'); seek(839.0)">
              ML flow and without an ML flow before
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:14:02,816'); seek(842.0)">
              that, this is how the model experiment looks like.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:14:05,760'); seek(845.0)">
              When I talk about model experiments,
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:14:08,990'); seek(848.0)">
              let's say I'm going to train an algorithm and I may
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:14:12,756'); seek(852.0)">
              train n number of times. So I wanted to know based
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:16,276'); seek(856.0)">
              on which seed and which parameter my model really performed.
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:19,578'); seek(859.0)">
              Well, considering the scenario, I'll take all
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:22,932'); seek(862.0)">
              the historical experiments which I performed that
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:26,328'); seek(866.0)">
              would be tracked in ML flow, which you can see each,
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:30,790'); seek(870.0)">
              along with the timestamp, you can see the model which I ran, and if
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:34,408'); seek(874.0)">
              you go deep and along with that features, what kind of features I configured,
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:39,042'); seek(879.0)">
              then I get an accuracy, precision, recall value, whatever. I have that in
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:42,748'); seek(882.0)">
              a metrics for confusion matrix. This really helps
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:46,562'); seek(886.0)">
              in performing in multiple iteration of experiments and to get the tracking
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:50,838'); seek(890.0)">
              of the models. All right, now coming
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:54,192'); seek(894.0)">
              to the code. So typically what we do to train and model,
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:57,504'); seek(897.0)">
              we load an input data and we extract some of the features.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:15:01,318'); seek(901.0)">
              And I'm using an Ingrams to extract the features. Then I'm going to
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:15:04,548'); seek(904.0)">
              train and model, and I'm going to compute the accuracy. Now,
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:15:08,212'); seek(908.0)">
              what version of my code was this result from? No idea.
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:15:12,610'); seek(912.0)">
              To perform this, we need an ML flow tracking,
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:15,998'); seek(915.0)">
              which is ideally used to track all the experiment
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:19,326'); seek(919.0)">
              results. Now let's see how the code looks like with
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:23,048'); seek(923.0)">
              ML flow. So with pythonic way, by having a
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:26,760'); seek(926.0)">
              packages import having ML flow and ML Tensorflow,
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:30,962'); seek(930.0)">
              then we say ML flow start run as a run.
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:34,636'); seek(934.0)">
              Then we start to log the metrics. Then we keep training our
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:38,816'); seek(938.0)">
              model along with fine tuning the parameters. In this
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:42,192'); seek(942.0)">
              way, everything get stored in a
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:45,744'); seek(945.0)">
              database, ideally in cloud environment. We configure with an
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:49,408'); seek(949.0)">
              S three bucket of AWS service, Amazon Web service.
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:53,490'); seek(953.0)">
              Then once the setup is done, then we
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:57,348'); seek(957.0)">
              add an implementation accordingly to make an ML flow start,
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:16:00,980'); seek(960.0)">
              and then iterate the model multiple times
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:16:04,840'); seek(964.0)">
              and keep having experiment results get stored.
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:16:08,150'); seek(968.0)">
              ML flow comes with a default UI where all the model
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:16:11,496'); seek(971.0)">
              experiment can be visualized, which I've shown you in the earlier
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:16:15,182'); seek(975.0)">
              slide. Now coming to the model training.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:19,116'); seek(979.0)">
              So now the reason why I kept explaining about the experiment and
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:23,132'); seek(983.0)">
              tracking and all. When you start the model training framework,
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:26,498'); seek(986.0)">
              you should have all the experiments needs to be tracked somehow.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:29,798'); seek(989.0)">
              Right now I've taken a small example code of
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:33,456'); seek(993.0)">
              how do we perform the model training activity.
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:37,150'); seek(997.0)">
              So ideally we are going to use a BERT model which you can see somewhere,
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:40,822'); seek(1000.0)">
              which I am using a pre trained BERT model. Then I'm using
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:44,132'); seek(1004.0)">
              auto model for sequence classification reasoning. I can
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:47,476'); seek(1007.0)">
              put this instead of BeRT model. I can try with robota
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:51,546'); seek(1011.0)">
              XLM, robota distal, Bird, Biobird,
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:55,010'); seek(1015.0)">
              GPT-2 or GPT-3 any of the pretrained models
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:58,718'); seek(1018.0)">
              I can put here. So when I build a framework I
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:17:02,488'); seek(1022.0)">
              have to call the transformer library. Then I use auto tokenizer
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:17:06,398'); seek(1026.0)">
              and auto model for sequence classification and load the pretrained
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:17:10,098'); seek(1030.0)">
              model and in the next line I'll show the code further.
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:17:13,826'); seek(1033.0)">
              But before that we are using a GPU machine to run all this model
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:17,756'); seek(1037.0)">
              training activity because this is a large language model.
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:21,504'); seek(1041.0)">
              Then again when we are doing a fine tuning, it recurs a GPU machine
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:25,030'); seek(1045.0)">
              with CUDA library to perform this activity,
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:29,070'); seek(1049.0)">
              the model two device and the torch device,
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:32,586'); seek(1052.0)">
              by using a CUDA library specifies
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:36,698'); seek(1056.0)">
              the GPU would be let's say I'm using an eight GPU, the processor
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:40,778'); seek(1060.0)">
              would get split into multiple GPUs and it starts to
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:44,950'); seek(1064.0)">
              perform the model training activity. The reason why we're using
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:48,312'); seek(1068.0)">
              auto model so this is a framework which we can build and
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:52,710'); seek(1072.0)">
              by passing in the command line prompt the model framework.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:56,546'); seek(1076.0)">
              Based on that we can further train the model.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:18:00,650'); seek(1080.0)">
              Now coming to the model train
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:18:04,076'); seek(1084.0)">
              which you can see is an abstract class to
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:18:07,308'); seek(1087.0)">
              perform the model training which has been given by transformer
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:18:10,838'); seek(1090.0)">
              architecture. Then I can start training
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:18:15,024'); seek(1095.0)">
              model will get trained. Then I can keep changing my training.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:18,416'); seek(1098.0)">
              Hyperparameters it's based on learning rate, number of
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:21,908'); seek(1101.0)">
              epochs and lock size.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:25,892'); seek(1105.0)">
              And there are additional parameters which we can use which
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:30,260'); seek(1110.0)">
              mainly we use learning rate and number of epochs which
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:34,392'); seek(1114.0)">
              is used for fine tuning the model parameters.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:37,774'); seek(1117.0)">
              Right? Now, once we train the model,
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:41,384'); seek(1121.0)">
              then we use a data loader, then we use a model fit to
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:45,672'); seek(1125.0)">
              train the model and along with that hyperparameters tuning,
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:50,090'); seek(1130.0)">
              the fine tuning job is nothing but take a pretrained model
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:54,410'); seek(1134.0)">
              and fine tune the model to a specific task. In Bert we are
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:57,808'); seek(1137.0)">
              going to perform a text classification for that
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:19:01,152'); seek(1141.0)">
              give an input data set and keep training the model
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:19:05,248'); seek(1145.0)">
              until you get the accuracy or recall
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:19:09,158'); seek(1149.0)">
              and precision to a certain benchmark. 85% or 95%.
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:19:12,996'); seek(1152.0)">
              How much would you require based on the problem definition or problem statement
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:19:17,066'); seek(1157.0)">
              which you defined? Now we
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:20,628'); seek(1160.0)">
              have done with the model training, then we have to evaluate the model.
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:23,652'); seek(1163.0)">
              As I said at the beginning of the conversation,
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:27,946'); seek(1167.0)">
              when we get the model we have to do the model evaluation metrics,
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:31,246'); seek(1171.0)">
              then split the data into train test and validation.
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:34,894'); seek(1174.0)">
              Then once the training has been performed with the
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:38,216'); seek(1178.0)">
              train data, we can use evaluation or validation
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:42,162'); seek(1182.0)">
              data set to evaluate the model. Then further we can
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:45,660'); seek(1185.0)">
              use a prediction logic which could be based on logics
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:49,186'); seek(1189.0)">
              or softmax classifier or neural network in the behind. I don't
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:52,614'); seek(1192.0)">
              want to go deeper in that. Our idea is to define a framework,
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:56,910'); seek(1196.0)">
              do a model training and productionize the model, or deploy the model
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:20:00,880'); seek(1200.0)">
              in cloud. That's where our focus is. If anything,
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:20:03,988'); seek(1203.0)">
              please feel free to reach out to me after the presentation or after
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:20:07,668'); seek(1207.0)">
              this live event. Then we can discuss or take the conversation
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:20:11,626'); seek(1211.0)">
              further. Now talked about model training and model evaluation
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:20:15,838'); seek(1215.0)">
              and we can use model accuracy, sorry, model prediction
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:20:19,630'); seek(1219.0)">
              and based on the model prediction we can compute the confusion
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:23,614'); seek(1223.0)">
              matrix score that can be used for
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:27,356'); seek(1227.0)">
              taking the model to production. Now how do
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:31,292'); seek(1231.0)">
              we take the model to production? That is another interesting problem which
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:35,116'); seek(1235.0)">
              can be solved by using Pytorch serving,
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:39,150'); seek(1239.0)">
              right? So now
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:42,624'); seek(1242.0)">
              when we say PyTorch serving of ML models,
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:47,070'); seek(1247.0)">
              Pytorch serving is nothing. But we have built a
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:50,368'); seek(1250.0)">
              model in Pytorch and how do we serve the model in production to
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:54,132'); seek(1254.0)">
              achieve the low latency and scalability problem,
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:57,810'); seek(1257.0)">
              right? So as you could see, once you train
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:21:01,300'); seek(1261.0)">
              the model and you evaluate the model, and if you feel the model
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:21:05,030'); seek(1265.0)">
              is good enough to take to higher environments,
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:21:08,230'); seek(1268.0)">
              then you have to convert the model into Mar file.
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:21:12,078'); seek(1272.0)">
              So which is nothing but a torch serve which we've been
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:21:15,992'); seek(1275.0)">
              using. And we have a model store where we have to convert the model
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:21:19,980'); seek(1279.0)">
              into Mar file and the model has to be deployed
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:21:23,682'); seek(1283.0)">
              into a pytot serving inference place.
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:27,050'); seek(1287.0)">
              So there is a logic which we have to follow. We have to build the
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:30,688'); seek(1290.0)">
              docker image. Once we have a model, then we have to deploy
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:34,246'); seek(1294.0)">
              that into the ECs that we can take in the next slide.
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:37,878'); seek(1297.0)">
              But overall torch serving will help us to
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:41,344'); seek(1301.0)">
              deploy the Mar files inside a model store. Then it will have
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:44,884'); seek(1304.0)">
              an inference API so that via invoking an API we can
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:48,548'); seek(1308.0)">
              call the model prediction results. So internally this has
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:52,228'); seek(1312.0)">
              an architecture where you can have multiple model can be served
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:56,122'); seek(1316.0)">
              in a single python serving instance.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:59,110'); seek(1319.0)">
              Right. Now, ideally this can be used for an API invocation
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:22:02,718'); seek(1322.0)">
              to call all the models which has been deployed inside the Python serving model
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:22:06,632'); seek(1326.0)">
              again. For more questions please feel free to reach out to me after the
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:22:10,108'); seek(1330.0)">
              event. Now once we have the
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:22:13,164'); seek(1333.0)">
              Pytotch serving, this is the most interesting piece. So we
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:22:17,452'); seek(1337.0)">
              have trained and deploy or sorry productionize the model
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:22:21,164'); seek(1341.0)">
              in cloud right now whenever
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:24,482'); seek(1344.0)">
              you talk about the model training activity. Once the Mar file
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:28,454'); seek(1348.0)">
              is generated, we have to push inside the S three bucket because all
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:31,728'); seek(1351.0)">
              the model can be stored in s three bucket because that
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:34,964'); seek(1354.0)">
              is a huge file and is a blob storage or it's a file storage.
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:37,786'); seek(1357.0)">
              S three bucket from Amazon can be used to store the models.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:41,898'); seek(1361.0)">
              Then we can use an ECR elastic container
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:45,038'); seek(1365.0)">
              registry to push the model. Or we
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:48,328'); seek(1368.0)">
              have to push the image into an ECS or EC two instance.
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:51,870'); seek(1371.0)">
              There we could see we build every model as a docker image.
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:56,318'); seek(1376.0)">
              Then once we have a docker image, then we have an EBS elastic
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:23:01,530'); seek(1381.0)">
              load balancing server or EBS storage is used in
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:23:05,004'); seek(1385.0)">
              the backend to connect to the ECS and the model can
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:23:08,268'); seek(1388.0)">
              be stored over there in the model store. Then from there via inference
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:23:12,038'); seek(1392.0)">
              API we can pick the model by writing a python function or Python
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:23:15,542'); seek(1395.0)">
              code that will be deployed as a docker image or docker container.
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:19,238'); seek(1399.0)">
              Then it can pull the model and it can perform the inference logic or
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:22,788'); seek(1402.0)">
              it can do the prediction. So now you can see how the old model
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:26,644'); seek(1406.0)">
              is getting developed from the time we
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:30,228'); seek(1410.0)">
              start the model development to productionize the algorithms.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:35,090'); seek(1415.0)">
              Now let's talk more about the AWS cloud environment.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:38,318'); seek(1418.0)">
              So already we have a sage maker, but let's not use a sage maker
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:42,094'); seek(1422.0)">
              or sage maker endpoint. But ideally we are saving a cost literally by
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:46,488'); seek(1426.0)">
              having ECs, ECR and S three bucket and perform
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:50,236'); seek(1430.0)">
              a model training in a GPU machine. And once the model has been trained,
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:55,050'); seek(1435.0)">
              push the file with and we can write
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:58,412'); seek(1438.0)">
              some scripts to push the trained model into S
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:24:01,868'); seek(1441.0)">
              three bucket. Then once that is done we can use Jenkins
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:24:05,350'); seek(1445.0)">
              Ci CD pipeline to
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:24:08,752'); seek(1448.0)">
              push the docker image into an ECS container which underneath it
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:24:12,768'); seek(1452.0)">
              uses Fargate or EC two. In my case I refer to EC two,
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:24:16,900'); seek(1456.0)">
              right? As I said, once a model has been built, all the models
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:20,954'); seek(1460.0)">
              will get stored under model store. Once the models are
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:24:24,808'); seek(1464.0)">
              stored under model store, there is a management API, an inference API
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:28,702'); seek(1468.0)">
              which by using an pytotch serving command where we
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:33,528'); seek(1473.0)">
              can provide inference API for the applications
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:38,658'); seek(1478.0)">
              to consume. I think this is a holistic step involved in
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:42,316'); seek(1482.0)">
              creating the model or developing the machine learning algorithm
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:45,810'); seek(1485.0)">
              or deploying the model in cloud environment
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:49,370'); seek(1489.0)">
              right. Any further questions? As I said, you can always reach out to me after
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:53,052'); seek(1493.0)">
              the event. All right. Thank you for
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:57,596'); seek(1497.0)">
              watching my video. Have they had.
            </span>
            
            </div>
          </div>
          

          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/cloud2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #7B2726;">
                <i class="fe fe-grid me-2"></i>
                See all 47 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Deepak%20Karunanidhi_cloud.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Deepak Karunanidhi
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Associate Director - Data Science | Machine Learning @ Novartis
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/akdeepak85" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Deepak Karunanidhi's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Deepak Karunanidhi"
                  data-url="https://www.conf42.com/cloud2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/cloud2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Cloud Native"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://sreday.com/2024-amsterdam/">
                  SREday Amsterdam 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>