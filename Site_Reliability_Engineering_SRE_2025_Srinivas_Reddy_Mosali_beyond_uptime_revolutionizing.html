<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Beyond Uptime: Revolutionizing Fintech Reliability Through SRE Innovation</title>
    <meta name="description" content="Defend your systems against intergalactic invaders!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Srinivas%20Reddy%20Mosali_sre.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Beyond Uptime: Revolutionizing Fintech Reliability Through SRE Innovation | Conf42"/>
    <meta property="og:description" content="Discover how a high-growth fintech startup mastered system reliability during a viral surge, slashed incident recovery times, and maintained perfect uptime through SRE innovation. Learn battle-tested strategies for balancing aggressive growth with regulatory compliance in the financial sector."/>
    <meta property="og:url" content="https://conf42.com/Site_Reliability_Engineering_SRE_2025_Srinivas_Reddy_Mosali_beyond_uptime_revolutionizing"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/MLOPS2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        MLOps 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-09-18
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/mlops2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #E36414;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Site Reliability Engineering (SRE) 2025 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2025-04-17">April 17 2025</time>
              
              - premiere 5PM GMT
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Defend your systems against intergalactic invaders!
 -->
              <script>
                const event_date = new Date("2025-04-17T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2025-04-17T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "wAWyJMwWuO4"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrCu4WV3_Ve7-ChGVhr_-DbU" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello everyone.", "timestamp": "00:00:00,500", "timestamp_s": 0.0}, {"text": "Thank you for joining me in this conference.", "timestamp": "00:00:01,519", "timestamp_s": 1.0}, {"text": "Today we will discuss on how FinTech reliability is achieved", "timestamp": "00:00:03,950", "timestamp_s": 3.0}, {"text": "through SRE Innovation.", "timestamp": "00:00:07,340", "timestamp_s": 7.0}, {"text": "So as organizations accelerate their AI initiatives, many are discovering", "timestamp": "00:00:08,735", "timestamp_s": 8.0}, {"text": "that traditional infrastructure was not built to handle the high demands", "timestamp": "00:00:13,264", "timestamp_s": 13.0}, {"text": "of machine learning operations.", "timestamp": "00:00:17,804", "timestamp_s": 17.0}, {"text": "These workloads are highly resource intensive and are unpredictable.", "timestamp": "00:00:20,064", "timestamp_s": 20.0}, {"text": "While Kubernetes has emerged as a platform of choice for managing", "timestamp": "00:00:24,244", "timestamp_s": 24.0}, {"text": "these environments, simply running the containers is not enough.", "timestamp": "00:00:28,594", "timestamp_s": 28.0}, {"text": "The challenge lies in running them optimally.", "timestamp": "00:00:32,444", "timestamp_s": 32.0}, {"text": "What we are seeing is that many teams are struggling, be it dealing with", "timestamp": "00:00:35,824", "timestamp_s": 35.0}, {"text": "over-provisioned resources, unpredictable costs and performance bottlenecks.", "timestamp": "00:00:40,774", "timestamp_s": 40.0}, {"text": "So in this particular session, I would like to walk you through a set of proven", "timestamp": "00:00:46,185", "timestamp_s": 46.0}, {"text": "SRE strategies that are designed to increase performance, reduce costs, and", "timestamp": "00:00:51,025", "timestamp_s": 51.0}, {"text": "significantly improve the reliability of the AI ML pipelines running in.", "timestamp": "00:00:56,685", "timestamp_s": 56.0}, {"text": "Kubernetes these approaches come directly from the high stakes environments", "timestamp": "00:01:02,040", "timestamp_s": 62.0}, {"text": "these where even minor inefficiencies can lead to a major financial losses.", "timestamp": "00:01:06,680", "timestamp_s": 66.0}, {"text": "So together we\u0027ll go through the techniques that have delivered up", "timestamp": "00:01:11,990", "timestamp_s": 71.0}, {"text": "to a 60% improvements in resource utilization and substantially", "timestamp": "00:01:15,720", "timestamp_s": 75.0}, {"text": "reductions in incident rates.", "timestamp": "00:01:20,540", "timestamp_s": 80.0}, {"text": "All while maintaining the agility and performance.", "timestamp": "00:01:22,915", "timestamp_s": 82.0}, {"text": "Ultimately, this presentation is about more than just keeping the systems online.", "timestamp": "00:01:26,955", "timestamp_s": 86.0}, {"text": "It\u0027s about engineering confidently at scale in an increased AI", "timestamp": "00:01:32,315", "timestamp_s": 92.0}, {"text": "driven financial ecosystem.", "timestamp": "00:01:36,845", "timestamp_s": 96.0}, {"text": "So let\u0027s take a closer look on how the transformation is happening.", "timestamp": "00:01:38,795", "timestamp_s": 98.0}, {"text": "Okay.", "timestamp": "00:01:43,365", "timestamp_s": 103.0}, {"text": "When we talk about scaling AI infrastructure efficiently, GPU", "timestamp": "00:01:43,574", "timestamp_s": 103.0}, {"text": "utilization is one of the most overlooked and most expensive", "timestamp": "00:01:47,114", "timestamp_s": 107.0}, {"text": "and has more bottlenecks.", "timestamp": "00:01:51,204", "timestamp_s": 111.0}, {"text": "So let\u0027s look at how we can dramatically shift that performance to cost ratio", "timestamp": "00:01:52,854", "timestamp_s": 112.0}, {"text": "using smart GPU level strategies.", "timestamp": "00:01:58,654", "timestamp_s": 118.0}, {"text": "Let\u0027s start with the MIG or the multi-instance GPU Partitioning.", "timestamp": "00:02:01,694", "timestamp_s": 121.0}, {"text": "So this is a feature available in NVIDIA\u0027s 800 and h hundred series.", "timestamp": "00:02:05,504", "timestamp_s": 125.0}, {"text": "It allows you to carve up a single GPU into a multiple isolated instances,", "timestamp": "00:02:10,704", "timestamp_s": 130.0}, {"text": "which enables like multiple workloads to run simultaneously without any issues.", "timestamp": "00:02:16,714", "timestamp_s": 136.0}, {"text": "Our real world tests have shown up to a seven x better utilization especially in", "timestamp": "00:02:21,364", "timestamp_s": 141.0}, {"text": "environments with varied inference loads.", "timestamp": "00:02:26,564", "timestamp_s": 146.0}, {"text": "Next in in memory efficiency QDA provides like a low level access to memory", "timestamp": "00:02:29,844", "timestamp_s": 149.0}, {"text": "management, allowing us to opti optimize.", "timestamp": "00:02:36,424", "timestamp_s": 156.0}, {"text": "Tensor core use and reduce the fragmentation.", "timestamp": "00:02:39,784", "timestamp_s": 159.0}, {"text": "So this can lead to 15 to 30% of throughput gains particularly for", "timestamp": "00:02:43,124", "timestamp_s": 163.0}, {"text": "vision heavy models like ID verification or biometric fraud detection.", "timestamp": "00:02:48,704", "timestamp_s": 168.0}, {"text": "Oh, coming to precision optimization.", "timestamp": "00:02:53,504", "timestamp_s": 173.0}, {"text": "In many FinTech companies like ML workloads, especially during inference.", "timestamp": "00:02:55,724", "timestamp_s": 175.0}, {"text": "Full FFP 32 precision is not always needed.", "timestamp": "00:03:00,984", "timestamp_s": 180.0}, {"text": "So by switching the, switching it to FP 16 or even INT eight, organizations can", "timestamp": "00:03:04,524", "timestamp_s": 184.0}, {"text": "drastically shrink the memory footprint.", "timestamp": "00:03:11,724", "timestamp_s": 191.0}, {"text": "So these techniques are game changers.", "timestamp": "00:03:14,454", "timestamp_s": 194.0}, {"text": "They maximize the GPU resource usage and provide critical improvements in", "timestamp": "00:03:16,414", "timestamp_s": 196.0}, {"text": "both performance and cost efficiency.", "timestamp": "00:03:21,324", "timestamp_s": 201.0}, {"text": "Moving on to the next slide.", "timestamp": "00:03:24,384", "timestamp_s": 204.0}, {"text": "On auto-scaling strategies, as organizations push to run ML workloads", "timestamp": "00:03:25,644", "timestamp_s": 205.0}, {"text": "more efficiently traditional auto-scaling strategies are not enough, especially", "timestamp": "00:03:29,984", "timestamp_s": 209.0}, {"text": "in GPU accelerated environments.", "timestamp": "00:03:34,264", "timestamp_s": 214.0}, {"text": "Here we\u0027ll go through four techniques that enable smarter ML strategies.", "timestamp": "00:03:36,334", "timestamp_s": 216.0}, {"text": "Which will help with the autoscaling.", "timestamp": "00:03:42,104", "timestamp_s": 222.0}, {"text": "First we\u0027ll go through the workload profiling.", "timestamp": "00:03:44,604", "timestamp_s": 224.0}, {"text": "So the foundation of autoscaling is understanding when and how much to scale.", "timestamp": "00:03:46,884", "timestamp_s": 226.0}, {"text": "So workload profiling uses the historical telemetry across both", "timestamp": "00:03:52,374", "timestamp_s": 232.0}, {"text": "training and inference cycles.", "timestamp": "00:03:56,334", "timestamp_s": 236.0}, {"text": "To uncover the usage patterns.", "timestamp": "00:03:59,024", "timestamp_s": 239.0}, {"text": "So this helps define the baseline resource needs, identify the spikes", "timestamp": "00:04:01,214", "timestamp_s": 241.0}, {"text": "during the market, events or frauds, fraud searches, and ensures that", "timestamp": "00:04:06,384", "timestamp_s": 246.0}, {"text": "the auto-scaling decisions are data informed and are not reactive.", "timestamp": "00:04:10,404", "timestamp_s": 250.0}, {"text": "Coming to the custom metrics pipeline.", "timestamp": "00:04:15,044", "timestamp_s": 255.0}, {"text": "So if you look at the out of box Kubernetes like Kubernetes HPA doesn\u0027t", "timestamp": "00:04:17,924", "timestamp_s": 257.0}, {"text": "know anything about the machine learning.", "timestamp": "00:04:22,584", "timestamp_s": 262.0}, {"text": "That\u0027s where like the custom metrics come in using Prometheus adapters.", "timestamp": "00:04:24,894", "timestamp_s": 264.0}, {"text": "We expose the ml. Relevant signals like the Q Depth batch processing duration,", "timestamp": "00:04:29,694", "timestamp_s": 269.0}, {"text": "and GPU memory pressure to the autoscaler.", "timestamp": "00:04:35,694", "timestamp_s": 275.0}, {"text": "So this enables a precise decision based on the actual workload characteristics,", "timestamp": "00:04:39,284", "timestamp_s": 279.0}, {"text": "not just the CPU load or the pod count.", "timestamp": "00:04:45,624", "timestamp_s": 285.0}, {"text": "Coming to predictive scaling for recurring or seasonal workloads.", "timestamp": "00:04:48,854", "timestamp_s": 288.0}, {"text": "Think EOD model model retaining or the batch scoring jobs.", "timestamp": "00:04:53,504", "timestamp_s": 293.0}, {"text": "We can apply the time-based or ML based scaling triggers.", "timestamp": "00:04:58,594", "timestamp_s": 298.0}, {"text": "So predictive models can anticipate the spikes before they happen.", "timestamp": "00:05:02,884", "timestamp_s": 302.0}, {"text": "Enabling warm pools to be spun up just in time.", "timestamp": "00:05:07,000", "timestamp_s": 307.0}, {"text": "This reduces the cold start latency by up to 85%, especially valuable", "timestamp": "00:05:10,340", "timestamp_s": 310.0}, {"text": "for real time FinTech systems.", "timestamp": "00:05:16,280", "timestamp_s": 316.0}, {"text": "Coming to the buffer management we can\u0027t forget the cluster resilience.", "timestamp": "00:05:19,160", "timestamp_s": 319.0}, {"text": "So the buffer management means deliberately over provisioning a", "timestamp": "00:05:24,004", "timestamp_s": 324.0}, {"text": "small GPU headroom during critical critical hours like trading sessions", "timestamp": "00:05:28,464", "timestamp_s": 328.0}, {"text": "or things like holiday season, right?", "timestamp": "00:05:33,774", "timestamp_s": 333.0}, {"text": "Then you can scale them back down more aggressively after the hours.", "timestamp": "00:05:36,594", "timestamp_s": 336.0}, {"text": "This balances the control cost control with high availability, which is", "timestamp": "00:05:40,960", "timestamp_s": 340.0}, {"text": "non-negotiable for financial applications.", "timestamp": "00:05:45,439", "timestamp_s": 345.0}, {"text": "So these autoscaling strategies go well beyond the HPA setup.", "timestamp": "00:05:48,529", "timestamp_s": 348.0}, {"text": "When implemented together, they enable the organizations to sustain peak", "timestamp": "00:05:53,669", "timestamp_s": 353.0}, {"text": "model performance with minimal costs.", "timestamp": "00:05:57,699", "timestamp_s": 357.0}, {"text": "Okay.", "timestamp": "00:06:00,954", "timestamp_s": 360.0}, {"text": "Coming to the multi GPU training orchestration the biggest bottlenecks", "timestamp": "00:06:01,164", "timestamp_s": 361.0}, {"text": "in scaling a machine learning training right is GPU job orchestrations.", "timestamp": "00:06:05,314", "timestamp_s": 365.0}, {"text": "It\u0027s not just having about having more GPUs.", "timestamp": "00:06:12,034", "timestamp_s": 372.0}, {"text": "It\u0027s about using them in intelligently.", "timestamp": "00:06:15,594", "timestamp_s": 375.0}, {"text": "When we orchestrate jobs sufficiently across the gps, especially in", "timestamp": "00:06:18,114", "timestamp_s": 378.0}, {"text": "distributed environments, we unlock, huge throughput gains in cost efficiency.", "timestamp": "00:06:21,954", "timestamp_s": 381.0}, {"text": "The first step is topology aware scheduling.", "timestamp": "00:06:27,654", "timestamp_s": 387.0}, {"text": "Why does this matter?", "timestamp": "00:06:30,754", "timestamp_s": 390.0}, {"text": "So when jobs span across multiple GPUs, especially across nodes network", "timestamp": "00:06:31,894", "timestamp_s": 391.0}, {"text": "latency becomes c enemy like we solve this by using the g Kubernetes node", "timestamp": "00:06:37,834", "timestamp_s": 397.0}, {"text": "affinity rules to schedule the jobs on nodes that are physically close to the", "timestamp": "00:06:43,194", "timestamp_s": 403.0}, {"text": "interconnect through high bandwidth.", "timestamp": "00:06:48,354", "timestamp_s": 408.0}, {"text": "Links like NV link, or PCIE.", "timestamp": "00:06:50,939", "timestamp_s": 410.0}, {"text": "This minimizes the communication overhead and drastically improves the", "timestamp": "00:06:54,319", "timestamp_s": 414.0}, {"text": "performance and distributed training.", "timestamp": "00:06:57,589", "timestamp_s": 417.0}, {"text": "Next we move on to distributed training frameworks.", "timestamp": "00:07:00,179", "timestamp_s": 420.0}, {"text": "So framework for frameworks like TensorFlows multi", "timestamp": "00:07:03,119", "timestamp_s": 423.0}, {"text": "worker needs to be tuned.", "timestamp": "00:07:06,959", "timestamp_s": 426.0}, {"text": "To our infrastructure, often we need poor performance because", "timestamp": "00:07:09,694", "timestamp_s": 429.0}, {"text": "training scripts are not optimized.", "timestamp": "00:07:14,124", "timestamp_s": 434.0}, {"text": "So we address this with Custom gate template that enable, like", "timestamp": "00:07:16,454", "timestamp_s": 436.0}, {"text": "dynamic scaling, reduces the training time variance, and", "timestamp": "00:07:21,344", "timestamp_s": 441.0}, {"text": "it minimizes the communication bottlenecks between the nodes.", "timestamp": "00:07:25,124", "timestamp_s": 445.0}, {"text": "Finally, there\u0027s a priority based preemptive, mechanism.", "timestamp": "00:07:28,459", "timestamp_s": 448.0}, {"text": "So in this particular technique if you look at our shared clusters.", "timestamp": "00:07:32,784", "timestamp_s": 452.0}, {"text": "When multiple teams submit the jobs, it\u0027s critical to ensure that high", "timestamp": "00:07:37,004", "timestamp_s": 457.0}, {"text": "priority jobs don\u0027t wait in line behind the rule, low priority ones.", "timestamp": "00:07:42,454", "timestamp_s": 462.0}, {"text": "So we implement intelligent queuing mechanisms that evaluate", "timestamp": "00:07:47,074", "timestamp_s": 467.0}, {"text": "the job priority and then.", "timestamp": "00:07:51,344", "timestamp_s": 471.0}, {"text": "Deadlines and then resource fairness.", "timestamp": "00:07:53,119", "timestamp_s": 473.0}, {"text": "It\u0027s like air traffic control for GPUs, like preventing the collisions,", "timestamp": "00:07:55,729", "timestamp_s": 475.0}, {"text": "reducing the idle time, and keeping the system fair and efficient.", "timestamp": "00:08:00,139", "timestamp_s": 480.0}, {"text": "The key takeaway is like when you bring all these three pillars together, the", "timestamp": "00:08:04,479", "timestamp_s": 484.0}, {"text": "topology awareness, smart frameworks, and priority based scheduling you can", "timestamp": "00:08:09,609", "timestamp_s": 489.0}, {"text": "cut the training time by 30 to 50%.", "timestamp": "00:08:14,519", "timestamp_s": 494.0}, {"text": "That\u0027s a massive impact when you are running the running hundreds or even", "timestamp": "00:08:17,939", "timestamp_s": 497.0}, {"text": "thousands of experiments each week.", "timestamp": "00:08:22,079", "timestamp_s": 502.0}, {"text": "Okay.", "timestamp": "00:08:24,829", "timestamp_s": 504.0}, {"text": "Moving on to the advanced monitoring and observability.", "timestamp": "00:08:25,159", "timestamp_s": 505.0}, {"text": "As machine learning workloads grow in complexity and scale, the need for robust", "timestamp": "00:08:27,889", "timestamp_s": 507.0}, {"text": "observability becomes absolutely critical.", "timestamp": "00:08:32,879", "timestamp_s": 512.0}, {"text": "Not just for performance, but for cost efficiency and op", "timestamp": "00:08:35,649", "timestamp_s": 515.0}, {"text": "and operational reliability.", "timestamp": "00:08:39,459", "timestamp_s": 519.0}, {"text": "So this.", "timestamp": "00:08:41,319", "timestamp_s": 521.0}, {"text": "Here outlines the layered approach.", "timestamp": "00:08:42,709", "timestamp_s": 522.0}, {"text": "We take towards building a comprehensive monitoring stack for", "timestamp": "00:08:45,884", "timestamp_s": 525.0}, {"text": "the machine learning infrastructure.", "timestamp": "00:08:49,414", "timestamp_s": 529.0}, {"text": "So let\u0027s start at the foundation, the historical data.", "timestamp": "00:08:51,834", "timestamp_s": 531.0}, {"text": "So long-term storage of metrics is essential for capacity", "timestamp": "00:08:55,659", "timestamp_s": 535.0}, {"text": "planning and trend analysis.", "timestamp": "00:08:59,779", "timestamp_s": 539.0}, {"text": "Whether you are sizing your GPU clusters for next quarter\u0027s workloads,", "timestamp": "00:09:01,729", "timestamp_s": 541.0}, {"text": "or preparing for upcoming model retraining cycles, you need to.", "timestamp": "00:09:06,479", "timestamp_s": 546.0}, {"text": "Analyze historical context, right?", "timestamp": "00:09:11,819", "timestamp_s": 551.0}, {"text": "So it helps avoid over provisioning and gives a factual basis", "timestamp": "00:09:14,249", "timestamp_s": 554.0}, {"text": "for infrastructure budgeting.", "timestamp": "00:09:19,159", "timestamp_s": 559.0}, {"text": "So next we move up to resource utilization.", "timestamp": "00:09:21,349", "timestamp_s": 561.0}, {"text": "So this is where the fine-grained metrics come in, like the CPU usage,", "timestamp": "00:09:24,890", "timestamp_s": 564.0}, {"text": "memory pressure, GPU utilization at the process level, right?", "timestamp": "00:09:29,780", "timestamp_s": 569.0}, {"text": "So this layer is all about visibility into how your infrastructure.", "timestamp": "00:09:34,270", "timestamp_s": 574.0}, {"text": "Is being consumed in real time without this, you are essentially", "timestamp": "00:09:38,590", "timestamp_s": 578.0}, {"text": "like flying blind about That is we have the performance insights.", "timestamp": "00:09:42,690", "timestamp_s": 582.0}, {"text": "So generic metrics like CPU usage won\u0027t tell you how long a training", "timestamp": "00:09:48,000", "timestamp_s": 588.0}, {"text": "model takes or when your data pipeline is getting congested.", "timestamp": "00:09:52,640", "timestamp_s": 592.0}, {"text": "So we create a custom dashboard that reflects that ML specific metrics", "timestamp": "00:09:56,970", "timestamp_s": 596.0}, {"text": "such as throughput or training loss.", "timestamp": "00:10:02,580", "timestamp_s": 602.0}, {"text": "Over the period of time.", "timestamp": "00:10:05,485", "timestamp_s": 605.0}, {"text": "And then the GPU memory fragmentation.", "timestamp": "00:10:06,745", "timestamp_s": 606.0}, {"text": "So these these give the engineers a much cleaner picture of how", "timestamp": "00:10:09,375", "timestamp_s": 609.0}, {"text": "their workloads are behaving.", "timestamp": "00:10:13,205", "timestamp_s": 613.0}, {"text": "At the top of the pyramid, if you look at, is the root cause analysis.", "timestamp": "00:10:15,455", "timestamp_s": 615.0}, {"text": "So this is the most advanced layer and arguably the most valuable as well.", "timestamp": "00:10:20,525", "timestamp_s": 620.0}, {"text": "So it.", "timestamp": "00:10:25,245", "timestamp_s": 625.0}, {"text": "Ties everything together through distributed tracing and correlation", "timestamp": "00:10:26,595", "timestamp_s": 626.0}, {"text": "correlates across the layers, like linking spikes in the GPU usages to a specific", "timestamp": "00:10:31,005", "timestamp_s": 631.0}, {"text": "phase of a model training while something breaks or slows down, this is how you", "timestamp": "00:10:37,195", "timestamp_s": 637.0}, {"text": "rapidly isolate and resolve the issue.", "timestamp": "00:10:42,635", "timestamp_s": 642.0}, {"text": "The systems we are talking are not just about alerting.", "timestamp": "00:10:45,815", "timestamp_s": 645.0}, {"text": "They\u0027re about creating a narrative that ML engineers and SREs can follow.", "timestamp": "00:10:49,445", "timestamp_s": 649.0}, {"text": "They support both issues at hand and also work on strategic decision making.", "timestamp": "00:10:54,395", "timestamp_s": 654.0}, {"text": "The most successful organizations like built all four of these layers", "timestamp": "00:11:00,575", "timestamp_s": 660.0}, {"text": "into their monitoring approach.", "timestamp": "00:11:05,135", "timestamp_s": 665.0}, {"text": "Together they create a holistic feedback.", "timestamp": "00:11:08,325", "timestamp_s": 668.0}, {"text": "That helps optimize models and improve uptime as well as cost controls.", "timestamp": "00:11:10,885", "timestamp_s": 670.0}, {"text": "Now let\u0027s explore how we can dramatically improve, data to a reduced latency in the", "timestamp": "00:11:16,885", "timestamp_s": 676.0}, {"text": "ML pipelines by optimizing the storage.", "timestamp": "00:11:23,025", "timestamp_s": 683.0}, {"text": "So as machine learning workloads become increasingly data intensive", "timestamp": "00:11:25,815", "timestamp_s": 685.0}, {"text": "storage becomes a critical bottleneck.", "timestamp": "00:11:30,645", "timestamp_s": 690.0}, {"text": "So this slides outlines like three key strategies to overcome that.", "timestamp": "00:11:33,165", "timestamp_s": 693.0}, {"text": "So if you look at the distributed file systems we have file systems", "timestamp": "00:11:37,520", "timestamp_s": 697.0}, {"text": "like GPFS, which are very distributed.", "timestamp": "00:11:41,280", "timestamp_s": 701.0}, {"text": "These are built for high throughput scenarios and are capable of parallel", "timestamp": "00:11:43,800", "timestamp_s": 703.0}, {"text": "data processing across hundreds of nodes.", "timestamp": "00:11:48,000", "timestamp_s": 708.0}, {"text": "So they outperform the traditional network storage solutions by up to eight 10 x.", "timestamp": "00:11:51,240", "timestamp_s": 711.0}, {"text": "Right, especially when working with the small, fragmented file types", "timestamp": "00:11:56,435", "timestamp_s": 716.0}, {"text": "which are common in the ML data sets.", "timestamp": "00:12:01,295", "timestamp_s": 721.0}, {"text": "The design allows like efficient aggregation of bandwidth and parallel", "timestamp": "00:12:04,205", "timestamp_s": 724.0}, {"text": "read and writes, so which accelerates the data ingestion during the training.", "timestamp": "00:12:09,365", "timestamp_s": 729.0}, {"text": "Next, we deploy the in-memory caching layers.", "timestamp": "00:12:13,885", "timestamp_s": 733.0}, {"text": "Between the persistent storage and compute resources these caching", "timestamp": "00:12:17,480", "timestamp_s": 737.0}, {"text": "layers drastically reduce IO wait times, especially for frequently", "timestamp": "00:12:22,870", "timestamp_s": 742.0}, {"text": "accessed data sets by 65% or more.", "timestamp": "00:12:26,440", "timestamp_s": 746.0}, {"text": "They also implement automatic eviction policies ensuring that the optimal", "timestamp": "00:12:29,370", "timestamp_s": 749.0}, {"text": "memory usage without manual intervention.", "timestamp": "00:12:33,760", "timestamp_s": 753.0}, {"text": "This is particularly beneficial in iterative workflows where", "timestamp": "00:12:36,460", "timestamp_s": 756.0}, {"text": "the same data sets are are.", "timestamp": "00:12:40,850", "timestamp_s": 760.0}, {"text": "Read multiple times during the model tuning.", "timestamp": "00:12:43,615", "timestamp_s": 763.0}, {"text": "Lastly, we have something called the automated storage tiering.", "timestamp": "00:12:46,605", "timestamp_s": 766.0}, {"text": "So this involves moving data between the hot and cold storage tiers based on the", "timestamp": "00:12:50,325", "timestamp_s": 770.0}, {"text": "access, frequency and performance needs.", "timestamp": "00:12:55,605", "timestamp_s": 775.0}, {"text": "So this not only ensures that the performance for the", "timestamp": "00:12:58,635", "timestamp_s": 778.0}, {"text": "active data sets, but also.", "timestamp": "00:13:02,455", "timestamp_s": 782.0}, {"text": "Achieves like the 40 to 50 percent of cost reductions by shifting the", "timestamp": "00:13:05,155", "timestamp_s": 785.0}, {"text": "inactive data to lower cost storage.", "timestamp": "00:13:10,010", "timestamp_s": 790.0}, {"text": "This is done transparently through a unified namespace.", "timestamp": "00:13:13,210", "timestamp_s": 793.0}, {"text": "So engineers don\u0027t need to worry about where the data physically rec resides.", "timestamp": "00:13:16,860", "timestamp_s": 796.0}, {"text": "So together with these kind of three layers, distributed file system, in", "timestamp": "00:13:22,500", "timestamp_s": 802.0}, {"text": "memory caching and intelligent tiering this create a robust, cost efficient", "timestamp": "00:13:28,260", "timestamp_s": 808.0}, {"text": "foundation for data driven ML pipelines.", "timestamp": "00:13:32,540", "timestamp_s": 812.0}, {"text": "The result is faster training, lower cost, and smoother operations.", "timestamp": "00:13:35,690", "timestamp_s": 815.0}, {"text": "Here we are looking at how we optimize cloud spend for machine learning", "timestamp": "00:13:40,030", "timestamp_s": 820.0}, {"text": "workloads using sport instances.", "timestamp": "00:13:44,520", "timestamp_s": 824.0}, {"text": "We start with the workload classifications.", "timestamp": "00:13:47,660", "timestamp_s": 827.0}, {"text": "So we assess the jobs based on the fall tolerance and run length", "timestamp": "00:13:50,390", "timestamp_s": 830.0}, {"text": "to identify which, which can safely run on the spot Instances.", "timestamp": "00:13:54,915", "timestamp_s": 834.0}, {"text": "Next we implement like 4,000.", "timestamp": "00:13:59,885", "timestamp_s": 839.0}, {"text": "By adding automated checkpoints and recovery systems.", "timestamp": "00:14:02,590", "timestamp_s": 842.0}, {"text": "So if a spot instance gets reclaimed, the job can resume smoothly.", "timestamp": "00:14:06,420", "timestamp_s": 846.0}, {"text": "Then comes the bidding strategy, optimization izing, the historical", "timestamp": "00:14:11,460", "timestamp_s": 851.0}, {"text": "and real time pricing data.", "timestamp": "00:14:16,920", "timestamp_s": 856.0}, {"text": "We just are bidding to maximize the savings while maintaining the job.", "timestamp": "00:14:19,020", "timestamp_s": 859.0}, {"text": "Re reliability.", "timestamp": "00:14:23,010", "timestamp_s": 863.0}, {"text": "Lastly, we adopt the hybrid deployment models.", "timestamp": "00:14:24,660", "timestamp_s": 864.0}, {"text": "So dynamic, clearly, like switching between the spot and on demand instances", "timestamp": "00:14:28,950", "timestamp_s": 868.0}, {"text": "based on cost and availability.", "timestamp": "00:14:33,920", "timestamp_s": 873.0}, {"text": "So this is one of the important models which we can deploy to make sure", "timestamp": "00:14:35,870", "timestamp_s": 875.0}, {"text": "we enable that the cost savings of up to 60 to 80% is achieved without", "timestamp": "00:14:39,705", "timestamp_s": 879.0}, {"text": "even sacrificing on the performance.", "timestamp": "00:14:46,165", "timestamp_s": 886.0}, {"text": "So moving on, let\u0027s talk about optimizing the no pool configurations.", "timestamp": "00:14:48,675", "timestamp_s": 888.0}, {"text": "So an area where we can unlock substantial performance and cost", "timestamp": "00:14:53,325", "timestamp_s": 893.0}, {"text": "benefits, especially in machine learning and compute heavy environments.", "timestamp": "00:14:57,165", "timestamp_s": 897.0}, {"text": "So first we start with heterogeneous hardware segmentation.", "timestamp": "00:15:01,905", "timestamp_s": 901.0}, {"text": "So rather than mixing all the GPUs, GU types into a single nor pool.", "timestamp": "00:15:05,855", "timestamp_s": 905.0}, {"text": "We create a specialized nor pools for different GPUs, like eight", "timestamp": "00:15:11,025", "timestamp_s": 911.0}, {"text": "hundreds going to one nor pool.", "timestamp": "00:15:15,075", "timestamp_s": 915.0}, {"text": "And then the v hundreds go to a different nor pool.", "timestamp": "00:15:16,875", "timestamp_s": 916.0}, {"text": "So each has a different compute and the memory characteristics.", "timestamp": "00:15:20,475", "timestamp_s": 920.0}, {"text": "So assigning the workloads without creating the nor pools", "timestamp": "00:15:24,075", "timestamp_s": 924.0}, {"text": "may lead to and inefficiencies.", "timestamp": "00:15:27,505", "timestamp_s": 927.0}, {"text": "So by using Ts and tolerations, we make sure workloads are scheduled only to.", "timestamp": "00:15:30,395", "timestamp_s": 930.0}, {"text": "Hardware data optimized for.", "timestamp": "00:15:36,525", "timestamp_s": 936.0}, {"text": "Next we have resource optimization.", "timestamp": "00:15:38,585", "timestamp_s": 938.0}, {"text": "This is about fine tuning the CPU and the GPU as well as the memory ratios", "timestamp": "00:15:41,315", "timestamp_s": 941.0}, {"text": "to match the ml workload profiles.", "timestamp": "00:15:46,385", "timestamp_s": 946.0}, {"text": "For example inference might require more CPU, whereas training", "timestamp": "00:15:49,400", "timestamp_s": 949.0}, {"text": "model could need more GPU memory.", "timestamp": "00:15:54,225", "timestamp_s": 954.0}, {"text": "So if.", "timestamp": "00:15:56,750", "timestamp_s": 956.0}, {"text": "If these ratios are mislead or miscalculated, we either end up with", "timestamp": "00:15:57,590", "timestamp_s": 957.0}, {"text": "bottlenecks or underutilized hardware, both of which are very costly.", "timestamp": "00:16:02,360", "timestamp_s": 962.0}, {"text": "Then there is network topology alignment where you\u0027ll you are running, when you\u0027re", "timestamp": "00:16:06,815", "timestamp_s": 966.0}, {"text": "running your distributed training network bandwidth, and latency matters a lot.", "timestamp": "00:16:11,395", "timestamp_s": 971.0}, {"text": "So we design a no pools to align with.", "timestamp": "00:16:17,165", "timestamp_s": 977.0}, {"text": "With the physical infrastructure, that means like grouping into a nodes nodes", "timestamp": "00:16:21,320", "timestamp_s": 981.0}, {"text": "with the high speed interconnects like NV link or InfiniBand into a same pool.", "timestamp": "00:16:26,860", "timestamp_s": 986.0}, {"text": "This ensures that the data exchanges between the GPUs in a fast and consistent", "timestamp": "00:16:33,060", "timestamp_s": 993.0}, {"text": "manner, which avoids the slowdowns also during the training process.", "timestamp": "00:16:37,960", "timestamp_s": 997.0}, {"text": "Now let\u0027s quickly look at how we can share resources fairly when multiple", "timestamp": "00:16:42,450", "timestamp_s": 1002.0}, {"text": "teams are working on the same system.", "timestamp": "00:16:47,180", "timestamp_s": 1007.0}, {"text": "First we use namespace to keep things organized.", "timestamp": "00:16:49,870", "timestamp_s": 1009.0}, {"text": "So each team or, type of work like production versus a non-production", "timestamp": "00:16:53,330", "timestamp_s": 1013.0}, {"text": "gets its own namespace.", "timestamp": "00:16:58,190", "timestamp_s": 1018.0}, {"text": "So this way we can set up different permissions use specific settings", "timestamp": "00:17:00,140", "timestamp_s": 1020.0}, {"text": "for each environment and keep the network traffic separate.", "timestamp": "00:17:04,540", "timestamp_s": 1024.0}, {"text": "Second, we can set up the resource controls.", "timestamp": "00:17:08,740", "timestamp_s": 1028.0}, {"text": "That means we limit how GPU memory and storage each team can use.", "timestamp": "00:17:11,710", "timestamp_s": 1031.0}, {"text": "This helps, make, making sure like team doesn\u0027t accidentally take", "timestamp": "00:17:17,110", "timestamp_s": 1037.0}, {"text": "more share than which is allocated.", "timestamp": "00:17:21,910", "timestamp_s": 1041.0}, {"text": "Then finally we give higher priority to critical jobs like live production jobs.", "timestamp": "00:17:24,900", "timestamp_s": 1044.0}, {"text": "At the same time, we make sure like every team gets at least a minimum resources", "timestamp": "00:17:30,640", "timestamp_s": 1050.0}, {"text": "which are needed for their operations.", "timestamp": "00:17:35,930", "timestamp_s": 1055.0}, {"text": "So this avoids the problem of one team overutilizing the resources while", "timestamp": "00:17:37,850", "timestamp_s": 1057.0}, {"text": "the other team doesn\u0027t get anything.", "timestamp": "00:17:42,810", "timestamp_s": 1062.0}, {"text": "So the overall this setup.", "timestamp": "00:17:44,940", "timestamp_s": 1064.0}, {"text": "Keeps fair, reliable, and running the infrastructure smoothly for everyone.", "timestamp": "00:17:46,985", "timestamp_s": 1066.0}, {"text": "Here\u0027s what an impact these optimization strategies can have.", "timestamp": "00:17:52,465", "timestamp_s": 1072.0}, {"text": "So in one of the case studies, it was observed that companies that", "timestamp": "00:17:56,270", "timestamp_s": 1076.0}, {"text": "applied this techniques saw an average of 43% reduction in costs.", "timestamp": "00:17:59,660", "timestamp_s": 1079.0}, {"text": "That, near that is nearly half of their infrastructure spending saved just by.", "timestamp": "00:18:04,530", "timestamp_s": 1084.0}, {"text": "Tuning the things right next.", "timestamp": "00:18:11,050", "timestamp_s": 1091.0}, {"text": "Training jobs became much faster, like 2.8 times faster on average.", "timestamp": "00:18:13,820", "timestamp_s": 1093.0}, {"text": "That means models are trained and ready in a bayless time which", "timestamp": "00:18:19,350", "timestamp_s": 1099.0}, {"text": "speeding up the entire development cycle study also suggests that.", "timestamp": "00:18:23,870", "timestamp_s": 1103.0}, {"text": "There was a big jump in the GPU usage, so which is up around 67%.", "timestamp": "00:18:28,640", "timestamp_s": 1108.0}, {"text": "So instead of having expensive GPU sitting idle, they\u0027re being", "timestamp": "00:18:35,720", "timestamp_s": 1115.0}, {"text": "used efficiently across a cluster.", "timestamp": "00:18:40,880", "timestamp_s": 1120.0}, {"text": "And finally, the production models became more reliable with 94% of the", "timestamp": "00:18:44,300", "timestamp_s": 1124.0}, {"text": "inherent requests, meeting the SLA goals.", "timestamp": "00:18:49,100", "timestamp_s": 1129.0}, {"text": "So that\u0027s a strong sign of stable production ready systems.", "timestamp": "00:18:52,120", "timestamp_s": 1132.0}, {"text": "So this slides gives us a step by step guide for applying the", "timestamp": "00:18:56,070", "timestamp_s": 1136.0}, {"text": "strategies we\u0027ve covered so far.", "timestamp": "00:19:00,130", "timestamp_s": 1140.0}, {"text": "Step one is to establish the baseline metrics.", "timestamp": "00:19:02,460", "timestamp_s": 1142.0}, {"text": "Like before making any changes, we need to understand how our", "timestamp": "00:19:06,120", "timestamp_s": 1146.0}, {"text": "resources are being used right now.", "timestamp": "00:19:10,000", "timestamp_s": 1150.0}, {"text": "That includes tracking the GPU and the CP usage.", "timestamp": "00:19:12,725", "timestamp_s": 1152.0}, {"text": "Training times and how much it\u0027s costing up per model.", "timestamp": "00:19:15,920", "timestamp_s": 1155.0}, {"text": "Then step two is to go for the quick wins.", "timestamp": "00:19:19,720", "timestamp_s": 1159.0}, {"text": "Like there are changes that are not.", "timestamp": "00:19:23,110", "timestamp_s": 1163.0}, {"text": "Easy to be implemented and give immediate value without disruption", "timestamp": "00:19:26,010", "timestamp_s": 1166.0}, {"text": "disrupting our current setup.", "timestamp": "00:19:30,510", "timestamp_s": 1170.0}, {"text": "So examples include resizing the resource limits correctly, and then enabling", "timestamp": "00:19:33,420", "timestamp_s": 1173.0}, {"text": "simple auto scaling, and then using the right storage classes for each workloads.", "timestamp": "00:19:38,645", "timestamp_s": 1178.0}, {"text": "Then step three is to deploy more advanced optimizations.", "timestamp": "00:19:44,445", "timestamp_s": 1184.0}, {"text": "Once the basics are in place, we can start using smarter tactics like custom", "timestamp": "00:19:48,330", "timestamp_s": 1188.0}, {"text": "autoscaling based on the real time metrics using spot instances to cut", "timestamp": "00:19:53,830", "timestamp_s": 1193.0}, {"text": "the costs or even fine tuning the node placement based on the hardware topology.", "timestamp": "00:19:58,430", "timestamp_s": 1198.0}, {"text": "And then finally step four is continuously continuous refinement.", "timestamp": "00:20:04,190", "timestamp_s": 1204.0}, {"text": "So optimization is not.", "timestamp": "00:20:09,500", "timestamp_s": 1209.0}, {"text": "One, one time thing.", "timestamp": "00:20:11,480", "timestamp_s": 1211.0}, {"text": "So it needs to evolve as workloads and technologies change.", "timestamp": "00:20:13,190", "timestamp_s": 1213.0}, {"text": "That means doing regular reviews, setting up alerts for unusual costs,", "timestamp": "00:20:17,090", "timestamp_s": 1217.0}, {"text": "and making sure our infrastructure and models are improved over over", "timestamp": "00:20:22,260", "timestamp_s": 1222.0}, {"text": "time which is very important, right?", "timestamp": "00:20:26,860", "timestamp_s": 1226.0}, {"text": "So in short, this roadmap helps balance the quick efficiency gains", "timestamp": "00:20:29,840", "timestamp_s": 1229.0}, {"text": "with smart long-term planning.", "timestamp": "00:20:33,860", "timestamp_s": 1233.0}, {"text": "Finally, thank you for joining me in this session and I hope you have a great", "timestamp": "00:20:36,140", "timestamp_s": 1236.0}, {"text": "time at this particular conference.", "timestamp": "00:20:39,950", "timestamp_s": 1239.0}, {"text": "Thank you.", "timestamp": "00:20:42,170", "timestamp_s": 1242.0}, {"text": "Hello everyone.", "timestamp": "00:20:43,060", "timestamp_s": 1243.0}, {"text": "Thank you for joining me in this conference.", "timestamp": "00:20:44,080", "timestamp_s": 1244.0}, {"text": "Today we will discuss on how FinTech reliability is achieved", "timestamp": "00:20:46,510", "timestamp_s": 1246.0}, {"text": "through SRE Innovation.", "timestamp": "00:20:49,900", "timestamp_s": 1249.0}, {"text": "So as organizations accelerate their AI initiatives, many are discovering", "timestamp": "00:20:51,295", "timestamp_s": 1251.0}, {"text": "that traditional infrastructure was not built to handle the high demands", "timestamp": "00:20:55,825", "timestamp_s": 1255.0}, {"text": "of machine learning operations.", "timestamp": "00:21:00,365", "timestamp_s": 1260.0}, {"text": "These workloads are highly resource intensive and are unpredictable.", "timestamp": "00:21:02,625", "timestamp_s": 1262.0}, {"text": "While Kubernetes has emerged as a platform of choice for managing", "timestamp": "00:21:06,805", "timestamp_s": 1266.0}, {"text": "these environments, simply running the containers is not enough.", "timestamp": "00:21:11,155", "timestamp_s": 1271.0}, {"text": "The challenge lies in running them optimally.", "timestamp": "00:21:15,005", "timestamp_s": 1275.0}, {"text": "What we are seeing is that many teams are struggling, be it dealing with", "timestamp": "00:21:18,385", "timestamp_s": 1278.0}, {"text": "over-provisioned resources, unpredictable costs and performance bottlenecks.", "timestamp": "00:21:23,335", "timestamp_s": 1283.0}, {"text": "So in this particular session, I would like to walk you through a set of proven", "timestamp": "00:21:28,745", "timestamp_s": 1288.0}, {"text": "SRE strategies that are designed to increase performance, reduce costs, and", "timestamp": "00:21:33,585", "timestamp_s": 1293.0}, {"text": "significantly improve the reliability of the AI ML pipelines running in.", "timestamp": "00:21:39,245", "timestamp_s": 1299.0}, {"text": "Kubernetes these approaches come directly from the high stakes environments", "timestamp": "00:21:44,600", "timestamp_s": 1304.0}, {"text": "these where even minor inefficiencies can lead to a major financial losses.", "timestamp": "00:21:49,240", "timestamp_s": 1309.0}, {"text": "So together we\u0027ll go through the techniques that have delivered up", "timestamp": "00:21:54,550", "timestamp_s": 1314.0}, {"text": "to a 60% improvements in resource utilization and substantially", "timestamp": "00:21:58,280", "timestamp_s": 1318.0}, {"text": "reductions in incident rates.", "timestamp": "00:22:03,100", "timestamp_s": 1323.0}, {"text": "All while maintaining the agility and performance.", "timestamp": "00:22:05,475", "timestamp_s": 1325.0}, {"text": "Ultimately, this presentation is about more than just keeping the systems online.", "timestamp": "00:22:09,515", "timestamp_s": 1329.0}, {"text": "It\u0027s about engineering confidently at scale in an increased AI", "timestamp": "00:22:14,875", "timestamp_s": 1334.0}, {"text": "driven financial ecosystem.", "timestamp": "00:22:19,405", "timestamp_s": 1339.0}, {"text": "So let\u0027s take a closer look on how the transformation is happening.", "timestamp": "00:22:21,355", "timestamp_s": 1341.0}, {"text": "Okay.", "timestamp": "00:22:25,925", "timestamp_s": 1345.0}, {"text": "When we talk about scaling AI infrastructure efficiently, GPU", "timestamp": "00:22:26,135", "timestamp_s": 1346.0}, {"text": "utilization is one of the most overlooked and most expensive", "timestamp": "00:22:29,675", "timestamp_s": 1349.0}, {"text": "and has more bottlenecks.", "timestamp": "00:22:33,765", "timestamp_s": 1353.0}, {"text": "So let\u0027s look at how we can dramatically shift that performance to cost ratio", "timestamp": "00:22:35,415", "timestamp_s": 1355.0}, {"text": "using smart GPU level strategies.", "timestamp": "00:22:41,215", "timestamp_s": 1361.0}, {"text": "Let\u0027s start with the MIG or the multi-instance GPU Partitioning.", "timestamp": "00:22:44,255", "timestamp_s": 1364.0}, {"text": "So this is a feature available in NVIDIA\u0027s 800 and h hundred series.", "timestamp": "00:22:48,065", "timestamp_s": 1368.0}, {"text": "It allows you to carve up a single GPU into a multiple isolated instances,", "timestamp": "00:22:53,265", "timestamp_s": 1373.0}, {"text": "which enables like multiple workloads to run simultaneously without any issues.", "timestamp": "00:22:59,275", "timestamp_s": 1379.0}, {"text": "Our real world tests have shown up to a seven x better utilization especially in", "timestamp": "00:23:03,925", "timestamp_s": 1383.0}, {"text": "environments with varied inference loads.", "timestamp": "00:23:09,125", "timestamp_s": 1389.0}, {"text": "Next in in memory efficiency QDA provides like a low level access to memory", "timestamp": "00:23:12,405", "timestamp_s": 1392.0}, {"text": "management, allowing us to opti optimize.", "timestamp": "00:23:18,985", "timestamp_s": 1398.0}, {"text": "Tensor core use and reduce the fragmentation.", "timestamp": "00:23:22,345", "timestamp_s": 1402.0}, {"text": "So this can lead to 15 to 30% of throughput gains particularly for", "timestamp": "00:23:25,685", "timestamp_s": 1405.0}, {"text": "vision heavy models like ID verification or biometric fraud detection.", "timestamp": "00:23:31,265", "timestamp_s": 1411.0}, {"text": "Oh, coming to precision optimization.", "timestamp": "00:23:36,065", "timestamp_s": 1416.0}, {"text": "In many FinTech companies like ML workloads, especially during inference.", "timestamp": "00:23:38,285", "timestamp_s": 1418.0}, {"text": "Full FFP 32 precision is not always needed.", "timestamp": "00:23:43,545", "timestamp_s": 1423.0}, {"text": "So by switching the, switching it to FP 16 or even INT eight, organizations can", "timestamp": "00:23:47,085", "timestamp_s": 1427.0}, {"text": "drastically shrink the memory footprint.", "timestamp": "00:23:54,285", "timestamp_s": 1434.0}, {"text": "So these techniques are game changers.", "timestamp": "00:23:57,015", "timestamp_s": 1437.0}, {"text": "They maximize the GPU resource usage and provide critical improvements in", "timestamp": "00:23:58,975", "timestamp_s": 1438.0}, {"text": "both performance and cost efficiency.", "timestamp": "00:24:03,885", "timestamp_s": 1443.0}, {"text": "Moving on to the next slide.", "timestamp": "00:24:06,945", "timestamp_s": 1446.0}, {"text": "On auto-scaling strategies, as organizations push to run ML workloads", "timestamp": "00:24:08,205", "timestamp_s": 1448.0}, {"text": "more efficiently traditional auto-scaling strategies are not enough, especially", "timestamp": "00:24:12,545", "timestamp_s": 1452.0}, {"text": "in GPU accelerated environments.", "timestamp": "00:24:16,825", "timestamp_s": 1456.0}, {"text": "Here we\u0027ll go through four techniques that enable smarter ML strategies.", "timestamp": "00:24:18,895", "timestamp_s": 1458.0}, {"text": "Which will help with the autoscaling.", "timestamp": "00:24:24,665", "timestamp_s": 1464.0}, {"text": "First we\u0027ll go through the workload profiling.", "timestamp": "00:24:27,165", "timestamp_s": 1467.0}, {"text": "So the foundation of autoscaling is understanding when and how much to scale.", "timestamp": "00:24:29,445", "timestamp_s": 1469.0}, {"text": "So workload profiling uses the historical telemetry across both", "timestamp": "00:24:34,935", "timestamp_s": 1474.0}, {"text": "training and inference cycles.", "timestamp": "00:24:38,895", "timestamp_s": 1478.0}, {"text": "To uncover the usage patterns.", "timestamp": "00:24:41,585", "timestamp_s": 1481.0}, {"text": "So this helps define the baseline resource needs, identify the spikes", "timestamp": "00:24:43,775", "timestamp_s": 1483.0}, {"text": "during the market, events or frauds, fraud searches, and ensures that", "timestamp": "00:24:48,945", "timestamp_s": 1488.0}, {"text": "the auto-scaling decisions are data informed and are not reactive.", "timestamp": "00:24:52,965", "timestamp_s": 1492.0}, {"text": "Coming to the custom metrics pipeline.", "timestamp": "00:24:57,605", "timestamp_s": 1497.0}, {"text": "So if you look at the out of box Kubernetes like Kubernetes HPA doesn\u0027t", "timestamp": "00:25:00,485", "timestamp_s": 1500.0}, {"text": "know anything about the machine learning.", "timestamp": "00:25:05,145", "timestamp_s": 1505.0}, {"text": "That\u0027s where like the custom metrics come in using Prometheus adapters.", "timestamp": "00:25:07,455", "timestamp_s": 1507.0}, {"text": "We expose the ml. Relevant signals like the Q Depth batch processing duration,", "timestamp": "00:25:12,255", "timestamp_s": 1512.0}, {"text": "and GPU memory pressure to the autoscaler.", "timestamp": "00:25:18,255", "timestamp_s": 1518.0}, {"text": "So this enables a precise decision based on the actual workload characteristics,", "timestamp": "00:25:21,845", "timestamp_s": 1521.0}, {"text": "not just the CPU load or the pod count.", "timestamp": "00:25:28,185", "timestamp_s": 1528.0}, {"text": "Coming to predictive scaling for recurring or seasonal workloads.", "timestamp": "00:25:31,415", "timestamp_s": 1531.0}, {"text": "Think EOD model model retaining or the batch scoring jobs.", "timestamp": "00:25:36,065", "timestamp_s": 1536.0}, {"text": "We can apply the time-based or ML based scaling triggers.", "timestamp": "00:25:41,155", "timestamp_s": 1541.0}, {"text": "So predictive models can anticipate the spikes before they happen.", "timestamp": "00:25:45,445", "timestamp_s": 1545.0}, {"text": "Enabling warm pools to be spun up just in time.", "timestamp": "00:25:49,560", "timestamp_s": 1549.0}, {"text": "This reduces the cold start latency by up to 85%, especially valuable", "timestamp": "00:25:52,900", "timestamp_s": 1552.0}, {"text": "for real time FinTech systems.", "timestamp": "00:25:58,840", "timestamp_s": 1558.0}, {"text": "Coming to the buffer management we can\u0027t forget the cluster resilience.", "timestamp": "00:26:01,720", "timestamp_s": 1561.0}, {"text": "So the buffer management means deliberately over provisioning a", "timestamp": "00:26:06,565", "timestamp_s": 1566.0}, {"text": "small GPU headroom during critical critical hours like trading sessions", "timestamp": "00:26:11,025", "timestamp_s": 1571.0}, {"text": "or things like holiday season, right?", "timestamp": "00:26:16,335", "timestamp_s": 1576.0}, {"text": "Then you can scale them back down more aggressively after the hours.", "timestamp": "00:26:19,155", "timestamp_s": 1579.0}, {"text": "This balances the control cost control with high availability, which is", "timestamp": "00:26:23,520", "timestamp_s": 1583.0}, {"text": "non-negotiable for financial applications.", "timestamp": "00:26:28,000", "timestamp_s": 1588.0}, {"text": "So these autoscaling strategies go well beyond the HPA setup.", "timestamp": "00:26:31,090", "timestamp_s": 1591.0}, {"text": "When implemented together, they enable the organizations to sustain peak", "timestamp": "00:26:36,230", "timestamp_s": 1596.0}, {"text": "model performance with minimal costs.", "timestamp": "00:26:40,260", "timestamp_s": 1600.0}, {"text": "Okay.", "timestamp": "00:26:43,515", "timestamp_s": 1603.0}, {"text": "Coming to the multi GPU training orchestration the biggest bottlenecks", "timestamp": "00:26:43,725", "timestamp_s": 1603.0}, {"text": "in scaling a machine learning training right is GPU job orchestrations.", "timestamp": "00:26:47,875", "timestamp_s": 1607.0}, {"text": "It\u0027s not just having about having more GPUs.", "timestamp": "00:26:54,595", "timestamp_s": 1614.0}, {"text": "It\u0027s about using them in intelligently.", "timestamp": "00:26:58,155", "timestamp_s": 1618.0}, {"text": "When we orchestrate jobs sufficiently across the gps, especially in", "timestamp": "00:27:00,675", "timestamp_s": 1620.0}, {"text": "distributed environments, we unlock, huge throughput gains in cost efficiency.", "timestamp": "00:27:04,515", "timestamp_s": 1624.0}, {"text": "The first step is topology aware scheduling.", "timestamp": "00:27:10,215", "timestamp_s": 1630.0}, {"text": "Why does this matter?", "timestamp": "00:27:13,315", "timestamp_s": 1633.0}, {"text": "So when jobs span across multiple GPUs, especially across nodes network", "timestamp": "00:27:14,455", "timestamp_s": 1634.0}, {"text": "latency becomes c enemy like we solve this by using the g Kubernetes node", "timestamp": "00:27:20,395", "timestamp_s": 1640.0}, {"text": "affinity rules to schedule the jobs on nodes that are physically close to the", "timestamp": "00:27:25,755", "timestamp_s": 1645.0}, {"text": "interconnect through high bandwidth.", "timestamp": "00:27:30,915", "timestamp_s": 1650.0}, {"text": "Links like NV link, or PCIE.", "timestamp": "00:27:33,500", "timestamp_s": 1653.0}, {"text": "This minimizes the communication overhead and drastically improves the", "timestamp": "00:27:36,880", "timestamp_s": 1656.0}, {"text": "performance and distributed training.", "timestamp": "00:27:40,150", "timestamp_s": 1660.0}, {"text": "Next we move on to distributed training frameworks.", "timestamp": "00:27:42,740", "timestamp_s": 1662.0}, {"text": "So framework for frameworks like TensorFlows multi", "timestamp": "00:27:45,680", "timestamp_s": 1665.0}, {"text": "worker needs to be tuned.", "timestamp": "00:27:49,520", "timestamp_s": 1669.0}, {"text": "To our infrastructure, often we need poor performance because", "timestamp": "00:27:52,255", "timestamp_s": 1672.0}, {"text": "training scripts are not optimized.", "timestamp": "00:27:56,685", "timestamp_s": 1676.0}, {"text": "So we address this with Custom gate template that enable, like", "timestamp": "00:27:59,015", "timestamp_s": 1679.0}, {"text": "dynamic scaling, reduces the training time variance, and", "timestamp": "00:28:03,905", "timestamp_s": 1683.0}, {"text": "it minimizes the communication bottlenecks between the nodes.", "timestamp": "00:28:07,685", "timestamp_s": 1687.0}, {"text": "Finally, there\u0027s a priority based preemptive, mechanism.", "timestamp": "00:28:11,020", "timestamp_s": 1691.0}, {"text": "So in this particular technique if you look at our shared clusters.", "timestamp": "00:28:15,345", "timestamp_s": 1695.0}, {"text": "When multiple teams submit the jobs, it\u0027s critical to ensure that high", "timestamp": "00:28:19,565", "timestamp_s": 1699.0}, {"text": "priority jobs don\u0027t wait in line behind the rule, low priority ones.", "timestamp": "00:28:25,015", "timestamp_s": 1705.0}, {"text": "So we implement intelligent queuing mechanisms that evaluate", "timestamp": "00:28:29,635", "timestamp_s": 1709.0}, {"text": "the job priority and then.", "timestamp": "00:28:33,905", "timestamp_s": 1713.0}, {"text": "Deadlines and then resource fairness.", "timestamp": "00:28:35,680", "timestamp_s": 1715.0}, {"text": "It\u0027s like air traffic control for GPUs, like preventing the collisions,", "timestamp": "00:28:38,290", "timestamp_s": 1718.0}, {"text": "reducing the idle time, and keeping the system fair and efficient.", "timestamp": "00:28:42,700", "timestamp_s": 1722.0}, {"text": "The key takeaway is like when you bring all these three pillars together, the", "timestamp": "00:28:47,040", "timestamp_s": 1727.0}, {"text": "topology awareness, smart frameworks, and priority based scheduling you can", "timestamp": "00:28:52,170", "timestamp_s": 1732.0}, {"text": "cut the training time by 30 to 50%.", "timestamp": "00:28:57,080", "timestamp_s": 1737.0}, {"text": "That\u0027s a massive impact when you are running the running hundreds or even", "timestamp": "00:29:00,500", "timestamp_s": 1740.0}, {"text": "thousands of experiments each week.", "timestamp": "00:29:04,640", "timestamp_s": 1744.0}, {"text": "Okay.", "timestamp": "00:29:07,390", "timestamp_s": 1747.0}, {"text": "Moving on to the advanced monitoring and observability.", "timestamp": "00:29:07,720", "timestamp_s": 1747.0}, {"text": "As machine learning workloads grow in complexity and scale, the need for robust", "timestamp": "00:29:10,450", "timestamp_s": 1750.0}, {"text": "observability becomes absolutely critical.", "timestamp": "00:29:15,440", "timestamp_s": 1755.0}, {"text": "Not just for performance, but for cost efficiency and op", "timestamp": "00:29:18,210", "timestamp_s": 1758.0}, {"text": "and operational reliability.", "timestamp": "00:29:22,020", "timestamp_s": 1762.0}, {"text": "So this.", "timestamp": "00:29:23,880", "timestamp_s": 1763.0}, {"text": "Here outlines the layered approach.", "timestamp": "00:29:25,270", "timestamp_s": 1765.0}, {"text": "We take towards building a comprehensive monitoring stack for", "timestamp": "00:29:28,445", "timestamp_s": 1768.0}, {"text": "the machine learning infrastructure.", "timestamp": "00:29:31,975", "timestamp_s": 1771.0}, {"text": "So let\u0027s start at the foundation, the historical data.", "timestamp": "00:29:34,395", "timestamp_s": 1774.0}, {"text": "So long-term storage of metrics is essential for capacity", "timestamp": "00:29:38,220", "timestamp_s": 1778.0}, {"text": "planning and trend analysis.", "timestamp": "00:29:42,340", "timestamp_s": 1782.0}, {"text": "Whether you are sizing your GPU clusters for next quarter\u0027s workloads,", "timestamp": "00:29:44,290", "timestamp_s": 1784.0}, {"text": "or preparing for upcoming model retraining cycles, you need to.", "timestamp": "00:29:49,040", "timestamp_s": 1789.0}, {"text": "Analyze historical context, right?", "timestamp": "00:29:54,380", "timestamp_s": 1794.0}, {"text": "So it helps avoid over provisioning and gives a factual basis", "timestamp": "00:29:56,810", "timestamp_s": 1796.0}, {"text": "for infrastructure budgeting.", "timestamp": "00:30:01,720", "timestamp_s": 1801.0}, {"text": "So next we move up to resource utilization.", "timestamp": "00:30:03,910", "timestamp_s": 1803.0}, {"text": "So this is where the fine-grained metrics come in, like the CPU usage,", "timestamp": "00:30:07,450", "timestamp_s": 1807.0}, {"text": "memory pressure, GPU utilization at the process level, right?", "timestamp": "00:30:12,340", "timestamp_s": 1812.0}, {"text": "So this layer is all about visibility into how your infrastructure.", "timestamp": "00:30:16,830", "timestamp_s": 1816.0}, {"text": "Is being consumed in real time without this, you are essentially", "timestamp": "00:30:21,150", "timestamp_s": 1821.0}, {"text": "like flying blind about That is we have the performance insights.", "timestamp": "00:30:25,250", "timestamp_s": 1825.0}, {"text": "So generic metrics like CPU usage won\u0027t tell you how long a training", "timestamp": "00:30:30,560", "timestamp_s": 1830.0}, {"text": "model takes or when your data pipeline is getting congested.", "timestamp": "00:30:35,200", "timestamp_s": 1835.0}, {"text": "So we create a custom dashboard that reflects that ML specific metrics", "timestamp": "00:30:39,530", "timestamp_s": 1839.0}, {"text": "such as throughput or training loss.", "timestamp": "00:30:45,140", "timestamp_s": 1845.0}, {"text": "Over the period of time.", "timestamp": "00:30:48,045", "timestamp_s": 1848.0}, {"text": "And then the GPU memory fragmentation.", "timestamp": "00:30:49,305", "timestamp_s": 1849.0}, {"text": "So these these give the engineers a much cleaner picture of how", "timestamp": "00:30:51,935", "timestamp_s": 1851.0}, {"text": "their workloads are behaving.", "timestamp": "00:30:55,765", "timestamp_s": 1855.0}, {"text": "At the top of the pyramid, if you look at, is the root cause analysis.", "timestamp": "00:30:58,015", "timestamp_s": 1858.0}, {"text": "So this is the most advanced layer and arguably the most valuable as well.", "timestamp": "00:31:03,085", "timestamp_s": 1863.0}, {"text": "So it.", "timestamp": "00:31:07,805", "timestamp_s": 1867.0}, {"text": "Ties everything together through distributed tracing and correlation", "timestamp": "00:31:09,155", "timestamp_s": 1869.0}, {"text": "correlates across the layers, like linking spikes in the GPU usages to a specific", "timestamp": "00:31:13,565", "timestamp_s": 1873.0}, {"text": "phase of a model training while something breaks or slows down, this is how you", "timestamp": "00:31:19,755", "timestamp_s": 1879.0}, {"text": "rapidly isolate and resolve the issue.", "timestamp": "00:31:25,195", "timestamp_s": 1885.0}, {"text": "The systems we are talking are not just about alerting.", "timestamp": "00:31:28,375", "timestamp_s": 1888.0}, {"text": "They\u0027re about creating a narrative that ML engineers and SREs can follow.", "timestamp": "00:31:32,005", "timestamp_s": 1892.0}, {"text": "They support both issues at hand and also work on strategic decision making.", "timestamp": "00:31:36,955", "timestamp_s": 1896.0}, {"text": "The most successful organizations like built all four of these layers", "timestamp": "00:31:43,135", "timestamp_s": 1903.0}, {"text": "into their monitoring approach.", "timestamp": "00:31:47,695", "timestamp_s": 1907.0}, {"text": "Together they create a holistic feedback.", "timestamp": "00:31:50,885", "timestamp_s": 1910.0}, {"text": "That helps optimize models and improve uptime as well as cost controls.", "timestamp": "00:31:53,445", "timestamp_s": 1913.0}, {"text": "Now let\u0027s explore how we can dramatically improve, data to a reduced latency in the", "timestamp": "00:31:59,445", "timestamp_s": 1919.0}, {"text": "ML pipelines by optimizing the storage.", "timestamp": "00:32:05,585", "timestamp_s": 1925.0}, {"text": "So as machine learning workloads become increasingly data intensive", "timestamp": "00:32:08,375", "timestamp_s": 1928.0}, {"text": "storage becomes a critical bottleneck.", "timestamp": "00:32:13,205", "timestamp_s": 1933.0}, {"text": "So this slides outlines like three key strategies to overcome that.", "timestamp": "00:32:15,725", "timestamp_s": 1935.0}, {"text": "So if you look at the distributed file systems we have file systems", "timestamp": "00:32:20,080", "timestamp_s": 1940.0}, {"text": "like GPFS, which are very distributed.", "timestamp": "00:32:23,840", "timestamp_s": 1943.0}, {"text": "These are built for high throughput scenarios and are capable of parallel", "timestamp": "00:32:26,360", "timestamp_s": 1946.0}, {"text": "data processing across hundreds of nodes.", "timestamp": "00:32:30,560", "timestamp_s": 1950.0}, {"text": "So they outperform the traditional network storage solutions by up to eight 10 x.", "timestamp": "00:32:33,800", "timestamp_s": 1953.0}, {"text": "Right, especially when working with the small, fragmented file types", "timestamp": "00:32:38,995", "timestamp_s": 1958.0}, {"text": "which are common in the ML data sets.", "timestamp": "00:32:43,855", "timestamp_s": 1963.0}, {"text": "The design allows like efficient aggregation of bandwidth and parallel", "timestamp": "00:32:46,765", "timestamp_s": 1966.0}, {"text": "read and writes, so which accelerates the data ingestion during the training.", "timestamp": "00:32:51,925", "timestamp_s": 1971.0}, {"text": "Next, we deploy the in-memory caching layers.", "timestamp": "00:32:56,445", "timestamp_s": 1976.0}, {"text": "Between the persistent storage and compute resources these caching", "timestamp": "00:33:00,040", "timestamp_s": 1980.0}, {"text": "layers drastically reduce IO wait times, especially for frequently", "timestamp": "00:33:05,430", "timestamp_s": 1985.0}, {"text": "accessed data sets by 65% or more.", "timestamp": "00:33:09,000", "timestamp_s": 1989.0}, {"text": "They also implement automatic eviction policies ensuring that the optimal", "timestamp": "00:33:11,930", "timestamp_s": 1991.0}, {"text": "memory usage without manual intervention.", "timestamp": "00:33:16,320", "timestamp_s": 1996.0}, {"text": "This is particularly beneficial in iterative workflows where", "timestamp": "00:33:19,020", "timestamp_s": 1999.0}, {"text": "the same data sets are are.", "timestamp": "00:33:23,410", "timestamp_s": 2003.0}, {"text": "Read multiple times during the model tuning.", "timestamp": "00:33:26,175", "timestamp_s": 2006.0}, {"text": "Lastly, we have something called the automated storage tiering.", "timestamp": "00:33:29,165", "timestamp_s": 2009.0}, {"text": "So this involves moving data between the hot and cold storage tiers based on the", "timestamp": "00:33:32,885", "timestamp_s": 2012.0}, {"text": "access, frequency and performance needs.", "timestamp": "00:33:38,165", "timestamp_s": 2018.0}, {"text": "So this not only ensures that the performance for the", "timestamp": "00:33:41,195", "timestamp_s": 2021.0}, {"text": "active data sets, but also.", "timestamp": "00:33:45,015", "timestamp_s": 2025.0}, {"text": "Achieves like the 40 to 50 percent of cost reductions by shifting the", "timestamp": "00:33:47,715", "timestamp_s": 2027.0}, {"text": "inactive data to lower cost storage.", "timestamp": "00:33:52,570", "timestamp_s": 2032.0}, {"text": "This is done transparently through a unified namespace.", "timestamp": "00:33:55,770", "timestamp_s": 2035.0}, {"text": "So engineers don\u0027t need to worry about where the data physically rec resides.", "timestamp": "00:33:59,420", "timestamp_s": 2039.0}, {"text": "So together with these kind of three layers, distributed file system, in", "timestamp": "00:34:05,060", "timestamp_s": 2045.0}, {"text": "memory caching and intelligent tiering this create a robust, cost efficient", "timestamp": "00:34:10,820", "timestamp_s": 2050.0}, {"text": "foundation for data driven ML pipelines.", "timestamp": "00:34:15,100", "timestamp_s": 2055.0}, {"text": "The result is faster training, lower cost, and smoother operations.", "timestamp": "00:34:18,250", "timestamp_s": 2058.0}, {"text": "Here we are looking at how we optimize cloud spend for machine learning", "timestamp": "00:34:22,590", "timestamp_s": 2062.0}, {"text": "workloads using sport instances.", "timestamp": "00:34:27,080", "timestamp_s": 2067.0}, {"text": "We start with the workload classifications.", "timestamp": "00:34:30,220", "timestamp_s": 2070.0}, {"text": "So we assess the jobs based on the fall tolerance and run length", "timestamp": "00:34:32,950", "timestamp_s": 2072.0}, {"text": "to identify which, which can safely run on the spot Instances.", "timestamp": "00:34:37,475", "timestamp_s": 2077.0}, {"text": "Next we implement like 4,000.", "timestamp": "00:34:42,445", "timestamp_s": 2082.0}, {"text": "By adding automated checkpoints and recovery systems.", "timestamp": "00:34:45,150", "timestamp_s": 2085.0}, {"text": "So if a spot instance gets reclaimed, the job can resume smoothly.", "timestamp": "00:34:48,980", "timestamp_s": 2088.0}, {"text": "Then comes the bidding strategy, optimization izing, the historical", "timestamp": "00:34:54,020", "timestamp_s": 2094.0}, {"text": "and real time pricing data.", "timestamp": "00:34:59,480", "timestamp_s": 2099.0}, {"text": "We just are bidding to maximize the savings while maintaining the job.", "timestamp": "00:35:01,580", "timestamp_s": 2101.0}, {"text": "Re reliability.", "timestamp": "00:35:05,570", "timestamp_s": 2105.0}, {"text": "Lastly, we adopt the hybrid deployment models.", "timestamp": "00:35:07,220", "timestamp_s": 2107.0}, {"text": "So dynamic, clearly, like switching between the spot and on demand instances", "timestamp": "00:35:11,510", "timestamp_s": 2111.0}, {"text": "based on cost and availability.", "timestamp": "00:35:16,480", "timestamp_s": 2116.0}, {"text": "So this is one of the important models which we can deploy to make sure", "timestamp": "00:35:18,430", "timestamp_s": 2118.0}, {"text": "we enable that the cost savings of up to 60 to 80% is achieved without", "timestamp": "00:35:22,265", "timestamp_s": 2122.0}, {"text": "even sacrificing on the performance.", "timestamp": "00:35:28,725", "timestamp_s": 2128.0}, {"text": "So moving on, let\u0027s talk about optimizing the no pool configurations.", "timestamp": "00:35:31,235", "timestamp_s": 2131.0}, {"text": "So an area where we can unlock substantial performance and cost", "timestamp": "00:35:35,885", "timestamp_s": 2135.0}, {"text": "benefits, especially in machine learning and compute heavy environments.", "timestamp": "00:35:39,725", "timestamp_s": 2139.0}, {"text": "So first we start with heterogeneous hardware segmentation.", "timestamp": "00:35:44,465", "timestamp_s": 2144.0}, {"text": "So rather than mixing all the GPUs, GU types into a single nor pool.", "timestamp": "00:35:48,415", "timestamp_s": 2148.0}, {"text": "We create a specialized nor pools for different GPUs, like eight", "timestamp": "00:35:53,585", "timestamp_s": 2153.0}, {"text": "hundreds going to one nor pool.", "timestamp": "00:35:57,635", "timestamp_s": 2157.0}, {"text": "And then the v hundreds go to a different nor pool.", "timestamp": "00:35:59,435", "timestamp_s": 2159.0}, {"text": "So each has a different compute and the memory characteristics.", "timestamp": "00:36:03,035", "timestamp_s": 2163.0}, {"text": "So assigning the workloads without creating the nor pools", "timestamp": "00:36:06,635", "timestamp_s": 2166.0}, {"text": "may lead to and inefficiencies.", "timestamp": "00:36:10,065", "timestamp_s": 2170.0}, {"text": "So by using Ts and tolerations, we make sure workloads are scheduled only to.", "timestamp": "00:36:12,955", "timestamp_s": 2172.0}, {"text": "Hardware data optimized for.", "timestamp": "00:36:19,085", "timestamp_s": 2179.0}, {"text": "Next we have resource optimization.", "timestamp": "00:36:21,145", "timestamp_s": 2181.0}, {"text": "This is about fine tuning the CPU and the GPU as well as the memory ratios", "timestamp": "00:36:23,875", "timestamp_s": 2183.0}, {"text": "to match the ml workload profiles.", "timestamp": "00:36:28,945", "timestamp_s": 2188.0}, {"text": "For example inference might require more CPU, whereas training", "timestamp": "00:36:31,960", "timestamp_s": 2191.0}, {"text": "model could need more GPU memory.", "timestamp": "00:36:36,785", "timestamp_s": 2196.0}, {"text": "So if.", "timestamp": "00:36:39,310", "timestamp_s": 2199.0}, {"text": "If these ratios are mislead or miscalculated, we either end up with", "timestamp": "00:36:40,150", "timestamp_s": 2200.0}, {"text": "bottlenecks or underutilized hardware, both of which are very costly.", "timestamp": "00:36:44,920", "timestamp_s": 2204.0}, {"text": "Then there is network topology alignment where you\u0027ll you are running, when you\u0027re", "timestamp": "00:36:49,375", "timestamp_s": 2209.0}, {"text": "running your distributed training network bandwidth, and latency matters a lot.", "timestamp": "00:36:53,955", "timestamp_s": 2213.0}, {"text": "So we design a no pools to align with.", "timestamp": "00:36:59,725", "timestamp_s": 2219.0}, {"text": "With the physical infrastructure, that means like grouping into a nodes nodes", "timestamp": "00:37:03,880", "timestamp_s": 2223.0}, {"text": "with the high speed interconnects like NV link or InfiniBand into a same pool.", "timestamp": "00:37:09,420", "timestamp_s": 2229.0}, {"text": "This ensures that the data exchanges between the GPUs in a fast and consistent", "timestamp": "00:37:15,620", "timestamp_s": 2235.0}, {"text": "manner, which avoids the slowdowns also during the training process.", "timestamp": "00:37:20,520", "timestamp_s": 2240.0}, {"text": "Now let\u0027s quickly look at how we can share resources fairly when multiple", "timestamp": "00:37:25,010", "timestamp_s": 2245.0}, {"text": "teams are working on the same system.", "timestamp": "00:37:29,740", "timestamp_s": 2249.0}, {"text": "First we use namespace to keep things organized.", "timestamp": "00:37:32,430", "timestamp_s": 2252.0}, {"text": "So each team or, type of work like production versus a non-production", "timestamp": "00:37:35,890", "timestamp_s": 2255.0}, {"text": "gets its own namespace.", "timestamp": "00:37:40,750", "timestamp_s": 2260.0}, {"text": "So this way we can set up different permissions use specific settings", "timestamp": "00:37:42,700", "timestamp_s": 2262.0}, {"text": "for each environment and keep the network traffic separate.", "timestamp": "00:37:47,100", "timestamp_s": 2267.0}, {"text": "Second, we can set up the resource controls.", "timestamp": "00:37:51,300", "timestamp_s": 2271.0}, {"text": "That means we limit how GPU memory and storage each team can use.", "timestamp": "00:37:54,270", "timestamp_s": 2274.0}, {"text": "This helps, make, making sure like team doesn\u0027t accidentally take", "timestamp": "00:37:59,670", "timestamp_s": 2279.0}, {"text": "more share than which is allocated.", "timestamp": "00:38:04,470", "timestamp_s": 2284.0}, {"text": "Then finally we give higher priority to critical jobs like live production jobs.", "timestamp": "00:38:07,460", "timestamp_s": 2287.0}, {"text": "At the same time, we make sure like every team gets at least a minimum resources", "timestamp": "00:38:13,200", "timestamp_s": 2293.0}, {"text": "which are needed for their operations.", "timestamp": "00:38:18,490", "timestamp_s": 2298.0}, {"text": "So this avoids the problem of one team overutilizing the resources while", "timestamp": "00:38:20,410", "timestamp_s": 2300.0}, {"text": "the other team doesn\u0027t get anything.", "timestamp": "00:38:25,370", "timestamp_s": 2305.0}, {"text": "So the overall this setup.", "timestamp": "00:38:27,500", "timestamp_s": 2307.0}, {"text": "Keeps fair, reliable, and running the infrastructure smoothly for everyone.", "timestamp": "00:38:29,545", "timestamp_s": 2309.0}, {"text": "Here\u0027s what an impact these optimization strategies can have.", "timestamp": "00:38:35,025", "timestamp_s": 2315.0}, {"text": "So in one of the case studies, it was observed that companies that", "timestamp": "00:38:38,830", "timestamp_s": 2318.0}, {"text": "applied this techniques saw an average of 43% reduction in costs.", "timestamp": "00:38:42,220", "timestamp_s": 2322.0}, {"text": "That, near that is nearly half of their infrastructure spending saved just by.", "timestamp": "00:38:47,090", "timestamp_s": 2327.0}, {"text": "Tuning the things right next.", "timestamp": "00:38:53,610", "timestamp_s": 2333.0}, {"text": "Training jobs became much faster, like 2.8 times faster on average.", "timestamp": "00:38:56,380", "timestamp_s": 2336.0}, {"text": "That means models are trained and ready in a bayless time which", "timestamp": "00:39:01,910", "timestamp_s": 2341.0}, {"text": "speeding up the entire development cycle study also suggests that.", "timestamp": "00:39:06,430", "timestamp_s": 2346.0}, {"text": "There was a big jump in the GPU usage, so which is up around 67%.", "timestamp": "00:39:11,200", "timestamp_s": 2351.0}, {"text": "So instead of having expensive GPU sitting idle, they\u0027re being", "timestamp": "00:39:18,280", "timestamp_s": 2358.0}, {"text": "used efficiently across a cluster.", "timestamp": "00:39:23,440", "timestamp_s": 2363.0}, {"text": "And finally, the production models became more reliable with 94% of the", "timestamp": "00:39:26,860", "timestamp_s": 2366.0}, {"text": "inherent requests, meeting the SLA goals.", "timestamp": "00:39:31,660", "timestamp_s": 2371.0}, {"text": "So that\u0027s a strong sign of stable production ready systems.", "timestamp": "00:39:34,680", "timestamp_s": 2374.0}, {"text": "So this slides gives us a step by step guide for applying the", "timestamp": "00:39:38,630", "timestamp_s": 2378.0}, {"text": "strategies we\u0027ve covered so far.", "timestamp": "00:39:42,690", "timestamp_s": 2382.0}, {"text": "Step one is to establish the baseline metrics.", "timestamp": "00:39:45,020", "timestamp_s": 2385.0}, {"text": "Like before making any changes, we need to understand how our", "timestamp": "00:39:48,680", "timestamp_s": 2388.0}, {"text": "resources are being used right now.", "timestamp": "00:39:52,560", "timestamp_s": 2392.0}, {"text": "That includes tracking the GPU and the CP usage.", "timestamp": "00:39:55,285", "timestamp_s": 2395.0}, {"text": "Training times and how much it\u0027s costing up per model.", "timestamp": "00:39:58,480", "timestamp_s": 2398.0}, {"text": "Then step two is to go for the quick wins.", "timestamp": "00:40:02,280", "timestamp_s": 2402.0}, {"text": "Like there are changes that are not.", "timestamp": "00:40:05,670", "timestamp_s": 2405.0}, {"text": "Easy to be implemented and give immediate value without disruption", "timestamp": "00:40:08,570", "timestamp_s": 2408.0}, {"text": "disrupting our current setup.", "timestamp": "00:40:13,070", "timestamp_s": 2413.0}, {"text": "So examples include resizing the resource limits correctly, and then enabling", "timestamp": "00:40:15,980", "timestamp_s": 2415.0}, {"text": "simple auto scaling, and then using the right storage classes for each workloads.", "timestamp": "00:40:21,205", "timestamp_s": 2421.0}, {"text": "Then step three is to deploy more advanced optimizations.", "timestamp": "00:40:27,005", "timestamp_s": 2427.0}, {"text": "Once the basics are in place, we can start using smarter tactics like custom", "timestamp": "00:40:30,890", "timestamp_s": 2430.0}, {"text": "autoscaling based on the real time metrics using spot instances to cut", "timestamp": "00:40:36,390", "timestamp_s": 2436.0}, {"text": "the costs or even fine tuning the node placement based on the hardware topology.", "timestamp": "00:40:40,990", "timestamp_s": 2440.0}, {"text": "And then finally step four is continuously continuous refinement.", "timestamp": "00:40:46,750", "timestamp_s": 2446.0}, {"text": "So optimization is not.", "timestamp": "00:40:52,060", "timestamp_s": 2452.0}, {"text": "One, one time thing.", "timestamp": "00:40:54,040", "timestamp_s": 2454.0}, {"text": "So it needs to evolve as workloads and technologies change.", "timestamp": "00:40:55,750", "timestamp_s": 2455.0}, {"text": "That means doing regular reviews, setting up alerts for unusual costs,", "timestamp": "00:40:59,650", "timestamp_s": 2459.0}, {"text": "and making sure our infrastructure and models are improved over over", "timestamp": "00:41:04,820", "timestamp_s": 2464.0}, {"text": "time which is very important, right?", "timestamp": "00:41:09,420", "timestamp_s": 2469.0}, {"text": "So in short, this roadmap helps balance the quick efficiency gains", "timestamp": "00:41:12,400", "timestamp_s": 2472.0}, {"text": "with smart long-term planning.", "timestamp": "00:41:16,420", "timestamp_s": 2476.0}, {"text": "Finally, thank you for joining me in this session and I hope you have a great", "timestamp": "00:41:18,700", "timestamp_s": 2478.0}, {"text": "time at this particular conference.", "timestamp": "00:41:22,510", "timestamp_s": 2482.0}, {"text": "Thank you.", "timestamp": "00:41:24,730", "timestamp_s": 2484.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'wAWyJMwWuO4',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Beyond Uptime: Revolutionizing Fintech Reliability Through SRE Innovation
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Discover how a high-growth fintech startup mastered system reliability during a viral surge, slashed incident recovery times, and maintained perfect uptime through SRE innovation. Learn battle-tested strategies for balancing aggressive growth with regulatory compliance in the financial sector.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./srt/sre2025_Srinivas_Reddy_Mosali.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:00,500'); seek(0.0)">
              Hello everyone.
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:01,519'); seek(1.0)">
              Thank you for joining me in this conference.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:03,950'); seek(3.0)">
              Today we will discuss on how FinTech reliability is achieved
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:07,340'); seek(7.0)">
              through SRE Innovation.
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:08,735'); seek(8.0)">
              So as organizations accelerate their AI initiatives, many are discovering
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:13,264'); seek(13.0)">
              that traditional infrastructure was not built to handle the high demands
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:17,804'); seek(17.0)">
              of machine learning operations.
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:20,064'); seek(20.0)">
              These workloads are highly resource intensive and are unpredictable.
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:24,244'); seek(24.0)">
              While Kubernetes has emerged as a platform of choice for managing
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:28,594'); seek(28.0)">
              these environments, simply running the containers is not enough.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:32,444'); seek(32.0)">
              The challenge lies in running them optimally.
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:00:35,824'); seek(35.0)">
              What we are seeing is that many teams are struggling, be it dealing with
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:00:40,774'); seek(40.0)">
              over-provisioned resources, unpredictable costs and performance bottlenecks.
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:00:46,185'); seek(46.0)">
              So in this particular session, I would like to walk you through a set of proven
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:00:51,025'); seek(51.0)">
              SRE strategies that are designed to increase performance, reduce costs, and
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:00:56,685'); seek(56.0)">
              significantly improve the reliability of the AI ML pipelines running in.
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:02,040'); seek(62.0)">
              Kubernetes these approaches come directly from the high stakes environments
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:06,680'); seek(66.0)">
              these where even minor inefficiencies can lead to a major financial losses.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:11,990'); seek(71.0)">
              So together we'll go through the techniques that have delivered up
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:15,720'); seek(75.0)">
              to a 60% improvements in resource utilization and substantially
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:20,540'); seek(80.0)">
              reductions in incident rates.
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:22,915'); seek(82.0)">
              All while maintaining the agility and performance.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:26,955'); seek(86.0)">
              Ultimately, this presentation is about more than just keeping the systems online.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:32,315'); seek(92.0)">
              It's about engineering confidently at scale in an increased AI
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:36,845'); seek(96.0)">
              driven financial ecosystem.
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:38,795'); seek(98.0)">
              So let's take a closer look on how the transformation is happening.
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:43,365'); seek(103.0)">
              Okay.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:43,574'); seek(103.0)">
              When we talk about scaling AI infrastructure efficiently, GPU
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:01:47,114'); seek(107.0)">
              utilization is one of the most overlooked and most expensive
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:01:51,204'); seek(111.0)">
              and has more bottlenecks.
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:01:52,854'); seek(112.0)">
              So let's look at how we can dramatically shift that performance to cost ratio
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:01:58,654'); seek(118.0)">
              using smart GPU level strategies.
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:01,694'); seek(121.0)">
              Let's start with the MIG or the multi-instance GPU Partitioning.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:05,504'); seek(125.0)">
              So this is a feature available in NVIDIA's 800 and h hundred series.
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:10,704'); seek(130.0)">
              It allows you to carve up a single GPU into a multiple isolated instances,
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:16,714'); seek(136.0)">
              which enables like multiple workloads to run simultaneously without any issues.
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:21,364'); seek(141.0)">
              Our real world tests have shown up to a seven x better utilization especially in
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:26,564'); seek(146.0)">
              environments with varied inference loads.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:29,844'); seek(149.0)">
              Next in in memory efficiency QDA provides like a low level access to memory
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:36,424'); seek(156.0)">
              management, allowing us to opti optimize.
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:39,784'); seek(159.0)">
              Tensor core use and reduce the fragmentation.
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:43,124'); seek(163.0)">
              So this can lead to 15 to 30% of throughput gains particularly for
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:48,704'); seek(168.0)">
              vision heavy models like ID verification or biometric fraud detection.
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:53,504'); seek(173.0)">
              Oh, coming to precision optimization.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:02:55,724'); seek(175.0)">
              In many FinTech companies like ML workloads, especially during inference.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:00,984'); seek(180.0)">
              Full FFP 32 precision is not always needed.
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:04,524'); seek(184.0)">
              So by switching the, switching it to FP 16 or even INT eight, organizations can
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:11,724'); seek(191.0)">
              drastically shrink the memory footprint.
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:14,454'); seek(194.0)">
              So these techniques are game changers.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:16,414'); seek(196.0)">
              They maximize the GPU resource usage and provide critical improvements in
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:21,324'); seek(201.0)">
              both performance and cost efficiency.
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:24,384'); seek(204.0)">
              Moving on to the next slide.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:25,644'); seek(205.0)">
              On auto-scaling strategies, as organizations push to run ML workloads
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:29,984'); seek(209.0)">
              more efficiently traditional auto-scaling strategies are not enough, especially
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:34,264'); seek(214.0)">
              in GPU accelerated environments.
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:36,334'); seek(216.0)">
              Here we'll go through four techniques that enable smarter ML strategies.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:42,104'); seek(222.0)">
              Which will help with the autoscaling.
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:44,604'); seek(224.0)">
              First we'll go through the workload profiling.
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:46,884'); seek(226.0)">
              So the foundation of autoscaling is understanding when and how much to scale.
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:52,374'); seek(232.0)">
              So workload profiling uses the historical telemetry across both
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:03:56,334'); seek(236.0)">
              training and inference cycles.
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:03:59,024'); seek(239.0)">
              To uncover the usage patterns.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:01,214'); seek(241.0)">
              So this helps define the baseline resource needs, identify the spikes
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:06,384'); seek(246.0)">
              during the market, events or frauds, fraud searches, and ensures that
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:10,404'); seek(250.0)">
              the auto-scaling decisions are data informed and are not reactive.
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:15,044'); seek(255.0)">
              Coming to the custom metrics pipeline.
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:17,924'); seek(257.0)">
              So if you look at the out of box Kubernetes like Kubernetes HPA doesn't
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:22,584'); seek(262.0)">
              know anything about the machine learning.
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:24,894'); seek(264.0)">
              That's where like the custom metrics come in using Prometheus adapters.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:29,694'); seek(269.0)">
              We expose the ml. Relevant signals like the Q Depth batch processing duration,
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:35,694'); seek(275.0)">
              and GPU memory pressure to the autoscaler.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:39,284'); seek(279.0)">
              So this enables a precise decision based on the actual workload characteristics,
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:45,624'); seek(285.0)">
              not just the CPU load or the pod count.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:48,854'); seek(288.0)">
              Coming to predictive scaling for recurring or seasonal workloads.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:53,504'); seek(293.0)">
              Think EOD model model retaining or the batch scoring jobs.
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:58,594'); seek(298.0)">
              We can apply the time-based or ML based scaling triggers.
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:02,884'); seek(302.0)">
              So predictive models can anticipate the spikes before they happen.
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:07,000'); seek(307.0)">
              Enabling warm pools to be spun up just in time.
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:10,340'); seek(310.0)">
              This reduces the cold start latency by up to 85%, especially valuable
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:16,280'); seek(316.0)">
              for real time FinTech systems.
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:19,160'); seek(319.0)">
              Coming to the buffer management we can't forget the cluster resilience.
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:24,004'); seek(324.0)">
              So the buffer management means deliberately over provisioning a
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:28,464'); seek(328.0)">
              small GPU headroom during critical critical hours like trading sessions
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:33,774'); seek(333.0)">
              or things like holiday season, right?
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:36,594'); seek(336.0)">
              Then you can scale them back down more aggressively after the hours.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:40,960'); seek(340.0)">
              This balances the control cost control with high availability, which is
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:45,439'); seek(345.0)">
              non-negotiable for financial applications.
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:48,529'); seek(348.0)">
              So these autoscaling strategies go well beyond the HPA setup.
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:53,669'); seek(353.0)">
              When implemented together, they enable the organizations to sustain peak
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:57,699'); seek(357.0)">
              model performance with minimal costs.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:00,954'); seek(360.0)">
              Okay.
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:01,164'); seek(361.0)">
              Coming to the multi GPU training orchestration the biggest bottlenecks
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:05,314'); seek(365.0)">
              in scaling a machine learning training right is GPU job orchestrations.
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:12,034'); seek(372.0)">
              It's not just having about having more GPUs.
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:15,594'); seek(375.0)">
              It's about using them in intelligently.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:18,114'); seek(378.0)">
              When we orchestrate jobs sufficiently across the gps, especially in
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:21,954'); seek(381.0)">
              distributed environments, we unlock, huge throughput gains in cost efficiency.
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:27,654'); seek(387.0)">
              The first step is topology aware scheduling.
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:30,754'); seek(390.0)">
              Why does this matter?
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:31,894'); seek(391.0)">
              So when jobs span across multiple GPUs, especially across nodes network
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:37,834'); seek(397.0)">
              latency becomes c enemy like we solve this by using the g Kubernetes node
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:43,194'); seek(403.0)">
              affinity rules to schedule the jobs on nodes that are physically close to the
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:48,354'); seek(408.0)">
              interconnect through high bandwidth.
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:50,939'); seek(410.0)">
              Links like NV link, or PCIE.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:54,319'); seek(414.0)">
              This minimizes the communication overhead and drastically improves the
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:57,589'); seek(417.0)">
              performance and distributed training.
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:00,179'); seek(420.0)">
              Next we move on to distributed training frameworks.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:03,119'); seek(423.0)">
              So framework for frameworks like TensorFlows multi
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:06,959'); seek(426.0)">
              worker needs to be tuned.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:09,694'); seek(429.0)">
              To our infrastructure, often we need poor performance because
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:14,124'); seek(434.0)">
              training scripts are not optimized.
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:16,454'); seek(436.0)">
              So we address this with Custom gate template that enable, like
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:21,344'); seek(441.0)">
              dynamic scaling, reduces the training time variance, and
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:25,124'); seek(445.0)">
              it minimizes the communication bottlenecks between the nodes.
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:28,459'); seek(448.0)">
              Finally, there's a priority based preemptive, mechanism.
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:32,784'); seek(452.0)">
              So in this particular technique if you look at our shared clusters.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:37,004'); seek(457.0)">
              When multiple teams submit the jobs, it's critical to ensure that high
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:42,454'); seek(462.0)">
              priority jobs don't wait in line behind the rule, low priority ones.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:47,074'); seek(467.0)">
              So we implement intelligent queuing mechanisms that evaluate
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:51,344'); seek(471.0)">
              the job priority and then.
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:53,119'); seek(473.0)">
              Deadlines and then resource fairness.
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:55,729'); seek(475.0)">
              It's like air traffic control for GPUs, like preventing the collisions,
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:00,139'); seek(480.0)">
              reducing the idle time, and keeping the system fair and efficient.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:04,479'); seek(484.0)">
              The key takeaway is like when you bring all these three pillars together, the
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:09,609'); seek(489.0)">
              topology awareness, smart frameworks, and priority based scheduling you can
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:14,519'); seek(494.0)">
              cut the training time by 30 to 50%.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:17,939'); seek(497.0)">
              That's a massive impact when you are running the running hundreds or even
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:22,079'); seek(502.0)">
              thousands of experiments each week.
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:24,829'); seek(504.0)">
              Okay.
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:25,159'); seek(505.0)">
              Moving on to the advanced monitoring and observability.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:27,889'); seek(507.0)">
              As machine learning workloads grow in complexity and scale, the need for robust
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:32,879'); seek(512.0)">
              observability becomes absolutely critical.
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:35,649'); seek(515.0)">
              Not just for performance, but for cost efficiency and op
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:39,459'); seek(519.0)">
              and operational reliability.
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:41,319'); seek(521.0)">
              So this.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:42,709'); seek(522.0)">
              Here outlines the layered approach.
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:45,884'); seek(525.0)">
              We take towards building a comprehensive monitoring stack for
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:49,414'); seek(529.0)">
              the machine learning infrastructure.
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:51,834'); seek(531.0)">
              So let's start at the foundation, the historical data.
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:55,659'); seek(535.0)">
              So long-term storage of metrics is essential for capacity
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:08:59,779'); seek(539.0)">
              planning and trend analysis.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:01,729'); seek(541.0)">
              Whether you are sizing your GPU clusters for next quarter's workloads,
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:06,479'); seek(546.0)">
              or preparing for upcoming model retraining cycles, you need to.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:11,819'); seek(551.0)">
              Analyze historical context, right?
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:14,249'); seek(554.0)">
              So it helps avoid over provisioning and gives a factual basis
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:19,159'); seek(559.0)">
              for infrastructure budgeting.
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:21,349'); seek(561.0)">
              So next we move up to resource utilization.
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:24,890'); seek(564.0)">
              So this is where the fine-grained metrics come in, like the CPU usage,
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:29,780'); seek(569.0)">
              memory pressure, GPU utilization at the process level, right?
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:34,270'); seek(574.0)">
              So this layer is all about visibility into how your infrastructure.
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:38,590'); seek(578.0)">
              Is being consumed in real time without this, you are essentially
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:42,690'); seek(582.0)">
              like flying blind about That is we have the performance insights.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:48,000'); seek(588.0)">
              So generic metrics like CPU usage won't tell you how long a training
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:52,640'); seek(592.0)">
              model takes or when your data pipeline is getting congested.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:56,970'); seek(596.0)">
              So we create a custom dashboard that reflects that ML specific metrics
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:02,580'); seek(602.0)">
              such as throughput or training loss.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:05,485'); seek(605.0)">
              Over the period of time.
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:06,745'); seek(606.0)">
              And then the GPU memory fragmentation.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:09,375'); seek(609.0)">
              So these these give the engineers a much cleaner picture of how
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:13,205'); seek(613.0)">
              their workloads are behaving.
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:15,455'); seek(615.0)">
              At the top of the pyramid, if you look at, is the root cause analysis.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:20,525'); seek(620.0)">
              So this is the most advanced layer and arguably the most valuable as well.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:25,245'); seek(625.0)">
              So it.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:26,595'); seek(626.0)">
              Ties everything together through distributed tracing and correlation
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:31,005'); seek(631.0)">
              correlates across the layers, like linking spikes in the GPU usages to a specific
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:37,195'); seek(637.0)">
              phase of a model training while something breaks or slows down, this is how you
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:42,635'); seek(642.0)">
              rapidly isolate and resolve the issue.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:45,815'); seek(645.0)">
              The systems we are talking are not just about alerting.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:49,445'); seek(649.0)">
              They're about creating a narrative that ML engineers and SREs can follow.
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:54,395'); seek(654.0)">
              They support both issues at hand and also work on strategic decision making.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:00,575'); seek(660.0)">
              The most successful organizations like built all four of these layers
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:05,135'); seek(665.0)">
              into their monitoring approach.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:08,325'); seek(668.0)">
              Together they create a holistic feedback.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:10,885'); seek(670.0)">
              That helps optimize models and improve uptime as well as cost controls.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:16,885'); seek(676.0)">
              Now let's explore how we can dramatically improve, data to a reduced latency in the
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:23,025'); seek(683.0)">
              ML pipelines by optimizing the storage.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:25,815'); seek(685.0)">
              So as machine learning workloads become increasingly data intensive
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:30,645'); seek(690.0)">
              storage becomes a critical bottleneck.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:33,165'); seek(693.0)">
              So this slides outlines like three key strategies to overcome that.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:37,520'); seek(697.0)">
              So if you look at the distributed file systems we have file systems
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:41,280'); seek(701.0)">
              like GPFS, which are very distributed.
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:43,800'); seek(703.0)">
              These are built for high throughput scenarios and are capable of parallel
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:48,000'); seek(708.0)">
              data processing across hundreds of nodes.
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:51,240'); seek(711.0)">
              So they outperform the traditional network storage solutions by up to eight 10 x.
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:56,435'); seek(716.0)">
              Right, especially when working with the small, fragmented file types
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:01,295'); seek(721.0)">
              which are common in the ML data sets.
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:04,205'); seek(724.0)">
              The design allows like efficient aggregation of bandwidth and parallel
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:09,365'); seek(729.0)">
              read and writes, so which accelerates the data ingestion during the training.
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:13,885'); seek(733.0)">
              Next, we deploy the in-memory caching layers.
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:17,480'); seek(737.0)">
              Between the persistent storage and compute resources these caching
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:22,870'); seek(742.0)">
              layers drastically reduce IO wait times, especially for frequently
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:26,440'); seek(746.0)">
              accessed data sets by 65% or more.
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:29,370'); seek(749.0)">
              They also implement automatic eviction policies ensuring that the optimal
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:33,760'); seek(753.0)">
              memory usage without manual intervention.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:36,460'); seek(756.0)">
              This is particularly beneficial in iterative workflows where
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:40,850'); seek(760.0)">
              the same data sets are are.
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:43,615'); seek(763.0)">
              Read multiple times during the model tuning.
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:46,605'); seek(766.0)">
              Lastly, we have something called the automated storage tiering.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:50,325'); seek(770.0)">
              So this involves moving data between the hot and cold storage tiers based on the
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:55,605'); seek(775.0)">
              access, frequency and performance needs.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:58,635'); seek(778.0)">
              So this not only ensures that the performance for the
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:13:02,455'); seek(782.0)">
              active data sets, but also.
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:05,155'); seek(785.0)">
              Achieves like the 40 to 50 percent of cost reductions by shifting the
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:10,010'); seek(790.0)">
              inactive data to lower cost storage.
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:13,210'); seek(793.0)">
              This is done transparently through a unified namespace.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:16,860'); seek(796.0)">
              So engineers don't need to worry about where the data physically rec resides.
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:22,500'); seek(802.0)">
              So together with these kind of three layers, distributed file system, in
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:28,260'); seek(808.0)">
              memory caching and intelligent tiering this create a robust, cost efficient
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:32,540'); seek(812.0)">
              foundation for data driven ML pipelines.
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:35,690'); seek(815.0)">
              The result is faster training, lower cost, and smoother operations.
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:40,030'); seek(820.0)">
              Here we are looking at how we optimize cloud spend for machine learning
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:44,520'); seek(824.0)">
              workloads using sport instances.
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:47,660'); seek(827.0)">
              We start with the workload classifications.
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:50,390'); seek(830.0)">
              So we assess the jobs based on the fall tolerance and run length
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:54,915'); seek(834.0)">
              to identify which, which can safely run on the spot Instances.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:59,885'); seek(839.0)">
              Next we implement like 4,000.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:14:02,590'); seek(842.0)">
              By adding automated checkpoints and recovery systems.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:14:06,420'); seek(846.0)">
              So if a spot instance gets reclaimed, the job can resume smoothly.
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:11,460'); seek(851.0)">
              Then comes the bidding strategy, optimization izing, the historical
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:16,920'); seek(856.0)">
              and real time pricing data.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:19,020'); seek(859.0)">
              We just are bidding to maximize the savings while maintaining the job.
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:23,010'); seek(863.0)">
              Re reliability.
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:24,660'); seek(864.0)">
              Lastly, we adopt the hybrid deployment models.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:28,950'); seek(868.0)">
              So dynamic, clearly, like switching between the spot and on demand instances
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:33,920'); seek(873.0)">
              based on cost and availability.
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:35,870'); seek(875.0)">
              So this is one of the important models which we can deploy to make sure
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:39,705'); seek(879.0)">
              we enable that the cost savings of up to 60 to 80% is achieved without
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:46,165'); seek(886.0)">
              even sacrificing on the performance.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:48,675'); seek(888.0)">
              So moving on, let's talk about optimizing the no pool configurations.
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:53,325'); seek(893.0)">
              So an area where we can unlock substantial performance and cost
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:57,165'); seek(897.0)">
              benefits, especially in machine learning and compute heavy environments.
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:15:01,905'); seek(901.0)">
              So first we start with heterogeneous hardware segmentation.
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:15:05,855'); seek(905.0)">
              So rather than mixing all the GPUs, GU types into a single nor pool.
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:15:11,025'); seek(911.0)">
              We create a specialized nor pools for different GPUs, like eight
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:15,075'); seek(915.0)">
              hundreds going to one nor pool.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:16,875'); seek(916.0)">
              And then the v hundreds go to a different nor pool.
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:20,475'); seek(920.0)">
              So each has a different compute and the memory characteristics.
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:24,075'); seek(924.0)">
              So assigning the workloads without creating the nor pools
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:27,505'); seek(927.0)">
              may lead to and inefficiencies.
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:30,395'); seek(930.0)">
              So by using Ts and tolerations, we make sure workloads are scheduled only to.
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:36,525'); seek(936.0)">
              Hardware data optimized for.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:38,585'); seek(938.0)">
              Next we have resource optimization.
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:41,315'); seek(941.0)">
              This is about fine tuning the CPU and the GPU as well as the memory ratios
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:46,385'); seek(946.0)">
              to match the ml workload profiles.
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:49,400'); seek(949.0)">
              For example inference might require more CPU, whereas training
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:54,225'); seek(954.0)">
              model could need more GPU memory.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:56,750'); seek(956.0)">
              So if.
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:57,590'); seek(957.0)">
              If these ratios are mislead or miscalculated, we either end up with
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:16:02,360'); seek(962.0)">
              bottlenecks or underutilized hardware, both of which are very costly.
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:16:06,815'); seek(966.0)">
              Then there is network topology alignment where you'll you are running, when you're
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:16:11,395'); seek(971.0)">
              running your distributed training network bandwidth, and latency matters a lot.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:17,165'); seek(977.0)">
              So we design a no pools to align with.
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:21,320'); seek(981.0)">
              With the physical infrastructure, that means like grouping into a nodes nodes
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:26,860'); seek(986.0)">
              with the high speed interconnects like NV link or InfiniBand into a same pool.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:33,060'); seek(993.0)">
              This ensures that the data exchanges between the GPUs in a fast and consistent
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:37,960'); seek(997.0)">
              manner, which avoids the slowdowns also during the training process.
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:42,450'); seek(1002.0)">
              Now let's quickly look at how we can share resources fairly when multiple
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:47,180'); seek(1007.0)">
              teams are working on the same system.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:49,870'); seek(1009.0)">
              First we use namespace to keep things organized.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:53,330'); seek(1013.0)">
              So each team or, type of work like production versus a non-production
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:58,190'); seek(1018.0)">
              gets its own namespace.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:17:00,140'); seek(1020.0)">
              So this way we can set up different permissions use specific settings
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:17:04,540'); seek(1024.0)">
              for each environment and keep the network traffic separate.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:17:08,740'); seek(1028.0)">
              Second, we can set up the resource controls.
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:17:11,710'); seek(1031.0)">
              That means we limit how GPU memory and storage each team can use.
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:17:17,110'); seek(1037.0)">
              This helps, make, making sure like team doesn't accidentally take
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:17:21,910'); seek(1041.0)">
              more share than which is allocated.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:24,900'); seek(1044.0)">
              Then finally we give higher priority to critical jobs like live production jobs.
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:30,640'); seek(1050.0)">
              At the same time, we make sure like every team gets at least a minimum resources
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:35,930'); seek(1055.0)">
              which are needed for their operations.
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:37,850'); seek(1057.0)">
              So this avoids the problem of one team overutilizing the resources while
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:42,810'); seek(1062.0)">
              the other team doesn't get anything.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:44,940'); seek(1064.0)">
              So the overall this setup.
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:46,985'); seek(1066.0)">
              Keeps fair, reliable, and running the infrastructure smoothly for everyone.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:52,465'); seek(1072.0)">
              Here's what an impact these optimization strategies can have.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:56,270'); seek(1076.0)">
              So in one of the case studies, it was observed that companies that
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:59,660'); seek(1079.0)">
              applied this techniques saw an average of 43% reduction in costs.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:18:04,530'); seek(1084.0)">
              That, near that is nearly half of their infrastructure spending saved just by.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:18:11,050'); seek(1091.0)">
              Tuning the things right next.
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:18:13,820'); seek(1093.0)">
              Training jobs became much faster, like 2.8 times faster on average.
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:18:19,350'); seek(1099.0)">
              That means models are trained and ready in a bayless time which
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:18:23,870'); seek(1103.0)">
              speeding up the entire development cycle study also suggests that.
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:18:28,640'); seek(1108.0)">
              There was a big jump in the GPU usage, so which is up around 67%.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:35,720'); seek(1115.0)">
              So instead of having expensive GPU sitting idle, they're being
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:40,880'); seek(1120.0)">
              used efficiently across a cluster.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:44,300'); seek(1124.0)">
              And finally, the production models became more reliable with 94% of the
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:49,100'); seek(1129.0)">
              inherent requests, meeting the SLA goals.
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:52,120'); seek(1132.0)">
              So that's a strong sign of stable production ready systems.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:56,070'); seek(1136.0)">
              So this slides gives us a step by step guide for applying the
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:19:00,130'); seek(1140.0)">
              strategies we've covered so far.
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:19:02,460'); seek(1142.0)">
              Step one is to establish the baseline metrics.
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:19:06,120'); seek(1146.0)">
              Like before making any changes, we need to understand how our
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:19:10,000'); seek(1150.0)">
              resources are being used right now.
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:19:12,725'); seek(1152.0)">
              That includes tracking the GPU and the CP usage.
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:19:15,920'); seek(1155.0)">
              Training times and how much it's costing up per model.
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:19:19,720'); seek(1159.0)">
              Then step two is to go for the quick wins.
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:19:23,110'); seek(1163.0)">
              Like there are changes that are not.
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:19:26,010'); seek(1166.0)">
              Easy to be implemented and give immediate value without disruption
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:19:30,510'); seek(1170.0)">
              disrupting our current setup.
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:33,420'); seek(1173.0)">
              So examples include resizing the resource limits correctly, and then enabling
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:38,645'); seek(1178.0)">
              simple auto scaling, and then using the right storage classes for each workloads.
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:44,445'); seek(1184.0)">
              Then step three is to deploy more advanced optimizations.
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:48,330'); seek(1188.0)">
              Once the basics are in place, we can start using smarter tactics like custom
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:53,830'); seek(1193.0)">
              autoscaling based on the real time metrics using spot instances to cut
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:58,430'); seek(1198.0)">
              the costs or even fine tuning the node placement based on the hardware topology.
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:20:04,190'); seek(1204.0)">
              And then finally step four is continuously continuous refinement.
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:20:09,500'); seek(1209.0)">
              So optimization is not.
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:20:11,480'); seek(1211.0)">
              One, one time thing.
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:20:13,190'); seek(1213.0)">
              So it needs to evolve as workloads and technologies change.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:20:17,090'); seek(1217.0)">
              That means doing regular reviews, setting up alerts for unusual costs,
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:20:22,260'); seek(1222.0)">
              and making sure our infrastructure and models are improved over over
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:20:26,860'); seek(1226.0)">
              time which is very important, right?
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:20:29,840'); seek(1229.0)">
              So in short, this roadmap helps balance the quick efficiency gains
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:20:33,860'); seek(1233.0)">
              with smart long-term planning.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:20:36,140'); seek(1236.0)">
              Finally, thank you for joining me in this session and I hope you have a great
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:20:39,950'); seek(1239.0)">
              time at this particular conference.
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:42,170'); seek(1242.0)">
              Thank you.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:43,060'); seek(1243.0)">
              Hello everyone.
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:44,080'); seek(1244.0)">
              Thank you for joining me in this conference.
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:46,510'); seek(1246.0)">
              Today we will discuss on how FinTech reliability is achieved
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:49,900'); seek(1249.0)">
              through SRE Innovation.
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:51,295'); seek(1251.0)">
              So as organizations accelerate their AI initiatives, many are discovering
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:55,825'); seek(1255.0)">
              that traditional infrastructure was not built to handle the high demands
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:21:00,365'); seek(1260.0)">
              of machine learning operations.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:21:02,625'); seek(1262.0)">
              These workloads are highly resource intensive and are unpredictable.
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:21:06,805'); seek(1266.0)">
              While Kubernetes has emerged as a platform of choice for managing
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:21:11,155'); seek(1271.0)">
              these environments, simply running the containers is not enough.
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:21:15,005'); seek(1275.0)">
              The challenge lies in running them optimally.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:21:18,385'); seek(1278.0)">
              What we are seeing is that many teams are struggling, be it dealing with
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:21:23,335'); seek(1283.0)">
              over-provisioned resources, unpredictable costs and performance bottlenecks.
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:21:28,745'); seek(1288.0)">
              So in this particular session, I would like to walk you through a set of proven
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:21:33,585'); seek(1293.0)">
              SRE strategies that are designed to increase performance, reduce costs, and
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:21:39,245'); seek(1299.0)">
              significantly improve the reliability of the AI ML pipelines running in.
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:44,600'); seek(1304.0)">
              Kubernetes these approaches come directly from the high stakes environments
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:49,240'); seek(1309.0)">
              these where even minor inefficiencies can lead to a major financial losses.
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:54,550'); seek(1314.0)">
              So together we'll go through the techniques that have delivered up
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:58,280'); seek(1318.0)">
              to a 60% improvements in resource utilization and substantially
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:22:03,100'); seek(1323.0)">
              reductions in incident rates.
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:22:05,475'); seek(1325.0)">
              All while maintaining the agility and performance.
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:22:09,515'); seek(1329.0)">
              Ultimately, this presentation is about more than just keeping the systems online.
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:22:14,875'); seek(1334.0)">
              It's about engineering confidently at scale in an increased AI
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:22:19,405'); seek(1339.0)">
              driven financial ecosystem.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:22:21,355'); seek(1341.0)">
              So let's take a closer look on how the transformation is happening.
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:22:25,925'); seek(1345.0)">
              Okay.
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:22:26,135'); seek(1346.0)">
              When we talk about scaling AI infrastructure efficiently, GPU
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:22:29,675'); seek(1349.0)">
              utilization is one of the most overlooked and most expensive
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:22:33,765'); seek(1353.0)">
              and has more bottlenecks.
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:22:35,415'); seek(1355.0)">
              So let's look at how we can dramatically shift that performance to cost ratio
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:22:41,215'); seek(1361.0)">
              using smart GPU level strategies.
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:44,255'); seek(1364.0)">
              Let's start with the MIG or the multi-instance GPU Partitioning.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:48,065'); seek(1368.0)">
              So this is a feature available in NVIDIA's 800 and h hundred series.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:53,265'); seek(1373.0)">
              It allows you to carve up a single GPU into a multiple isolated instances,
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:59,275'); seek(1379.0)">
              which enables like multiple workloads to run simultaneously without any issues.
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:23:03,925'); seek(1383.0)">
              Our real world tests have shown up to a seven x better utilization especially in
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:23:09,125'); seek(1389.0)">
              environments with varied inference loads.
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:23:12,405'); seek(1392.0)">
              Next in in memory efficiency QDA provides like a low level access to memory
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:23:18,985'); seek(1398.0)">
              management, allowing us to opti optimize.
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:23:22,345'); seek(1402.0)">
              Tensor core use and reduce the fragmentation.
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:23:25,685'); seek(1405.0)">
              So this can lead to 15 to 30% of throughput gains particularly for
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:23:31,265'); seek(1411.0)">
              vision heavy models like ID verification or biometric fraud detection.
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:23:36,065'); seek(1416.0)">
              Oh, coming to precision optimization.
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:23:38,285'); seek(1418.0)">
              In many FinTech companies like ML workloads, especially during inference.
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:23:43,545'); seek(1423.0)">
              Full FFP 32 precision is not always needed.
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:23:47,085'); seek(1427.0)">
              So by switching the, switching it to FP 16 or even INT eight, organizations can
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:54,285'); seek(1434.0)">
              drastically shrink the memory footprint.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:57,015'); seek(1437.0)">
              So these techniques are game changers.
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:58,975'); seek(1438.0)">
              They maximize the GPU resource usage and provide critical improvements in
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:24:03,885'); seek(1443.0)">
              both performance and cost efficiency.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:24:06,945'); seek(1446.0)">
              Moving on to the next slide.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:24:08,205'); seek(1448.0)">
              On auto-scaling strategies, as organizations push to run ML workloads
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:24:12,545'); seek(1452.0)">
              more efficiently traditional auto-scaling strategies are not enough, especially
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:24:16,825'); seek(1456.0)">
              in GPU accelerated environments.
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:24:18,895'); seek(1458.0)">
              Here we'll go through four techniques that enable smarter ML strategies.
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:24:24,665'); seek(1464.0)">
              Which will help with the autoscaling.
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:24:27,165'); seek(1467.0)">
              First we'll go through the workload profiling.
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:24:29,445'); seek(1469.0)">
              So the foundation of autoscaling is understanding when and how much to scale.
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:24:34,935'); seek(1474.0)">
              So workload profiling uses the historical telemetry across both
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:24:38,895'); seek(1478.0)">
              training and inference cycles.
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:24:41,585'); seek(1481.0)">
              To uncover the usage patterns.
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:24:43,775'); seek(1483.0)">
              So this helps define the baseline resource needs, identify the spikes
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:48,945'); seek(1488.0)">
              during the market, events or frauds, fraud searches, and ensures that
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:24:52,965'); seek(1492.0)">
              the auto-scaling decisions are data informed and are not reactive.
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:57,605'); seek(1497.0)">
              Coming to the custom metrics pipeline.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:25:00,485'); seek(1500.0)">
              So if you look at the out of box Kubernetes like Kubernetes HPA doesn't
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:25:05,145'); seek(1505.0)">
              know anything about the machine learning.
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:25:07,455'); seek(1507.0)">
              That's where like the custom metrics come in using Prometheus adapters.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:25:12,255'); seek(1512.0)">
              We expose the ml. Relevant signals like the Q Depth batch processing duration,
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:25:18,255'); seek(1518.0)">
              and GPU memory pressure to the autoscaler.
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:25:21,845'); seek(1521.0)">
              So this enables a precise decision based on the actual workload characteristics,
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:25:28,185'); seek(1528.0)">
              not just the CPU load or the pod count.
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:25:31,415'); seek(1531.0)">
              Coming to predictive scaling for recurring or seasonal workloads.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:25:36,065'); seek(1536.0)">
              Think EOD model model retaining or the batch scoring jobs.
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:25:41,155'); seek(1541.0)">
              We can apply the time-based or ML based scaling triggers.
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:25:45,445'); seek(1545.0)">
              So predictive models can anticipate the spikes before they happen.
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:25:49,560'); seek(1549.0)">
              Enabling warm pools to be spun up just in time.
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:25:52,900'); seek(1552.0)">
              This reduces the cold start latency by up to 85%, especially valuable
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:25:58,840'); seek(1558.0)">
              for real time FinTech systems.
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:26:01,720'); seek(1561.0)">
              Coming to the buffer management we can't forget the cluster resilience.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:26:06,565'); seek(1566.0)">
              So the buffer management means deliberately over provisioning a
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:26:11,025'); seek(1571.0)">
              small GPU headroom during critical critical hours like trading sessions
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:26:16,335'); seek(1576.0)">
              or things like holiday season, right?
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:26:19,155'); seek(1579.0)">
              Then you can scale them back down more aggressively after the hours.
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:26:23,520'); seek(1583.0)">
              This balances the control cost control with high availability, which is
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:26:28,000'); seek(1588.0)">
              non-negotiable for financial applications.
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:26:31,090'); seek(1591.0)">
              So these autoscaling strategies go well beyond the HPA setup.
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:26:36,230'); seek(1596.0)">
              When implemented together, they enable the organizations to sustain peak
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:26:40,260'); seek(1600.0)">
              model performance with minimal costs.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:26:43,515'); seek(1603.0)">
              Okay.
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:26:43,725'); seek(1603.0)">
              Coming to the multi GPU training orchestration the biggest bottlenecks
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:26:47,875'); seek(1607.0)">
              in scaling a machine learning training right is GPU job orchestrations.
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:26:54,595'); seek(1614.0)">
              It's not just having about having more GPUs.
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:26:58,155'); seek(1618.0)">
              It's about using them in intelligently.
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:27:00,675'); seek(1620.0)">
              When we orchestrate jobs sufficiently across the gps, especially in
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:27:04,515'); seek(1624.0)">
              distributed environments, we unlock, huge throughput gains in cost efficiency.
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:27:10,215'); seek(1630.0)">
              The first step is topology aware scheduling.
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:27:13,315'); seek(1633.0)">
              Why does this matter?
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:27:14,455'); seek(1634.0)">
              So when jobs span across multiple GPUs, especially across nodes network
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:27:20,395'); seek(1640.0)">
              latency becomes c enemy like we solve this by using the g Kubernetes node
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:27:25,755'); seek(1645.0)">
              affinity rules to schedule the jobs on nodes that are physically close to the
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:27:30,915'); seek(1650.0)">
              interconnect through high bandwidth.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:27:33,500'); seek(1653.0)">
              Links like NV link, or PCIE.
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:27:36,880'); seek(1656.0)">
              This minimizes the communication overhead and drastically improves the
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:27:40,150'); seek(1660.0)">
              performance and distributed training.
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:27:42,740'); seek(1662.0)">
              Next we move on to distributed training frameworks.
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:27:45,680'); seek(1665.0)">
              So framework for frameworks like TensorFlows multi
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:27:49,520'); seek(1669.0)">
              worker needs to be tuned.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:27:52,255'); seek(1672.0)">
              To our infrastructure, often we need poor performance because
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:27:56,685'); seek(1676.0)">
              training scripts are not optimized.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:27:59,015'); seek(1679.0)">
              So we address this with Custom gate template that enable, like
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:28:03,905'); seek(1683.0)">
              dynamic scaling, reduces the training time variance, and
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:28:07,685'); seek(1687.0)">
              it minimizes the communication bottlenecks between the nodes.
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:28:11,020'); seek(1691.0)">
              Finally, there's a priority based preemptive, mechanism.
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:28:15,345'); seek(1695.0)">
              So in this particular technique if you look at our shared clusters.
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:28:19,565'); seek(1699.0)">
              When multiple teams submit the jobs, it's critical to ensure that high
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:28:25,015'); seek(1705.0)">
              priority jobs don't wait in line behind the rule, low priority ones.
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:28:29,635'); seek(1709.0)">
              So we implement intelligent queuing mechanisms that evaluate
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:28:33,905'); seek(1713.0)">
              the job priority and then.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:28:35,680'); seek(1715.0)">
              Deadlines and then resource fairness.
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:28:38,290'); seek(1718.0)">
              It's like air traffic control for GPUs, like preventing the collisions,
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:28:42,700'); seek(1722.0)">
              reducing the idle time, and keeping the system fair and efficient.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:28:47,040'); seek(1727.0)">
              The key takeaway is like when you bring all these three pillars together, the
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:28:52,170'); seek(1732.0)">
              topology awareness, smart frameworks, and priority based scheduling you can
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:28:57,080'); seek(1737.0)">
              cut the training time by 30 to 50%.
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:29:00,500'); seek(1740.0)">
              That's a massive impact when you are running the running hundreds or even
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:29:04,640'); seek(1744.0)">
              thousands of experiments each week.
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:29:07,390'); seek(1747.0)">
              Okay.
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:29:07,720'); seek(1747.0)">
              Moving on to the advanced monitoring and observability.
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:29:10,450'); seek(1750.0)">
              As machine learning workloads grow in complexity and scale, the need for robust
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:29:15,440'); seek(1755.0)">
              observability becomes absolutely critical.
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:29:18,210'); seek(1758.0)">
              Not just for performance, but for cost efficiency and op
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:29:22,020'); seek(1762.0)">
              and operational reliability.
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:29:23,880'); seek(1763.0)">
              So this.
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:29:25,270'); seek(1765.0)">
              Here outlines the layered approach.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:29:28,445'); seek(1768.0)">
              We take towards building a comprehensive monitoring stack for
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:29:31,975'); seek(1771.0)">
              the machine learning infrastructure.
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:29:34,395'); seek(1774.0)">
              So let's start at the foundation, the historical data.
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:29:38,220'); seek(1778.0)">
              So long-term storage of metrics is essential for capacity
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:29:42,340'); seek(1782.0)">
              planning and trend analysis.
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:29:44,290'); seek(1784.0)">
              Whether you are sizing your GPU clusters for next quarter's workloads,
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:29:49,040'); seek(1789.0)">
              or preparing for upcoming model retraining cycles, you need to.
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:29:54,380'); seek(1794.0)">
              Analyze historical context, right?
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:29:56,810'); seek(1796.0)">
              So it helps avoid over provisioning and gives a factual basis
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:30:01,720'); seek(1801.0)">
              for infrastructure budgeting.
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:30:03,910'); seek(1803.0)">
              So next we move up to resource utilization.
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:30:07,450'); seek(1807.0)">
              So this is where the fine-grained metrics come in, like the CPU usage,
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:30:12,340'); seek(1812.0)">
              memory pressure, GPU utilization at the process level, right?
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:30:16,830'); seek(1816.0)">
              So this layer is all about visibility into how your infrastructure.
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:30:21,150'); seek(1821.0)">
              Is being consumed in real time without this, you are essentially
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:30:25,250'); seek(1825.0)">
              like flying blind about That is we have the performance insights.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:30:30,560'); seek(1830.0)">
              So generic metrics like CPU usage won't tell you how long a training
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:30:35,200'); seek(1835.0)">
              model takes or when your data pipeline is getting congested.
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:30:39,530'); seek(1839.0)">
              So we create a custom dashboard that reflects that ML specific metrics
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:30:45,140'); seek(1845.0)">
              such as throughput or training loss.
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:30:48,045'); seek(1848.0)">
              Over the period of time.
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:30:49,305'); seek(1849.0)">
              And then the GPU memory fragmentation.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:30:51,935'); seek(1851.0)">
              So these these give the engineers a much cleaner picture of how
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:30:55,765'); seek(1855.0)">
              their workloads are behaving.
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:30:58,015'); seek(1858.0)">
              At the top of the pyramid, if you look at, is the root cause analysis.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:31:03,085'); seek(1863.0)">
              So this is the most advanced layer and arguably the most valuable as well.
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:31:07,805'); seek(1867.0)">
              So it.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:31:09,155'); seek(1869.0)">
              Ties everything together through distributed tracing and correlation
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:31:13,565'); seek(1873.0)">
              correlates across the layers, like linking spikes in the GPU usages to a specific
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:31:19,755'); seek(1879.0)">
              phase of a model training while something breaks or slows down, this is how you
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:31:25,195'); seek(1885.0)">
              rapidly isolate and resolve the issue.
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:31:28,375'); seek(1888.0)">
              The systems we are talking are not just about alerting.
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:31:32,005'); seek(1892.0)">
              They're about creating a narrative that ML engineers and SREs can follow.
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:31:36,955'); seek(1896.0)">
              They support both issues at hand and also work on strategic decision making.
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:31:43,135'); seek(1903.0)">
              The most successful organizations like built all four of these layers
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:31:47,695'); seek(1907.0)">
              into their monitoring approach.
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:31:50,885'); seek(1910.0)">
              Together they create a holistic feedback.
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:31:53,445'); seek(1913.0)">
              That helps optimize models and improve uptime as well as cost controls.
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:31:59,445'); seek(1919.0)">
              Now let's explore how we can dramatically improve, data to a reduced latency in the
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:32:05,585'); seek(1925.0)">
              ML pipelines by optimizing the storage.
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:32:08,375'); seek(1928.0)">
              So as machine learning workloads become increasingly data intensive
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:32:13,205'); seek(1933.0)">
              storage becomes a critical bottleneck.
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:32:15,725'); seek(1935.0)">
              So this slides outlines like three key strategies to overcome that.
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:32:20,080'); seek(1940.0)">
              So if you look at the distributed file systems we have file systems
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:32:23,840'); seek(1943.0)">
              like GPFS, which are very distributed.
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:32:26,360'); seek(1946.0)">
              These are built for high throughput scenarios and are capable of parallel
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:32:30,560'); seek(1950.0)">
              data processing across hundreds of nodes.
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:32:33,800'); seek(1953.0)">
              So they outperform the traditional network storage solutions by up to eight 10 x.
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:32:38,995'); seek(1958.0)">
              Right, especially when working with the small, fragmented file types
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:32:43,855'); seek(1963.0)">
              which are common in the ML data sets.
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:32:46,765'); seek(1966.0)">
              The design allows like efficient aggregation of bandwidth and parallel
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:32:51,925'); seek(1971.0)">
              read and writes, so which accelerates the data ingestion during the training.
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:32:56,445'); seek(1976.0)">
              Next, we deploy the in-memory caching layers.
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:33:00,040'); seek(1980.0)">
              Between the persistent storage and compute resources these caching
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:33:05,430'); seek(1985.0)">
              layers drastically reduce IO wait times, especially for frequently
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:33:09,000'); seek(1989.0)">
              accessed data sets by 65% or more.
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:33:11,930'); seek(1991.0)">
              They also implement automatic eviction policies ensuring that the optimal
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:33:16,320'); seek(1996.0)">
              memory usage without manual intervention.
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:33:19,020'); seek(1999.0)">
              This is particularly beneficial in iterative workflows where
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:33:23,410'); seek(2003.0)">
              the same data sets are are.
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:33:26,175'); seek(2006.0)">
              Read multiple times during the model tuning.
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:33:29,165'); seek(2009.0)">
              Lastly, we have something called the automated storage tiering.
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:33:32,885'); seek(2012.0)">
              So this involves moving data between the hot and cold storage tiers based on the
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:33:38,165'); seek(2018.0)">
              access, frequency and performance needs.
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:33:41,195'); seek(2021.0)">
              So this not only ensures that the performance for the
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:33:45,015'); seek(2025.0)">
              active data sets, but also.
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:33:47,715'); seek(2027.0)">
              Achieves like the 40 to 50 percent of cost reductions by shifting the
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:33:52,570'); seek(2032.0)">
              inactive data to lower cost storage.
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:33:55,770'); seek(2035.0)">
              This is done transparently through a unified namespace.
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:33:59,420'); seek(2039.0)">
              So engineers don't need to worry about where the data physically rec resides.
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:34:05,060'); seek(2045.0)">
              So together with these kind of three layers, distributed file system, in
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:34:10,820'); seek(2050.0)">
              memory caching and intelligent tiering this create a robust, cost efficient
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:34:15,100'); seek(2055.0)">
              foundation for data driven ML pipelines.
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:34:18,250'); seek(2058.0)">
              The result is faster training, lower cost, and smoother operations.
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:34:22,590'); seek(2062.0)">
              Here we are looking at how we optimize cloud spend for machine learning
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:34:27,080'); seek(2067.0)">
              workloads using sport instances.
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:34:30,220'); seek(2070.0)">
              We start with the workload classifications.
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:34:32,950'); seek(2072.0)">
              So we assess the jobs based on the fall tolerance and run length
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:34:37,475'); seek(2077.0)">
              to identify which, which can safely run on the spot Instances.
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:34:42,445'); seek(2082.0)">
              Next we implement like 4,000.
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:34:45,150'); seek(2085.0)">
              By adding automated checkpoints and recovery systems.
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:34:48,980'); seek(2088.0)">
              So if a spot instance gets reclaimed, the job can resume smoothly.
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:34:54,020'); seek(2094.0)">
              Then comes the bidding strategy, optimization izing, the historical
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:34:59,480'); seek(2099.0)">
              and real time pricing data.
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:35:01,580'); seek(2101.0)">
              We just are bidding to maximize the savings while maintaining the job.
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:35:05,570'); seek(2105.0)">
              Re reliability.
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:35:07,220'); seek(2107.0)">
              Lastly, we adopt the hybrid deployment models.
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:35:11,510'); seek(2111.0)">
              So dynamic, clearly, like switching between the spot and on demand instances
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:35:16,480'); seek(2116.0)">
              based on cost and availability.
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:35:18,430'); seek(2118.0)">
              So this is one of the important models which we can deploy to make sure
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:35:22,265'); seek(2122.0)">
              we enable that the cost savings of up to 60 to 80% is achieved without
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:35:28,725'); seek(2128.0)">
              even sacrificing on the performance.
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:35:31,235'); seek(2131.0)">
              So moving on, let's talk about optimizing the no pool configurations.
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:35:35,885'); seek(2135.0)">
              So an area where we can unlock substantial performance and cost
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:35:39,725'); seek(2139.0)">
              benefits, especially in machine learning and compute heavy environments.
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:35:44,465'); seek(2144.0)">
              So first we start with heterogeneous hardware segmentation.
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:35:48,415'); seek(2148.0)">
              So rather than mixing all the GPUs, GU types into a single nor pool.
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:35:53,585'); seek(2153.0)">
              We create a specialized nor pools for different GPUs, like eight
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:35:57,635'); seek(2157.0)">
              hundreds going to one nor pool.
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:35:59,435'); seek(2159.0)">
              And then the v hundreds go to a different nor pool.
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:36:03,035'); seek(2163.0)">
              So each has a different compute and the memory characteristics.
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:36:06,635'); seek(2166.0)">
              So assigning the workloads without creating the nor pools
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:36:10,065'); seek(2170.0)">
              may lead to and inefficiencies.
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:36:12,955'); seek(2172.0)">
              So by using Ts and tolerations, we make sure workloads are scheduled only to.
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:36:19,085'); seek(2179.0)">
              Hardware data optimized for.
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:36:21,145'); seek(2181.0)">
              Next we have resource optimization.
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:36:23,875'); seek(2183.0)">
              This is about fine tuning the CPU and the GPU as well as the memory ratios
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:36:28,945'); seek(2188.0)">
              to match the ml workload profiles.
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:36:31,960'); seek(2191.0)">
              For example inference might require more CPU, whereas training
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:36:36,785'); seek(2196.0)">
              model could need more GPU memory.
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:36:39,310'); seek(2199.0)">
              So if.
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:36:40,150'); seek(2200.0)">
              If these ratios are mislead or miscalculated, we either end up with
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:36:44,920'); seek(2204.0)">
              bottlenecks or underutilized hardware, both of which are very costly.
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:36:49,375'); seek(2209.0)">
              Then there is network topology alignment where you'll you are running, when you're
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:36:53,955'); seek(2213.0)">
              running your distributed training network bandwidth, and latency matters a lot.
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:36:59,725'); seek(2219.0)">
              So we design a no pools to align with.
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:37:03,880'); seek(2223.0)">
              With the physical infrastructure, that means like grouping into a nodes nodes
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:37:09,420'); seek(2229.0)">
              with the high speed interconnects like NV link or InfiniBand into a same pool.
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:37:15,620'); seek(2235.0)">
              This ensures that the data exchanges between the GPUs in a fast and consistent
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:37:20,520'); seek(2240.0)">
              manner, which avoids the slowdowns also during the training process.
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:37:25,010'); seek(2245.0)">
              Now let's quickly look at how we can share resources fairly when multiple
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:37:29,740'); seek(2249.0)">
              teams are working on the same system.
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:37:32,430'); seek(2252.0)">
              First we use namespace to keep things organized.
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:37:35,890'); seek(2255.0)">
              So each team or, type of work like production versus a non-production
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:37:40,750'); seek(2260.0)">
              gets its own namespace.
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:37:42,700'); seek(2262.0)">
              So this way we can set up different permissions use specific settings
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:37:47,100'); seek(2267.0)">
              for each environment and keep the network traffic separate.
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:37:51,300'); seek(2271.0)">
              Second, we can set up the resource controls.
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:37:54,270'); seek(2274.0)">
              That means we limit how GPU memory and storage each team can use.
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:37:59,670'); seek(2279.0)">
              This helps, make, making sure like team doesn't accidentally take
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:38:04,470'); seek(2284.0)">
              more share than which is allocated.
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:38:07,460'); seek(2287.0)">
              Then finally we give higher priority to critical jobs like live production jobs.
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:38:13,200'); seek(2293.0)">
              At the same time, we make sure like every team gets at least a minimum resources
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:38:18,490'); seek(2298.0)">
              which are needed for their operations.
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:38:20,410'); seek(2300.0)">
              So this avoids the problem of one team overutilizing the resources while
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:38:25,370'); seek(2305.0)">
              the other team doesn't get anything.
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:38:27,500'); seek(2307.0)">
              So the overall this setup.
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:38:29,545'); seek(2309.0)">
              Keeps fair, reliable, and running the infrastructure smoothly for everyone.
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:38:35,025'); seek(2315.0)">
              Here's what an impact these optimization strategies can have.
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:38:38,830'); seek(2318.0)">
              So in one of the case studies, it was observed that companies that
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:38:42,220'); seek(2322.0)">
              applied this techniques saw an average of 43% reduction in costs.
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:38:47,090'); seek(2327.0)">
              That, near that is nearly half of their infrastructure spending saved just by.
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:38:53,610'); seek(2333.0)">
              Tuning the things right next.
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:38:56,380'); seek(2336.0)">
              Training jobs became much faster, like 2.8 times faster on average.
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:39:01,910'); seek(2341.0)">
              That means models are trained and ready in a bayless time which
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:39:06,430'); seek(2346.0)">
              speeding up the entire development cycle study also suggests that.
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:39:11,200'); seek(2351.0)">
              There was a big jump in the GPU usage, so which is up around 67%.
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:39:18,280'); seek(2358.0)">
              So instead of having expensive GPU sitting idle, they're being
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:39:23,440'); seek(2363.0)">
              used efficiently across a cluster.
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:39:26,860'); seek(2366.0)">
              And finally, the production models became more reliable with 94% of the
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:39:31,660'); seek(2371.0)">
              inherent requests, meeting the SLA goals.
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:39:34,680'); seek(2374.0)">
              So that's a strong sign of stable production ready systems.
            </span>
            
            <span id="chunk-605" class="transcript-chunks" onclick="console.log('00:39:38,630'); seek(2378.0)">
              So this slides gives us a step by step guide for applying the
            </span>
            
            <span id="chunk-606" class="transcript-chunks" onclick="console.log('00:39:42,690'); seek(2382.0)">
              strategies we've covered so far.
            </span>
            
            <span id="chunk-607" class="transcript-chunks" onclick="console.log('00:39:45,020'); seek(2385.0)">
              Step one is to establish the baseline metrics.
            </span>
            
            <span id="chunk-608" class="transcript-chunks" onclick="console.log('00:39:48,680'); seek(2388.0)">
              Like before making any changes, we need to understand how our
            </span>
            
            <span id="chunk-609" class="transcript-chunks" onclick="console.log('00:39:52,560'); seek(2392.0)">
              resources are being used right now.
            </span>
            
            <span id="chunk-610" class="transcript-chunks" onclick="console.log('00:39:55,285'); seek(2395.0)">
              That includes tracking the GPU and the CP usage.
            </span>
            
            <span id="chunk-611" class="transcript-chunks" onclick="console.log('00:39:58,480'); seek(2398.0)">
              Training times and how much it's costing up per model.
            </span>
            
            <span id="chunk-612" class="transcript-chunks" onclick="console.log('00:40:02,280'); seek(2402.0)">
              Then step two is to go for the quick wins.
            </span>
            
            <span id="chunk-613" class="transcript-chunks" onclick="console.log('00:40:05,670'); seek(2405.0)">
              Like there are changes that are not.
            </span>
            
            <span id="chunk-614" class="transcript-chunks" onclick="console.log('00:40:08,570'); seek(2408.0)">
              Easy to be implemented and give immediate value without disruption
            </span>
            
            <span id="chunk-615" class="transcript-chunks" onclick="console.log('00:40:13,070'); seek(2413.0)">
              disrupting our current setup.
            </span>
            
            <span id="chunk-616" class="transcript-chunks" onclick="console.log('00:40:15,980'); seek(2415.0)">
              So examples include resizing the resource limits correctly, and then enabling
            </span>
            
            <span id="chunk-617" class="transcript-chunks" onclick="console.log('00:40:21,205'); seek(2421.0)">
              simple auto scaling, and then using the right storage classes for each workloads.
            </span>
            
            <span id="chunk-618" class="transcript-chunks" onclick="console.log('00:40:27,005'); seek(2427.0)">
              Then step three is to deploy more advanced optimizations.
            </span>
            
            <span id="chunk-619" class="transcript-chunks" onclick="console.log('00:40:30,890'); seek(2430.0)">
              Once the basics are in place, we can start using smarter tactics like custom
            </span>
            
            <span id="chunk-620" class="transcript-chunks" onclick="console.log('00:40:36,390'); seek(2436.0)">
              autoscaling based on the real time metrics using spot instances to cut
            </span>
            
            <span id="chunk-621" class="transcript-chunks" onclick="console.log('00:40:40,990'); seek(2440.0)">
              the costs or even fine tuning the node placement based on the hardware topology.
            </span>
            
            <span id="chunk-622" class="transcript-chunks" onclick="console.log('00:40:46,750'); seek(2446.0)">
              And then finally step four is continuously continuous refinement.
            </span>
            
            <span id="chunk-623" class="transcript-chunks" onclick="console.log('00:40:52,060'); seek(2452.0)">
              So optimization is not.
            </span>
            
            <span id="chunk-624" class="transcript-chunks" onclick="console.log('00:40:54,040'); seek(2454.0)">
              One, one time thing.
            </span>
            
            <span id="chunk-625" class="transcript-chunks" onclick="console.log('00:40:55,750'); seek(2455.0)">
              So it needs to evolve as workloads and technologies change.
            </span>
            
            <span id="chunk-626" class="transcript-chunks" onclick="console.log('00:40:59,650'); seek(2459.0)">
              That means doing regular reviews, setting up alerts for unusual costs,
            </span>
            
            <span id="chunk-627" class="transcript-chunks" onclick="console.log('00:41:04,820'); seek(2464.0)">
              and making sure our infrastructure and models are improved over over
            </span>
            
            <span id="chunk-628" class="transcript-chunks" onclick="console.log('00:41:09,420'); seek(2469.0)">
              time which is very important, right?
            </span>
            
            <span id="chunk-629" class="transcript-chunks" onclick="console.log('00:41:12,400'); seek(2472.0)">
              So in short, this roadmap helps balance the quick efficiency gains
            </span>
            
            <span id="chunk-630" class="transcript-chunks" onclick="console.log('00:41:16,420'); seek(2476.0)">
              with smart long-term planning.
            </span>
            
            <span id="chunk-631" class="transcript-chunks" onclick="console.log('00:41:18,700'); seek(2478.0)">
              Finally, thank you for joining me in this session and I hope you have a great
            </span>
            
            <span id="chunk-632" class="transcript-chunks" onclick="console.log('00:41:22,510'); seek(2482.0)">
              time at this particular conference.
            </span>
            
            <span id="chunk-633" class="transcript-chunks" onclick="console.log('00:41:24,730'); seek(2484.0)">
              Thank you.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Srinivas%20Reddy%20Mosali%20-%20Conf42%20Site%20Reliablity%20Engineering%20%28SRE%29%202025.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Srinivas%20Reddy%20Mosali%20-%20Conf42%20Site%20Reliablity%20Engineering%20%28SRE%29%202025.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #E36414;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/sre2025" class="btn btn-sm btn-danger shadow lift" style="background-color: #E36414;">
                <i class="fe fe-grid me-2"></i>
                See all 109 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Srinivas%20Reddy%20Mosali_sre.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Srinivas Reddy Mosali
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Senior Consultant - Systems Engineer @ Visa
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/srinivas-reddy-mosali-b9548b141/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Srinivas Reddy Mosali's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Srinivas Reddy Mosali"
                  data-url="https://www.conf42.com/sre2025"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/sre2025"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Site Reliability Engineering"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>