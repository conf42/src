<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Containing an Elephant: How we took HBase and HDFS from private data centers into Kubernetes and Public Cloud</title>
    <meta name="description" content="Everything Cloud Native and Cloud Security. It came from the Cloud!!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/cloud_dhiraj.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Containing an Elephant: How we took HBase and HDFS from private data centers into Kubernetes and Public Cloud | Conf42"/>
    <meta property="og:description" content="Operating a distributed system like HBase/Hadoop FS at peta byte scale took years to master in our private data centers. This talk describes our dramatic shift towards running a mission critical stateful application on Kubernetes in Public Cloud, why we did it and the challenges we had to overcome.  Salesforce runs a very large footprint of HBase and HDFS clusters in our data centers with multiple petabytes of data, billions of queries per day over thousands of machines. After more than a decade of running our own data centers, we pivoted towards public cloud for its scalability and availability. As part of this foray, we made a bold decision to move our HBase clusters from staid bare metal hosts to the dynamic and immutable world of containers and Kubernetes. This move brought with it a number of challenges which are likely to find echoes in other such mature stateful applications adapting to public cloud and Kubernetes. The challenges include 1. Limitations in Kubernetes while deploying large scale stateful applications 2. Failures in HBase/HDFS as the DNS records keep changing in Kubernetes 3. Resilience of HBase/HDFS even when a whole availability zone fails in Public Cloud 4. Introducing encryption in communication over untrusted public cloud networks without application being aware   This talk will go over how we overcame these challenges and the benefits we are beginning to see from these efforts."/>
    <meta property="og:url" content="https://conf42.com/Cloud_Native_2021_Dhiraj_Hegde_hbase_hdfs_kubernetes_public_cloud"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/LLM2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Large Language Models (LLMs) 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-03-20
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/llms2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #7B2726;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Cloud Native 2021 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Everything Cloud Native and Cloud Security. It came from the Cloud!!
 -->
              <script>
                const event_date = new Date("2021-04-29T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2021-04-29T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "T3_UMV6_tKw"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "KblJ1XbXtDs"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://youtube.com/playlist?list=PLIuxSyKxlQrDISBJgFU1hAD4f5ivuVgIq" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hi, my name is Dhiraj Hegde and I\u0027m from Salesforce. Today I\u0027ll be", "timestamp": "00:00:40,930", "timestamp_s": 40.0}, {"text": "talking about how we manage hbase in public cloud.", "timestamp": "00:00:44,748", "timestamp_s": 44.0}, {"text": "Hbase is a distribute at key value store that is", "timestamp": "00:00:48,156", "timestamp_s": 48.0}, {"text": "horizontally scalable and it runs on top of Hadoop file system. For many", "timestamp": "00:00:51,428", "timestamp_s": 51.0}, {"text": "years, Salesforce has been running HBS in its own private data centers.", "timestamp": "00:00:55,332", "timestamp_s": 55.0}, {"text": "It\u0027s been running a very large footprint of clusters with thousands of machines,", "timestamp": "00:00:59,498", "timestamp_s": 59.0}, {"text": "petabytes of data storage and billions", "timestamp": "00:01:03,742", "timestamp_s": 63.0}, {"text": "of queries per day. But it had been using very traditional mechanisms", "timestamp": "00:01:07,006", "timestamp_s": 67.0}, {"text": "of managing these clusters on bare metal hosts", "timestamp": "00:01:10,798", "timestamp_s": 70.0}, {"text": "using open source tools like puppet and ambari.", "timestamp": "00:01:14,542", "timestamp_s": 74.0}, {"text": "When we started moving some of our clusters into public cloud,", "timestamp": "00:01:17,630", "timestamp_s": 77.0}, {"text": "we decided to take a very different approach, using kubernetes to manage these clusters.", "timestamp": "00:01:20,684", "timestamp_s": 80.0}, {"text": "We\u0027ll explain in this talk why we chose kubernetes, what challenges", "timestamp": "00:01:25,298", "timestamp_s": 85.0}, {"text": "we ran into with our choice, and how we overcame those challenges.", "timestamp": "00:01:28,946", "timestamp_s": 88.0}, {"text": "So why did we pick kubernetes? There are a number of reasons.", "timestamp": "00:01:33,078", "timestamp_s": 93.0}, {"text": "One of the main problems that we ran into in an old deployment", "timestamp": "00:01:36,790", "timestamp_s": 96.0}, {"text": "mechanism was that it was in place in a hosts,", "timestamp": "00:01:40,582", "timestamp_s": 100.0}, {"text": "meaning you would go into a host that already had software", "timestamp": "00:01:43,414", "timestamp_s": 103.0}, {"text": "installed and try to modify it. The problem with that is that when", "timestamp": "00:01:46,890", "timestamp_s": 106.0}, {"text": "you are trying to deploying on thousands of hosts, some of those hosts,", "timestamp": "00:01:50,148", "timestamp_s": 110.0}, {"text": "the installation could fail, leaving it in an uncertain", "timestamp": "00:01:53,418", "timestamp_s": 113.0}, {"text": "stateful, with some binaries present or some configs present and", "timestamp": "00:01:56,558", "timestamp_s": 116.0}, {"text": "others missing. And if you happen to miss these failures, those hosts", "timestamp": "00:02:00,008", "timestamp_s": 120.0}, {"text": "would remain in this inconsistent stateful for a very long time. The other thing", "timestamp": "00:02:03,438", "timestamp_s": 123.0}, {"text": "we have noticed is there\u0027s a temptation when you\u0027re dealing", "timestamp": "00:02:07,468", "timestamp_s": 127.0}, {"text": "with emergency issues, maybe a site issue that people", "timestamp": "00:02:10,898", "timestamp_s": 130.0}, {"text": "just go and log into. Hosts go in and modify these configurations locally", "timestamp": "00:02:14,556", "timestamp_s": 134.0}, {"text": "and completely forget that they did that. And the problem there again", "timestamp": "00:02:19,026", "timestamp_s": 139.0}, {"text": "is that once the config is forgotten, it looks a little different on that one", "timestamp": "00:02:22,992", "timestamp_s": 142.0}, {"text": "host compared to many of the other hosts. With containers. A lot of", "timestamp": "00:02:26,672", "timestamp_s": 146.0}, {"text": "these problems go away because they have this thing called immutability,", "timestamp": "00:02:30,608", "timestamp_s": 150.0}, {"text": "meaning with a container, it is actually created from", "timestamp": "00:02:34,058", "timestamp_s": 154.0}, {"text": "an image, and that image contains all the configuration,", "timestamp": "00:02:37,812", "timestamp_s": 157.0}, {"text": "all the binaries already present. And as that", "timestamp": "00:02:41,578", "timestamp_s": 161.0}, {"text": "container comes up, it is an exact replica of what that image has.", "timestamp": "00:02:45,028", "timestamp_s": 165.0}, {"text": "And if you even attempt to make a change after it\u0027s running, what happens is", "timestamp": "00:02:48,824", "timestamp_s": 168.0}, {"text": "that the next time the container is restarted could be a crash or a deliberate", "timestamp": "00:02:52,728", "timestamp_s": 172.0}, {"text": "restart. It would once again start from the image. So whatever", "timestamp": "00:02:56,862", "timestamp_s": 176.0}, {"text": "you made, changes locally would be totally lost. So in that", "timestamp": "00:03:00,380", "timestamp_s": 180.0}, {"text": "kind of immutable environment, it is pretty easy to make sure", "timestamp": "00:03:03,852", "timestamp_s": 183.0}, {"text": "that over a period of time, the images are all consistent,", "timestamp": "00:03:07,740", "timestamp_s": 187.0}, {"text": "no matter what people try to do to it or what failures", "timestamp": "00:03:11,590", "timestamp_s": 191.0}, {"text": "happen with it. The second reason why we felt Kubernetes", "timestamp": "00:03:15,046", "timestamp_s": 195.0}, {"text": "and containerization was a good thing was availability and scalability.", "timestamp": "00:03:18,566", "timestamp_s": 198.0}, {"text": "With Kubernetes, it\u0027s really good about making sure", "timestamp": "00:03:22,758", "timestamp_s": 202.0}, {"text": "that if you are running your containers on a particular host or", "timestamp": "00:03:26,276", "timestamp_s": 206.0}, {"text": "set of hosts, and if something happens to those hosts, kubernetes can monitor", "timestamp": "00:03:30,132", "timestamp_s": 210.0}, {"text": "and cache those issues and immediately restart those containers", "timestamp": "00:03:34,446", "timestamp_s": 214.0}, {"text": "on a different hosts. By the way, just to be clear here", "timestamp": "00:03:37,982", "timestamp_s": 217.0}, {"text": "in Kubernetes, containers are managed by", "timestamp": "00:03:41,990", "timestamp_s": 221.0}, {"text": "a construct called pods. But in this talk we are going to treat", "timestamp": "00:03:45,048", "timestamp_s": 225.0}, {"text": "the term containers and pods as one and these same to", "timestamp": "00:03:48,696", "timestamp_s": 228.0}, {"text": "keep it simple. They are not exactly the same, but we are not going to", "timestamp": "00:03:52,028", "timestamp_s": 232.0}, {"text": "get into it. And for the purposes of this talk, it doesn\u0027t", "timestamp": "00:03:54,828", "timestamp_s": 234.0}, {"text": "really matter as much. So coming back to the question of availability,", "timestamp": "00:03:58,338", "timestamp_s": 238.0}, {"text": "it makes sure that when pods fail", "timestamp": "00:04:02,230", "timestamp_s": 242.0}, {"text": "on a particular host, it will make sure that", "timestamp": "00:04:05,942", "timestamp_s": 245.0}, {"text": "if it can find another healthy host out there, it\u0027ll move these pods to those", "timestamp": "00:04:09,488", "timestamp_s": 249.0}, {"text": "other hosts. Similarly, when you need to scale", "timestamp": "00:04:12,772", "timestamp_s": 252.0}, {"text": "up your application, it\u0027s very easy in", "timestamp": "00:04:16,282", "timestamp_s": 256.0}, {"text": "Kubernetes to specify a higher number of pods to meet", "timestamp": "00:04:19,428", "timestamp_s": 259.0}, {"text": "to the requirements of that particular traffic. And once the traffic", "timestamp": "00:04:22,856", "timestamp_s": 262.0}, {"text": "spike is gone, you can also equally", "timestamp": "00:04:27,270", "timestamp_s": 267.0}, {"text": "quickly reduce the number of pods that are run to", "timestamp": "00:04:31,342", "timestamp_s": 271.0}, {"text": "serve the traffic. So this ability to scale up and down", "timestamp": "00:04:34,952", "timestamp_s": 274.0}, {"text": "is very valuable to us, especially when you are moving to the cloud.", "timestamp": "00:04:38,392", "timestamp_s": 278.0}, {"text": "And one of the big claims of cloud is that you got this elasticity.", "timestamp": "00:04:41,500", "timestamp_s": 281.0}, {"text": "So how can you take advantage of this elasticity?", "timestamp": "00:04:45,794", "timestamp_s": 285.0}, {"text": "Using Kubernetes and its elastic management of", "timestamp": "00:04:48,970", "timestamp_s": 288.0}, {"text": "pods is one great way of achieving it. The third", "timestamp": "00:04:52,592", "timestamp_s": 292.0}, {"text": "reason is actually one of the most important reasons as to why we went with", "timestamp": "00:04:55,696", "timestamp_s": 295.0}, {"text": "Kubernetes. We had this desire or goal to make", "timestamp": "00:04:59,248", "timestamp_s": 299.0}, {"text": "sure that whatever we built, and we started by these way with AWS,", "timestamp": "00:05:03,092", "timestamp_s": 303.0}, {"text": "which is Amazon\u0027s cloud offering. But our plan was to try and", "timestamp": "00:05:07,338", "timestamp_s": 307.0}, {"text": "build something there that would apply to other clouds,", "timestamp": "00:05:11,316", "timestamp_s": 311.0}, {"text": "because it is very likely that we would be running a software in other", "timestamp": "00:05:14,542", "timestamp_s": 314.0}, {"text": "clouds. Example of other clouds are things like Azure from Microsoft", "timestamp": "00:05:18,536", "timestamp_s": 318.0}, {"text": "or GCP from Google.", "timestamp": "00:05:22,478", "timestamp_s": 322.0}, {"text": "So we wanted to be able to run our software in", "timestamp": "00:05:25,670", "timestamp_s": 325.0}, {"text": "all those different environments. But the problem there is when", "timestamp": "00:05:29,452", "timestamp_s": 329.0}, {"text": "you\u0027re building your software deployment processes for one cloud, you are so intimately", "timestamp": "00:05:33,132", "timestamp_s": 333.0}, {"text": "tied to the APIs and the way they manage compute, storage network", "timestamp": "00:05:37,346", "timestamp_s": 337.0}, {"text": "that whatever you build there doesn\u0027t naturally apply to", "timestamp": "00:05:41,510", "timestamp_s": 341.0}, {"text": "these other clouds. But fortunately with Kubernetes, things turn", "timestamp": "00:05:45,040", "timestamp_s": 345.0}, {"text": "upside down. Kubernetes is actually very opinionated. It actually", "timestamp": "00:05:48,656", "timestamp_s": 348.0}, {"text": "specifies exactly how compute should be managed,", "timestamp": "00:05:52,756", "timestamp_s": 352.0}, {"text": "exactly how storage should be managed, and how network should be", "timestamp": "00:05:55,674", "timestamp_s": 355.0}, {"text": "managed. At that point, it becomes incumbent upon", "timestamp": "00:05:58,948", "timestamp_s": 358.0}, {"text": "the other cloud providers to make sure that they manage storage,", "timestamp": "00:06:02,794", "timestamp_s": 362.0}, {"text": "network and compute in the way Kubernetes", "timestamp": "00:06:07,342", "timestamp_s": 367.0}, {"text": "expects it to be managed. So when you build your software", "timestamp": "00:06:10,670", "timestamp_s": 370.0}, {"text": "deployment processes around kubernetes on any one of those cloud", "timestamp": "00:06:14,526", "timestamp_s": 374.0}, {"text": "providers, you automatically get the same deployment", "timestamp": "00:06:18,076", "timestamp_s": 378.0}, {"text": "processes working in all these other different clouds, because it sort of", "timestamp": "00:06:22,082", "timestamp_s": 382.0}, {"text": "enforces that sort of behavior. So for those three reasons,", "timestamp": "00:06:26,012", "timestamp_s": 386.0}, {"text": "we found Kubernetes to be very interesting to us. Okay,", "timestamp": "00:06:29,458", "timestamp_s": 389.0}, {"text": "now we\u0027ll get into a little bit about some of the challenges that we had", "timestamp": "00:06:32,800", "timestamp_s": 392.0}, {"text": "with Kubernetes once we started using it. To understand it a little", "timestamp": "00:06:36,096", "timestamp_s": 396.0}, {"text": "better, you would need to know the difference between stateful and stateless applications.", "timestamp": "00:06:39,648", "timestamp_s": 399.0}, {"text": "Here we show you a typical stateless application,", "timestamp": "00:06:43,962", "timestamp_s": 403.0}, {"text": "basically HTTP servers that run in a website. Each one of these", "timestamp": "00:06:46,900", "timestamp_s": 406.0}, {"text": "server instances has nothing that is unique in them. They kind", "timestamp": "00:06:50,452", "timestamp_s": 410.0}, {"text": "of serve the same content. If you ever try to make a request to them", "timestamp": "00:06:54,068", "timestamp_s": 414.0}, {"text": "that changes the content, it usually goes to some back end database", "timestamp": "00:06:57,736", "timestamp_s": 417.0}, {"text": "that is shared across all those instances, or maybe even a", "timestamp": "00:07:01,422", "timestamp_s": 421.0}, {"text": "shared file system across all these instances. So essentially each", "timestamp": "00:07:04,632", "timestamp_s": 424.0}, {"text": "one of those running servers in Kubernetes", "timestamp": "00:07:08,572", "timestamp_s": 428.0}, {"text": "world, these would be pods. They\u0027re all stateless.", "timestamp": "00:07:12,226", "timestamp_s": 432.0}, {"text": "And when a client is trying to access the service", "timestamp": "00:07:15,298", "timestamp_s": 435.0}, {"text": "from them, they typically go to a load balancer, as you can see here,", "timestamp": "00:07:18,940", "timestamp_s": 438.0}, {"text": "and the load balancer then forwards those requests,", "timestamp": "00:07:22,336", "timestamp_s": 442.0}, {"text": "each or any one of those HTTP servers in the back end. Now the interesting", "timestamp": "00:07:25,526", "timestamp_s": 445.0}, {"text": "thing to note here is that the client only needs to know the hostname and", "timestamp": "00:07:29,760", "timestamp_s": 449.0}, {"text": "IP address of the load balancer. It doesn\u0027t even need to know the", "timestamp": "00:07:33,492", "timestamp_s": 453.0}, {"text": "host names of the HTTP servers running behind it,", "timestamp": "00:07:36,948", "timestamp_s": 456.0}, {"text": "because they are just getting traffic forwarded to them by the cloud balancer.", "timestamp": "00:07:40,196", "timestamp_s": 460.0}, {"text": "Kubernetes usually manages these kinds of environments by", "timestamp": "00:07:44,062", "timestamp_s": 464.0}, {"text": "specifying a manifest, which is really a document", "timestamp": "00:07:47,496", "timestamp_s": 467.0}, {"text": "describing how many instances of these HTTP servers to", "timestamp": "00:07:51,198", "timestamp_s": 471.0}, {"text": "run as pods. It can also describe the configuration of", "timestamp": "00:07:55,048", "timestamp_s": 475.0}, {"text": "the cloud balancer, which is called a service in Kubernetes, but again it", "timestamp": "00:07:58,808", "timestamp_s": 478.0}, {"text": "is specified using a document called a manifest. And with that you can", "timestamp": "00:08:02,668", "timestamp_s": 482.0}, {"text": "nicely set up all of this. And in the early years, this is what Kubernetes", "timestamp": "00:08:06,348", "timestamp_s": 486.0}, {"text": "was really known for, the ability to very quickly spin up", "timestamp": "00:08:10,406", "timestamp_s": 490.0}, {"text": "a set of stateless applications and provide some", "timestamp": "00:08:14,272", "timestamp_s": 494.0}, {"text": "sort of service. But when you look at something like Hadoop", "timestamp": "00:08:17,888", "timestamp_s": 497.0}, {"text": "hbase, which is a very stateful application, it\u0027s a", "timestamp": "00:08:22,422", "timestamp_s": 502.0}, {"text": "database. Typically the way it behaves is that you got a client", "timestamp": "00:08:26,004", "timestamp_s": 506.0}, {"text": "there. And when it needs to access data for reasons", "timestamp": "00:08:29,626", "timestamp_s": 509.0}, {"text": "of reading, for querying, that is for modification,", "timestamp": "00:08:33,434", "timestamp_s": 513.0}, {"text": "for deletion, so on and so forth, it needs access", "timestamp": "00:08:36,686", "timestamp_s": 516.0}, {"text": "to the data servers which you can see at the bottom,", "timestamp": "00:08:40,056", "timestamp_s": 520.0}, {"text": "lined up at the bottom, a number of these data servers.", "timestamp": "00:08:43,430", "timestamp_s": 523.0}, {"text": "And each one of those data servers has very different data in", "timestamp": "00:08:46,862", "timestamp_s": 526.0}, {"text": "them. It typically does not have the same data across all of them", "timestamp": "00:08:50,188", "timestamp_s": 530.0}, {"text": "because then every data server would have to have huge amount of storage.", "timestamp": "00:08:53,852", "timestamp_s": 533.0}, {"text": "Instead you break it up into pieces and you spread it across a large", "timestamp": "00:08:57,554", "timestamp_s": 537.0}, {"text": "number of data servers. And that\u0027s how you scale as you need to store", "timestamp": "00:09:00,876", "timestamp_s": 540.0}, {"text": "more data. You just add more data servers. And on the left hand side you", "timestamp": "00:09:04,336", "timestamp_s": 544.0}, {"text": "typically have something called a metadata server which is responsible", "timestamp": "00:09:07,888", "timestamp_s": 547.0}, {"text": "for knowing where all this data is present. It basically knows the", "timestamp": "00:09:11,766", "timestamp_s": 551.0}, {"text": "geography of things. So a client, when it is", "timestamp": "00:09:15,620", "timestamp_s": 555.0}, {"text": "trying to access data, it goes to the metadata server", "timestamp": "00:09:19,012", "timestamp_s": 559.0}, {"text": "with a key saying that I want to access this. The metadata", "timestamp": "00:09:22,650", "timestamp_s": 562.0}, {"text": "server then gives a location of where these can find the data.", "timestamp": "00:09:26,638", "timestamp_s": 566.0}, {"text": "And then the client directly goes to the data server based", "timestamp": "00:09:30,168", "timestamp_s": 570.0}, {"text": "on these information and accesses the data. Now in all of this, what you will", "timestamp": "00:09:33,832", "timestamp_s": 573.0}, {"text": "notice is that the client actually needs to know the identity,", "timestamp": "00:09:37,688", "timestamp_s": 577.0}, {"text": "the hostname of all the elements. I mean it needs to", "timestamp": "00:09:41,250", "timestamp_s": 581.0}, {"text": "know the host names of the metadata server that can provide this", "timestamp": "00:09:44,604", "timestamp_s": 584.0}, {"text": "information. And once a metadata server provides the hostname information", "timestamp": "00:09:48,672", "timestamp_s": 588.0}, {"text": "of the data server, it needs to directly deal with those", "timestamp": "00:09:52,640", "timestamp_s": 592.0}, {"text": "particular data servers. There\u0027s no magical load balancer", "timestamp": "00:09:55,968", "timestamp_s": 595.0}, {"text": "hiding things from you, which is why the DNS that you see on", "timestamp": "00:10:00,134", "timestamp_s": 600.0}, {"text": "the right hand top corner is very important because it has to have all these", "timestamp": "00:10:03,744", "timestamp_s": 603.0}, {"text": "information of the host names and IP addresses.", "timestamp": "00:10:07,220", "timestamp_s": 607.0}, {"text": "And by the way, all the clients usually have a cache", "timestamp": "00:10:10,290", "timestamp_s": 610.0}, {"text": "because once you discover this information, having to rediscover", "timestamp": "00:10:13,870", "timestamp_s": 613.0}, {"text": "this information every time for a request is very inefficient.", "timestamp": "00:10:17,662", "timestamp_s": 617.0}, {"text": "So you hbase to kind of cache this information and", "timestamp": "00:10:21,118", "timestamp_s": 621.0}, {"text": "hopefully the stuff that you cache doesn\u0027t change too often because that is", "timestamp": "00:10:25,592", "timestamp_s": 625.0}, {"text": "a little bit of a disturbance that the client had to deal with. So you", "timestamp": "00:10:29,292", "timestamp_s": 629.0}, {"text": "try to minimize this disturbance. So all of this makes it", "timestamp": "00:10:31,948", "timestamp_s": 631.0}, {"text": "very important that the identity of", "timestamp": "00:10:35,628", "timestamp_s": 635.0}, {"text": "the servers that you\u0027re accessing tend to be", "timestamp": "00:10:39,232", "timestamp_s": 639.0}, {"text": "stable. They do not change very much. And given a host", "timestamp": "00:10:42,928", "timestamp_s": 642.0}, {"text": "name, the location of what it contains, it becomes very", "timestamp": "00:10:46,742", "timestamp_s": 646.0}, {"text": "important. Took so that association between the name of the", "timestamp": "00:10:50,192", "timestamp_s": 650.0}, {"text": "server and its content, the state it contains is very,", "timestamp": "00:10:53,748", "timestamp_s": 653.0}, {"text": "very important to the client for good performance. And this", "timestamp": "00:10:57,412", "timestamp_s": 657.0}, {"text": "is a very good example of what a stateful application is, and you can", "timestamp": "00:11:01,492", "timestamp_s": 661.0}, {"text": "see how it\u0027s a little different from stateless applications. So the", "timestamp": "00:11:05,128", "timestamp_s": 665.0}, {"text": "Kubernetes community, when it decided to support use", "timestamp": "00:11:08,328", "timestamp_s": 668.0}, {"text": "cases like Hadoop, hbase or Cassandra,", "timestamp": "00:11:11,576", "timestamp_s": 671.0}, {"text": "they introduced a feature called stateful.", "timestamp": "00:11:15,086", "timestamp_s": 675.0}, {"text": "What stateful provides is these same ability to create", "timestamp": "00:11:18,542", "timestamp_s": 678.0}, {"text": "pods, but in this case, when it creates pods, it gives it", "timestamp": "00:11:22,076", "timestamp_s": 682.0}, {"text": "unique names. And the names are kind of easy to guess.", "timestamp": "00:11:25,980", "timestamp_s": 685.0}, {"text": "They usually begin with the same prefix. In this case", "timestamp": "00:11:29,840", "timestamp_s": 689.0}, {"text": "it was pod. As an example I took. It can", "timestamp": "00:11:32,992", "timestamp_s": 692.0}, {"text": "even be HTTP or hadoop or whatever,", "timestamp": "00:11:36,288", "timestamp_s": 696.0}, {"text": "but then it would associate with each instance of the pod", "timestamp": "00:11:39,712", "timestamp_s": 699.0}, {"text": "a unique number, like zero, one, two,", "timestamp": "00:11:43,338", "timestamp_s": 703.0}, {"text": "depending upon how many of these you want. So each pod would", "timestamp": "00:11:46,772", "timestamp_s": 706.0}, {"text": "have a unique name. And also you can see in the right hand top corner", "timestamp": "00:11:50,148", "timestamp_s": 710.0}, {"text": "that the DNS is also modified to give these pods a", "timestamp": "00:11:53,866", "timestamp_s": 713.0}, {"text": "proper hostname and IP address. So you got everything with", "timestamp": "00:11:57,448", "timestamp_s": 717.0}, {"text": "it. You got unique names, hostnames and IP address associated", "timestamp": "00:12:01,048", "timestamp_s": 721.0}, {"text": "with each one of these pods. In addition, since this is a stateful application,", "timestamp": "00:12:04,558", "timestamp_s": 724.0}, {"text": "you could also define how much storage you want to associate", "timestamp": "00:12:08,970", "timestamp_s": 728.0}, {"text": "with each one of these pods, the size of it,", "timestamp": "00:12:13,602", "timestamp_s": 733.0}, {"text": "also the class, whether you want SSD or HDD. All of this can be specified", "timestamp": "00:12:16,716", "timestamp_s": 736.0}, {"text": "using a construct called a persistent volume claim.", "timestamp": "00:12:21,550", "timestamp_s": 741.0}, {"text": "It\u0027s basically a claim for storage. It\u0027s not the actual storage you are requesting", "timestamp": "00:12:25,526", "timestamp_s": 745.0}, {"text": "storage. And this is embedded in each one of these pod definitions", "timestamp": "00:12:29,318", "timestamp_s": 749.0}, {"text": "where a pv claim is specified. When this is defined,", "timestamp": "00:12:33,754", "timestamp_s": 753.0}, {"text": "what happens is that the providers or cloud providers who run", "timestamp": "00:12:37,610", "timestamp_s": 757.0}, {"text": "Kubernetes like AWS, Azure or GCP,", "timestamp": "00:12:41,492", "timestamp_s": 761.0}, {"text": "they will notice this claim and these immediately carve out a disk in the cloud", "timestamp": "00:12:45,194", "timestamp_s": 765.0}, {"text": "which has these size and the class of storage that is being requested,", "timestamp": "00:12:49,624", "timestamp_s": 769.0}, {"text": "and then it is made available to Kubernetes. Kubernetes then mounts that", "timestamp": "00:12:53,230", "timestamp_s": 773.0}, {"text": "disk in each one of these pods so that it becomes a locally available storage", "timestamp": "00:12:57,640", "timestamp_s": 777.0}, {"text": "in each one of those pods. And at that point", "timestamp": "00:13:01,842", "timestamp_s": 781.0}, {"text": "you\u0027ve got a unique name which is well defined in DNS", "timestamp": "00:13:05,292", "timestamp_s": 785.0}, {"text": "associated with storage. And this is a one to one mapping", "timestamp": "00:13:08,754", "timestamp_s": 788.0}, {"text": "between the two. Now what\u0027s interesting to note here is that", "timestamp": "00:13:12,518", "timestamp_s": 792.0}, {"text": "let\u0027s take an example of any one of these pods which is", "timestamp": "00:13:16,528", "timestamp_s": 796.0}, {"text": "running on host a right now. It has a claim and it is accessing pv", "timestamp": "00:13:20,608", "timestamp_s": 800.0}, {"text": "zero which is mounted as a disk in it. Let\u0027s say for some reason host", "timestamp": "00:13:24,202", "timestamp_s": 804.0}, {"text": "a has some problem, it goes up in smoke.", "timestamp": "00:13:28,266", "timestamp_s": 808.0}, {"text": "Kubernetes would notice that. And it", "timestamp": "00:13:31,146", "timestamp_s": 811.0}, {"text": "will then say that okay, this pod is gone, it\u0027ll remove it from", "timestamp": "00:13:34,452", "timestamp_s": 814.0}, {"text": "its system and you\u0027ll notice in the right hand top corner that the DNS also", "timestamp": "00:13:38,072", "timestamp_s": 818.0}, {"text": "is modified to remove any DNS entries related to it.", "timestamp": "00:13:41,672", "timestamp_s": 821.0}, {"text": "You can still see the storage is present because claim that created", "timestamp": "00:13:45,640", "timestamp_s": 825.0}, {"text": "the storage is still present. These pod is gone but the claim is still there.", "timestamp": "00:13:49,618", "timestamp_s": 829.0}, {"text": "Eventually what Kubernetes will do is it will find another free", "timestamp": "00:13:53,340", "timestamp_s": 833.0}, {"text": "host like host d in this example and recreate", "timestamp": "00:13:56,636", "timestamp_s": 836.0}, {"text": "the same pod so it has the same hostname. Pod zero,", "timestamp": "00:14:00,598", "timestamp_s": 840.0}, {"text": "the one that got destroyed, is recreated with the same hostname.", "timestamp": "00:14:04,336", "timestamp_s": 844.0}, {"text": "And because it has the same claim embedded inside of it,", "timestamp": "00:14:08,670", "timestamp_s": 848.0}, {"text": "these same storage is again associated with it. And even", "timestamp": "00:14:12,644", "timestamp_s": 852.0}, {"text": "DNS is updated to have the DNS record.", "timestamp": "00:14:16,420", "timestamp_s": 856.0}, {"text": "One thing that is different here is that when the DNS record is", "timestamp": "00:14:20,084", "timestamp_s": 860.0}, {"text": "recreated, it did not get the same IP address.", "timestamp": "00:14:23,732", "timestamp_s": 863.0}, {"text": "So it had the same hostname, but the IP address had to change then.", "timestamp": "00:14:26,900", "timestamp_s": 866.0}, {"text": "That\u0027s just the nature of networking. When you move from one compute", "timestamp": "00:14:30,472", "timestamp_s": 870.0}, {"text": "unit to another, the IP addresses that", "timestamp": "00:14:34,830", "timestamp_s": 874.0}, {"text": "you associate with that compute unit has to change.", "timestamp": "00:14:38,172", "timestamp_s": 878.0}, {"text": "It\u0027s just how network is managed in kubernetes, but otherwise", "timestamp": "00:14:41,724", "timestamp_s": 881.0}, {"text": "you basically achieve something quite interesting, which is a", "timestamp": "00:14:45,634", "timestamp_s": 885.0}, {"text": "given host name is always associated with the same volume", "timestamp": "00:14:49,292", "timestamp_s": 889.0}, {"text": "no matter where your compute goes, moves around inside the Kubernetes", "timestamp": "00:14:52,806", "timestamp_s": 892.0}, {"text": "clusters a very interesting and useful property.", "timestamp": "00:14:57,286", "timestamp_s": 897.0}, {"text": "That stickiness between hostname and the", "timestamp": "00:15:00,064", "timestamp_s": 900.0}, {"text": "volume that you use. You also notice that the IP addresses", "timestamp": "00:15:03,504", "timestamp_s": 903.0}, {"text": "can change even if the hostname doesn\u0027t change. And this is kind of important because", "timestamp": "00:15:07,706", "timestamp_s": 907.0}, {"text": "we\u0027ll get into some of the issues we have because of this a few minutes", "timestamp": "00:15:11,332", "timestamp_s": 911.0}, {"text": "from now. So using stateful set we were able", "timestamp": "00:15:14,456", "timestamp_s": 914.0}, {"text": "to deploy Hadoop hbase and going back to", "timestamp": "00:15:18,408", "timestamp_s": 918.0}, {"text": "the same slide that I showed you a while back, you can see that each", "timestamp": "00:15:22,248", "timestamp_s": 922.0}, {"text": "one of the data servers is being deployed as a stateful set.", "timestamp": "00:15:25,752", "timestamp_s": 925.0}, {"text": "And every one of them has a unique name, like DS 123-4123", "timestamp": "00:15:29,436", "timestamp_s": 929.0}, {"text": "rather. And similarly the metadata server, which is also", "timestamp": "00:15:34,812", "timestamp_s": 934.0}, {"text": "stateful, has a unique name and disks associated", "timestamp": "00:15:38,252", "timestamp_s": 938.0}, {"text": "with it. So we were able to model and deploy a software", "timestamp": "00:15:42,198", "timestamp_s": 942.0}, {"text": "using stateful set pretty well. Stateful sets are managed", "timestamp": "00:15:45,766", "timestamp_s": 945.0}, {"text": "by a controller called as the stateful set controller or the STS", "timestamp": "00:15:49,078", "timestamp_s": 949.0}, {"text": "controller in Kubernetes. And while we found", "timestamp": "00:15:52,906", "timestamp_s": 952.0}, {"text": "many of its features around managing compute storage", "timestamp": "00:15:56,420", "timestamp_s": 956.0}, {"text": "failover et centers very useful, we also", "timestamp": "00:16:00,234", "timestamp_s": 960.0}, {"text": "had some challenges with it. One area where there", "timestamp": "00:16:03,860", "timestamp_s": 963.0}, {"text": "was a problem was with its rolling upgrade process where you\u0027re trying to upgrade", "timestamp": "00:16:07,048", "timestamp_s": 967.0}, {"text": "the software. And the way it does upgrade is it starts with the", "timestamp": "00:16:10,958", "timestamp_s": 970.0}, {"text": "pod with the highest number and goes one by one,", "timestamp": "00:16:14,648", "timestamp_s": 974.0}, {"text": "upgrading each one of them in", "timestamp": "00:16:18,060", "timestamp_s": 978.0}, {"text": "strict order all the way down to zero. And while this is a very", "timestamp": "00:16:21,500", "timestamp_s": 981.0}, {"text": "nice and careful way of upgrading software.", "timestamp": "00:16:25,292", "timestamp_s": 985.0}, {"text": "It is also very slow. You can imagine", "timestamp": "00:16:28,910", "timestamp_s": 988.0}, {"text": "in the world of Hadoop hbase,", "timestamp": "00:16:33,014", "timestamp_s": 993.0}, {"text": "you got hundreds of these pods, and each one of", "timestamp": "00:16:36,070", "timestamp_s": 996.0}, {"text": "them is a heavy server that takes around five minutes to boot up and", "timestamp": "00:16:39,504", "timestamp_s": 999.0}, {"text": "initialize and set up its security credentials,", "timestamp": "00:16:43,028", "timestamp_s": 1003.0}, {"text": "cordless kerberos, key tabs, et cetera. So going", "timestamp": "00:16:45,866", "timestamp_s": 1005.0}, {"text": "through it one by one would take a very long time and", "timestamp": "00:16:49,316", "timestamp_s": 1009.0}, {"text": "almost make it impractical for us to use such a valuable feature.", "timestamp": "00:16:53,736", "timestamp_s": 1013.0}, {"text": "Fortunately, Kubernetes is also very extensible,", "timestamp": "00:16:57,630", "timestamp_s": 1017.0}, {"text": "so you can kind of go in and modify behavior", "timestamp": "00:17:00,830", "timestamp_s": 1020.0}, {"text": "or introduce new behavior by providing your own controllers in certain", "timestamp": "00:17:04,814", "timestamp_s": 1024.0}, {"text": "areas. And in this particular case, we were able to", "timestamp": "00:17:08,700", "timestamp_s": 1028.0}, {"text": "build a new controller, which we call the custom controller,", "timestamp": "00:17:12,332", "timestamp_s": 1032.0}, {"text": "which actually works in communication with the", "timestamp": "00:17:16,010", "timestamp_s": 1036.0}, {"text": "default stateful set controller. So the stateful set", "timestamp": "00:17:19,440", "timestamp_s": 1039.0}, {"text": "controller would continue to create pods and create storage", "timestamp": "00:17:23,312", "timestamp_s": 1043.0}, {"text": "and coordinate the mounting and all of", "timestamp": "00:17:27,286", "timestamp_s": 1047.0}, {"text": "that, whereas the custom controller that we built would be", "timestamp": "00:17:30,768", "timestamp_s": 1050.0}, {"text": "in charge of deciding which pods would be", "timestamp": "00:17:34,516", "timestamp_s": 1054.0}, {"text": "deleted next in order to be replaced. So the deletion", "timestamp": "00:17:37,828", "timestamp_s": 1057.0}, {"text": "would be the custom controller\u0027s job and rest of", "timestamp": "00:17:41,402", "timestamp_s": 1061.0}, {"text": "it would be these existing stateful set controller\u0027s job. So once", "timestamp": "00:17:45,028", "timestamp_s": 1065.0}, {"text": "we had this ability, and this is enabled by a flag called on", "timestamp": "00:17:48,872", "timestamp_s": 1068.0}, {"text": "delete strategy in stateful, if you\u0027re interested in looking it up,", "timestamp": "00:17:52,632", "timestamp_s": 1072.0}, {"text": "basically, these custom controller would then enable batching where", "timestamp": "00:17:57,030", "timestamp_s": 1077.0}, {"text": "it would go after a batch of pods, delete them first,", "timestamp": "00:18:00,508", "timestamp_s": 1080.0}, {"text": "and then the stateful controller would notice that these pods", "timestamp": "00:18:04,220", "timestamp_s": 1084.0}, {"text": "are missing and would recreate them with the new configuration,", "timestamp": "00:18:07,538", "timestamp_s": 1087.0}, {"text": "though. Similarly, the custom controller would then move to the next batch", "timestamp": "00:18:11,046", "timestamp_s": 1091.0}, {"text": "of three, in this case, delete them, and stateful", "timestamp": "00:18:14,662", "timestamp_s": 1094.0}, {"text": "would do the remaining part of bringing up these new ones.", "timestamp": "00:18:18,598", "timestamp_s": 1098.0}, {"text": "So in this manner, by coordinating with these existing behavior,", "timestamp": "00:18:22,112", "timestamp_s": 1102.0}, {"text": "we were able to get batch upgrades enabled in Kubernetes,", "timestamp": "00:18:25,946", "timestamp_s": 1105.0}, {"text": "which is a very big problem when we initially faced it.", "timestamp": "00:18:29,834", "timestamp_s": 1109.0}, {"text": "Another limitation that we had was that in Kubernetes,", "timestamp": "00:18:33,890", "timestamp_s": 1113.0}, {"text": "when you are deploying your services, you also define what is called", "timestamp": "00:18:37,818", "timestamp_s": 1117.0}, {"text": "as the pod disruption budget. This is important to make sure that", "timestamp": "00:18:41,448", "timestamp_s": 1121.0}, {"text": "whatever operations you do in your cluster, you don\u0027t let the number of", "timestamp": "00:18:45,336", "timestamp_s": 1125.0}, {"text": "unhealthy pods or disrupted pods. To use", "timestamp": "00:18:48,828", "timestamp_s": 1128.0}, {"text": "that terminology, you make sure that you put a limit on how many pods", "timestamp": "00:18:52,412", "timestamp_s": 1132.0}, {"text": "are disrupted. In this case, for example, let\u0027s consider that", "timestamp": "00:18:56,018", "timestamp_s": 1136.0}, {"text": "the pod disruption budget is one. What you\u0027re saying is that at", "timestamp": "00:18:59,228", "timestamp_s": 1139.0}, {"text": "any given time in your cluster, you\u0027d at most disrupt one pod", "timestamp": "00:19:02,368", "timestamp_s": 1142.0}, {"text": "and not more than that when you\u0027re doing any of your administrative tasks.", "timestamp": "00:19:06,326", "timestamp_s": 1146.0}, {"text": "Now, the problem here is that if more than one of your pods", "timestamp": "00:19:09,942", "timestamp_s": 1149.0}, {"text": "is unhealthy, in this case, pod three and pod one are in an unhealthy state", "timestamp": "00:19:13,498", "timestamp_s": 1153.0}, {"text": "because of some issue with them, and you are trying to", "timestamp": "00:19:17,444", "timestamp_s": 1157.0}, {"text": "upgrade that particular stateful set,", "timestamp": "00:19:20,596", "timestamp_s": 1160.0}, {"text": "maybe because you want to fix the issue by deploying new code.", "timestamp": "00:19:24,260", "timestamp_s": 1164.0}, {"text": "Unfortunately, since it always starts with the highest number,", "timestamp": "00:19:27,832", "timestamp_s": 1167.0}, {"text": "pod five in this case,", "timestamp": "00:19:31,080", "timestamp_s": 1171.0}, {"text": "when it tries to upgrade, Kubernetes will prevent it", "timestamp": "00:19:33,910", "timestamp_s": 1173.0}, {"text": "from being upgraded because it would increase the number", "timestamp": "00:19:37,048", "timestamp_s": 1177.0}, {"text": "of unhealthy pods, because you hbase to destroy a healthy pod to create a", "timestamp": "00:19:40,252", "timestamp_s": 1180.0}, {"text": "new one, and it bold increase the number of unhealthy pods as a result.", "timestamp": "00:19:44,124", "timestamp_s": 1184.0}, {"text": "So in this case, again, a custom controller is really useful.", "timestamp": "00:19:47,516", "timestamp_s": 1187.0}, {"text": "What it did was it went after the unhealthy pods first while", "timestamp": "00:19:51,030", "timestamp_s": 1191.0}, {"text": "doing upgrades, instead of just being according to a strict ordering,", "timestamp": "00:19:54,688", "timestamp_s": 1194.0}, {"text": "delete the first unhealthy pod and replace it with", "timestamp": "00:19:58,502", "timestamp_s": 1198.0}, {"text": "the healthy pod as a result, and then go after the next unhealthy pod,", "timestamp": "00:20:01,728", "timestamp_s": 1201.0}, {"text": "replace it with these new one, and then finally go to the healthy", "timestamp": "00:20:05,094", "timestamp_s": 1205.0}, {"text": "pods which can now be replaced because there are no unhealthy pods left.", "timestamp": "00:20:08,378", "timestamp_s": 1208.0}, {"text": "So in this way, we were able to overcome any blockage", "timestamp": "00:20:11,892", "timestamp_s": 1211.0}, {"text": "due to pod disruption budget and move the rolling upgrade forward.", "timestamp": "00:20:15,502", "timestamp_s": 1215.0}, {"text": "Another interesting problem we had, which is kind of unique to", "timestamp": "00:20:20,230", "timestamp_s": 1220.0}, {"text": "stateful applications, I guess, especially things like Zookeeper", "timestamp": "00:20:24,790", "timestamp_s": 1224.0}, {"text": "and many other such services. You have a number of", "timestamp": "00:20:28,754", "timestamp_s": 1228.0}, {"text": "instances, but one of them is elected a leader", "timestamp": "00:20:32,348", "timestamp_s": 1232.0}, {"text": "and it\u0027s the leader of the group, and it has certain responsibilities as", "timestamp": "00:20:36,410", "timestamp_s": 1236.0}, {"text": "a result. And to create a leader, you have to go through an", "timestamp": "00:20:39,708", "timestamp_s": 1239.0}, {"text": "election process. So there is some activity and delay involved", "timestamp": "00:20:43,168", "timestamp_s": 1243.0}, {"text": "in doing some of these things. Unfortunately, Kubernetes at its level", "timestamp": "00:20:46,758", "timestamp_s": 1246.0}, {"text": "knows nothing about these leader business. So the", "timestamp": "00:20:50,720", "timestamp_s": 1250.0}, {"text": "controller would typically just go after the highest number of", "timestamp": "00:20:54,388", "timestamp_s": 1254.0}, {"text": "pod, and if that pod is disrupted and a new one", "timestamp": "00:20:57,812", "timestamp_s": 1257.0}, {"text": "is created, the leader might be reelected into one of the older pods.", "timestamp": "00:21:01,108", "timestamp_s": 1261.0}, {"text": "So the next upgrade would hit that leader again, and once again you would have", "timestamp": "00:21:05,322", "timestamp_s": 1265.0}, {"text": "election. And if you\u0027re really unlucky, the third pod also would be from", "timestamp": "00:21:08,808", "timestamp_s": 1268.0}, {"text": "the older set of pods. So you end up disrupting the", "timestamp": "00:21:12,328", "timestamp_s": 1272.0}, {"text": "leader these times in this case. But you can imagine in a real cluster,", "timestamp": "00:21:15,624", "timestamp_s": 1275.0}, {"text": "this repeated leader election bold be very disruptive", "timestamp": "00:21:20,090", "timestamp_s": 1280.0}, {"text": "to the cluster. So to avoid this,", "timestamp": "00:21:23,794", "timestamp_s": 1283.0}, {"text": "once again, the custom controller came to a rescue.", "timestamp": "00:21:27,292", "timestamp_s": 1287.0}, {"text": "We built sidecar containers. These are basically logic", "timestamp": "00:21:30,598", "timestamp_s": 1290.0}, {"text": "that runs inside each one of these pods, which checks", "timestamp": "00:21:34,678", "timestamp_s": 1294.0}, {"text": "to see if it\u0027s a leader, and it makes that information available through", "timestamp": "00:21:38,694", "timestamp_s": 1298.0}, {"text": "labels in the pod. And the custom controller is basically monitoring", "timestamp": "00:21:42,976", "timestamp_s": 1302.0}, {"text": "all these pods to see which one of them has this leader label on it.", "timestamp": "00:21:47,098", "timestamp_s": 1307.0}, {"text": "And it would then avoid that particular leader and update", "timestamp": "00:21:50,484", "timestamp_s": 1310.0}, {"text": "all the other pods first, then finally go and update the leader.", "timestamp": "00:21:53,882", "timestamp_s": 1313.0}, {"text": "So you end up disrupting the leader pod only once", "timestamp": "00:21:57,438", "timestamp_s": 1317.0}, {"text": "throughout this process, which was a nice capability that we could", "timestamp": "00:22:01,000", "timestamp_s": 1321.0}, {"text": "have thanks to this custom controller. So another area", "timestamp": "00:22:04,552", "timestamp_s": 1324.0}, {"text": "of problems that we experienced was around DNS.", "timestamp": "00:22:08,636", "timestamp_s": 1328.0}, {"text": "As you can imagine in kubernetes,", "timestamp": "00:22:12,890", "timestamp_s": 1332.0}, {"text": "it\u0027s a very dynamic world. As pods move from one hosts", "timestamp": "00:22:16,226", "timestamp_s": 1336.0}, {"text": "to another, even though they keep the same hosts names, the IP", "timestamp": "00:22:19,862", "timestamp_s": 1339.0}, {"text": "addresses keep changing. And I kind of went over that earlier.", "timestamp": "00:22:23,558", "timestamp_s": 1343.0}, {"text": "This creates a strange problem because", "timestamp": "00:22:27,310", "timestamp_s": 1347.0}, {"text": "traditional software like hbase, Hadoop file", "timestamp": "00:22:30,704", "timestamp_s": 1350.0}, {"text": "system, et cetera, they were largely developed in an environment where", "timestamp": "00:22:33,834", "timestamp_s": 1353.0}, {"text": "DNS did not change so much. So as a result,", "timestamp": "00:22:37,572", "timestamp_s": 1357.0}, {"text": "there was a lot of bugs in this code base where", "timestamp": "00:22:40,900", "timestamp_s": 1360.0}, {"text": "it would resolve DNS hostname to IP address and", "timestamp": "00:22:44,648", "timestamp_s": 1364.0}, {"text": "cache that information for literally forever in its", "timestamp": "00:22:48,168", "timestamp_s": 1368.0}, {"text": "code. So you can imagine if you had that kind of code,", "timestamp": "00:22:52,136", "timestamp_s": 1372.0}, {"text": "you would have invalid information in the software pretty quickly.", "timestamp": "00:22:56,170", "timestamp_s": 1376.0}, {"text": "And in particular, what we noticed is that if the metadata", "timestamp": "00:23:00,412", "timestamp_s": 1380.0}, {"text": "servers had these IP addresses changing", "timestamp": "00:23:04,498", "timestamp_s": 1384.0}, {"text": "and if a large number of data servers sort of had to", "timestamp": "00:23:08,730", "timestamp_s": 1388.0}, {"text": "talk to these metadata servers, they were kind of losing connection", "timestamp": "00:23:12,288", "timestamp_s": 1392.0}, {"text": "to this metadata server as its ip address changed.", "timestamp": "00:23:15,958", "timestamp_s": 1395.0}, {"text": "Now obviously the fix to this kind of problem is to go into", "timestamp": "00:23:19,254", "timestamp_s": 1399.0}, {"text": "the open source code, find where all these bugs are.", "timestamp": "00:23:22,852", "timestamp_s": 1402.0}, {"text": "These it is holding on to these addresses and fix", "timestamp": "00:23:25,892", "timestamp_s": 1405.0}, {"text": "those bugs. But with a large code base like", "timestamp": "00:23:29,956", "timestamp_s": 1409.0}, {"text": "Hadoop file system and hbase, it\u0027s kind", "timestamp": "00:23:33,748", "timestamp_s": 1413.0}, {"text": "of challenging to find all the places that this issue exists. And especially", "timestamp": "00:23:36,904", "timestamp_s": 1416.0}, {"text": "when we had to get our software out and very sort", "timestamp": "00:23:40,856", "timestamp_s": 1420.0}, {"text": "of depending upon our eyeballing capabilities to find", "timestamp": "00:23:45,128", "timestamp_s": 1425.0}, {"text": "all these issues or a testing test matrix to", "timestamp": "00:23:48,860", "timestamp_s": 1428.0}, {"text": "find all these issues seemed a little risky.", "timestamp": "00:23:52,508", "timestamp_s": 1432.0}, {"text": "So what we ended up doing was that even as we went about fixing", "timestamp": "00:23:55,986", "timestamp_s": 1435.0}, {"text": "these bugs, we came up with a solution where for", "timestamp": "00:23:59,638", "timestamp_s": 1439.0}, {"text": "each one of our pods we put a load balancer", "timestamp": "00:24:03,616", "timestamp_s": 1443.0}, {"text": "and it\u0027s called a service in Kubernetes. And it\u0027s actually not a physical", "timestamp": "00:24:07,126", "timestamp_s": 1447.0}, {"text": "load balancer, it\u0027s a virtual one which works using network", "timestamp": "00:24:11,062", "timestamp_s": 1451.0}, {"text": "magic, really there\u0027s no physical load balancer involved. So we", "timestamp": "00:24:14,346", "timestamp_s": 1454.0}, {"text": "created this virtual load balancer in front of each one of", "timestamp": "00:24:18,292", "timestamp_s": 1458.0}, {"text": "these metadata server instances. So now", "timestamp": "00:24:21,732", "timestamp_s": 1461.0}, {"text": "what that does is that when you create a load balancer, not only does it", "timestamp": "00:24:25,092", "timestamp_s": 1465.0}, {"text": "get a host name but also an IP address. And that IP address is", "timestamp": "00:24:28,248", "timestamp_s": 1468.0}, {"text": "very static in nature. It doesn\u0027t change as long as you don\u0027t delete the", "timestamp": "00:24:32,152", "timestamp_s": 1472.0}, {"text": "load balancer. So even though your pods may be changing the", "timestamp": "00:24:35,544", "timestamp_s": 1475.0}, {"text": "IP addresses, the load balancer does not. So when", "timestamp": "00:24:39,308", "timestamp_s": 1479.0}, {"text": "the client is trying to contact these pod, it would first go to the load", "timestamp": "00:24:43,212", "timestamp_s": 1483.0}, {"text": "balancer and then the load balancer would forward the request to the pod.", "timestamp": "00:24:46,338", "timestamp_s": 1486.0}, {"text": "So we sort of recreated that stateless applications methodology,", "timestamp": "00:24:50,118", "timestamp_s": 1490.0}, {"text": "at least for metadata servers, so that we", "timestamp": "00:24:54,822", "timestamp_s": 1494.0}, {"text": "can kind of protect ourselves from IP address related issues.", "timestamp": "00:24:58,608", "timestamp_s": 1498.0}, {"text": "And in the meantime we also went about finding", "timestamp": "00:25:02,308", "timestamp_s": 1502.0}, {"text": "all these bugs using various testing mechanisms and eliminating", "timestamp": "00:25:05,818", "timestamp_s": 1505.0}, {"text": "it. But it gave us some breathing time. Another interesting", "timestamp": "00:25:09,642", "timestamp_s": 1509.0}, {"text": "issue that you have with kubernetes is how DNS actually", "timestamp": "00:25:13,704", "timestamp_s": 1513.0}, {"text": "works inside it. There\u0027s actually a dedicated DNS server", "timestamp": "00:25:17,304", "timestamp_s": 1517.0}, {"text": "that is providing all this support for the changing IP", "timestamp": "00:25:21,422", "timestamp_s": 1521.0}, {"text": "addresses. It\u0027s called core DNS. It actually runs inside the", "timestamp": "00:25:25,022", "timestamp_s": 1525.0}, {"text": "Kubernetes cluster. And as you create pods of various", "timestamp": "00:25:28,684", "timestamp_s": 1528.0}, {"text": "type and delete it, this core DNS is the one", "timestamp": "00:25:32,242", "timestamp_s": 1532.0}, {"text": "which keeps track of when to create a DNS record and when to delete", "timestamp": "00:25:35,452", "timestamp_s": 1535.0}, {"text": "it. The problem with this approach is that while it all works", "timestamp": "00:25:39,058", "timestamp_s": 1539.0}, {"text": "great on the server side, there\u0027s no guarantee that your clients are", "timestamp": "00:25:42,816", "timestamp_s": 1542.0}, {"text": "actually running in the same Kubernetes cluster. Really in the real", "timestamp": "00:25:46,448", "timestamp_s": 1546.0}, {"text": "world your client is typically outside of a Kubernetes cluster.", "timestamp": "00:25:50,096", "timestamp_s": 1550.0}, {"text": "It\u0027s probably running in some external host or VM,", "timestamp": "00:25:53,722", "timestamp_s": 1553.0}, {"text": "but not necessarily inside your Kubernetes cluster. And that", "timestamp": "00:25:57,626", "timestamp_s": 1557.0}, {"text": "client is actually depending upon typically a different DNS server,", "timestamp": "00:26:00,772", "timestamp_s": 1560.0}, {"text": "which is the global DNS server that is visible across a", "timestamp": "00:26:04,398", "timestamp_s": 1564.0}, {"text": "large number of environments and not the core DNS, which is", "timestamp": "00:26:08,808", "timestamp_s": 1568.0}, {"text": "visible only inside the Kubernetes cluster. So to deal", "timestamp": "00:26:12,248", "timestamp_s": 1572.0}, {"text": "with this issue, what you have to do is find a way of getting your", "timestamp": "00:26:15,992", "timestamp_s": 1575.0}, {"text": "records from the core DNS into the global DNS.", "timestamp": "00:26:19,788", "timestamp_s": 1579.0}, {"text": "Otherwise your client would not know how to contact all", "timestamp": "00:26:23,010", "timestamp_s": 1583.0}, {"text": "your services. So in our case we use an open source", "timestamp": "00:26:26,332", "timestamp_s": 1586.0}, {"text": "tool called external DNS. It\u0027s something that is open source", "timestamp": "00:26:29,942", "timestamp_s": 1589.0}, {"text": "and most people use it when they\u0027re trying to deal with", "timestamp": "00:26:33,734", "timestamp_s": 1593.0}, {"text": "this particular scenario. And what external DNS does is", "timestamp": "00:26:37,504", "timestamp_s": 1597.0}, {"text": "that it transfers these DNS records that are within the Kubernetes cluster", "timestamp": "00:26:40,848", "timestamp_s": 1600.0}, {"text": "into this global DNS server. I\u0027ve simplified the picture here by showing", "timestamp": "00:26:45,306", "timestamp_s": 1605.0}, {"text": "that it\u0027s actually moving data from core DNS to global", "timestamp": "00:26:49,418", "timestamp_s": 1609.0}, {"text": "DNS. In reality that\u0027s not exactly how it does it,", "timestamp": "00:26:52,778", "timestamp_s": 1612.0}, {"text": "but in effect it has the same impact.", "timestamp": "00:26:56,072", "timestamp_s": 1616.0}, {"text": "It makes sure that those DNS records are available in global", "timestamp": "00:26:59,870", "timestamp_s": 1619.0}, {"text": "DNS. Once they are available in global DNS, the client is able", "timestamp": "00:27:03,822", "timestamp_s": 1623.0}, {"text": "to then contact your data servers", "timestamp": "00:27:07,468", "timestamp_s": 1627.0}, {"text": "and communicate with them effectively now one challenge with", "timestamp": "00:27:10,866", "timestamp_s": 1630.0}, {"text": "this approach is that external DNS only runs periodically,", "timestamp": "00:27:14,268", "timestamp_s": 1634.0}, {"text": "every minute or so. So your DNS", "timestamp": "00:27:17,330", "timestamp_s": 1637.0}, {"text": "records are not immediately available in global DNS.", "timestamp": "00:27:20,982", "timestamp_s": 1640.0}, {"text": "So for example, if data server four here is just booting", "timestamp": "00:27:24,166", "timestamp_s": 1644.0}, {"text": "up, it should not go online until it\u0027s absolutely", "timestamp": "00:27:27,798", "timestamp_s": 1647.0}, {"text": "certain that its DNS records are available in global DNS.", "timestamp": "00:27:32,032", "timestamp_s": 1652.0}, {"text": "So we have to actually build logic to make sure that it can validate", "timestamp": "00:27:35,754", "timestamp_s": 1655.0}, {"text": "that global DNS has actually got its DNS records.", "timestamp": "00:27:39,978", "timestamp_s": 1659.0}, {"text": "Once it\u0027s confirmed, only then would the data server declare", "timestamp": "00:27:43,530", "timestamp_s": 1663.0}, {"text": "itself as available for traffic. So this is one of those steps", "timestamp": "00:27:46,794", "timestamp_s": 1666.0}, {"text": "you kind of might have to deal with in the real world when you\u0027re trying", "timestamp": "00:27:50,318", "timestamp_s": 1670.0}, {"text": "to use Kubernetes and stateful applications in", "timestamp": "00:27:53,288", "timestamp_s": 1673.0}, {"text": "general. Now finally, I want to talk", "timestamp": "00:27:56,988", "timestamp_s": 1676.0}, {"text": "a little bit about scalability architecture in", "timestamp": "00:28:00,172", "timestamp_s": 1680.0}, {"text": "public cloud. Typically you deploy software in a", "timestamp": "00:28:03,612", "timestamp_s": 1683.0}, {"text": "certain region. You can deploy it across multiple", "timestamp": "00:28:07,052", "timestamp_s": 1687.0}, {"text": "regions, but if you are doing a high performance software that needs", "timestamp": "00:28:10,642", "timestamp_s": 1690.0}, {"text": "very low latency, you deploy that software", "timestamp": "00:28:14,080", "timestamp_s": 1694.0}, {"text": "in a particular region, which is really a geographical region like", "timestamp": "00:28:17,846", "timestamp_s": 1697.0}, {"text": "us east or US west.", "timestamp": "00:28:21,652", "timestamp_s": 1701.0}, {"text": "And within that region you can also spread your software", "timestamp": "00:28:24,530", "timestamp_s": 1704.0}, {"text": "across different availability zones. Availability zones can", "timestamp": "00:28:28,490", "timestamp_s": 1708.0}, {"text": "mean different things for different cloud providers, but typically", "timestamp": "00:28:32,292", "timestamp_s": 1712.0}, {"text": "it is either a separate building, a separate", "timestamp": "00:28:36,174", "timestamp_s": 1716.0}, {"text": "campus even, but very close to each other, so that", "timestamp": "00:28:40,062", "timestamp_s": 1720.0}, {"text": "the network latency between the different availability zones is", "timestamp": "00:28:43,368", "timestamp_s": 1723.0}, {"text": "not too high. So you can actually spread your software across it without", "timestamp": "00:28:46,428", "timestamp_s": 1726.0}, {"text": "experiencing too much of a performance issue.", "timestamp": "00:28:50,396", "timestamp_s": 1730.0}, {"text": "So I\u0027ll be calling availability zones AZ for", "timestamp": "00:28:53,610", "timestamp_s": 1733.0}, {"text": "short here. So the goal is typically for you", "timestamp": "00:28:57,452", "timestamp_s": 1737.0}, {"text": "to take a few AZ. In our case", "timestamp": "00:29:00,592", "timestamp_s": 1740.0}, {"text": "we took three AZ approach and make sure that your software", "timestamp": "00:29:03,872", "timestamp_s": 1743.0}, {"text": "is spread across the instances of your software are spread across each", "timestamp": "00:29:07,702", "timestamp_s": 1747.0}, {"text": "one of these AZ to achieve this.", "timestamp": "00:29:11,408", "timestamp_s": 1751.0}, {"text": "Fortunately in kubernetes there has been significant", "timestamp": "00:29:14,356", "timestamp_s": 1754.0}, {"text": "effort to make sure that you can support this kind of deployment.", "timestamp": "00:29:17,562", "timestamp_s": 1757.0}, {"text": "So they have got something called affinity and anti affinity rules", "timestamp": "00:29:21,242", "timestamp_s": 1761.0}, {"text": "where you can tell the Kubernetes scheduler", "timestamp": "00:29:25,422", "timestamp_s": 1765.0}, {"text": "to spread the pods across different AZ.", "timestamp": "00:29:29,278", "timestamp_s": 1769.0}, {"text": "And the way they do it is that the hosts that run in each AZ", "timestamp": "00:29:32,654", "timestamp_s": 1772.0}, {"text": "have a certain label indicating what AZ that hosts is", "timestamp": "00:29:36,510", "timestamp_s": 1776.0}, {"text": "running in. And then you can tell Kubernetes that,", "timestamp": "00:29:39,948", "timestamp_s": 1779.0}, {"text": "make sure that when you deploy these pods, they run on", "timestamp": "00:29:43,100", "timestamp_s": 1783.0}, {"text": "hosts that have different label values as much as possible.", "timestamp": "00:29:46,876", "timestamp_s": 1786.0}, {"text": "Obviously you will have more than three pods, so you\u0027re not", "timestamp": "00:29:51,630", "timestamp_s": 1791.0}, {"text": "going to be able to spread these all on different AZ, but you do your", "timestamp": "00:29:55,216", "timestamp_s": 1795.0}, {"text": "best effort to equally balance it across different AZ.", "timestamp": "00:29:58,752", "timestamp_s": 1798.0}, {"text": "Now that takes care of making sure that your software", "timestamp": "00:30:02,690", "timestamp_s": 1802.0}, {"text": "is running on different azs, but what about the data inside that", "timestamp": "00:30:06,362", "timestamp_s": 1806.0}, {"text": "software. A good example of it is Hadoop file system,", "timestamp": "00:30:10,372", "timestamp_s": 1810.0}, {"text": "which keeps three copies of data for high availability reasons.", "timestamp": "00:30:14,100", "timestamp_s": 1814.0}, {"text": "Now you want to make sure that that copy each one of those", "timestamp": "00:30:18,270", "timestamp_s": 1818.0}, {"text": "copies is running in different AZ for safety reasons.", "timestamp": "00:30:21,672", "timestamp_s": 1821.0}, {"text": "So fortunately in Hadoop itself, when they designed", "timestamp": "00:30:25,870", "timestamp_s": 1825.0}, {"text": "it, they introduced this concept called rack topology, which is sort", "timestamp": "00:30:29,378", "timestamp_s": 1829.0}, {"text": "of the traditional data center terminology where you tell", "timestamp": "00:30:33,468", "timestamp_s": 1833.0}, {"text": "Hadoop fails system. In particular it\u0027s metadata server.", "timestamp": "00:30:37,356", "timestamp_s": 1837.0}, {"text": "What is the topology of your servers?", "timestamp": "00:30:42,430", "timestamp_s": 1842.0}, {"text": "In which racks do they run in? And Hadoop will make sure that", "timestamp": "00:30:45,814", "timestamp_s": 1845.0}, {"text": "these replicas are kind of distributed on", "timestamp": "00:30:49,440", "timestamp_s": 1849.0}, {"text": "different racks, so that if one whole rack goes down,", "timestamp": "00:30:53,156", "timestamp_s": 1853.0}, {"text": "you still got other racks that can serve the data.", "timestamp": "00:30:56,692", "timestamp_s": 1856.0}, {"text": "We were then able to convince Hadoop through using its", "timestamp": "00:31:00,084", "timestamp_s": 1860.0}, {"text": "script based interface that each AZ is a different rack.", "timestamp": "00:31:04,184", "timestamp_s": 1864.0}, {"text": "And thereby Hadoop was able to spread these replicas", "timestamp": "00:31:08,366", "timestamp_s": 1868.0}, {"text": "across different azs using that mapping. The metadata", "timestamp": "00:31:12,446", "timestamp_s": 1872.0}, {"text": "server itself had multiple instances which are", "timestamp": "00:31:16,658", "timestamp_s": 1876.0}, {"text": "again spread across different AZ", "timestamp": "00:31:20,108", "timestamp_s": 1880.0}, {"text": "using the same affinity anti affinity rules that Kubernetes", "timestamp": "00:31:23,778", "timestamp_s": 1883.0}, {"text": "supports. So what you achieve with all these spread is", "timestamp": "00:31:27,586", "timestamp_s": 1887.0}, {"text": "that if an entire AZ goes away due to a power", "timestamp": "00:31:31,212", "timestamp_s": 1891.0}, {"text": "outage or a network outage, you still have the software and", "timestamp": "00:31:34,956", "timestamp_s": 1894.0}, {"text": "its data available in the other AZ and still", "timestamp": "00:31:38,652", "timestamp_s": 1898.0}, {"text": "serving traffic in spite of this outage. So it\u0027s really useful", "timestamp": "00:31:42,724", "timestamp_s": 1902.0}, {"text": "for resilience in general. So that\u0027s", "timestamp": "00:31:46,602", "timestamp_s": 1906.0}, {"text": "pretty much all I had for today\u0027s talk. Thank you", "timestamp": "00:31:49,802", "timestamp_s": 1909.0}, {"text": "so much for listening.", "timestamp": "00:31:53,732", "timestamp_s": 1913.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'T3_UMV6_tKw',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Containing an Elephant: How we took HBase and HDFS from private data centers into Kubernetes and Public Cloud
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Operating a distributed system like HBase/Hadoop FS at peta byte scale took years to master in our private data centers. This talk describes our dramatic shift towards running a mission critical stateful application on Kubernetes in Public Cloud, why we did it and the challenges we had to overcome.</p>
<p>Salesforce runs a very large footprint of HBase and HDFS clusters in our data centers with multiple petabytes of data, billions of queries per day over thousands of machines. After more than a decade of running our own data centers, we pivoted towards public cloud for its scalability and availability. As part of this foray, we made a bold decision to move our HBase clusters from staid bare metal hosts to the dynamic and immutable world of containers and Kubernetes. This move brought with it a number of challenges which are likely to find echoes in other such mature stateful applications adapting to public cloud and Kubernetes. The challenges include
1. Limitations in Kubernetes while deploying large scale stateful applications
2. Failures in HBase/HDFS as the DNS records keep changing in Kubernetes
3. Resilience of HBase/HDFS even when a whole availability zone fails in Public Cloud
4. Introducing encryption in communication over untrusted public cloud networks without application being aware</p>
<p>This talk will go over how we overcame these challenges and the benefits we are beginning to see from these efforts.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Salesforce has been running HBS in its own private data centers. When it moved some of its clusters into public cloud, it used kubernetes to manage them. Using Kubernetes and its elastic management of pods is one of the most important reasons.

              </li>
              
              <li>
                The difference between stateful and stateless applications. Typical stateless application is basically HTTP servers that run in a website. What Kubernetes provides is the ability to create these pods. Here are some of the challenges we faced once we started using it.

              </li>
              
              <li>
                Using stateful set we were able to deploy Hadoop hbase. We found many of its features around managing compute storage failover et centers very useful. But there were challenges with its rolling upgrade process where you're trying to upgrade the software. Fortunately, Kubernetes is also very extensible.

              </li>
              
              <li>
                Another area of problems that we experienced was around DNS. As pods move from one hosts to another, the IP addresses keep changing. We came up with a solution where for each one of our pods we put a load balancer. But there's no guarantee that your clients are actually running in the same Kubernetes cluster.

              </li>
              
              <li>
                Typically you deploy software in a certain region. You can also spread your software across different availability zones. In kubernetes there has been significant effort to make sure that you can support this kind of deployment. It's really useful for resilience in general.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/T3_UMV6_tKw.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:40,930'); seek(40.0)">
              Hi, my name is Dhiraj Hegde and I'm from Salesforce. Today I'll be
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:44,748'); seek(44.0)">
              talking about how we manage hbase in public cloud.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:48,156'); seek(48.0)">
              Hbase is a distribute at key value store that is
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:51,428'); seek(51.0)">
              horizontally scalable and it runs on top of Hadoop file system. For many
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:55,332'); seek(55.0)">
              years, Salesforce has been running HBS in its own private data centers.
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:59,498'); seek(59.0)">
              It's been running a very large footprint of clusters with thousands of machines,
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:01:03,742'); seek(63.0)">
              petabytes of data storage and billions
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:01:07,006'); seek(67.0)">
              of queries per day. But it had been using very traditional mechanisms
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:01:10,798'); seek(70.0)">
              of managing these clusters on bare metal hosts
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:01:14,542'); seek(74.0)">
              using open source tools like puppet and ambari.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:01:17,630'); seek(77.0)">
              When we started moving some of our clusters into public cloud,
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:20,684'); seek(80.0)">
              we decided to take a very different approach, using kubernetes to manage these clusters.
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:25,298'); seek(85.0)">
              We'll explain in this talk why we chose kubernetes, what challenges
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:28,946'); seek(88.0)">
              we ran into with our choice, and how we overcame those challenges.
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:33,078'); seek(93.0)">
              So why did we pick kubernetes? There are a number of reasons.
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:36,790'); seek(96.0)">
              One of the main problems that we ran into in an old deployment
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:40,582'); seek(100.0)">
              mechanism was that it was in place in a hosts,
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:43,414'); seek(103.0)">
              meaning you would go into a host that already had software
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:46,890'); seek(106.0)">
              installed and try to modify it. The problem with that is that when
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:50,148'); seek(110.0)">
              you are trying to deploying on thousands of hosts, some of those hosts,
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:53,418'); seek(113.0)">
              the installation could fail, leaving it in an uncertain
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:56,558'); seek(116.0)">
              stateful, with some binaries present or some configs present and
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:02:00,008'); seek(120.0)">
              others missing. And if you happen to miss these failures, those hosts
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:02:03,438'); seek(123.0)">
              would remain in this inconsistent stateful for a very long time. The other thing
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:02:07,468'); seek(127.0)">
              we have noticed is there's a temptation when you're dealing
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:02:10,898'); seek(130.0)">
              with emergency issues, maybe a site issue that people
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:02:14,556'); seek(134.0)">
              just go and log into. Hosts go in and modify these configurations locally
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:19,026'); seek(139.0)">
              and completely forget that they did that. And the problem there again
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:22,992'); seek(142.0)">
              is that once the config is forgotten, it looks a little different on that one
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:26,672'); seek(146.0)">
              host compared to many of the other hosts. With containers. A lot of
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:30,608'); seek(150.0)">
              these problems go away because they have this thing called immutability,
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:34,058'); seek(154.0)">
              meaning with a container, it is actually created from
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:37,812'); seek(157.0)">
              an image, and that image contains all the configuration,
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:41,578'); seek(161.0)">
              all the binaries already present. And as that
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:45,028'); seek(165.0)">
              container comes up, it is an exact replica of what that image has.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:48,824'); seek(168.0)">
              And if you even attempt to make a change after it's running, what happens is
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:52,728'); seek(172.0)">
              that the next time the container is restarted could be a crash or a deliberate
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:56,862'); seek(176.0)">
              restart. It would once again start from the image. So whatever
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:03:00,380'); seek(180.0)">
              you made, changes locally would be totally lost. So in that
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:03:03,852'); seek(183.0)">
              kind of immutable environment, it is pretty easy to make sure
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:03:07,740'); seek(187.0)">
              that over a period of time, the images are all consistent,
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:03:11,590'); seek(191.0)">
              no matter what people try to do to it or what failures
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:15,046'); seek(195.0)">
              happen with it. The second reason why we felt Kubernetes
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:18,566'); seek(198.0)">
              and containerization was a good thing was availability and scalability.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:22,758'); seek(202.0)">
              With Kubernetes, it's really good about making sure
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:26,276'); seek(206.0)">
              that if you are running your containers on a particular host or
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:30,132'); seek(210.0)">
              set of hosts, and if something happens to those hosts, kubernetes can monitor
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:34,446'); seek(214.0)">
              and cache those issues and immediately restart those containers
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:37,982'); seek(217.0)">
              on a different hosts. By the way, just to be clear here
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:41,990'); seek(221.0)">
              in Kubernetes, containers are managed by
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:45,048'); seek(225.0)">
              a construct called pods. But in this talk we are going to treat
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:48,696'); seek(228.0)">
              the term containers and pods as one and these same to
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:52,028'); seek(232.0)">
              keep it simple. They are not exactly the same, but we are not going to
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:54,828'); seek(234.0)">
              get into it. And for the purposes of this talk, it doesn't
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:58,338'); seek(238.0)">
              really matter as much. So coming back to the question of availability,
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:04:02,230'); seek(242.0)">
              it makes sure that when pods fail
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:04:05,942'); seek(245.0)">
              on a particular host, it will make sure that
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:04:09,488'); seek(249.0)">
              if it can find another healthy host out there, it'll move these pods to those
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:12,772'); seek(252.0)">
              other hosts. Similarly, when you need to scale
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:16,282'); seek(256.0)">
              up your application, it's very easy in
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:19,428'); seek(259.0)">
              Kubernetes to specify a higher number of pods to meet
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:22,856'); seek(262.0)">
              to the requirements of that particular traffic. And once the traffic
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:27,270'); seek(267.0)">
              spike is gone, you can also equally
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:31,342'); seek(271.0)">
              quickly reduce the number of pods that are run to
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:34,952'); seek(274.0)">
              serve the traffic. So this ability to scale up and down
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:38,392'); seek(278.0)">
              is very valuable to us, especially when you are moving to the cloud.
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:41,500'); seek(281.0)">
              And one of the big claims of cloud is that you got this elasticity.
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:45,794'); seek(285.0)">
              So how can you take advantage of this elasticity?
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:48,970'); seek(288.0)">
              Using Kubernetes and its elastic management of
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:52,592'); seek(292.0)">
              pods is one great way of achieving it. The third
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:55,696'); seek(295.0)">
              reason is actually one of the most important reasons as to why we went with
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:59,248'); seek(299.0)">
              Kubernetes. We had this desire or goal to make
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:05:03,092'); seek(303.0)">
              sure that whatever we built, and we started by these way with AWS,
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:05:07,338'); seek(307.0)">
              which is Amazon's cloud offering. But our plan was to try and
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:11,316'); seek(311.0)">
              build something there that would apply to other clouds,
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:14,542'); seek(314.0)">
              because it is very likely that we would be running a software in other
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:18,536'); seek(318.0)">
              clouds. Example of other clouds are things like Azure from Microsoft
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:22,478'); seek(322.0)">
              or GCP from Google.
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:25,670'); seek(325.0)">
              So we wanted to be able to run our software in
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:29,452'); seek(329.0)">
              all those different environments. But the problem there is when
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:33,132'); seek(333.0)">
              you're building your software deployment processes for one cloud, you are so intimately
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:37,346'); seek(337.0)">
              tied to the APIs and the way they manage compute, storage network
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:41,510'); seek(341.0)">
              that whatever you build there doesn't naturally apply to
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:45,040'); seek(345.0)">
              these other clouds. But fortunately with Kubernetes, things turn
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:48,656'); seek(348.0)">
              upside down. Kubernetes is actually very opinionated. It actually
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:52,756'); seek(352.0)">
              specifies exactly how compute should be managed,
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:55,674'); seek(355.0)">
              exactly how storage should be managed, and how network should be
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:58,948'); seek(358.0)">
              managed. At that point, it becomes incumbent upon
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:06:02,794'); seek(362.0)">
              the other cloud providers to make sure that they manage storage,
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:06:07,342'); seek(367.0)">
              network and compute in the way Kubernetes
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:10,670'); seek(370.0)">
              expects it to be managed. So when you build your software
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:14,526'); seek(374.0)">
              deployment processes around kubernetes on any one of those cloud
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:18,076'); seek(378.0)">
              providers, you automatically get the same deployment
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:22,082'); seek(382.0)">
              processes working in all these other different clouds, because it sort of
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:26,012'); seek(386.0)">
              enforces that sort of behavior. So for those three reasons,
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:29,458'); seek(389.0)">
              we found Kubernetes to be very interesting to us. Okay,
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:32,800'); seek(392.0)">
              now we'll get into a little bit about some of the challenges that we had
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:36,096'); seek(396.0)">
              with Kubernetes once we started using it. To understand it a little
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:39,648'); seek(399.0)">
              better, you would need to know the difference between stateful and stateless applications.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:43,962'); seek(403.0)">
              Here we show you a typical stateless application,
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:46,900'); seek(406.0)">
              basically HTTP servers that run in a website. Each one of these
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:50,452'); seek(410.0)">
              server instances has nothing that is unique in them. They kind
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:54,068'); seek(414.0)">
              of serve the same content. If you ever try to make a request to them
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:57,736'); seek(417.0)">
              that changes the content, it usually goes to some back end database
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:07:01,422'); seek(421.0)">
              that is shared across all those instances, or maybe even a
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:04,632'); seek(424.0)">
              shared file system across all these instances. So essentially each
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:08,572'); seek(428.0)">
              one of those running servers in Kubernetes
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:12,226'); seek(432.0)">
              world, these would be pods. They're all stateless.
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:15,298'); seek(435.0)">
              And when a client is trying to access the service
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:18,940'); seek(438.0)">
              from them, they typically go to a load balancer, as you can see here,
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:22,336'); seek(442.0)">
              and the load balancer then forwards those requests,
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:25,526'); seek(445.0)">
              each or any one of those HTTP servers in the back end. Now the interesting
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:29,760'); seek(449.0)">
              thing to note here is that the client only needs to know the hostname and
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:33,492'); seek(453.0)">
              IP address of the load balancer. It doesn't even need to know the
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:36,948'); seek(456.0)">
              host names of the HTTP servers running behind it,
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:40,196'); seek(460.0)">
              because they are just getting traffic forwarded to them by the cloud balancer.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:44,062'); seek(464.0)">
              Kubernetes usually manages these kinds of environments by
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:47,496'); seek(467.0)">
              specifying a manifest, which is really a document
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:51,198'); seek(471.0)">
              describing how many instances of these HTTP servers to
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:55,048'); seek(475.0)">
              run as pods. It can also describe the configuration of
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:58,808'); seek(478.0)">
              the cloud balancer, which is called a service in Kubernetes, but again it
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:02,668'); seek(482.0)">
              is specified using a document called a manifest. And with that you can
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:06,348'); seek(486.0)">
              nicely set up all of this. And in the early years, this is what Kubernetes
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:10,406'); seek(490.0)">
              was really known for, the ability to very quickly spin up
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:14,272'); seek(494.0)">
              a set of stateless applications and provide some
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:17,888'); seek(497.0)">
              sort of service. But when you look at something like Hadoop
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:22,422'); seek(502.0)">
              hbase, which is a very stateful application, it's a
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:26,004'); seek(506.0)">
              database. Typically the way it behaves is that you got a client
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:29,626'); seek(509.0)">
              there. And when it needs to access data for reasons
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:33,434'); seek(513.0)">
              of reading, for querying, that is for modification,
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:36,686'); seek(516.0)">
              for deletion, so on and so forth, it needs access
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:40,056'); seek(520.0)">
              to the data servers which you can see at the bottom,
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:43,430'); seek(523.0)">
              lined up at the bottom, a number of these data servers.
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:46,862'); seek(526.0)">
              And each one of those data servers has very different data in
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:50,188'); seek(530.0)">
              them. It typically does not have the same data across all of them
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:53,852'); seek(533.0)">
              because then every data server would have to have huge amount of storage.
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:57,554'); seek(537.0)">
              Instead you break it up into pieces and you spread it across a large
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:00,876'); seek(540.0)">
              number of data servers. And that's how you scale as you need to store
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:04,336'); seek(544.0)">
              more data. You just add more data servers. And on the left hand side you
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:07,888'); seek(547.0)">
              typically have something called a metadata server which is responsible
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:11,766'); seek(551.0)">
              for knowing where all this data is present. It basically knows the
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:15,620'); seek(555.0)">
              geography of things. So a client, when it is
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:19,012'); seek(559.0)">
              trying to access data, it goes to the metadata server
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:22,650'); seek(562.0)">
              with a key saying that I want to access this. The metadata
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:26,638'); seek(566.0)">
              server then gives a location of where these can find the data.
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:30,168'); seek(570.0)">
              And then the client directly goes to the data server based
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:33,832'); seek(573.0)">
              on these information and accesses the data. Now in all of this, what you will
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:37,688'); seek(577.0)">
              notice is that the client actually needs to know the identity,
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:41,250'); seek(581.0)">
              the hostname of all the elements. I mean it needs to
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:44,604'); seek(584.0)">
              know the host names of the metadata server that can provide this
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:48,672'); seek(588.0)">
              information. And once a metadata server provides the hostname information
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:52,640'); seek(592.0)">
              of the data server, it needs to directly deal with those
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:55,968'); seek(595.0)">
              particular data servers. There's no magical load balancer
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:00,134'); seek(600.0)">
              hiding things from you, which is why the DNS that you see on
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:03,744'); seek(603.0)">
              the right hand top corner is very important because it has to have all these
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:07,220'); seek(607.0)">
              information of the host names and IP addresses.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:10,290'); seek(610.0)">
              And by the way, all the clients usually have a cache
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:13,870'); seek(613.0)">
              because once you discover this information, having to rediscover
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:17,662'); seek(617.0)">
              this information every time for a request is very inefficient.
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:21,118'); seek(621.0)">
              So you hbase to kind of cache this information and
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:25,592'); seek(625.0)">
              hopefully the stuff that you cache doesn't change too often because that is
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:29,292'); seek(629.0)">
              a little bit of a disturbance that the client had to deal with. So you
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:31,948'); seek(631.0)">
              try to minimize this disturbance. So all of this makes it
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:35,628'); seek(635.0)">
              very important that the identity of
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:39,232'); seek(639.0)">
              the servers that you're accessing tend to be
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:42,928'); seek(642.0)">
              stable. They do not change very much. And given a host
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:46,742'); seek(646.0)">
              name, the location of what it contains, it becomes very
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:50,192'); seek(650.0)">
              important. Took so that association between the name of the
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:53,748'); seek(653.0)">
              server and its content, the state it contains is very,
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:57,412'); seek(657.0)">
              very important to the client for good performance. And this
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:01,492'); seek(661.0)">
              is a very good example of what a stateful application is, and you can
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:05,128'); seek(665.0)">
              see how it's a little different from stateless applications. So the
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:08,328'); seek(668.0)">
              Kubernetes community, when it decided to support use
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:11,576'); seek(671.0)">
              cases like Hadoop, hbase or Cassandra,
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:15,086'); seek(675.0)">
              they introduced a feature called stateful.
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:18,542'); seek(678.0)">
              What stateful provides is these same ability to create
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:22,076'); seek(682.0)">
              pods, but in this case, when it creates pods, it gives it
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:25,980'); seek(685.0)">
              unique names. And the names are kind of easy to guess.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:29,840'); seek(689.0)">
              They usually begin with the same prefix. In this case
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:32,992'); seek(692.0)">
              it was pod. As an example I took. It can
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:36,288'); seek(696.0)">
              even be HTTP or hadoop or whatever,
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:39,712'); seek(699.0)">
              but then it would associate with each instance of the pod
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:43,338'); seek(703.0)">
              a unique number, like zero, one, two,
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:46,772'); seek(706.0)">
              depending upon how many of these you want. So each pod would
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:50,148'); seek(710.0)">
              have a unique name. And also you can see in the right hand top corner
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:53,866'); seek(713.0)">
              that the DNS is also modified to give these pods a
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:57,448'); seek(717.0)">
              proper hostname and IP address. So you got everything with
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:01,048'); seek(721.0)">
              it. You got unique names, hostnames and IP address associated
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:04,558'); seek(724.0)">
              with each one of these pods. In addition, since this is a stateful application,
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:08,970'); seek(728.0)">
              you could also define how much storage you want to associate
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:13,602'); seek(733.0)">
              with each one of these pods, the size of it,
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:16,716'); seek(736.0)">
              also the class, whether you want SSD or HDD. All of this can be specified
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:21,550'); seek(741.0)">
              using a construct called a persistent volume claim.
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:25,526'); seek(745.0)">
              It's basically a claim for storage. It's not the actual storage you are requesting
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:29,318'); seek(749.0)">
              storage. And this is embedded in each one of these pod definitions
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:33,754'); seek(753.0)">
              where a pv claim is specified. When this is defined,
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:37,610'); seek(757.0)">
              what happens is that the providers or cloud providers who run
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:41,492'); seek(761.0)">
              Kubernetes like AWS, Azure or GCP,
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:45,194'); seek(765.0)">
              they will notice this claim and these immediately carve out a disk in the cloud
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:49,624'); seek(769.0)">
              which has these size and the class of storage that is being requested,
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:53,230'); seek(773.0)">
              and then it is made available to Kubernetes. Kubernetes then mounts that
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:57,640'); seek(777.0)">
              disk in each one of these pods so that it becomes a locally available storage
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:01,842'); seek(781.0)">
              in each one of those pods. And at that point
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:05,292'); seek(785.0)">
              you've got a unique name which is well defined in DNS
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:08,754'); seek(788.0)">
              associated with storage. And this is a one to one mapping
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:12,518'); seek(792.0)">
              between the two. Now what's interesting to note here is that
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:16,528'); seek(796.0)">
              let's take an example of any one of these pods which is
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:20,608'); seek(800.0)">
              running on host a right now. It has a claim and it is accessing pv
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:24,202'); seek(804.0)">
              zero which is mounted as a disk in it. Let's say for some reason host
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:28,266'); seek(808.0)">
              a has some problem, it goes up in smoke.
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:31,146'); seek(811.0)">
              Kubernetes would notice that. And it
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:34,452'); seek(814.0)">
              will then say that okay, this pod is gone, it'll remove it from
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:38,072'); seek(818.0)">
              its system and you'll notice in the right hand top corner that the DNS also
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:41,672'); seek(821.0)">
              is modified to remove any DNS entries related to it.
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:45,640'); seek(825.0)">
              You can still see the storage is present because claim that created
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:49,618'); seek(829.0)">
              the storage is still present. These pod is gone but the claim is still there.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:53,340'); seek(833.0)">
              Eventually what Kubernetes will do is it will find another free
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:56,636'); seek(836.0)">
              host like host d in this example and recreate
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:00,598'); seek(840.0)">
              the same pod so it has the same hostname. Pod zero,
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:04,336'); seek(844.0)">
              the one that got destroyed, is recreated with the same hostname.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:08,670'); seek(848.0)">
              And because it has the same claim embedded inside of it,
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:12,644'); seek(852.0)">
              these same storage is again associated with it. And even
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:16,420'); seek(856.0)">
              DNS is updated to have the DNS record.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:20,084'); seek(860.0)">
              One thing that is different here is that when the DNS record is
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:23,732'); seek(863.0)">
              recreated, it did not get the same IP address.
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:26,900'); seek(866.0)">
              So it had the same hostname, but the IP address had to change then.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:30,472'); seek(870.0)">
              That's just the nature of networking. When you move from one compute
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:34,830'); seek(874.0)">
              unit to another, the IP addresses that
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:38,172'); seek(878.0)">
              you associate with that compute unit has to change.
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:41,724'); seek(881.0)">
              It's just how network is managed in kubernetes, but otherwise
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:45,634'); seek(885.0)">
              you basically achieve something quite interesting, which is a
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:49,292'); seek(889.0)">
              given host name is always associated with the same volume
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:52,806'); seek(892.0)">
              no matter where your compute goes, moves around inside the Kubernetes
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:57,286'); seek(897.0)">
              clusters a very interesting and useful property.
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:00,064'); seek(900.0)">
              That stickiness between hostname and the
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:03,504'); seek(903.0)">
              volume that you use. You also notice that the IP addresses
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:07,706'); seek(907.0)">
              can change even if the hostname doesn't change. And this is kind of important because
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:11,332'); seek(911.0)">
              we'll get into some of the issues we have because of this a few minutes
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:14,456'); seek(914.0)">
              from now. So using stateful set we were able
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:18,408'); seek(918.0)">
              to deploy Hadoop hbase and going back to
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:22,248'); seek(922.0)">
              the same slide that I showed you a while back, you can see that each
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:25,752'); seek(925.0)">
              one of the data servers is being deployed as a stateful set.
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:29,436'); seek(929.0)">
              And every one of them has a unique name, like DS 123-4123
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:34,812'); seek(934.0)">
              rather. And similarly the metadata server, which is also
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:38,252'); seek(938.0)">
              stateful, has a unique name and disks associated
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:42,198'); seek(942.0)">
              with it. So we were able to model and deploy a software
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:45,766'); seek(945.0)">
              using stateful set pretty well. Stateful sets are managed
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:49,078'); seek(949.0)">
              by a controller called as the stateful set controller or the STS
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:52,906'); seek(952.0)">
              controller in Kubernetes. And while we found
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:56,420'); seek(956.0)">
              many of its features around managing compute storage
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:16:00,234'); seek(960.0)">
              failover et centers very useful, we also
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:03,860'); seek(963.0)">
              had some challenges with it. One area where there
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:07,048'); seek(967.0)">
              was a problem was with its rolling upgrade process where you're trying to upgrade
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:10,958'); seek(970.0)">
              the software. And the way it does upgrade is it starts with the
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:14,648'); seek(974.0)">
              pod with the highest number and goes one by one,
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:18,060'); seek(978.0)">
              upgrading each one of them in
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:21,500'); seek(981.0)">
              strict order all the way down to zero. And while this is a very
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:25,292'); seek(985.0)">
              nice and careful way of upgrading software.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:28,910'); seek(988.0)">
              It is also very slow. You can imagine
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:33,014'); seek(993.0)">
              in the world of Hadoop hbase,
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:36,070'); seek(996.0)">
              you got hundreds of these pods, and each one of
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:39,504'); seek(999.0)">
              them is a heavy server that takes around five minutes to boot up and
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:43,028'); seek(1003.0)">
              initialize and set up its security credentials,
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:45,866'); seek(1005.0)">
              cordless kerberos, key tabs, et cetera. So going
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:49,316'); seek(1009.0)">
              through it one by one would take a very long time and
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:53,736'); seek(1013.0)">
              almost make it impractical for us to use such a valuable feature.
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:57,630'); seek(1017.0)">
              Fortunately, Kubernetes is also very extensible,
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:00,830'); seek(1020.0)">
              so you can kind of go in and modify behavior
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:04,814'); seek(1024.0)">
              or introduce new behavior by providing your own controllers in certain
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:08,700'); seek(1028.0)">
              areas. And in this particular case, we were able to
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:12,332'); seek(1032.0)">
              build a new controller, which we call the custom controller,
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:16,010'); seek(1036.0)">
              which actually works in communication with the
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:19,440'); seek(1039.0)">
              default stateful set controller. So the stateful set
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:23,312'); seek(1043.0)">
              controller would continue to create pods and create storage
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:27,286'); seek(1047.0)">
              and coordinate the mounting and all of
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:30,768'); seek(1050.0)">
              that, whereas the custom controller that we built would be
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:34,516'); seek(1054.0)">
              in charge of deciding which pods would be
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:37,828'); seek(1057.0)">
              deleted next in order to be replaced. So the deletion
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:41,402'); seek(1061.0)">
              would be the custom controller's job and rest of
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:45,028'); seek(1065.0)">
              it would be these existing stateful set controller's job. So once
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:48,872'); seek(1068.0)">
              we had this ability, and this is enabled by a flag called on
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:52,632'); seek(1072.0)">
              delete strategy in stateful, if you're interested in looking it up,
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:57,030'); seek(1077.0)">
              basically, these custom controller would then enable batching where
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:00,508'); seek(1080.0)">
              it would go after a batch of pods, delete them first,
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:04,220'); seek(1084.0)">
              and then the stateful controller would notice that these pods
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:07,538'); seek(1087.0)">
              are missing and would recreate them with the new configuration,
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:11,046'); seek(1091.0)">
              though. Similarly, the custom controller would then move to the next batch
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:14,662'); seek(1094.0)">
              of three, in this case, delete them, and stateful
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:18,598'); seek(1098.0)">
              would do the remaining part of bringing up these new ones.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:22,112'); seek(1102.0)">
              So in this manner, by coordinating with these existing behavior,
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:25,946'); seek(1105.0)">
              we were able to get batch upgrades enabled in Kubernetes,
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:29,834'); seek(1109.0)">
              which is a very big problem when we initially faced it.
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:33,890'); seek(1113.0)">
              Another limitation that we had was that in Kubernetes,
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:37,818'); seek(1117.0)">
              when you are deploying your services, you also define what is called
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:41,448'); seek(1121.0)">
              as the pod disruption budget. This is important to make sure that
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:45,336'); seek(1125.0)">
              whatever operations you do in your cluster, you don't let the number of
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:48,828'); seek(1128.0)">
              unhealthy pods or disrupted pods. To use
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:52,412'); seek(1132.0)">
              that terminology, you make sure that you put a limit on how many pods
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:56,018'); seek(1136.0)">
              are disrupted. In this case, for example, let's consider that
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:59,228'); seek(1139.0)">
              the pod disruption budget is one. What you're saying is that at
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:02,368'); seek(1142.0)">
              any given time in your cluster, you'd at most disrupt one pod
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:06,326'); seek(1146.0)">
              and not more than that when you're doing any of your administrative tasks.
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:09,942'); seek(1149.0)">
              Now, the problem here is that if more than one of your pods
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:13,498'); seek(1153.0)">
              is unhealthy, in this case, pod three and pod one are in an unhealthy state
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:17,444'); seek(1157.0)">
              because of some issue with them, and you are trying to
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:20,596'); seek(1160.0)">
              upgrade that particular stateful set,
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:24,260'); seek(1164.0)">
              maybe because you want to fix the issue by deploying new code.
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:27,832'); seek(1167.0)">
              Unfortunately, since it always starts with the highest number,
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:31,080'); seek(1171.0)">
              pod five in this case,
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:33,910'); seek(1173.0)">
              when it tries to upgrade, Kubernetes will prevent it
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:37,048'); seek(1177.0)">
              from being upgraded because it would increase the number
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:40,252'); seek(1180.0)">
              of unhealthy pods, because you hbase to destroy a healthy pod to create a
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:44,124'); seek(1184.0)">
              new one, and it bold increase the number of unhealthy pods as a result.
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:47,516'); seek(1187.0)">
              So in this case, again, a custom controller is really useful.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:51,030'); seek(1191.0)">
              What it did was it went after the unhealthy pods first while
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:54,688'); seek(1194.0)">
              doing upgrades, instead of just being according to a strict ordering,
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:19:58,502'); seek(1198.0)">
              delete the first unhealthy pod and replace it with
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:01,728'); seek(1201.0)">
              the healthy pod as a result, and then go after the next unhealthy pod,
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:05,094'); seek(1205.0)">
              replace it with these new one, and then finally go to the healthy
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:08,378'); seek(1208.0)">
              pods which can now be replaced because there are no unhealthy pods left.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:11,892'); seek(1211.0)">
              So in this way, we were able to overcome any blockage
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:15,502'); seek(1215.0)">
              due to pod disruption budget and move the rolling upgrade forward.
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:20,230'); seek(1220.0)">
              Another interesting problem we had, which is kind of unique to
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:24,790'); seek(1224.0)">
              stateful applications, I guess, especially things like Zookeeper
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:28,754'); seek(1228.0)">
              and many other such services. You have a number of
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:32,348'); seek(1232.0)">
              instances, but one of them is elected a leader
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:36,410'); seek(1236.0)">
              and it's the leader of the group, and it has certain responsibilities as
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:39,708'); seek(1239.0)">
              a result. And to create a leader, you have to go through an
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:43,168'); seek(1243.0)">
              election process. So there is some activity and delay involved
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:46,758'); seek(1246.0)">
              in doing some of these things. Unfortunately, Kubernetes at its level
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:50,720'); seek(1250.0)">
              knows nothing about these leader business. So the
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:20:54,388'); seek(1254.0)">
              controller would typically just go after the highest number of
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:20:57,812'); seek(1257.0)">
              pod, and if that pod is disrupted and a new one
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:01,108'); seek(1261.0)">
              is created, the leader might be reelected into one of the older pods.
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:05,322'); seek(1265.0)">
              So the next upgrade would hit that leader again, and once again you would have
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:08,808'); seek(1268.0)">
              election. And if you're really unlucky, the third pod also would be from
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:12,328'); seek(1272.0)">
              the older set of pods. So you end up disrupting the
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:15,624'); seek(1275.0)">
              leader these times in this case. But you can imagine in a real cluster,
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:20,090'); seek(1280.0)">
              this repeated leader election bold be very disruptive
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:23,794'); seek(1283.0)">
              to the cluster. So to avoid this,
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:27,292'); seek(1287.0)">
              once again, the custom controller came to a rescue.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:30,598'); seek(1290.0)">
              We built sidecar containers. These are basically logic
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:34,678'); seek(1294.0)">
              that runs inside each one of these pods, which checks
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:38,694'); seek(1298.0)">
              to see if it's a leader, and it makes that information available through
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:42,976'); seek(1302.0)">
              labels in the pod. And the custom controller is basically monitoring
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:47,098'); seek(1307.0)">
              all these pods to see which one of them has this leader label on it.
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:50,484'); seek(1310.0)">
              And it would then avoid that particular leader and update
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:21:53,882'); seek(1313.0)">
              all the other pods first, then finally go and update the leader.
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:21:57,438'); seek(1317.0)">
              So you end up disrupting the leader pod only once
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:01,000'); seek(1321.0)">
              throughout this process, which was a nice capability that we could
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:04,552'); seek(1324.0)">
              have thanks to this custom controller. So another area
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:08,636'); seek(1328.0)">
              of problems that we experienced was around DNS.
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:12,890'); seek(1332.0)">
              As you can imagine in kubernetes,
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:16,226'); seek(1336.0)">
              it's a very dynamic world. As pods move from one hosts
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:19,862'); seek(1339.0)">
              to another, even though they keep the same hosts names, the IP
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:23,558'); seek(1343.0)">
              addresses keep changing. And I kind of went over that earlier.
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:27,310'); seek(1347.0)">
              This creates a strange problem because
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:30,704'); seek(1350.0)">
              traditional software like hbase, Hadoop file
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:33,834'); seek(1353.0)">
              system, et cetera, they were largely developed in an environment where
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:37,572'); seek(1357.0)">
              DNS did not change so much. So as a result,
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:22:40,900'); seek(1360.0)">
              there was a lot of bugs in this code base where
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:44,648'); seek(1364.0)">
              it would resolve DNS hostname to IP address and
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:22:48,168'); seek(1368.0)">
              cache that information for literally forever in its
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:52,136'); seek(1372.0)">
              code. So you can imagine if you had that kind of code,
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:22:56,170'); seek(1376.0)">
              you would have invalid information in the software pretty quickly.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:00,412'); seek(1380.0)">
              And in particular, what we noticed is that if the metadata
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:04,498'); seek(1384.0)">
              servers had these IP addresses changing
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:08,730'); seek(1388.0)">
              and if a large number of data servers sort of had to
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:12,288'); seek(1392.0)">
              talk to these metadata servers, they were kind of losing connection
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:15,958'); seek(1395.0)">
              to this metadata server as its ip address changed.
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:19,254'); seek(1399.0)">
              Now obviously the fix to this kind of problem is to go into
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:22,852'); seek(1402.0)">
              the open source code, find where all these bugs are.
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:25,892'); seek(1405.0)">
              These it is holding on to these addresses and fix
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:29,956'); seek(1409.0)">
              those bugs. But with a large code base like
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:33,748'); seek(1413.0)">
              Hadoop file system and hbase, it's kind
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:23:36,904'); seek(1416.0)">
              of challenging to find all the places that this issue exists. And especially
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:40,856'); seek(1420.0)">
              when we had to get our software out and very sort
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:45,128'); seek(1425.0)">
              of depending upon our eyeballing capabilities to find
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:23:48,860'); seek(1428.0)">
              all these issues or a testing test matrix to
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:23:52,508'); seek(1432.0)">
              find all these issues seemed a little risky.
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:23:55,986'); seek(1435.0)">
              So what we ended up doing was that even as we went about fixing
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:23:59,638'); seek(1439.0)">
              these bugs, we came up with a solution where for
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:03,616'); seek(1443.0)">
              each one of our pods we put a load balancer
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:07,126'); seek(1447.0)">
              and it's called a service in Kubernetes. And it's actually not a physical
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:11,062'); seek(1451.0)">
              load balancer, it's a virtual one which works using network
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:14,346'); seek(1454.0)">
              magic, really there's no physical load balancer involved. So we
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:18,292'); seek(1458.0)">
              created this virtual load balancer in front of each one of
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:21,732'); seek(1461.0)">
              these metadata server instances. So now
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:25,092'); seek(1465.0)">
              what that does is that when you create a load balancer, not only does it
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:28,248'); seek(1468.0)">
              get a host name but also an IP address. And that IP address is
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:24:32,152'); seek(1472.0)">
              very static in nature. It doesn't change as long as you don't delete the
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:24:35,544'); seek(1475.0)">
              load balancer. So even though your pods may be changing the
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:24:39,308'); seek(1479.0)">
              IP addresses, the load balancer does not. So when
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:24:43,212'); seek(1483.0)">
              the client is trying to contact these pod, it would first go to the load
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:24:46,338'); seek(1486.0)">
              balancer and then the load balancer would forward the request to the pod.
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:24:50,118'); seek(1490.0)">
              So we sort of recreated that stateless applications methodology,
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:24:54,822'); seek(1494.0)">
              at least for metadata servers, so that we
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:24:58,608'); seek(1498.0)">
              can kind of protect ourselves from IP address related issues.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:25:02,308'); seek(1502.0)">
              And in the meantime we also went about finding
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:25:05,818'); seek(1505.0)">
              all these bugs using various testing mechanisms and eliminating
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:25:09,642'); seek(1509.0)">
              it. But it gave us some breathing time. Another interesting
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:25:13,704'); seek(1513.0)">
              issue that you have with kubernetes is how DNS actually
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:17,304'); seek(1517.0)">
              works inside it. There's actually a dedicated DNS server
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:21,422'); seek(1521.0)">
              that is providing all this support for the changing IP
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:25,022'); seek(1525.0)">
              addresses. It's called core DNS. It actually runs inside the
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:25:28,684'); seek(1528.0)">
              Kubernetes cluster. And as you create pods of various
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:25:32,242'); seek(1532.0)">
              type and delete it, this core DNS is the one
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:25:35,452'); seek(1535.0)">
              which keeps track of when to create a DNS record and when to delete
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:25:39,058'); seek(1539.0)">
              it. The problem with this approach is that while it all works
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:25:42,816'); seek(1542.0)">
              great on the server side, there's no guarantee that your clients are
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:25:46,448'); seek(1546.0)">
              actually running in the same Kubernetes cluster. Really in the real
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:25:50,096'); seek(1550.0)">
              world your client is typically outside of a Kubernetes cluster.
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:25:53,722'); seek(1553.0)">
              It's probably running in some external host or VM,
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:25:57,626'); seek(1557.0)">
              but not necessarily inside your Kubernetes cluster. And that
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:26:00,772'); seek(1560.0)">
              client is actually depending upon typically a different DNS server,
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:26:04,398'); seek(1564.0)">
              which is the global DNS server that is visible across a
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:26:08,808'); seek(1568.0)">
              large number of environments and not the core DNS, which is
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:26:12,248'); seek(1572.0)">
              visible only inside the Kubernetes cluster. So to deal
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:15,992'); seek(1575.0)">
              with this issue, what you have to do is find a way of getting your
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:19,788'); seek(1579.0)">
              records from the core DNS into the global DNS.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:23,010'); seek(1583.0)">
              Otherwise your client would not know how to contact all
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:26:26,332'); seek(1586.0)">
              your services. So in our case we use an open source
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:26:29,942'); seek(1589.0)">
              tool called external DNS. It's something that is open source
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:26:33,734'); seek(1593.0)">
              and most people use it when they're trying to deal with
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:26:37,504'); seek(1597.0)">
              this particular scenario. And what external DNS does is
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:26:40,848'); seek(1600.0)">
              that it transfers these DNS records that are within the Kubernetes cluster
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:26:45,306'); seek(1605.0)">
              into this global DNS server. I've simplified the picture here by showing
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:26:49,418'); seek(1609.0)">
              that it's actually moving data from core DNS to global
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:26:52,778'); seek(1612.0)">
              DNS. In reality that's not exactly how it does it,
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:26:56,072'); seek(1616.0)">
              but in effect it has the same impact.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:26:59,870'); seek(1619.0)">
              It makes sure that those DNS records are available in global
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:27:03,822'); seek(1623.0)">
              DNS. Once they are available in global DNS, the client is able
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:27:07,468'); seek(1627.0)">
              to then contact your data servers
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:27:10,866'); seek(1630.0)">
              and communicate with them effectively now one challenge with
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:27:14,268'); seek(1634.0)">
              this approach is that external DNS only runs periodically,
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:17,330'); seek(1637.0)">
              every minute or so. So your DNS
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:27:20,982'); seek(1640.0)">
              records are not immediately available in global DNS.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:27:24,166'); seek(1644.0)">
              So for example, if data server four here is just booting
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:27:27,798'); seek(1647.0)">
              up, it should not go online until it's absolutely
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:27:32,032'); seek(1652.0)">
              certain that its DNS records are available in global DNS.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:27:35,754'); seek(1655.0)">
              So we have to actually build logic to make sure that it can validate
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:27:39,978'); seek(1659.0)">
              that global DNS has actually got its DNS records.
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:27:43,530'); seek(1663.0)">
              Once it's confirmed, only then would the data server declare
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:27:46,794'); seek(1666.0)">
              itself as available for traffic. So this is one of those steps
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:27:50,318'); seek(1670.0)">
              you kind of might have to deal with in the real world when you're trying
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:27:53,288'); seek(1673.0)">
              to use Kubernetes and stateful applications in
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:27:56,988'); seek(1676.0)">
              general. Now finally, I want to talk
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:28:00,172'); seek(1680.0)">
              a little bit about scalability architecture in
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:28:03,612'); seek(1683.0)">
              public cloud. Typically you deploy software in a
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:28:07,052'); seek(1687.0)">
              certain region. You can deploy it across multiple
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:28:10,642'); seek(1690.0)">
              regions, but if you are doing a high performance software that needs
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:14,080'); seek(1694.0)">
              very low latency, you deploy that software
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:28:17,846'); seek(1697.0)">
              in a particular region, which is really a geographical region like
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:28:21,652'); seek(1701.0)">
              us east or US west.
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:28:24,530'); seek(1704.0)">
              And within that region you can also spread your software
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:28:28,490'); seek(1708.0)">
              across different availability zones. Availability zones can
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:28:32,292'); seek(1712.0)">
              mean different things for different cloud providers, but typically
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:28:36,174'); seek(1716.0)">
              it is either a separate building, a separate
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:28:40,062'); seek(1720.0)">
              campus even, but very close to each other, so that
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:28:43,368'); seek(1723.0)">
              the network latency between the different availability zones is
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:28:46,428'); seek(1726.0)">
              not too high. So you can actually spread your software across it without
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:28:50,396'); seek(1730.0)">
              experiencing too much of a performance issue.
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:28:53,610'); seek(1733.0)">
              So I'll be calling availability zones AZ for
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:28:57,452'); seek(1737.0)">
              short here. So the goal is typically for you
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:29:00,592'); seek(1740.0)">
              to take a few AZ. In our case
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:29:03,872'); seek(1743.0)">
              we took three AZ approach and make sure that your software
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:29:07,702'); seek(1747.0)">
              is spread across the instances of your software are spread across each
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:11,408'); seek(1751.0)">
              one of these AZ to achieve this.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:29:14,356'); seek(1754.0)">
              Fortunately in kubernetes there has been significant
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:29:17,562'); seek(1757.0)">
              effort to make sure that you can support this kind of deployment.
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:29:21,242'); seek(1761.0)">
              So they have got something called affinity and anti affinity rules
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:29:25,422'); seek(1765.0)">
              where you can tell the Kubernetes scheduler
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:29:29,278'); seek(1769.0)">
              to spread the pods across different AZ.
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:29:32,654'); seek(1772.0)">
              And the way they do it is that the hosts that run in each AZ
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:29:36,510'); seek(1776.0)">
              have a certain label indicating what AZ that hosts is
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:29:39,948'); seek(1779.0)">
              running in. And then you can tell Kubernetes that,
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:29:43,100'); seek(1783.0)">
              make sure that when you deploy these pods, they run on
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:29:46,876'); seek(1786.0)">
              hosts that have different label values as much as possible.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:29:51,630'); seek(1791.0)">
              Obviously you will have more than three pods, so you're not
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:29:55,216'); seek(1795.0)">
              going to be able to spread these all on different AZ, but you do your
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:29:58,752'); seek(1798.0)">
              best effort to equally balance it across different AZ.
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:30:02,690'); seek(1802.0)">
              Now that takes care of making sure that your software
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:30:06,362'); seek(1806.0)">
              is running on different azs, but what about the data inside that
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:10,372'); seek(1810.0)">
              software. A good example of it is Hadoop file system,
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:30:14,100'); seek(1814.0)">
              which keeps three copies of data for high availability reasons.
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:30:18,270'); seek(1818.0)">
              Now you want to make sure that that copy each one of those
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:30:21,672'); seek(1821.0)">
              copies is running in different AZ for safety reasons.
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:30:25,870'); seek(1825.0)">
              So fortunately in Hadoop itself, when they designed
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:30:29,378'); seek(1829.0)">
              it, they introduced this concept called rack topology, which is sort
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:30:33,468'); seek(1833.0)">
              of the traditional data center terminology where you tell
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:30:37,356'); seek(1837.0)">
              Hadoop fails system. In particular it's metadata server.
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:30:42,430'); seek(1842.0)">
              What is the topology of your servers?
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:30:45,814'); seek(1845.0)">
              In which racks do they run in? And Hadoop will make sure that
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:30:49,440'); seek(1849.0)">
              these replicas are kind of distributed on
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:30:53,156'); seek(1853.0)">
              different racks, so that if one whole rack goes down,
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:30:56,692'); seek(1856.0)">
              you still got other racks that can serve the data.
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:31:00,084'); seek(1860.0)">
              We were then able to convince Hadoop through using its
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:31:04,184'); seek(1864.0)">
              script based interface that each AZ is a different rack.
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:31:08,366'); seek(1868.0)">
              And thereby Hadoop was able to spread these replicas
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:31:12,446'); seek(1872.0)">
              across different azs using that mapping. The metadata
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:31:16,658'); seek(1876.0)">
              server itself had multiple instances which are
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:31:20,108'); seek(1880.0)">
              again spread across different AZ
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:31:23,778'); seek(1883.0)">
              using the same affinity anti affinity rules that Kubernetes
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:31:27,586'); seek(1887.0)">
              supports. So what you achieve with all these spread is
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:31:31,212'); seek(1891.0)">
              that if an entire AZ goes away due to a power
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:31:34,956'); seek(1894.0)">
              outage or a network outage, you still have the software and
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:31:38,652'); seek(1898.0)">
              its data available in the other AZ and still
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:31:42,724'); seek(1902.0)">
              serving traffic in spite of this outage. So it's really useful
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:31:46,602'); seek(1906.0)">
              for resilience in general. So that's
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:31:49,802'); seek(1909.0)">
              pretty much all I had for today's talk. Thank you
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:31:53,732'); seek(1913.0)">
              so much for listening.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Conf42%20Cloud%20Native%202021%20-%20Dhiraj%20Hegde%20-%20Containing%20An%20Elephant.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Conf42%20Cloud%20Native%202021%20-%20Dhiraj%20Hegde%20-%20Containing%20An%20Elephant.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #7B2726;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/cloud2021" class="btn btn-sm btn-danger shadow lift" style="background-color: #7B2726;">
                <i class="fe fe-grid me-2"></i>
                See all 45 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/cloud_dhiraj.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Dhiraj Hegde
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Architect @ Salesforce
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/dhirajlink/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Dhiraj Hegde's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/@dhirajtxt" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Dhiraj Hegde's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @dhirajtxt"
                  data-url="https://www.conf42.com/cloud2021"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/cloud2021"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Cloud Native"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>