<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Edge-Ready GenAI: Engineering Low-Latency Solutions for Resource-Constrained Environments</title>
    <meta name="description" content="Help us build a dystopian, machine-ated future!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Sai%20KR%20Pentaparthi_ml.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Edge-Ready GenAI: Engineering Low-Latency Solutions for Resource-Constrained Environments | Conf42"/>
    <meta property="og:description" content="Unlock the secrets to deploying powerful generative AI on edge devices with minimal resources. Learn practical compression techniques that dramatically reduce latency while maintaining performance. Transform your edge devices into AI powerhousesâ€”no cloud required."/>
    <meta property="og:url" content="https://conf42.com/Machine_Learning_2025_Sai_KR_Pentaparthi_latency_solutions_genai"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/IOT2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Internet of Things (IoT) 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-12-18
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/iot2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2026
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2026">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2026">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2026">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2026">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2026">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/dbd2026">
                            Database DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2026">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2026">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/agents2026">
                            AI Agents
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2026">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2026">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2026">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2026">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2026">
                            Chaos Engineering
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <!-- <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a> -->
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #198B91;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Machine Learning 2025 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2025-05-08">May 08 2025</time>
              
              - premiere 5PM GMT
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Help us build a dystopian, machine-ated future!
 -->
              <script>
                const event_date = new Date("2025-05-08T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2025-05-08T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "7QdWz6xIBY8"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrDbH1VBaoA60lLAAVqmiLPe" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Morning or afternoon everyone.", "timestamp": "00:00:01,000", "timestamp_s": 1.0}, {"text": "Today we are diving into a critical and exciting area, HDD generative ai.", "timestamp": "00:00:02,830", "timestamp_s": 2.0}, {"text": "Specifically we\u0027ll be exploring how to engineer low latency AI solutions", "timestamp": "00:00:09,400", "timestamp_s": 9.0}, {"text": "designed for resource constrained environments as businesses increasingly", "timestamp": "00:00:14,410", "timestamp_s": 14.0}, {"text": "look for real-time AI capabilities, right where the action happens at the edge.", "timestamp": "00:00:19,510", "timestamp_s": 19.0}, {"text": "Understanding how to optimize these powerful generative", "timestamp": "00:00:24,849", "timestamp_s": 24.0}, {"text": "models becomes essential.", "timestamp": "00:00:27,759", "timestamp_s": 27.0}, {"text": "We look at systematic ways to maintain performance while cutting", "timestamp": "00:00:30,310", "timestamp_s": 30.0}, {"text": "down on computational needs, power consumption, and latency.", "timestamp": "00:00:34,269", "timestamp_s": 34.0}, {"text": "This opens up fascinating new possibilities for intelligent", "timestamp": "00:00:39,099", "timestamp_s": 39.0}, {"text": "applications directly on edge devices.", "timestamp": "00:00:42,250", "timestamp_s": 42.0}, {"text": "Just a little bit about myself.", "timestamp": "00:00:45,649", "timestamp_s": 45.0}, {"text": "I\u0027m Ian, principal software engineer at SD Engineering.", "timestamp": "00:00:47,999", "timestamp_s": 47.0}, {"text": "I direct my work focuses on enhancing global connectivity through advanced", "timestamp": "00:00:52,349", "timestamp_s": 52.0}, {"text": "IP satellite network infrastructure.", "timestamp": "00:00:56,999", "timestamp_s": 56.0}, {"text": "My experience in network engineering and software development gives me a", "timestamp": "00:00:59,969", "timestamp_s": 59.0}, {"text": "practical perspective on challenges and opportunities of deploying", "timestamp": "00:01:04,079", "timestamp_s": 64.0}, {"text": "complex systems like AI in diverse environments, including the Edge.", "timestamp": "00:01:07,919", "timestamp_s": 67.0}, {"text": "You can find more details or connect with me on LinkedIn.", "timestamp": "00:01:13,470", "timestamp_s": 73.0}, {"text": "So why is Edge AI challenging?", "timestamp": "00:01:17,470", "timestamp_s": 77.0}, {"text": "Let\u0027s look at edge computing Frontier.", "timestamp": "00:01:19,840", "timestamp_s": 79.0}, {"text": "Unlike the seemingly limitless resources in the cloud, edge devices", "timestamp": "00:01:22,750", "timestamp_s": 82.0}, {"text": "operate under significant constraints.", "timestamp": "00:01:26,679", "timestamp_s": 86.0}, {"text": "First, limited computational resources.", "timestamp": "00:01:29,200", "timestamp_s": 89.0}, {"text": "Think processing power, memory storage edge devices typically", "timestamp": "00:01:32,500", "timestamp_s": 92.0}, {"text": "have much less than cloud servers.", "timestamp": "00:01:36,340", "timestamp_s": 96.0}, {"text": "Second, connectivity challenges.", "timestamp": "00:01:38,830", "timestamp_s": 98.0}, {"text": "Edge solutions often need to work reliably, even with spotty, slow,", "timestamp": "00:01:41,530", "timestamp_s": 101.0}, {"text": "or sometimes no internet connection.", "timestamp": "00:01:46,150", "timestamp_s": 106.0}, {"text": "Third, energy constraints.", "timestamp": "00:01:48,670", "timestamp_s": 108.0}, {"text": "Many edge devices run on batteries or have strict power budgets limiting", "timestamp": "00:01:51,160", "timestamp_s": 111.0}, {"text": "how complex our AI models can be.", "timestamp": "00:01:55,810", "timestamp_s": 115.0}, {"text": "And finally, realtime requirements.", "timestamp": "00:01:58,570", "timestamp_s": 118.0}, {"text": "Many edge applications like autonomous vehicles or industrial monitoring", "timestamp": "00:02:00,160", "timestamp_s": 120.0}, {"text": "demand, immediate low latency responses, putting high performance demands.", "timestamp": "00:02:05,530", "timestamp_s": 125.0}, {"text": "On these constant devices.", "timestamp": "00:02:10,105", "timestamp_s": 130.0}, {"text": "Despite this the challenges, the demand for Edge AI is booming.", "timestamp": "00:02:12,635", "timestamp_s": 132.0}, {"text": "Let\u0027s look at market trends.", "timestamp": "00:02:17,035", "timestamp_s": 137.0}, {"text": "We are seeing strong expansion.", "timestamp": "00:02:18,865", "timestamp_s": 138.0}, {"text": "The Edge AI accelerator market, the specialized hardware is growing incredibly", "timestamp": "00:02:21,535", "timestamp_s": 141.0}, {"text": "fast, nearly 36 39% CHER, projected to hit almost 7.7 billion by 2027.", "timestamp": "00:02:26,275", "timestamp_s": 146.0}, {"text": "This signals huge demand for on-device ai.", "timestamp": "00:02:35,120", "timestamp_s": 155.0}, {"text": "What\u0027s driving this key adoption drivers are primarily the need for low", "timestamp": "00:02:38,660", "timestamp_s": 158.0}, {"text": "latency, over 78% of organization site.", "timestamp": "00:02:43,100", "timestamp_s": 163.0}, {"text": "This is crucial for realtime responses.", "timestamp": "00:02:46,970", "timestamp_s": 166.0}, {"text": "Then data privacy is another major factor.", "timestamp": "00:02:50,570", "timestamp_s": 170.0}, {"text": "Almost 65% of organizations cite this as another major factor.", "timestamp": "00:02:53,750", "timestamp_s": 173.0}, {"text": "Pushing processing locally instead of sending sensitive data to", "timestamp": "00:02:59,160", "timestamp_s": 179.0}, {"text": "the cloud is important for them.", "timestamp": "00:03:02,100", "timestamp_s": 182.0}, {"text": "And the industry adapting is adapting.", "timestamp": "00:03:05,100", "timestamp_s": 185.0}, {"text": "Model optimization techniques, which we\u0027ll discuss heavily today, are", "timestamp": "00:03:08,280", "timestamp_s": 188.0}, {"text": "already used in 82% of deployments.", "timestamp": "00:03:11,880", "timestamp_s": 191.0}, {"text": "This shows a clear focus on making complex AI run effectively on edge hardware,", "timestamp": "00:03:15,060", "timestamp_s": 195.0}, {"text": "edge AI market evolution.", "timestamp": "00:03:21,610", "timestamp_s": 201.0}, {"text": "Let\u0027s look ahead.", "timestamp": "00:03:23,500", "timestamp_s": 203.0}, {"text": "We are really at an inflection point.", "timestamp": "00:03:24,805", "timestamp_s": 204.0}, {"text": "Currently generative AI at the edge is still limited by those resources,", "timestamp": "00:03:27,010", "timestamp_s": 207.0}, {"text": "resource constraints that were mentioned.", "timestamp": "00:03:30,880", "timestamp_s": 210.0}, {"text": "But over the next 12 months, we expect increasing adoption of optimized edge", "timestamp": "00:03:33,710", "timestamp_s": 213.0}, {"text": "AI solutions within eight to 24 months.", "timestamp": "00:03:39,170", "timestamp_s": 219.0}, {"text": "The prediction is that most enterprise need real time edge capabilities", "timestamp": "00:03:42,050", "timestamp_s": 222.0}, {"text": "will be act actively deploying them.", "timestamp": "00:03:46,640", "timestamp_s": 226.0}, {"text": "Beyond 2025 H, AI is likely to become ubiquitous, supported by", "timestamp": "00:03:49,040", "timestamp_s": 229.0}, {"text": "specialized hardware acceleration.", "timestamp": "00:03:54,800", "timestamp_s": 234.0}, {"text": "This rabbit evolution is driven by hardware advancements, better optimization", "timestamp": "00:03:57,290", "timestamp_s": 237.0}, {"text": "techniques like composition and pruning, and the growing need for privacy.", "timestamp": "00:04:02,750", "timestamp_s": 242.0}, {"text": "Preserving local computation.", "timestamp": "00:04:06,890", "timestamp_s": 246.0}, {"text": "The shift away from cloud dependency for realtime tasks is well underway.", "timestamp": "00:04:09,140", "timestamp_s": 249.0}, {"text": "Market trends.", "timestamp": "00:04:14,429", "timestamp_s": 254.0}, {"text": "Just to reiterate, the those key market drives drivers because they underscore", "timestamp": "00:04:15,899", "timestamp_s": 255.0}, {"text": "the why behind the edge optimization.", "timestamp": "00:04:22,030", "timestamp_s": 262.0}, {"text": "The market growth is significant.", "timestamp": "00:04:25,120", "timestamp_s": 265.0}, {"text": "As mentioned, 39% of CAGR for accelerators and 7.6 billion market cap soon.", "timestamp": "00:04:28,150", "timestamp_s": 268.0}, {"text": "The primary need for low latency, for immediate responses and data privacy, both", "timestamp": "00:04:36,150", "timestamp_s": 276.0}, {"text": "favoring local on device processing and crucially, the industry is already heavily", "timestamp": "00:04:43,080", "timestamp_s": 283.0}, {"text": "invested in model optimizations, which is done in like almost 82% of deployments", "timestamp": "00:04:48,240", "timestamp_s": 288.0}, {"text": "to overcome hardware limitations.", "timestamp": "00:04:53,940", "timestamp_s": 293.0}, {"text": "This sets the stage for the techniques we are going to", "timestamp": "00:04:56,310", "timestamp_s": 296.0}, {"text": "explore in the rest of the talk.", "timestamp": "00:04:59,190", "timestamp_s": 299.0}, {"text": "Acceleration solutions?", "timestamp": "00:05:01,780", "timestamp_s": 301.0}, {"text": "Software optimization is key Hardware plays a vital role.", "timestamp": "00:05:04,340", "timestamp_s": 304.0}, {"text": "Specialized hardware acceleration solution dramatically boost performance", "timestamp": "00:05:08,910", "timestamp_s": 308.0}, {"text": "and energy efficiency for the edge.", "timestamp": "00:05:13,380", "timestamp_s": 313.0}, {"text": "Yeah.", "timestamp": "00:05:16,245", "timestamp_s": 316.0}, {"text": "We have neural processing units, N ps, which are dedicated AI accelerators.", "timestamp": "00:05:18,360", "timestamp_s": 318.0}, {"text": "Now common in modern systems on chips they offer 10 to 15 times the", "timestamp": "00:05:23,340", "timestamp_s": 323.0}, {"text": "energy efficiency for a standard CPU for typical neural network tasks.", "timestamp": "00:05:29,380", "timestamp_s": 329.0}, {"text": "Google offers HT ps. Purposely built edge accelerators providing", "timestamp": "00:05:34,700", "timestamp_s": 334.0}, {"text": "significant processing power, like up to four terra operations in small", "timestamp": "00:05:40,250", "timestamp_s": 340.0}, {"text": "low power around two watts package.", "timestamp": "00:05:45,050", "timestamp_s": 345.0}, {"text": "That\u0027s impressive.", "timestamp": "00:05:47,480", "timestamp_s": 347.0}, {"text": "Then mobile GPUs are also increasingly optimized for AI leveraging their", "timestamp": "00:05:49,220", "timestamp_s": 349.0}, {"text": "parallel processing power, and the FPG accelerators offer re comfortable hardware", "timestamp": "00:05:54,680", "timestamp_s": 354.0}, {"text": "adapted for specific AI models and often very power efficient in production.", "timestamp": "00:06:01,100", "timestamp_s": 361.0}, {"text": "These accelerators are designed specifically for the math", "timestamp": "00:06:07,205", "timestamp_s": 367.0}, {"text": "intensive operations common in ai, like matrix, multiplication,", "timestamp": "00:06:10,685", "timestamp_s": 370.0}, {"text": "it, GPU\u0027s, overview.", "timestamp": "00:06:15,495", "timestamp_s": 375.0}, {"text": "This table gives a snapshot of the diverse landscape of it, GPU and accelerators.", "timestamp": "00:06:17,655", "timestamp_s": 377.0}, {"text": "Don\u0027t worry about memorizing the details, but notice how the key players like", "timestamp": "00:06:23,715", "timestamp_s": 383.0}, {"text": "Nvidia Jetson series like Nano Nx, A GX RTX, ADA AMD rise, AI inter core Intel", "timestamp": "00:06:28,305", "timestamp_s": 388.0}, {"text": "score Ultra and PS and R gpu, Google score, HT ps, Callcom, snapdragons and", "timestamp": "00:06:37,750", "timestamp_s": 397.0}, {"text": "pos, and specialists like H Cortex.", "timestamp": "00:06:44,230", "timestamp_s": 404.0}, {"text": "Important takeaway here is the variation in ai performance,", "timestamp": "00:06:48,355", "timestamp_s": 408.0}, {"text": "memory, capacity, and bandwidth, power consumption, and form factor.", "timestamp": "00:06:52,045", "timestamp_s": 412.0}, {"text": "Choosing the right hardware depends heavily on specific application", "timestamp": "00:06:56,515", "timestamp_s": 416.0}, {"text": "needs regarding performance, power, budget, and physical size.", "timestamp": "00:07:00,805", "timestamp_s": 420.0}, {"text": "This plays a critical role in the process of developing these models.", "timestamp": "00:07:05,935", "timestamp_s": 425.0}, {"text": "Now let\u0027s look at the inference performance specifically for", "timestamp": "00:07:11,345", "timestamp_s": 431.0}, {"text": "large language models, which are notoriously resource intensive.", "timestamp": "00:07:14,975", "timestamp_s": 434.0}, {"text": "Again, this table is illustrative based on available data, which can be fragmented.", "timestamp": "00:07:19,385", "timestamp_s": 439.0}, {"text": "Key things to note.", "timestamp": "00:07:25,640", "timestamp_s": 445.0}, {"text": "High-end desktop GPOs like RTX 4 0 9 0 or 3 0 9 0 achieve very", "timestamp": "00:07:26,930", "timestamp_s": 446.0}, {"text": "high throughput tokens per second.", "timestamp": "00:07:32,450", "timestamp_s": 452.0}, {"text": "Using optimized frameworks like Nvidia sensors R-T-L-L-M significantly", "timestamp": "00:07:34,820", "timestamp_s": 454.0}, {"text": "outperforming less optimized methods like Lama CCP on the same", "timestamp": "00:07:39,470", "timestamp_s": 459.0}, {"text": "hardware for it specific hardware like Nvidia Jetson\u0027s Orient Series.", "timestamp": "00:07:43,520", "timestamp_s": 463.0}, {"text": "A MD Rise, AI and Intel Core ultra running models like LAMA or SSL", "timestamp": "00:07:48,860", "timestamp_s": 468.0}, {"text": "seven B, often using ization like integer four is becoming feasible,", "timestamp": "00:07:54,470", "timestamp_s": 474.0}, {"text": "especially with dedicated frameworks like Tensor, R-T-L-L-M-O-N-N-X.", "timestamp": "00:07:59,810", "timestamp_s": 479.0}, {"text": "Runtime with V is ai.", "timestamp": "00:08:03,920", "timestamp_s": 483.0}, {"text": "Iex, LLM, open V. However, specific standardized benchmarking numbers", "timestamp": "00:08:06,930", "timestamp_s": 486.0}, {"text": "for throughput and latency on these edge platforms is still emerging", "timestamp": "00:08:12,690", "timestamp_s": 492.0}, {"text": "and depend heavily on software maturity, on automation levels.", "timestamp": "00:08:16,530", "timestamp_s": 496.0}, {"text": "That\u0027s why we see, needed data in the, in specific rows and columns.", "timestamp": "00:08:20,220", "timestamp_s": 500.0}, {"text": "The power consumption is drastically lower for each devices compared to", "timestamp": "00:08:26,160", "timestamp_s": 506.0}, {"text": "desktop GPUs highlighting the efficiency gains, but also performance trade-offs.", "timestamp": "00:08:30,390", "timestamp_s": 510.0}, {"text": "Beyond physical hardware.", "timestamp": "00:08:36,490", "timestamp_s": 516.0}, {"text": "Hardware is only part of equation Software specifically, AI frameworks", "timestamp": "00:08:38,200", "timestamp_s": 518.0}, {"text": "and optimization engines is crucial for actually running models", "timestamp": "00:08:42,970", "timestamp_s": 522.0}, {"text": "efficiently on the hardware key.", "timestamp": "00:08:47,200", "timestamp_s": 527.0}, {"text": "AI frameworks like TensorFlow Light and PyTorch Mobile are", "timestamp": "00:08:49,750", "timestamp_s": 529.0}, {"text": "designed for edge deployment.", "timestamp": "00:08:53,860", "timestamp_s": 533.0}, {"text": "NVIDIA sensor RT and its LLM variant are highly optimized for their GPOs.", "timestamp": "00:08:56,080", "timestamp_s": 536.0}, {"text": "Inference automation engines like ONNX runtime and Intel.", "timestamp": "00:09:02,095", "timestamp_s": 542.0}, {"text": "So open wino.", "timestamp": "00:09:05,605", "timestamp_s": 545.0}, {"text": "Focus on optimizing models for efficient execution across various hardware", "timestamp": "00:09:07,765", "timestamp_s": 547.0}, {"text": "vendors.", "timestamp": "00:09:14,485", "timestamp_s": 554.0}, {"text": "Also provide specific software stacks like Nvidia, Jetpack,", "timestamp": "00:09:14,965", "timestamp_s": 554.0}, {"text": "A MD Rise, and AI software.", "timestamp": "00:09:18,985", "timestamp_s": 558.0}, {"text": "Intel\u0027s one API Tools and Qualcomm\u0027s AI stack, which bundles drivers, libraries,", "timestamp": "00:09:21,085", "timestamp_s": 561.0}, {"text": "and tools for their respective hardware.", "timestamp": "00:09:26,875", "timestamp_s": 566.0}, {"text": "The right framework involves trade offs.", "timestamp": "00:09:30,520", "timestamp_s": 570.0}, {"text": "As this chart illustrates TensorFlow light generally offers excellent", "timestamp": "00:09:33,085", "timestamp_s": 573.0}, {"text": "hardware support through its de delegation system, allowing it to", "timestamp": "00:09:38,065", "timestamp_s": 578.0}, {"text": "leverage n ps and GPUs effectively, or in an next runtime, often provide", "timestamp": "00:09:43,155", "timestamp_s": 583.0}, {"text": "slightly better raw execution, speed, and great cross-platform portability.", "timestamp": "00:09:48,735", "timestamp_s": 588.0}, {"text": "Py Touch Mobile is often placed for its developer experience and", "timestamp": "00:09:54,690", "timestamp_s": 594.0}, {"text": "see simpler deployment, making it good for rapid prototyping.", "timestamp": "00:09:59,100", "timestamp_s": 599.0}, {"text": "TVM Apache TVM can achieve the best performance through deep", "timestamp": "00:10:03,210", "timestamp_s": 603.0}, {"text": "compiler optimizations, but also comes with significantly", "timestamp": "00:10:07,530", "timestamp_s": 607.0}, {"text": "higher deployment complexity.", "timestamp": "00:10:10,980", "timestamp_s": 610.0}, {"text": "So the choice depends on your priorities.", "timestamp": "00:10:13,530", "timestamp_s": 613.0}, {"text": "Broad hardware support.", "timestamp": "00:10:16,380", "timestamp_s": 616.0}, {"text": "ITF flight, raw speed and portability.", "timestamp": "00:10:18,600", "timestamp_s": 618.0}, {"text": "Linux runtime is the one to explore ease of use by touch", "timestamp": "00:10:21,660", "timestamp_s": 621.0}, {"text": "mobile, our maximum performance at the far cost of complexity, TVR", "timestamp": "00:10:25,320", "timestamp_s": 625.0}, {"text": "automation techniques.", "timestamp": "00:10:31,330", "timestamp_s": 631.0}, {"text": "Now let\u0027s get to the core software automation techniques, especially", "timestamp": "00:10:32,830", "timestamp_s": 632.0}, {"text": "critical for large models like LLMs.", "timestamp": "00:10:36,430", "timestamp_s": 636.0}, {"text": "On the edge.", "timestamp": "00:10:38,410", "timestamp_s": 638.0}, {"text": "The most crucial technique and the one we will spend significant time", "timestamp": "00:10:39,490", "timestamp_s": 639.0}, {"text": "on is ization, reducing the precision of numbers used in the model.", "timestamp": "00:10:43,840", "timestamp_s": 643.0}, {"text": "Next is network pruning, removing redundant pairs of the neural network.", "timestamp": "00:10:49,510", "timestamp_s": 649.0}, {"text": "Then.", "timestamp": "00:10:54,310", "timestamp_s": 654.0}, {"text": "There is a knowledge distillation training a smaller student model", "timestamp": "00:10:55,420", "timestamp_s": 655.0}, {"text": "to mimic a larger teacher model.", "timestamp": "00:10:59,050", "timestamp_s": 659.0}, {"text": "Other methods include low rank approximation or factorization", "timestamp": "00:11:01,420", "timestamp_s": 661.0}, {"text": "and various memory optimizations.", "timestamp": "00:11:05,200", "timestamp_s": 665.0}, {"text": "Today we\u0027ll focus primarily on top three ization, pruning,", "timestamp": "00:11:07,330", "timestamp_s": 667.0}, {"text": "and knowledge distillation.", "timestamp": "00:11:10,930", "timestamp_s": 670.0}, {"text": "Start with ization.", "timestamp": "00:11:13,160", "timestamp_s": 673.0}, {"text": "The fundamental idea is to use fewer bits to represent the model\u0027s weight", "timestamp": "00:11:14,925", "timestamp_s": 674.0}, {"text": "and activations they\u0027re using size and often speeding up comp computation.", "timestamp": "00:11:19,215", "timestamp_s": 679.0}, {"text": "There are several approaches post-training ization.", "timestamp": "00:11:24,595", "timestamp_s": 684.0}, {"text": "PTQ is the simplest.", "timestamp": "00:11:27,595", "timestamp_s": 687.0}, {"text": "We train our model normally, usually in 32 bit floating point numbers,", "timestamp": "00:11:30,745", "timestamp_s": 690.0}, {"text": "and then convert it into lower precision like eight bit in TJ.", "timestamp": "00:11:36,075", "timestamp_s": 696.0}, {"text": "A represented as in eight or even in four.", "timestamp": "00:11:41,120", "timestamp_s": 701.0}, {"text": "Afterwards.", "timestamp": "00:11:43,965", "timestamp_s": 703.0}, {"text": "It\u0027s easier, but might have a moderate accuracy cost.", "timestamp": "00:11:45,195", "timestamp_s": 705.0}, {"text": "Next is ization.", "timestamp": "00:11:50,325", "timestamp_s": 710.0}, {"text": "Our training incorporates the effects of ization during the training process.", "timestamp": "00:11:51,465", "timestamp_s": 711.0}, {"text": "The model learns to be robust to lower precision.", "timestamp": "00:11:56,985", "timestamp_s": 716.0}, {"text": "It\u0027s more complex, but usually preserves better accuracy.", "timestamp": "00:12:00,135", "timestamp_s": 720.0}, {"text": "Then dynamic range ization adjust how contestation is applied", "timestamp": "00:12:04,335", "timestamp_s": 724.0}, {"text": "based on the actual values encountered during inference.", "timestamp": "00:12:08,385", "timestamp_s": 728.0}, {"text": "Offering a balance between accuracy and performance,", "timestamp": "00:12:11,595", "timestamp_s": 731.0}, {"text": "especially good at varying inputs.", "timestamp": "00:12:14,775", "timestamp_s": 734.0}, {"text": "Precis, precis, our ization.", "timestamp": "00:12:17,875", "timestamp_s": 737.0}, {"text": "This visual helps illustrate the concept.", "timestamp": "00:12:20,580", "timestamp_s": 740.0}, {"text": "Standard training uses FP 32.", "timestamp": "00:12:22,980", "timestamp_s": 742.0}, {"text": "Simply converts this F 32 words into eight or in four.", "timestamp": "00:12:26,370", "timestamp_s": 746.0}, {"text": "After training Q 80 simulates this conver conversion during training, allowing the", "timestamp": "00:12:29,970", "timestamp_s": 749.0}, {"text": "model to adapt and advanced technique is mixed to precision deployment.", "timestamp": "00:12:35,760", "timestamp_s": 755.0}, {"text": "Here, you don\u0027t have to ize the entire model uniformly.", "timestamp": "00:12:41,055", "timestamp_s": 761.0}, {"text": "You can analyze which layers are most sensitive to precision loss and keep", "timestamp": "00:12:44,775", "timestamp_s": 764.0}, {"text": "them at higher precision like FP 32 or FP 16, while aggressively contacting", "timestamp": "00:12:48,975", "timestamp_s": 768.0}, {"text": "less sensitive layers to inte or integer.", "timestamp": "00:12:54,525", "timestamp_s": 774.0}, {"text": "Four.", "timestamp": "00:12:57,615", "timestamp_s": 777.0}, {"text": "This offers a fine grain trade off.", "timestamp": "00:12:58,575", "timestamp_s": 778.0}, {"text": "Ization is powerful.", "timestamp": "00:13:01,515", "timestamp_s": 781.0}, {"text": "Moving from floating point 32 to integer eight can make models four times smaller", "timestamp": "00:13:02,745", "timestamp_s": 782.0}, {"text": "and inference three to four times faster.", "timestamp": "00:13:08,235", "timestamp_s": 788.0}, {"text": "Especially on hardware with inte support.", "timestamp": "00:13:11,175", "timestamp_s": 791.0}, {"text": "Inference is a stage where model makes prediction on not seen data.", "timestamp": "00:13:14,595", "timestamp_s": 794.0}, {"text": "Let\u0027s talk about ization technique, PTQ, diving deeper into it.", "timestamp": "00:13:19,105", "timestamp_s": 799.0}, {"text": "What is it?", "timestamp": "00:13:24,565", "timestamp_s": 804.0}, {"text": "Converting pre-trained FP 32 model after training is complete pro.", "timestamp": "00:13:25,165", "timestamp_s": 805.0}, {"text": "It\u0027s similar it\u0027s simpler and faster.", "timestamp": "00:13:30,505", "timestamp_s": 810.0}, {"text": "To implement because we don\u0027t need to modify the training pipeline.", "timestamp": "00:13:33,135", "timestamp_s": 813.0}, {"text": "No retraining is needed.", "timestamp": "00:13:37,185", "timestamp_s": 817.0}, {"text": "Cons, it generally leads to higher accuracy loss compared to QA,", "timestamp": "00:13:38,805", "timestamp_s": 818.0}, {"text": "especially when going to a very low precision like integer four.", "timestamp": "00:13:45,855", "timestamp_s": 825.0}, {"text": "It often requests calibration data set.", "timestamp": "00:13:50,145", "timestamp_s": 830.0}, {"text": "A small representative data set used to determine the best", "timestamp": "00:13:52,785", "timestamp_s": 832.0}, {"text": "way to map the floating point.", "timestamp": "00:13:56,535", "timestamp_s": 836.0}, {"text": "32 ranges to integer or integer four ranges.", "timestamp": "00:13:58,425", "timestamp_s": 838.0}, {"text": "For many models, PT Q2 in eight results in less than 2% accuracy degradation,", "timestamp": "00:14:03,900", "timestamp_s": 843.0}, {"text": "which is often acceptable because h the use cases are like not very generic,", "timestamp": "00:14:11,670", "timestamp_s": 851.0}, {"text": "like this very domain specific actually.", "timestamp": "00:14:17,310", "timestamp_s": 857.0}, {"text": "Why is ization particularly to integer eight, so beneficial for hardware?", "timestamp": "00:14:20,080", "timestamp_s": 860.0}, {"text": "Let\u0027s look at the specs for NVIDIA eight 10 GPU often found", "timestamp": "00:14:24,250", "timestamp_s": 864.0}, {"text": "in servers, but illustrating a common principle in accelerators.", "timestamp": "00:14:28,390", "timestamp_s": 868.0}, {"text": "Notice the performance figure for standard floating point 32 math.", "timestamp": "00:14:32,350", "timestamp_s": 872.0}, {"text": "It delivers 31.2 terra flops, but look at the par integer eight", "timestamp": "00:14:36,430", "timestamp_s": 876.0}, {"text": "performance using its tensor course.", "timestamp": "00:14:41,140", "timestamp_s": 881.0}, {"text": "Two 50 Terra operations per second.", "timestamp": "00:14:43,810", "timestamp_s": 883.0}, {"text": "Potentially up to 500 tops with sparsity features.", "timestamp": "00:14:46,105", "timestamp_s": 886.0}, {"text": "This massive increase in throughput for integer eight operations compared", "timestamp": "00:14:50,155", "timestamp_s": 890.0}, {"text": "to floating point 32 is why position is so effective and so critical.", "timestamp": "00:14:54,295", "timestamp_s": 894.0}, {"text": "Hardware accelerators often specifically designed to perform low precision", "timestamp": "00:14:59,275", "timestamp_s": 899.0}, {"text": "integer math much faster, and more power efficiently than a floating point math", "timestamp": "00:15:04,105", "timestamp_s": 904.0}, {"text": "ization technique.", "timestamp": "00:15:10,395", "timestamp_s": 910.0}, {"text": "What is it simulating the effects of conversation during", "timestamp": "00:15:12,115", "timestamp_s": 912.0}, {"text": "the model training process?", "timestamp": "00:15:15,055", "timestamp_s": 915.0}, {"text": "It generally achieves better accuracy preservation compared to PTQ,", "timestamp": "00:15:16,855", "timestamp_s": 916.0}, {"text": "especially at lower bid depths because the model learns to compensate for", "timestamp": "00:15:21,265", "timestamp_s": 921.0}, {"text": "the precision laws during training.", "timestamp": "00:15:25,965", "timestamp_s": 925.0}, {"text": "It.", "timestamp": "00:15:28,785", "timestamp_s": 928.0}, {"text": "What are the cons?", "timestamp": "00:15:29,265", "timestamp_s": 929.0}, {"text": "It\u0027s more complex to implement as you need to modify your.", "timestamp": "00:15:30,165", "timestamp_s": 930.0}, {"text": "The training code and pipeline, it requires access to the original", "timestamp": "00:15:33,900", "timestamp_s": 933.0}, {"text": "training data and that MA massive infrastructure training time is", "timestamp": "00:15:37,290", "timestamp_s": 937.0}, {"text": "longer two 80, often keep accuracy degradation below 1.5% for integer", "timestamp": "00:15:41,940", "timestamp_s": 941.0}, {"text": "eight, potentially even better than PTQ", "timestamp": "00:15:47,340", "timestamp_s": 947.0}, {"text": "dynamic rate ation.", "timestamp": "00:15:51,160", "timestamp_s": 951.0}, {"text": "Adapting the ization parameters like the scaling factor based on", "timestamp": "00:15:54,400", "timestamp_s": 954.0}, {"text": "the range of activation values observed at runtime, rather than", "timestamp": "00:15:58,480", "timestamp_s": 958.0}, {"text": "using fixed parameters determined offline, what are the pros offers?", "timestamp": "00:16:02,440", "timestamp_s": 962.0}, {"text": "A good balance between the simplicity of PTQ and the accuracy of Q 80.", "timestamp": "00:16:08,380", "timestamp_s": 968.0}, {"text": "It adapts to characteristics of the input data being processed.", "timestamp": "00:16:13,090", "timestamp_s": 973.0}, {"text": "It can also reduce.", "timestamp": "00:16:17,590", "timestamp_s": 977.0}, {"text": "Memory bandwidth needs as activation ranges are determined on the floor.", "timestamp": "00:16:19,225", "timestamp_s": 979.0}, {"text": "What are the cons?", "timestamp": "00:16:24,115", "timestamp_s": 984.0}, {"text": "There can be a potential runtime overhead associated with calculating", "timestamp": "00:16:25,735", "timestamp_s": 985.0}, {"text": "this ranges during inference process.", "timestamp": "00:16:29,455", "timestamp_s": 989.0}, {"text": "Because this runs on the edge, although often minimal, unsupported hardware.", "timestamp": "00:16:31,405", "timestamp_s": 991.0}, {"text": "Next condensation technique mixed to precision.", "timestamp": "00:16:37,475", "timestamp_s": 997.0}, {"text": "Using different numerical precision levels for different", "timestamp": "00:16:41,660", "timestamp_s": 1001.0}, {"text": "layers within the same model.", "timestamp": "00:16:44,690", "timestamp_s": 1004.0}, {"text": "So what are the cons with this approach?", "timestamp": "00:16:48,110", "timestamp_s": 1008.0}, {"text": "Provides a excellent way to balance compression and accuracy.", "timestamp": "00:16:50,090", "timestamp_s": 1010.0}, {"text": "We can keep critical sensitive layers at higher precision.", "timestamp": "00:16:56,060", "timestamp_s": 1016.0}, {"text": "Example, floating point 16 or floating point 32, while izing other more robust.", "timestamp": "00:16:59,150", "timestamp_s": 1019.0}, {"text": "Layers aggressively into integer eight or integer four.", "timestamp": "00:17:05,865", "timestamp_s": 1025.0}, {"text": "It requires careful analysis to determine which layers are sensitive.", "timestamp": "00:17:10,425", "timestamp_s": 1030.0}, {"text": "It also needs framework and hardware support to handle computations", "timestamp": "00:17:14,445", "timestamp_s": 1034.0}, {"text": "involving multiple different data types within the same inference path.", "timestamp": "00:17:18,705", "timestamp_s": 1038.0}, {"text": "Studies have shown potential for significant compression.", "timestamp": "00:17:25,050", "timestamp_s": 1045.0}, {"text": "Example, up to 30 70% with minimal accuracy loss, like less than 1% using", "timestamp": "00:17:27,990", "timestamp_s": 1047.0}, {"text": "mixed precision because when you you convert some of the layers to inte,", "timestamp": "00:17:33,240", "timestamp_s": 1053.0}, {"text": "it offers more model compression.", "timestamp": "00:17:37,735", "timestamp_s": 1057.0}, {"text": "Also,", "timestamp": "00:17:39,980", "timestamp_s": 1059.0}, {"text": "next to major m automation technique is neural network pruning.", "timestamp": "00:17:41,370", "timestamp_s": 1061.0}, {"text": "The core idea is that many large neural networks are over parameterized.", "timestamp": "00:17:46,750", "timestamp_s": 1066.0}, {"text": "They have redundant weights or neurons that don\u0027t contribute", "timestamp": "00:17:51,730", "timestamp_s": 1071.0}, {"text": "much to final output.", "timestamp": "00:17:55,000", "timestamp_s": 1075.0}, {"text": "Crooning aims to identify and remove these non-essential parts.", "timestamp": "00:17:56,890", "timestamp_s": 1076.0}, {"text": "The process generally involves identifying redundancy by analyzing", "timestamp": "00:18:01,030", "timestamp_s": 1081.0}, {"text": "weights or activation patterns, then removing these elements.", "timestamp": "00:18:04,930", "timestamp_s": 1084.0}, {"text": "This can be structured pruning where entire filters or neurons are remote.", "timestamp": "00:18:09,340", "timestamp_s": 1089.0}, {"text": "Often better for hardware speedups or unstructured pruning where", "timestamp": "00:18:13,705", "timestamp_s": 1093.0}, {"text": "individual weights are remote leading to sparks networks.", "timestamp": "00:18:17,665", "timestamp_s": 1097.0}, {"text": "Techniques like the iterative magnitude pruning gradually remove small weights", "timestamp": "00:18:21,805", "timestamp_s": 1101.0}, {"text": "while re retraining the network.", "timestamp": "00:18:26,185", "timestamp_s": 1106.0}, {"text": "To maintain accuracy sensitive allow VT analysis is often ever helps", "timestamp": "00:18:28,765", "timestamp_s": 1108.0}, {"text": "to evaluate how removing certain parts impact overall performance.", "timestamp": "00:18:33,965", "timestamp_s": 1113.0}, {"text": "Effective pruning can reduce models significantly, sometimes 50 to 90%", "timestamp": "00:18:38,825", "timestamp_s": 1118.0}, {"text": "with very minimal impact on accuracy, creating leaner, faster networks.", "timestamp": "00:18:44,285", "timestamp_s": 1124.0}, {"text": "So here\u0027s a overview of the pruning process.", "timestamp": "00:18:52,295", "timestamp_s": 1132.0}, {"text": "Step one, training initial network.", "timestamp": "00:18:56,555", "timestamp_s": 1136.0}, {"text": "We start by training a potentially over network until it converges", "timestamp": "00:18:59,345", "timestamp_s": 1139.0}, {"text": "and performs step two, identify and remove unimportant elements.", "timestamp": "00:19:03,725", "timestamp_s": 1143.0}, {"text": "This is crucial step where you, we apply the criteria like weight", "timestamp": "00:19:07,995", "timestamp_s": 1147.0}, {"text": "magnitude activation, frequency, impact on loss, to decide which parts", "timestamp": "00:19:13,305", "timestamp_s": 1153.0}, {"text": "are unimportant and remove them.", "timestamp": "00:19:17,745", "timestamp_s": 1157.0}, {"text": "This might involve setting weights to zero, unstructured and removing", "timestamp": "00:19:19,935", "timestamp_s": 1159.0}, {"text": "entire structures like filters, which is a structured way.", "timestamp": "00:19:23,475", "timestamp_s": 1163.0}, {"text": "Then step three, fine tune, prune network.", "timestamp": "00:19:27,600", "timestamp_s": 1167.0}, {"text": "After removing elements, the network\u0027s performance usually", "timestamp": "00:19:30,750", "timestamp_s": 1170.0}, {"text": "drops, so we retain the smaller prune network for a few cycles.", "timestamp": "00:19:34,020", "timestamp_s": 1174.0}, {"text": "This allows the remaining weights to adapt and often recovers most,", "timestamp": "00:19:39,650", "timestamp_s": 1179.0}, {"text": "if not all of the lowest accuracy.", "timestamp": "00:19:43,580", "timestamp_s": 1183.0}, {"text": "This cycle of prune and fine tune can be repeated iteratively to reach", "timestamp": "00:19:46,610", "timestamp_s": 1186.0}, {"text": "a desired level of pars and size.", "timestamp": "00:19:51,320", "timestamp_s": 1191.0}, {"text": "Let\u0027s look at specific pruning types.", "timestamp": "00:19:54,330", "timestamp_s": 1194.0}, {"text": "Magnitude based pruning is perhaps the simplest and most common.", "timestamp": "00:19:56,280", "timestamp_s": 1196.0}, {"text": "It operates at the granularity of either individual weights, weight pruning,", "timestamp": "00:20:00,570", "timestamp_s": 1200.0}, {"text": "which is unstructured or entire units, neurons or filters, which are structured.", "timestamp": "00:20:04,840", "timestamp_s": 1204.0}, {"text": "Pruning", "timestamp": "00:20:09,070", "timestamp_s": 1209.0}, {"text": "For weight pruning we simply remove weights with the lower absolute", "timestamp": "00:20:11,410", "timestamp_s": 1211.0}, {"text": "values closest to zero, assuming they contribute least for structured pruning.", "timestamp": "00:20:15,200", "timestamp_s": 1215.0}, {"text": "We might remove entire filters based on the sum or average", "timestamp": "00:20:20,645", "timestamp_s": 1220.0}, {"text": "magnitude of their weights.", "timestamp": "00:20:25,055", "timestamp_s": 1225.0}, {"text": "And the result, unstructured pruning leads to sparse irregular weight", "timestamp": "00:20:28,625", "timestamp_s": 1228.0}, {"text": "mattresses, structured pruning results in smaller but still dense", "timestamp": "00:20:32,465", "timestamp_s": 1232.0}, {"text": "mattresses, which are often easier for standard hardware to accelerate.", "timestamp": "00:20:36,515", "timestamp_s": 1236.0}, {"text": "What are the pros?", "timestamp": "00:20:41,800", "timestamp_s": 1241.0}, {"text": "Simple concept, high compression potential, especially unstructured.", "timestamp": "00:20:43,385", "timestamp_s": 1243.0}, {"text": "Structured pruning often gives better inference setups on standard hardware.", "timestamp": "00:20:47,360", "timestamp_s": 1247.0}, {"text": "What are the cons?", "timestamp": "00:20:52,700", "timestamp_s": 1252.0}, {"text": "Unstructured pruning often needs specialized hardware or libraries for", "timestamp": "00:20:53,570", "timestamp_s": 1253.0}, {"text": "speed up while structured pruning is ser and might impact accuracy, more", "timestamp": "00:20:57,740", "timestamp_s": 1257.0}, {"text": "pruning technique.", "timestamp": "00:21:05,430", "timestamp_s": 1265.0}, {"text": "Importance based beyond just magnitude important based pruning", "timestamp": "00:21:06,780", "timestamp_s": 1266.0}, {"text": "uses more sophisticated metrices to decide what to remove.", "timestamp": "00:21:10,830", "timestamp_s": 1270.0}, {"text": "What is a mechanism?", "timestamp": "00:21:17,520", "timestamp_s": 1277.0}, {"text": "Instead of just looking at the weight values, it might consider", "timestamp": "00:21:18,660", "timestamp_s": 1278.0}, {"text": "the effect on the loss function.", "timestamp": "00:21:21,540", "timestamp_s": 1281.0}, {"text": "If the weight were remote, using techniques like tailor expansion", "timestamp": "00:21:23,100", "timestamp_s": 1283.0}, {"text": "or optimal brain damage, or other algorithms are they analyzed", "timestamp": "00:21:26,310", "timestamp_s": 1286.0}, {"text": "neural neuron activation.", "timestamp": "00:21:31,700", "timestamp_s": 1291.0}, {"text": "Or analyze gradient information during training.", "timestamp": "00:21:33,955", "timestamp_s": 1293.0}, {"text": "Goal is to make a more informed, sophisticated selection of which elements", "timestamp": "00:21:37,165", "timestamp_s": 1297.0}, {"text": "are truly unimportant for the need for function, potentially preserving accuracy", "timestamp": "00:21:41,905", "timestamp_s": 1301.0}, {"text": "better than simple magnitude training", "timestamp": "00:21:46,735", "timestamp_s": 1306.0}, {"text": "based pruning technique.", "timestamp": "00:21:49,475", "timestamp_s": 1309.0}, {"text": "What\u0027s the mechanism we use here?", "timestamp": "00:21:52,805", "timestamp_s": 1312.0}, {"text": "As we see in the process diagram, it involves repeating the cycles of prune.", "timestamp": "00:21:54,635", "timestamp_s": 1314.0}, {"text": "Fine tune.", "timestamp": "00:21:58,595", "timestamp_s": 1318.0}, {"text": "We remove a small percentage of weights or neurons, retain briefly, then", "timestamp": "00:22:00,275", "timestamp_s": 1320.0}, {"text": "remove some more written and so on.", "timestamp": "00:22:04,475", "timestamp_s": 1324.0}, {"text": "The goal is to reach the targets parity or size gradually, rather", "timestamp": "00:22:07,445", "timestamp_s": 1327.0}, {"text": "than removing everything at once.", "timestamp": "00:22:10,745", "timestamp_s": 1330.0}, {"text": "This gradual approach generally leads to better accuracy prevention", "timestamp": "00:22:13,475", "timestamp_s": 1333.0}, {"text": "for a given level of parity compared to one shot pruning.", "timestamp": "00:22:17,345", "timestamp_s": 1337.0}, {"text": "What are the trade-offs it\u0027s most time consuming due to multiple", "timestamp": "00:22:22,745", "timestamp_s": 1342.0}, {"text": "pruning and retraining steps?", "timestamp": "00:22:25,805", "timestamp_s": 1345.0}, {"text": "A fascinating related concept is lottery ticket hypothesis.", "timestamp": "00:22:30,185", "timestamp_s": 1350.0}, {"text": "It proposes that dense randomly initiated networks contains spar", "timestamp": "00:22:34,835", "timestamp_s": 1354.0}, {"text": "subnets called winning tickets.", "timestamp": "00:22:39,365", "timestamp_s": 1359.0}, {"text": "That.", "timestamp": "00:22:41,045", "timestamp_s": 1361.0}, {"text": "Trained in isolation from the start, using their original initial", "timestamp": "00:22:42,035", "timestamp_s": 1362.0}, {"text": "waste can achieve performance comparable to full dense network", "timestamp": "00:22:45,485", "timestamp_s": 1365.0}, {"text": "magnitude Pruning, especially iterative pruning, is effectively", "timestamp": "00:22:51,815", "timestamp_s": 1371.0}, {"text": "a method to find these winning tickets within the large network.", "timestamp": "00:22:55,715", "timestamp_s": 1375.0}, {"text": "What are the implication?", "timestamp": "00:23:00,275", "timestamp_s": 1380.0}, {"text": "It suggests that the inherent sparsity might be fundamental", "timestamp": "00:23:01,655", "timestamp_s": 1381.0}, {"text": "property of trainable neural networks.", "timestamp": "00:23:05,045", "timestamp_s": 1385.0}, {"text": "We don\u0027t necessarily need the huge dense network.", "timestamp": "00:23:07,955", "timestamp_s": 1387.0}, {"text": "We just need to find the right sparse structure within it.", "timestamp": "00:23:11,045", "timestamp_s": 1391.0}, {"text": "This provides theoretical backing for why pruning can be so effective", "timestamp": "00:23:14,615", "timestamp_s": 1394.0}, {"text": "without catastrophic accuracy loss, which is important for the H", "timestamp": "00:23:18,725", "timestamp_s": 1398.0}, {"text": "loss technique, knowledge distillation for the h. Imagine we have a large,", "timestamp": "00:23:23,175", "timestamp_s": 1403.0}, {"text": "complex, highly accurate teacher model.", "timestamp": "00:23:29,130", "timestamp_s": 1409.0}, {"text": "It performs great, but it\u0027s too slow or large for the edge.", "timestamp": "00:23:31,950", "timestamp_s": 1411.0}, {"text": "We also have a much smaller student model, which is lightweight enough", "timestamp": "00:23:37,050", "timestamp_s": 1417.0}, {"text": "for the edge deployment, but might not be as accurate on its own knowledge.", "timestamp": "00:23:40,590", "timestamp_s": 1420.0}, {"text": "Stands for is a distillation process, which works by training the.", "timestamp": "00:23:46,170", "timestamp_s": 1426.0}, {"text": "Student model, not just on the ground truth labels, but also to", "timestamp": "00:23:52,855", "timestamp_s": 1432.0}, {"text": "mimic the output of teacher model.", "timestamp": "00:23:56,545", "timestamp_s": 1436.0}, {"text": "Specifically, the student learns from the teacher soft targets", "timestamp": "00:23:58,645", "timestamp_s": 1438.0}, {"text": "the full potential distribution across all classes that teacher", "timestamp": "00:24:02,365", "timestamp_s": 1442.0}, {"text": "predicts even for incorrect classes.", "timestamp": "00:24:05,485", "timestamp_s": 1445.0}, {"text": "This encodes richer information about how the teacher model thinks and generalizes.", "timestamp": "00:24:08,695", "timestamp_s": 1448.0}, {"text": "This allows the small student model to benefit from the knowledge learned by", "timestamp": "00:24:13,840", "timestamp_s": 1453.0}, {"text": "the much larger teacher, often achieving significant better accuracy than if", "timestamp": "00:24:17,770", "timestamp_s": 1457.0}, {"text": "it were trained only on hard levels.", "timestamp": "00:24:22,750", "timestamp_s": 1462.0}, {"text": "So for knowledge, distillation based technique based on the type input,", "timestamp": "00:24:26,000", "timestamp_s": 1466.0}, {"text": "this is the most common type, focusing on matching the output", "timestamp": "00:24:32,160", "timestamp_s": 1472.0}, {"text": "layer of the student to the teacher.", "timestamp": "00:24:35,580", "timestamp_s": 1475.0}, {"text": "Using those soft labels that were mentioned, it captures the teacher\u0027s", "timestamp": "00:24:38,100", "timestamp_s": 1478.0}, {"text": "final pre pre prediction reasoning.", "timestamp": "00:24:42,000", "timestamp_s": 1482.0}, {"text": "Example show a significant parameter reduction while retaining high", "timestamp": "00:24:44,875", "timestamp_s": 1484.0}, {"text": "quality up to 95% of teacher.", "timestamp": "00:24:49,075", "timestamp_s": 1489.0}, {"text": "This goes deeper trying to mass the activations in intermediate", "timestamp": "00:24:53,035", "timestamp_s": 1493.0}, {"text": "layers of the student and teacher.", "timestamp": "00:24:56,215", "timestamp_s": 1496.0}, {"text": "The goal is to encourage the student to develop similar internal feature", "timestamp": "00:24:58,435", "timestamp_s": 1498.0}, {"text": "representations as that of the teacher.", "timestamp": "00:25:02,335", "timestamp_s": 1502.0}, {"text": "We can also categorize distillation by how the training happens.", "timestamp": "00:25:05,345", "timestamp_s": 1505.0}, {"text": "This is the standard approach.", "timestamp": "00:25:08,855", "timestamp_s": 1508.0}, {"text": "First, train the teacher model completely, and then we can, we", "timestamp": "00:25:10,895", "timestamp_s": 1510.0}, {"text": "use the frozen pre-trained teacher model to train the student.", "timestamp": "00:25:15,275", "timestamp_s": 1515.0}, {"text": "Then there is online, offline, online distillation.", "timestamp": "00:25:18,995", "timestamp_s": 1518.0}, {"text": "Here the teacher and student models are trained simultaneously.", "timestamp": "00:25:21,575", "timestamp_s": 1521.0}, {"text": "They learn together, potentially influencing each other, which can", "timestamp": "00:25:25,295", "timestamp_s": 1525.0}, {"text": "sometimes lead to better results than the sequential offline train approach.", "timestamp": "00:25:28,625", "timestamp_s": 1528.0}, {"text": "An interesting variant where a model distills knowledge from itself.", "timestamp": "00:25:34,595", "timestamp_s": 1534.0}, {"text": "Often deeper layers of network act as a teacher for shallower", "timestamp": "00:25:39,335", "timestamp_s": 1539.0}, {"text": "layers acting as a student.", "timestamp": "00:25:43,535", "timestamp_s": 1543.0}, {"text": "This can improve the performance of the single model without", "timestamp": "00:25:46,055", "timestamp_s": 1546.0}, {"text": "needing a separate teacher used.", "timestamp": "00:25:49,775", "timestamp_s": 1549.0}, {"text": "This is used when there is a very large gap between teacher and", "timestamp": "00:25:53,945", "timestamp_s": 1553.0}, {"text": "student size or model capabilities.", "timestamp": "00:25:57,635", "timestamp_s": 1557.0}, {"text": "We\u0027re optimizing existing large models.", "timestamp": "00:26:00,825", "timestamp_s": 1560.0}, {"text": "Another crucial strategy is to use edge optimized model", "timestamp": "00:26:03,760", "timestamp_s": 1563.0}, {"text": "architectures from the start.", "timestamp": "00:26:07,000", "timestamp_s": 1567.0}, {"text": "The, these are neural network designs created specifically for efficiency", "timestamp": "00:26:09,310", "timestamp_s": 1569.0}, {"text": "on resource constraint devices.", "timestamp": "00:26:13,810", "timestamp_s": 1573.0}, {"text": "Example includes mobile nets.", "timestamp": "00:26:15,700", "timestamp_s": 1575.0}, {"text": "Famous for using depth device separable convolution, which drastically reduces", "timestamp": "00:26:18,190", "timestamp_s": 1578.0}, {"text": "computations like eight to nine times compared to standard convolution.", "timestamp": "00:26:24,010", "timestamp_s": 1584.0}, {"text": "Great for vision tasks, shuffle nets, use channel shuffling and group convolution", "timestamp": "00:26:27,790", "timestamp_s": 1587.0}, {"text": "further automations for low power devices.", "timestamp": "00:26:33,300", "timestamp_s": 1593.0}, {"text": "And then squeeze net employs fire modules to reduce parameter counts while", "timestamp": "00:26:36,685", "timestamp_s": 1596.0}, {"text": "maintaining classification accuracy.", "timestamp": "00:26:41,975", "timestamp_s": 1601.0}, {"text": "Then the Efficient Net uses a compounding scaling method to", "timestamp": "00:26:44,705", "timestamp_s": 1604.0}, {"text": "intelligently balance network with depth and input resolution for excellent", "timestamp": "00:26:48,905", "timestamp_s": 1608.0}, {"text": "efficiency and accuracy trade-offs.", "timestamp": "00:26:54,485", "timestamp_s": 1614.0}, {"text": "Using this pre-built architectures is often more effective than simply", "timestamp": "00:26:57,575", "timestamp_s": 1617.0}, {"text": "thinking a standard large architecture as they incorporate efficiency", "timestamp": "00:27:01,715", "timestamp_s": 1621.0}, {"text": "principles directly into their design.", "timestamp": "00:27:06,095", "timestamp_s": 1626.0}, {"text": "Let\u0027s look at a practical example, an H Voice assistant using a combination", "timestamp": "00:27:10,445", "timestamp_s": 1630.0}, {"text": "of the techniques, likely neural network pruning, and eight bit ization.", "timestamp": "00:27:15,215", "timestamp_s": 1635.0}, {"text": "A baseline model was optimized.", "timestamp": "00:27:19,295", "timestamp_s": 1639.0}, {"text": "The results.", "timestamp": "00:27:21,635", "timestamp_s": 1641.0}, {"text": "The optimized model achieved 98.2% wake word accuracy with a very low", "timestamp": "00:27:22,625", "timestamp_s": 1642.0}, {"text": "false activation rate under 0.5%.", "timestamp": "00:27:27,965", "timestamp_s": 1647.0}, {"text": "It managed 87.3 command recognition accuracy across", "timestamp": "00:27:31,025", "timestamp_s": 1651.0}, {"text": "different noisy environments.", "timestamp": "00:27:35,915", "timestamp_s": 1655.0}, {"text": "Crucially, the model size for the entire system Wake.", "timestamp": "00:27:38,930", "timestamp_s": 1658.0}, {"text": "Workplace command recognition was reduced to just 76 mb, a 73% reduction", "timestamp": "00:27:41,630", "timestamp_s": 1661.0}, {"text": "from baseline, and the response latency was only 85 milliseconds.", "timestamp": "00:27:47,570", "timestamp_s": 1667.0}, {"text": "End to end well within the threshold for a real time fee.", "timestamp": "00:27:52,010", "timestamp_s": 1672.0}, {"text": "Knowledge distillation from a larger teacher model likely help", "timestamp": "00:27:56,000", "timestamp_s": 1676.0}, {"text": "maintain the higher accuracy despite the significant size reduction.", "timestamp": "00:27:59,270", "timestamp_s": 1679.0}, {"text": "This demonstrates how these techniques deliver tangible, real world benchmark", "timestamp": "00:28:04,010", "timestamp_s": 1684.0}, {"text": "performance benefits on the edge.", "timestamp": "00:28:09,020", "timestamp_s": 1689.0}, {"text": "Takeaway and implementation details go What are the key table covers?", "timestamp": "00:28:12,240", "timestamp_s": 1692.0}, {"text": "First, establish performance targets.", "timestamp": "00:28:17,775", "timestamp_s": 1697.0}, {"text": "Be clear about the latency, accuracy, power, size, budgets before the", "timestamp": "00:28:20,205", "timestamp_s": 1700.0}, {"text": "start start of optimization.", "timestamp": "00:28:24,805", "timestamp_s": 1704.0}, {"text": "Secondly, apply the integrated optimization.", "timestamp": "00:28:26,835", "timestamp_s": 1706.0}, {"text": "Don\u0027t just rely on just one technique.", "timestamp": "00:28:29,715", "timestamp_s": 1709.0}, {"text": "Combine pruning, ization, distillation, and potentially architectural choice.", "timestamp": "00:28:32,025", "timestamp_s": 1712.0}, {"text": "For the best reserves, their effects are often multi multiplicative.", "timestamp": "00:28:36,705", "timestamp_s": 1716.0}, {"text": "Third test on target hardware ator arent enough.", "timestamp": "00:28:41,565", "timestamp_s": 1721.0}, {"text": "Benchmark and profile directly on the edge devices we deploy onto", "timestamp": "00:28:45,105", "timestamp_s": 1725.0}, {"text": "to find the real bottlenecks.", "timestamp": "00:28:49,635", "timestamp_s": 1729.0}, {"text": "Fourth, iterate with real world data.", "timestamp": "00:28:51,495", "timestamp_s": 1731.0}, {"text": "Edge environments can be unpredictable, continuously collect data and refine the", "timestamp": "00:28:54,075", "timestamp_s": 1734.0}, {"text": "models after the deployment successfully.", "timestamp": "00:28:59,175", "timestamp_s": 1739.0}, {"text": "Deploying the generative AI at the edge needs the systematic, iterative approach.", "timestamp": "00:29:01,725", "timestamp_s": 1741.0}, {"text": "Considering the whole pipeline,", "timestamp": "00:29:06,525", "timestamp_s": 1746.0}, {"text": "a quick word on benchmarking, it\u0027s absolutely essential.", "timestamp": "00:29:09,025", "timestamp_s": 1749.0}, {"text": "To know this, we need to define metrics clearly.", "timestamp": "00:29:11,725", "timestamp_s": 1751.0}, {"text": "Latency, throughput, accuracy, power, create a standardized test environment,", "timestamp": "00:29:14,935", "timestamp_s": 1754.0}, {"text": "simulating real deployment conditions, measure performance comprehensively", "timestamp": "00:29:19,255", "timestamp_s": 1759.0}, {"text": "across different hardware and workloads, and critically use benchmark results to", "timestamp": "00:29:23,665", "timestamp_s": 1763.0}, {"text": "further guide optimization iterations.", "timestamp": "00:29:28,090", "timestamp_s": 1768.0}, {"text": "Systematic benchmarking validates the automation efforts and ensure", "timestamp": "00:29:30,865", "timestamp_s": 1770.0}, {"text": "the solutions perform reliably and consistently in the real world.", "timestamp": "00:29:34,825", "timestamp_s": 1774.0}, {"text": "This is important because if you have seen, there are multiple choices to make.", "timestamp": "00:29:38,335", "timestamp_s": 1778.0}, {"text": "So what choices need to be made and which techniques, which combination", "timestamp": "00:29:43,115", "timestamp_s": 1783.0}, {"text": "of techniques needs to be done?", "timestamp": "00:29:47,835", "timestamp_s": 1787.0}, {"text": "We can be only determined if we have a standard benchmarking, so we can run this", "timestamp": "00:29:49,515", "timestamp_s": 1789.0}, {"text": "iteratively and find if the combination actually reaches our benchmarks.", "timestamp": "00:29:54,015", "timestamp_s": 1794.0}, {"text": "Then finally, the implementation roadmap.", "timestamp": "00:29:59,485", "timestamp_s": 1799.0}, {"text": "What are the practical steps if we have to do this First audit model", "timestamp": "00:30:02,665", "timestamp_s": 1802.0}, {"text": "requirements, understand the Constance and performance needs, and then prototype", "timestamp": "00:30:06,865", "timestamp_s": 1806.0}, {"text": "optimization pipelines, test techniques individually first to see their impact", "timestamp": "00:30:11,915", "timestamp_s": 1811.0}, {"text": "and implement combined approach.", "timestamp": "00:30:16,895", "timestamp_s": 1816.0}, {"text": "Integrate the chosen techniques, pruning, ization, distillation", "timestamp": "00:30:19,055", "timestamp_s": 1819.0}, {"text": "carefully, and deploy and monitor.", "timestamp": "00:30:22,385", "timestamp_s": 1822.0}, {"text": "The observability aspect.", "timestamp": "00:30:25,125", "timestamp_s": 1825.0}, {"text": "Realize the optimized model, but but include telemetry to gather", "timestamp": "00:30:26,600", "timestamp_s": 1826.0}, {"text": "real world data performance data for continuous improvement.", "timestamp": "00:30:30,630", "timestamp_s": 1830.0}, {"text": "Start with audit, build systematically, validate thoroughly,", "timestamp": "00:30:35,050", "timestamp_s": 1835.0}, {"text": "and monitor continuously.", "timestamp": "00:30:38,500", "timestamp_s": 1838.0}, {"text": "That concludes our look into engineering, low latency, generative AI for the edge.", "timestamp": "00:30:42,550", "timestamp_s": 1842.0}, {"text": "By applying techniques like ization pruning, knowledge distillation, and", "timestamp": "00:30:47,020", "timestamp_s": 1847.0}, {"text": "choosing appropriate architectures and hardware, we can overcome resource", "timestamp": "00:30:50,980", "timestamp_s": 1850.0}, {"text": "constraints and unlock powerful AI K capabilities directly on the edge.", "timestamp": "00:30:55,030", "timestamp_s": 1855.0}, {"text": "Thank you for your time and attention, and have a good day.", "timestamp": "00:30:59,980", "timestamp_s": 1859.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: '7QdWz6xIBY8',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Edge-Ready GenAI: Engineering Low-Latency Solutions for Resource-Constrained Environments
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Unlock the secrets to deploying powerful generative AI on edge devices with minimal resources. Learn practical compression techniques that dramatically reduce latency while maintaining performance. Transform your edge devices into AI powerhousesâ€”no cloud required.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./srt/ml2025_Sai_KR_Pentaparthi.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:01,000'); seek(1.0)">
              Morning or afternoon everyone.
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:02,830'); seek(2.0)">
              Today we are diving into a critical and exciting area, HDD generative ai.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:09,400'); seek(9.0)">
              Specifically we'll be exploring how to engineer low latency AI solutions
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:14,410'); seek(14.0)">
              designed for resource constrained environments as businesses increasingly
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:19,510'); seek(19.0)">
              look for real-time AI capabilities, right where the action happens at the edge.
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:24,849'); seek(24.0)">
              Understanding how to optimize these powerful generative
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:27,759'); seek(27.0)">
              models becomes essential.
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:30,310'); seek(30.0)">
              We look at systematic ways to maintain performance while cutting
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:34,269'); seek(34.0)">
              down on computational needs, power consumption, and latency.
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:39,099'); seek(39.0)">
              This opens up fascinating new possibilities for intelligent
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:42,250'); seek(42.0)">
              applications directly on edge devices.
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:00:45,649'); seek(45.0)">
              Just a little bit about myself.
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:00:47,999'); seek(47.0)">
              I'm Ian, principal software engineer at SD Engineering.
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:00:52,349'); seek(52.0)">
              I direct my work focuses on enhancing global connectivity through advanced
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:00:56,999'); seek(56.0)">
              IP satellite network infrastructure.
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:00:59,969'); seek(59.0)">
              My experience in network engineering and software development gives me a
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:04,079'); seek(64.0)">
              practical perspective on challenges and opportunities of deploying
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:07,919'); seek(67.0)">
              complex systems like AI in diverse environments, including the Edge.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:13,470'); seek(73.0)">
              You can find more details or connect with me on LinkedIn.
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:17,470'); seek(77.0)">
              So why is Edge AI challenging?
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:19,840'); seek(79.0)">
              Let's look at edge computing Frontier.
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:22,750'); seek(82.0)">
              Unlike the seemingly limitless resources in the cloud, edge devices
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:26,679'); seek(86.0)">
              operate under significant constraints.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:29,200'); seek(89.0)">
              First, limited computational resources.
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:32,500'); seek(92.0)">
              Think processing power, memory storage edge devices typically
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:36,340'); seek(96.0)">
              have much less than cloud servers.
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:38,830'); seek(98.0)">
              Second, connectivity challenges.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:41,530'); seek(101.0)">
              Edge solutions often need to work reliably, even with spotty, slow,
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:01:46,150'); seek(106.0)">
              or sometimes no internet connection.
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:01:48,670'); seek(108.0)">
              Third, energy constraints.
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:01:51,160'); seek(111.0)">
              Many edge devices run on batteries or have strict power budgets limiting
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:01:55,810'); seek(115.0)">
              how complex our AI models can be.
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:01:58,570'); seek(118.0)">
              And finally, realtime requirements.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:00,160'); seek(120.0)">
              Many edge applications like autonomous vehicles or industrial monitoring
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:05,530'); seek(125.0)">
              demand, immediate low latency responses, putting high performance demands.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:10,105'); seek(130.0)">
              On these constant devices.
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:12,635'); seek(132.0)">
              Despite this the challenges, the demand for Edge AI is booming.
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:17,035'); seek(137.0)">
              Let's look at market trends.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:18,865'); seek(138.0)">
              We are seeing strong expansion.
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:21,535'); seek(141.0)">
              The Edge AI accelerator market, the specialized hardware is growing incredibly
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:26,275'); seek(146.0)">
              fast, nearly 36 39% CHER, projected to hit almost 7.7 billion by 2027.
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:35,120'); seek(155.0)">
              This signals huge demand for on-device ai.
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:38,660'); seek(158.0)">
              What's driving this key adoption drivers are primarily the need for low
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:43,100'); seek(163.0)">
              latency, over 78% of organization site.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:02:46,970'); seek(166.0)">
              This is crucial for realtime responses.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:02:50,570'); seek(170.0)">
              Then data privacy is another major factor.
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:02:53,750'); seek(173.0)">
              Almost 65% of organizations cite this as another major factor.
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:02:59,160'); seek(179.0)">
              Pushing processing locally instead of sending sensitive data to
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:02,100'); seek(182.0)">
              the cloud is important for them.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:05,100'); seek(185.0)">
              And the industry adapting is adapting.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:08,280'); seek(188.0)">
              Model optimization techniques, which we'll discuss heavily today, are
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:11,880'); seek(191.0)">
              already used in 82% of deployments.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:15,060'); seek(195.0)">
              This shows a clear focus on making complex AI run effectively on edge hardware,
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:21,610'); seek(201.0)">
              edge AI market evolution.
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:23,500'); seek(203.0)">
              Let's look ahead.
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:24,805'); seek(204.0)">
              We are really at an inflection point.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:27,010'); seek(207.0)">
              Currently generative AI at the edge is still limited by those resources,
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:30,880'); seek(210.0)">
              resource constraints that were mentioned.
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:33,710'); seek(213.0)">
              But over the next 12 months, we expect increasing adoption of optimized edge
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:39,170'); seek(219.0)">
              AI solutions within eight to 24 months.
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:03:42,050'); seek(222.0)">
              The prediction is that most enterprise need real time edge capabilities
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:03:46,640'); seek(226.0)">
              will be act actively deploying them.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:03:49,040'); seek(229.0)">
              Beyond 2025 H, AI is likely to become ubiquitous, supported by
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:03:54,800'); seek(234.0)">
              specialized hardware acceleration.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:03:57,290'); seek(237.0)">
              This rabbit evolution is driven by hardware advancements, better optimization
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:02,750'); seek(242.0)">
              techniques like composition and pruning, and the growing need for privacy.
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:06,890'); seek(246.0)">
              Preserving local computation.
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:09,140'); seek(249.0)">
              The shift away from cloud dependency for realtime tasks is well underway.
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:14,429'); seek(254.0)">
              Market trends.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:15,899'); seek(255.0)">
              Just to reiterate, the those key market drives drivers because they underscore
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:22,030'); seek(262.0)">
              the why behind the edge optimization.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:25,120'); seek(265.0)">
              The market growth is significant.
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:28,150'); seek(268.0)">
              As mentioned, 39% of CAGR for accelerators and 7.6 billion market cap soon.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:36,150'); seek(276.0)">
              The primary need for low latency, for immediate responses and data privacy, both
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:43,080'); seek(283.0)">
              favoring local on device processing and crucially, the industry is already heavily
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:48,240'); seek(288.0)">
              invested in model optimizations, which is done in like almost 82% of deployments
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:04:53,940'); seek(293.0)">
              to overcome hardware limitations.
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:04:56,310'); seek(296.0)">
              This sets the stage for the techniques we are going to
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:04:59,190'); seek(299.0)">
              explore in the rest of the talk.
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:01,780'); seek(301.0)">
              Acceleration solutions?
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:04,340'); seek(304.0)">
              Software optimization is key Hardware plays a vital role.
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:08,910'); seek(308.0)">
              Specialized hardware acceleration solution dramatically boost performance
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:13,380'); seek(313.0)">
              and energy efficiency for the edge.
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:16,245'); seek(316.0)">
              Yeah.
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:18,360'); seek(318.0)">
              We have neural processing units, N ps, which are dedicated AI accelerators.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:23,340'); seek(323.0)">
              Now common in modern systems on chips they offer 10 to 15 times the
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:29,380'); seek(329.0)">
              energy efficiency for a standard CPU for typical neural network tasks.
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:34,700'); seek(334.0)">
              Google offers HT ps. Purposely built edge accelerators providing
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:40,250'); seek(340.0)">
              significant processing power, like up to four terra operations in small
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:45,050'); seek(345.0)">
              low power around two watts package.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:47,480'); seek(347.0)">
              That's impressive.
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:49,220'); seek(349.0)">
              Then mobile GPUs are also increasingly optimized for AI leveraging their
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:05:54,680'); seek(354.0)">
              parallel processing power, and the FPG accelerators offer re comfortable hardware
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:01,100'); seek(361.0)">
              adapted for specific AI models and often very power efficient in production.
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:07,205'); seek(367.0)">
              These accelerators are designed specifically for the math
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:10,685'); seek(370.0)">
              intensive operations common in ai, like matrix, multiplication,
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:15,495'); seek(375.0)">
              it, GPU's, overview.
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:17,655'); seek(377.0)">
              This table gives a snapshot of the diverse landscape of it, GPU and accelerators.
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:23,715'); seek(383.0)">
              Don't worry about memorizing the details, but notice how the key players like
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:28,305'); seek(388.0)">
              Nvidia Jetson series like Nano Nx, A GX RTX, ADA AMD rise, AI inter core Intel
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:37,750'); seek(397.0)">
              score Ultra and PS and R gpu, Google score, HT ps, Callcom, snapdragons and
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:44,230'); seek(404.0)">
              pos, and specialists like H Cortex.
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:48,355'); seek(408.0)">
              Important takeaway here is the variation in ai performance,
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:52,045'); seek(412.0)">
              memory, capacity, and bandwidth, power consumption, and form factor.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:56,515'); seek(416.0)">
              Choosing the right hardware depends heavily on specific application
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:00,805'); seek(420.0)">
              needs regarding performance, power, budget, and physical size.
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:05,935'); seek(425.0)">
              This plays a critical role in the process of developing these models.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:11,345'); seek(431.0)">
              Now let's look at the inference performance specifically for
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:14,975'); seek(434.0)">
              large language models, which are notoriously resource intensive.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:19,385'); seek(439.0)">
              Again, this table is illustrative based on available data, which can be fragmented.
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:25,640'); seek(445.0)">
              Key things to note.
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:26,930'); seek(446.0)">
              High-end desktop GPOs like RTX 4 0 9 0 or 3 0 9 0 achieve very
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:32,450'); seek(452.0)">
              high throughput tokens per second.
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:34,820'); seek(454.0)">
              Using optimized frameworks like Nvidia sensors R-T-L-L-M significantly
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:39,470'); seek(459.0)">
              outperforming less optimized methods like Lama CCP on the same
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:43,520'); seek(463.0)">
              hardware for it specific hardware like Nvidia Jetson's Orient Series.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:48,860'); seek(468.0)">
              A MD Rise, AI and Intel Core ultra running models like LAMA or SSL
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:54,470'); seek(474.0)">
              seven B, often using ization like integer four is becoming feasible,
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:59,810'); seek(479.0)">
              especially with dedicated frameworks like Tensor, R-T-L-L-M-O-N-N-X.
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:08:03,920'); seek(483.0)">
              Runtime with V is ai.
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:08:06,930'); seek(486.0)">
              Iex, LLM, open V. However, specific standardized benchmarking numbers
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:12,690'); seek(492.0)">
              for throughput and latency on these edge platforms is still emerging
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:16,530'); seek(496.0)">
              and depend heavily on software maturity, on automation levels.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:20,220'); seek(500.0)">
              That's why we see, needed data in the, in specific rows and columns.
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:26,160'); seek(506.0)">
              The power consumption is drastically lower for each devices compared to
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:30,390'); seek(510.0)">
              desktop GPUs highlighting the efficiency gains, but also performance trade-offs.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:36,490'); seek(516.0)">
              Beyond physical hardware.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:38,200'); seek(518.0)">
              Hardware is only part of equation Software specifically, AI frameworks
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:42,970'); seek(522.0)">
              and optimization engines is crucial for actually running models
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:47,200'); seek(527.0)">
              efficiently on the hardware key.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:49,750'); seek(529.0)">
              AI frameworks like TensorFlow Light and PyTorch Mobile are
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:53,860'); seek(533.0)">
              designed for edge deployment.
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:56,080'); seek(536.0)">
              NVIDIA sensor RT and its LLM variant are highly optimized for their GPOs.
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:09:02,095'); seek(542.0)">
              Inference automation engines like ONNX runtime and Intel.
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:09:05,605'); seek(545.0)">
              So open wino.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:09:07,765'); seek(547.0)">
              Focus on optimizing models for efficient execution across various hardware
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:09:14,485'); seek(554.0)">
              vendors.
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:14,965'); seek(554.0)">
              Also provide specific software stacks like Nvidia, Jetpack,
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:18,985'); seek(558.0)">
              A MD Rise, and AI software.
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:21,085'); seek(561.0)">
              Intel's one API Tools and Qualcomm's AI stack, which bundles drivers, libraries,
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:26,875'); seek(566.0)">
              and tools for their respective hardware.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:30,520'); seek(570.0)">
              The right framework involves trade offs.
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:33,085'); seek(573.0)">
              As this chart illustrates TensorFlow light generally offers excellent
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:38,065'); seek(578.0)">
              hardware support through its de delegation system, allowing it to
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:43,155'); seek(583.0)">
              leverage n ps and GPUs effectively, or in an next runtime, often provide
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:48,735'); seek(588.0)">
              slightly better raw execution, speed, and great cross-platform portability.
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:54,690'); seek(594.0)">
              Py Touch Mobile is often placed for its developer experience and
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:59,100'); seek(599.0)">
              see simpler deployment, making it good for rapid prototyping.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:10:03,210'); seek(603.0)">
              TVM Apache TVM can achieve the best performance through deep
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:10:07,530'); seek(607.0)">
              compiler optimizations, but also comes with significantly
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:10:10,980'); seek(610.0)">
              higher deployment complexity.
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:10:13,530'); seek(613.0)">
              So the choice depends on your priorities.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:10:16,380'); seek(616.0)">
              Broad hardware support.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:18,600'); seek(618.0)">
              ITF flight, raw speed and portability.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:21,660'); seek(621.0)">
              Linux runtime is the one to explore ease of use by touch
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:25,320'); seek(625.0)">
              mobile, our maximum performance at the far cost of complexity, TVR
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:31,330'); seek(631.0)">
              automation techniques.
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:32,830'); seek(632.0)">
              Now let's get to the core software automation techniques, especially
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:36,430'); seek(636.0)">
              critical for large models like LLMs.
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:38,410'); seek(638.0)">
              On the edge.
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:39,490'); seek(639.0)">
              The most crucial technique and the one we will spend significant time
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:43,840'); seek(643.0)">
              on is ization, reducing the precision of numbers used in the model.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:49,510'); seek(649.0)">
              Next is network pruning, removing redundant pairs of the neural network.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:54,310'); seek(654.0)">
              Then.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:55,420'); seek(655.0)">
              There is a knowledge distillation training a smaller student model
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:59,050'); seek(659.0)">
              to mimic a larger teacher model.
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:11:01,420'); seek(661.0)">
              Other methods include low rank approximation or factorization
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:11:05,200'); seek(665.0)">
              and various memory optimizations.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:11:07,330'); seek(667.0)">
              Today we'll focus primarily on top three ization, pruning,
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:11:10,930'); seek(670.0)">
              and knowledge distillation.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:13,160'); seek(673.0)">
              Start with ization.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:14,925'); seek(674.0)">
              The fundamental idea is to use fewer bits to represent the model's weight
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:19,215'); seek(679.0)">
              and activations they're using size and often speeding up comp computation.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:24,595'); seek(684.0)">
              There are several approaches post-training ization.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:27,595'); seek(687.0)">
              PTQ is the simplest.
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:30,745'); seek(690.0)">
              We train our model normally, usually in 32 bit floating point numbers,
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:36,075'); seek(696.0)">
              and then convert it into lower precision like eight bit in TJ.
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:41,120'); seek(701.0)">
              A represented as in eight or even in four.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:43,965'); seek(703.0)">
              Afterwards.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:45,195'); seek(705.0)">
              It's easier, but might have a moderate accuracy cost.
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:50,325'); seek(710.0)">
              Next is ization.
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:51,465'); seek(711.0)">
              Our training incorporates the effects of ization during the training process.
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:56,985'); seek(716.0)">
              The model learns to be robust to lower precision.
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:12:00,135'); seek(720.0)">
              It's more complex, but usually preserves better accuracy.
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:12:04,335'); seek(724.0)">
              Then dynamic range ization adjust how contestation is applied
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:08,385'); seek(728.0)">
              based on the actual values encountered during inference.
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:11,595'); seek(731.0)">
              Offering a balance between accuracy and performance,
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:14,775'); seek(734.0)">
              especially good at varying inputs.
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:17,875'); seek(737.0)">
              Precis, precis, our ization.
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:20,580'); seek(740.0)">
              This visual helps illustrate the concept.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:22,980'); seek(742.0)">
              Standard training uses FP 32.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:26,370'); seek(746.0)">
              Simply converts this F 32 words into eight or in four.
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:29,970'); seek(749.0)">
              After training Q 80 simulates this conver conversion during training, allowing the
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:35,760'); seek(755.0)">
              model to adapt and advanced technique is mixed to precision deployment.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:41,055'); seek(761.0)">
              Here, you don't have to ize the entire model uniformly.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:44,775'); seek(764.0)">
              You can analyze which layers are most sensitive to precision loss and keep
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:48,975'); seek(768.0)">
              them at higher precision like FP 32 or FP 16, while aggressively contacting
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:54,525'); seek(774.0)">
              less sensitive layers to inte or integer.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:57,615'); seek(777.0)">
              Four.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:58,575'); seek(778.0)">
              This offers a fine grain trade off.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:13:01,515'); seek(781.0)">
              Ization is powerful.
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:13:02,745'); seek(782.0)">
              Moving from floating point 32 to integer eight can make models four times smaller
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:08,235'); seek(788.0)">
              and inference three to four times faster.
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:11,175'); seek(791.0)">
              Especially on hardware with inte support.
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:14,595'); seek(794.0)">
              Inference is a stage where model makes prediction on not seen data.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:19,105'); seek(799.0)">
              Let's talk about ization technique, PTQ, diving deeper into it.
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:24,565'); seek(804.0)">
              What is it?
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:25,165'); seek(805.0)">
              Converting pre-trained FP 32 model after training is complete pro.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:30,505'); seek(810.0)">
              It's similar it's simpler and faster.
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:33,135'); seek(813.0)">
              To implement because we don't need to modify the training pipeline.
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:37,185'); seek(817.0)">
              No retraining is needed.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:38,805'); seek(818.0)">
              Cons, it generally leads to higher accuracy loss compared to QA,
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:45,855'); seek(825.0)">
              especially when going to a very low precision like integer four.
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:50,145'); seek(830.0)">
              It often requests calibration data set.
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:52,785'); seek(832.0)">
              A small representative data set used to determine the best
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:56,535'); seek(836.0)">
              way to map the floating point.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:58,425'); seek(838.0)">
              32 ranges to integer or integer four ranges.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:14:03,900'); seek(843.0)">
              For many models, PT Q2 in eight results in less than 2% accuracy degradation,
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:11,670'); seek(851.0)">
              which is often acceptable because h the use cases are like not very generic,
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:17,310'); seek(857.0)">
              like this very domain specific actually.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:20,080'); seek(860.0)">
              Why is ization particularly to integer eight, so beneficial for hardware?
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:24,250'); seek(864.0)">
              Let's look at the specs for NVIDIA eight 10 GPU often found
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:28,390'); seek(868.0)">
              in servers, but illustrating a common principle in accelerators.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:32,350'); seek(872.0)">
              Notice the performance figure for standard floating point 32 math.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:36,430'); seek(876.0)">
              It delivers 31.2 terra flops, but look at the par integer eight
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:41,140'); seek(881.0)">
              performance using its tensor course.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:43,810'); seek(883.0)">
              Two 50 Terra operations per second.
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:46,105'); seek(886.0)">
              Potentially up to 500 tops with sparsity features.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:50,155'); seek(890.0)">
              This massive increase in throughput for integer eight operations compared
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:54,295'); seek(894.0)">
              to floating point 32 is why position is so effective and so critical.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:59,275'); seek(899.0)">
              Hardware accelerators often specifically designed to perform low precision
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:15:04,105'); seek(904.0)">
              integer math much faster, and more power efficiently than a floating point math
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:15:10,395'); seek(910.0)">
              ization technique.
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:15:12,115'); seek(912.0)">
              What is it simulating the effects of conversation during
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:15,055'); seek(915.0)">
              the model training process?
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:16,855'); seek(916.0)">
              It generally achieves better accuracy preservation compared to PTQ,
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:21,265'); seek(921.0)">
              especially at lower bid depths because the model learns to compensate for
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:25,965'); seek(925.0)">
              the precision laws during training.
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:28,785'); seek(928.0)">
              It.
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:29,265'); seek(929.0)">
              What are the cons?
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:30,165'); seek(930.0)">
              It's more complex to implement as you need to modify your.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:33,900'); seek(933.0)">
              The training code and pipeline, it requires access to the original
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:37,290'); seek(937.0)">
              training data and that MA massive infrastructure training time is
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:41,940'); seek(941.0)">
              longer two 80, often keep accuracy degradation below 1.5% for integer
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:47,340'); seek(947.0)">
              eight, potentially even better than PTQ
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:51,160'); seek(951.0)">
              dynamic rate ation.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:54,400'); seek(954.0)">
              Adapting the ization parameters like the scaling factor based on
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:58,480'); seek(958.0)">
              the range of activation values observed at runtime, rather than
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:16:02,440'); seek(962.0)">
              using fixed parameters determined offline, what are the pros offers?
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:16:08,380'); seek(968.0)">
              A good balance between the simplicity of PTQ and the accuracy of Q 80.
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:16:13,090'); seek(973.0)">
              It adapts to characteristics of the input data being processed.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:17,590'); seek(977.0)">
              It can also reduce.
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:19,225'); seek(979.0)">
              Memory bandwidth needs as activation ranges are determined on the floor.
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:24,115'); seek(984.0)">
              What are the cons?
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:25,735'); seek(985.0)">
              There can be a potential runtime overhead associated with calculating
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:29,455'); seek(989.0)">
              this ranges during inference process.
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:31,405'); seek(991.0)">
              Because this runs on the edge, although often minimal, unsupported hardware.
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:37,475'); seek(997.0)">
              Next condensation technique mixed to precision.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:41,660'); seek(1001.0)">
              Using different numerical precision levels for different
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:44,690'); seek(1004.0)">
              layers within the same model.
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:48,110'); seek(1008.0)">
              So what are the cons with this approach?
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:50,090'); seek(1010.0)">
              Provides a excellent way to balance compression and accuracy.
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:56,060'); seek(1016.0)">
              We can keep critical sensitive layers at higher precision.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:59,150'); seek(1019.0)">
              Example, floating point 16 or floating point 32, while izing other more robust.
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:17:05,865'); seek(1025.0)">
              Layers aggressively into integer eight or integer four.
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:17:10,425'); seek(1030.0)">
              It requires careful analysis to determine which layers are sensitive.
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:17:14,445'); seek(1034.0)">
              It also needs framework and hardware support to handle computations
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:18,705'); seek(1038.0)">
              involving multiple different data types within the same inference path.
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:25,050'); seek(1045.0)">
              Studies have shown potential for significant compression.
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:27,990'); seek(1047.0)">
              Example, up to 30 70% with minimal accuracy loss, like less than 1% using
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:33,240'); seek(1053.0)">
              mixed precision because when you you convert some of the layers to inte,
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:37,735'); seek(1057.0)">
              it offers more model compression.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:39,980'); seek(1059.0)">
              Also,
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:41,370'); seek(1061.0)">
              next to major m automation technique is neural network pruning.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:46,750'); seek(1066.0)">
              The core idea is that many large neural networks are over parameterized.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:51,730'); seek(1071.0)">
              They have redundant weights or neurons that don't contribute
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:55,000'); seek(1075.0)">
              much to final output.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:56,890'); seek(1076.0)">
              Crooning aims to identify and remove these non-essential parts.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:18:01,030'); seek(1081.0)">
              The process generally involves identifying redundancy by analyzing
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:18:04,930'); seek(1084.0)">
              weights or activation patterns, then removing these elements.
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:18:09,340'); seek(1089.0)">
              This can be structured pruning where entire filters or neurons are remote.
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:18:13,705'); seek(1093.0)">
              Often better for hardware speedups or unstructured pruning where
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:18:17,665'); seek(1097.0)">
              individual weights are remote leading to sparks networks.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:21,805'); seek(1101.0)">
              Techniques like the iterative magnitude pruning gradually remove small weights
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:26,185'); seek(1106.0)">
              while re retraining the network.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:28,765'); seek(1108.0)">
              To maintain accuracy sensitive allow VT analysis is often ever helps
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:33,965'); seek(1113.0)">
              to evaluate how removing certain parts impact overall performance.
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:38,825'); seek(1118.0)">
              Effective pruning can reduce models significantly, sometimes 50 to 90%
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:44,285'); seek(1124.0)">
              with very minimal impact on accuracy, creating leaner, faster networks.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:52,295'); seek(1132.0)">
              So here's a overview of the pruning process.
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:56,555'); seek(1136.0)">
              Step one, training initial network.
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:59,345'); seek(1139.0)">
              We start by training a potentially over network until it converges
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:19:03,725'); seek(1143.0)">
              and performs step two, identify and remove unimportant elements.
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:19:07,995'); seek(1147.0)">
              This is crucial step where you, we apply the criteria like weight
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:19:13,305'); seek(1153.0)">
              magnitude activation, frequency, impact on loss, to decide which parts
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:19:17,745'); seek(1157.0)">
              are unimportant and remove them.
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:19:19,935'); seek(1159.0)">
              This might involve setting weights to zero, unstructured and removing
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:19:23,475'); seek(1163.0)">
              entire structures like filters, which is a structured way.
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:19:27,600'); seek(1167.0)">
              Then step three, fine tune, prune network.
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:30,750'); seek(1170.0)">
              After removing elements, the network's performance usually
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:34,020'); seek(1174.0)">
              drops, so we retain the smaller prune network for a few cycles.
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:39,650'); seek(1179.0)">
              This allows the remaining weights to adapt and often recovers most,
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:43,580'); seek(1183.0)">
              if not all of the lowest accuracy.
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:46,610'); seek(1186.0)">
              This cycle of prune and fine tune can be repeated iteratively to reach
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:51,320'); seek(1191.0)">
              a desired level of pars and size.
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:54,330'); seek(1194.0)">
              Let's look at specific pruning types.
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:56,280'); seek(1196.0)">
              Magnitude based pruning is perhaps the simplest and most common.
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:20:00,570'); seek(1200.0)">
              It operates at the granularity of either individual weights, weight pruning,
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:20:04,840'); seek(1204.0)">
              which is unstructured or entire units, neurons or filters, which are structured.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:20:09,070'); seek(1209.0)">
              Pruning
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:20:11,410'); seek(1211.0)">
              For weight pruning we simply remove weights with the lower absolute
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:20:15,200'); seek(1215.0)">
              values closest to zero, assuming they contribute least for structured pruning.
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:20:20,645'); seek(1220.0)">
              We might remove entire filters based on the sum or average
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:20:25,055'); seek(1225.0)">
              magnitude of their weights.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:20:28,625'); seek(1228.0)">
              And the result, unstructured pruning leads to sparse irregular weight
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:20:32,465'); seek(1232.0)">
              mattresses, structured pruning results in smaller but still dense
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:36,515'); seek(1236.0)">
              mattresses, which are often easier for standard hardware to accelerate.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:41,800'); seek(1241.0)">
              What are the pros?
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:43,385'); seek(1243.0)">
              Simple concept, high compression potential, especially unstructured.
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:47,360'); seek(1247.0)">
              Structured pruning often gives better inference setups on standard hardware.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:52,700'); seek(1252.0)">
              What are the cons?
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:53,570'); seek(1253.0)">
              Unstructured pruning often needs specialized hardware or libraries for
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:57,740'); seek(1257.0)">
              speed up while structured pruning is ser and might impact accuracy, more
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:21:05,430'); seek(1265.0)">
              pruning technique.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:21:06,780'); seek(1266.0)">
              Importance based beyond just magnitude important based pruning
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:21:10,830'); seek(1270.0)">
              uses more sophisticated metrices to decide what to remove.
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:21:17,520'); seek(1277.0)">
              What is a mechanism?
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:21:18,660'); seek(1278.0)">
              Instead of just looking at the weight values, it might consider
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:21:21,540'); seek(1281.0)">
              the effect on the loss function.
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:21:23,100'); seek(1283.0)">
              If the weight were remote, using techniques like tailor expansion
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:21:26,310'); seek(1286.0)">
              or optimal brain damage, or other algorithms are they analyzed
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:21:31,700'); seek(1291.0)">
              neural neuron activation.
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:21:33,955'); seek(1293.0)">
              Or analyze gradient information during training.
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:37,165'); seek(1297.0)">
              Goal is to make a more informed, sophisticated selection of which elements
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:41,905'); seek(1301.0)">
              are truly unimportant for the need for function, potentially preserving accuracy
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:46,735'); seek(1306.0)">
              better than simple magnitude training
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:49,475'); seek(1309.0)">
              based pruning technique.
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:52,805'); seek(1312.0)">
              What's the mechanism we use here?
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:54,635'); seek(1314.0)">
              As we see in the process diagram, it involves repeating the cycles of prune.
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:58,595'); seek(1318.0)">
              Fine tune.
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:22:00,275'); seek(1320.0)">
              We remove a small percentage of weights or neurons, retain briefly, then
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:22:04,475'); seek(1324.0)">
              remove some more written and so on.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:22:07,445'); seek(1327.0)">
              The goal is to reach the targets parity or size gradually, rather
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:22:10,745'); seek(1330.0)">
              than removing everything at once.
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:22:13,475'); seek(1333.0)">
              This gradual approach generally leads to better accuracy prevention
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:22:17,345'); seek(1337.0)">
              for a given level of parity compared to one shot pruning.
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:22:22,745'); seek(1342.0)">
              What are the trade-offs it's most time consuming due to multiple
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:22:25,805'); seek(1345.0)">
              pruning and retraining steps?
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:22:30,185'); seek(1350.0)">
              A fascinating related concept is lottery ticket hypothesis.
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:34,835'); seek(1354.0)">
              It proposes that dense randomly initiated networks contains spar
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:39,365'); seek(1359.0)">
              subnets called winning tickets.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:41,045'); seek(1361.0)">
              That.
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:42,035'); seek(1362.0)">
              Trained in isolation from the start, using their original initial
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:45,485'); seek(1365.0)">
              waste can achieve performance comparable to full dense network
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:51,815'); seek(1371.0)">
              magnitude Pruning, especially iterative pruning, is effectively
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:55,715'); seek(1375.0)">
              a method to find these winning tickets within the large network.
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:23:00,275'); seek(1380.0)">
              What are the implication?
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:23:01,655'); seek(1381.0)">
              It suggests that the inherent sparsity might be fundamental
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:23:05,045'); seek(1385.0)">
              property of trainable neural networks.
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:23:07,955'); seek(1387.0)">
              We don't necessarily need the huge dense network.
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:23:11,045'); seek(1391.0)">
              We just need to find the right sparse structure within it.
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:23:14,615'); seek(1394.0)">
              This provides theoretical backing for why pruning can be so effective
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:23:18,725'); seek(1398.0)">
              without catastrophic accuracy loss, which is important for the H
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:23:23,175'); seek(1403.0)">
              loss technique, knowledge distillation for the h. Imagine we have a large,
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:29,130'); seek(1409.0)">
              complex, highly accurate teacher model.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:31,950'); seek(1411.0)">
              It performs great, but it's too slow or large for the edge.
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:37,050'); seek(1417.0)">
              We also have a much smaller student model, which is lightweight enough
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:40,590'); seek(1420.0)">
              for the edge deployment, but might not be as accurate on its own knowledge.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:46,170'); seek(1426.0)">
              Stands for is a distillation process, which works by training the.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:52,855'); seek(1432.0)">
              Student model, not just on the ground truth labels, but also to
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:56,545'); seek(1436.0)">
              mimic the output of teacher model.
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:58,645'); seek(1438.0)">
              Specifically, the student learns from the teacher soft targets
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:24:02,365'); seek(1442.0)">
              the full potential distribution across all classes that teacher
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:24:05,485'); seek(1445.0)">
              predicts even for incorrect classes.
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:24:08,695'); seek(1448.0)">
              This encodes richer information about how the teacher model thinks and generalizes.
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:24:13,840'); seek(1453.0)">
              This allows the small student model to benefit from the knowledge learned by
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:24:17,770'); seek(1457.0)">
              the much larger teacher, often achieving significant better accuracy than if
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:24:22,750'); seek(1462.0)">
              it were trained only on hard levels.
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:24:26,000'); seek(1466.0)">
              So for knowledge, distillation based technique based on the type input,
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:24:32,160'); seek(1472.0)">
              this is the most common type, focusing on matching the output
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:35,580'); seek(1475.0)">
              layer of the student to the teacher.
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:24:38,100'); seek(1478.0)">
              Using those soft labels that were mentioned, it captures the teacher's
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:42,000'); seek(1482.0)">
              final pre pre prediction reasoning.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:44,875'); seek(1484.0)">
              Example show a significant parameter reduction while retaining high
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:49,075'); seek(1489.0)">
              quality up to 95% of teacher.
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:53,035'); seek(1493.0)">
              This goes deeper trying to mass the activations in intermediate
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:56,215'); seek(1496.0)">
              layers of the student and teacher.
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:58,435'); seek(1498.0)">
              The goal is to encourage the student to develop similar internal feature
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:25:02,335'); seek(1502.0)">
              representations as that of the teacher.
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:25:05,345'); seek(1505.0)">
              We can also categorize distillation by how the training happens.
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:25:08,855'); seek(1508.0)">
              This is the standard approach.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:25:10,895'); seek(1510.0)">
              First, train the teacher model completely, and then we can, we
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:25:15,275'); seek(1515.0)">
              use the frozen pre-trained teacher model to train the student.
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:25:18,995'); seek(1518.0)">
              Then there is online, offline, online distillation.
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:25:21,575'); seek(1521.0)">
              Here the teacher and student models are trained simultaneously.
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:25:25,295'); seek(1525.0)">
              They learn together, potentially influencing each other, which can
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:25:28,625'); seek(1528.0)">
              sometimes lead to better results than the sequential offline train approach.
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:25:34,595'); seek(1534.0)">
              An interesting variant where a model distills knowledge from itself.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:25:39,335'); seek(1539.0)">
              Often deeper layers of network act as a teacher for shallower
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:25:43,535'); seek(1543.0)">
              layers acting as a student.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:25:46,055'); seek(1546.0)">
              This can improve the performance of the single model without
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:25:49,775'); seek(1549.0)">
              needing a separate teacher used.
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:53,945'); seek(1553.0)">
              This is used when there is a very large gap between teacher and
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:57,635'); seek(1557.0)">
              student size or model capabilities.
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:26:00,825'); seek(1560.0)">
              We're optimizing existing large models.
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:26:03,760'); seek(1563.0)">
              Another crucial strategy is to use edge optimized model
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:26:07,000'); seek(1567.0)">
              architectures from the start.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:26:09,310'); seek(1569.0)">
              The, these are neural network designs created specifically for efficiency
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:26:13,810'); seek(1573.0)">
              on resource constraint devices.
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:26:15,700'); seek(1575.0)">
              Example includes mobile nets.
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:26:18,190'); seek(1578.0)">
              Famous for using depth device separable convolution, which drastically reduces
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:26:24,010'); seek(1584.0)">
              computations like eight to nine times compared to standard convolution.
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:26:27,790'); seek(1587.0)">
              Great for vision tasks, shuffle nets, use channel shuffling and group convolution
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:26:33,300'); seek(1593.0)">
              further automations for low power devices.
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:26:36,685'); seek(1596.0)">
              And then squeeze net employs fire modules to reduce parameter counts while
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:26:41,975'); seek(1601.0)">
              maintaining classification accuracy.
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:26:44,705'); seek(1604.0)">
              Then the Efficient Net uses a compounding scaling method to
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:26:48,905'); seek(1608.0)">
              intelligently balance network with depth and input resolution for excellent
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:54,485'); seek(1614.0)">
              efficiency and accuracy trade-offs.
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:57,575'); seek(1617.0)">
              Using this pre-built architectures is often more effective than simply
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:27:01,715'); seek(1621.0)">
              thinking a standard large architecture as they incorporate efficiency
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:27:06,095'); seek(1626.0)">
              principles directly into their design.
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:27:10,445'); seek(1630.0)">
              Let's look at a practical example, an H Voice assistant using a combination
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:27:15,215'); seek(1635.0)">
              of the techniques, likely neural network pruning, and eight bit ization.
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:27:19,295'); seek(1639.0)">
              A baseline model was optimized.
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:27:21,635'); seek(1641.0)">
              The results.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:27:22,625'); seek(1642.0)">
              The optimized model achieved 98.2% wake word accuracy with a very low
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:27:27,965'); seek(1647.0)">
              false activation rate under 0.5%.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:27:31,025'); seek(1651.0)">
              It managed 87.3 command recognition accuracy across
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:27:35,915'); seek(1655.0)">
              different noisy environments.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:27:38,930'); seek(1658.0)">
              Crucially, the model size for the entire system Wake.
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:27:41,630'); seek(1661.0)">
              Workplace command recognition was reduced to just 76 mb, a 73% reduction
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:27:47,570'); seek(1667.0)">
              from baseline, and the response latency was only 85 milliseconds.
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:27:52,010'); seek(1672.0)">
              End to end well within the threshold for a real time fee.
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:27:56,000'); seek(1676.0)">
              Knowledge distillation from a larger teacher model likely help
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:59,270'); seek(1679.0)">
              maintain the higher accuracy despite the significant size reduction.
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:28:04,010'); seek(1684.0)">
              This demonstrates how these techniques deliver tangible, real world benchmark
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:28:09,020'); seek(1689.0)">
              performance benefits on the edge.
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:28:12,240'); seek(1692.0)">
              Takeaway and implementation details go What are the key table covers?
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:28:17,775'); seek(1697.0)">
              First, establish performance targets.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:28:20,205'); seek(1700.0)">
              Be clear about the latency, accuracy, power, size, budgets before the
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:28:24,805'); seek(1704.0)">
              start start of optimization.
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:28:26,835'); seek(1706.0)">
              Secondly, apply the integrated optimization.
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:28:29,715'); seek(1709.0)">
              Don't just rely on just one technique.
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:28:32,025'); seek(1712.0)">
              Combine pruning, ization, distillation, and potentially architectural choice.
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:28:36,705'); seek(1716.0)">
              For the best reserves, their effects are often multi multiplicative.
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:28:41,565'); seek(1721.0)">
              Third test on target hardware ator arent enough.
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:28:45,105'); seek(1725.0)">
              Benchmark and profile directly on the edge devices we deploy onto
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:28:49,635'); seek(1729.0)">
              to find the real bottlenecks.
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:28:51,495'); seek(1731.0)">
              Fourth, iterate with real world data.
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:28:54,075'); seek(1734.0)">
              Edge environments can be unpredictable, continuously collect data and refine the
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:59,175'); seek(1739.0)">
              models after the deployment successfully.
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:29:01,725'); seek(1741.0)">
              Deploying the generative AI at the edge needs the systematic, iterative approach.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:29:06,525'); seek(1746.0)">
              Considering the whole pipeline,
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:29:09,025'); seek(1749.0)">
              a quick word on benchmarking, it's absolutely essential.
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:29:11,725'); seek(1751.0)">
              To know this, we need to define metrics clearly.
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:29:14,935'); seek(1754.0)">
              Latency, throughput, accuracy, power, create a standardized test environment,
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:29:19,255'); seek(1759.0)">
              simulating real deployment conditions, measure performance comprehensively
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:29:23,665'); seek(1763.0)">
              across different hardware and workloads, and critically use benchmark results to
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:29:28,090'); seek(1768.0)">
              further guide optimization iterations.
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:29:30,865'); seek(1770.0)">
              Systematic benchmarking validates the automation efforts and ensure
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:29:34,825'); seek(1774.0)">
              the solutions perform reliably and consistently in the real world.
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:29:38,335'); seek(1778.0)">
              This is important because if you have seen, there are multiple choices to make.
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:29:43,115'); seek(1783.0)">
              So what choices need to be made and which techniques, which combination
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:29:47,835'); seek(1787.0)">
              of techniques needs to be done?
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:29:49,515'); seek(1789.0)">
              We can be only determined if we have a standard benchmarking, so we can run this
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:29:54,015'); seek(1794.0)">
              iteratively and find if the combination actually reaches our benchmarks.
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:59,485'); seek(1799.0)">
              Then finally, the implementation roadmap.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:30:02,665'); seek(1802.0)">
              What are the practical steps if we have to do this First audit model
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:30:06,865'); seek(1806.0)">
              requirements, understand the Constance and performance needs, and then prototype
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:30:11,915'); seek(1811.0)">
              optimization pipelines, test techniques individually first to see their impact
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:30:16,895'); seek(1816.0)">
              and implement combined approach.
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:30:19,055'); seek(1819.0)">
              Integrate the chosen techniques, pruning, ization, distillation
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:30:22,385'); seek(1822.0)">
              carefully, and deploy and monitor.
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:30:25,125'); seek(1825.0)">
              The observability aspect.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:30:26,600'); seek(1826.0)">
              Realize the optimized model, but but include telemetry to gather
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:30:30,630'); seek(1830.0)">
              real world data performance data for continuous improvement.
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:30:35,050'); seek(1835.0)">
              Start with audit, build systematically, validate thoroughly,
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:30:38,500'); seek(1838.0)">
              and monitor continuously.
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:30:42,550'); seek(1842.0)">
              That concludes our look into engineering, low latency, generative AI for the edge.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:30:47,020'); seek(1847.0)">
              By applying techniques like ization pruning, knowledge distillation, and
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:30:50,980'); seek(1850.0)">
              choosing appropriate architectures and hardware, we can overcome resource
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:30:55,030'); seek(1855.0)">
              constraints and unlock powerful AI K capabilities directly on the edge.
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:59,980'); seek(1859.0)">
              Thank you for your time and attention, and have a good day.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Sai%20KR%20Pentaparthi%20-%20Conf42%20Machine%20Learning%202025.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Sai%20KR%20Pentaparthi%20-%20Conf42%20Machine%20Learning%202025.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #198B91;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/ml2025" class="btn btn-sm btn-danger shadow lift" style="background-color: #198B91;">
                <i class="fe fe-grid me-2"></i>
                See all 136 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Sai%20KR%20Pentaparthi_ml.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Sai KR Pentaparthi
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                     @ ST Engineering iDirect, Inc.
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/saikalyanrp/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Sai KR Pentaparthi's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Sai KR Pentaparthi"
                  data-url="https://www.conf42.com/ml2025"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/ml2025"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
            </p>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4 justify-content-center">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Access to all content</b>
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Machine Learning"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe for FREE<i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2026
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2026">
                  DevOps 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2026">
                  Machine Learning 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2026">
                  Site Reliability Engineering (SRE) 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2026">
                  Cloud Native 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2026">
                  Golang 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/dbd2026">
                  Database DevOps 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2026">
                  Large Language Models (LLMs) 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2026">
                  Observability 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/agents2026">
                  AI Agents 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2026">
                  DevSecOps 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2026">
                  Prompt Engineering 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2026">
                  Platform Engineering 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2026">
                  MLOps 2026
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2026">
                  Chaos Engineering 2026
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>