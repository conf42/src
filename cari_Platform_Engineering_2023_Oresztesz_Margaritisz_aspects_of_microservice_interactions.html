<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Aspects of Microservice Interactions</title>
    <meta name="description" content="Level-up your DevOps!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Oresztesz%20Margaritisz_platform.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Aspects of Microservice Interactions | Conf42"/>
    <meta property="og:description" content="What are the recent challenges of microservice interactions? Have you ever had any issues with service-to-service calls which were especially hard to manage? Were you wondering about the reasons? Unfortunately, there are some rules we can not bend. But luckily some problems are easily solvable."/>
    <meta property="og:url" content="https://conf42.com/Platform_Engineering_2023_Oresztesz_Margaritisz_aspects_of_microservice_interactions"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/GOLANG2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Golang 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-04-03
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/golang2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #6F9471;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Platform Engineering 2023 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Level-up your DevOps!
 -->
              <script>
                const event_date = new Date("2023-09-07T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2023-09-07T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "Y3exQA1iY7k"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "8DBTffP4KK8"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://youtube.com/playlist?list=PLIuxSyKxlQrCD1FfkoF8-6lJPzNFia-KV" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello everyone. Today I\u0027m going to talk about", "timestamp": "00:00:27,810", "timestamp_s": 27.0}, {"text": "the aspects of microservice interactions,", "timestamp": "00:00:31,532", "timestamp_s": 31.0}, {"text": "and you might wonder what does Devstar is", "timestamp": "00:00:34,890", "timestamp_s": 34.0}, {"text": "doing on the screen. I put this", "timestamp": "00:00:38,252", "timestamp_s": 38.0}, {"text": "image here because microservice architectures are", "timestamp": "00:00:41,852", "timestamp_s": 41.0}, {"text": "also often referred to as", "timestamp": "00:00:45,916", "timestamp_s": 45.0}, {"text": "Dev star architectures. This is coming from", "timestamp": "00:00:49,388", "timestamp_s": 49.0}, {"text": "these very complex interaction diagrams", "timestamp": "00:00:53,140", "timestamp_s": 53.0}, {"text": "that were generated in well known microservice infrastructures", "timestamp": "00:00:56,842", "timestamp_s": 56.0}, {"text": "like you can see on the screen. Left one is from Netflix,", "timestamp": "00:01:02,470", "timestamp_s": 62.0}, {"text": "the middle one you can see from one from Twitter,", "timestamp": "00:01:07,110", "timestamp_s": 67.0}, {"text": "and the right one is from Amazon.", "timestamp": "00:01:11,110", "timestamp_s": 71.0}, {"text": "So having this huge amount of", "timestamp": "00:01:15,450", "timestamp_s": 75.0}, {"text": "network communications has its own implications.", "timestamp": "00:01:19,052", "timestamp_s": 79.0}, {"text": "And that\u0027s why I think it\u0027s very important to understand the", "timestamp": "00:01:23,370", "timestamp_s": 83.0}, {"text": "upcoming challenges of a network communication", "timestamp": "00:01:29,132", "timestamp_s": 89.0}, {"text": "at such a large scale. So that drove", "timestamp": "00:01:33,026", "timestamp_s": 93.0}, {"text": "me into this topic, into this area,", "timestamp": "00:01:37,138", "timestamp_s": 97.0}, {"text": "and encouraged me to look behind", "timestamp": "00:01:40,220", "timestamp_s": 100.0}, {"text": "the scenes and understand the details. And that\u0027s how this", "timestamp": "00:01:44,740", "timestamp_s": 104.0}, {"text": "presentation was born. So the first section,", "timestamp": "00:01:48,612", "timestamp_s": 108.0}, {"text": "let\u0027s talk about the", "timestamp": "00:01:52,970", "timestamp_s": 112.0}, {"text": "reasons, the driving", "timestamp": "00:01:57,432", "timestamp_s": 117.0}, {"text": "forces that are affecting these communications", "timestamp": "00:02:01,646", "timestamp_s": 121.0}, {"text": "channels. But first, I would", "timestamp": "00:02:05,650", "timestamp_s": 125.0}, {"text": "like to just have a little recap on", "timestamp": "00:02:08,972", "timestamp_s": 128.0}, {"text": "what are the main characteristics of the microservice", "timestamp": "00:02:12,268", "timestamp_s": 132.0}, {"text": "architecture style and why", "timestamp": "00:02:16,402", "timestamp_s": 136.0}, {"text": "are we doing this overall, what\u0027s the benefit if we are doing it", "timestamp": "00:02:20,752", "timestamp_s": 140.0}, {"text": "well? So one", "timestamp": "00:02:24,592", "timestamp_s": 144.0}, {"text": "of the most important aspect of these things is", "timestamp": "00:02:29,728", "timestamp_s": 149.0}, {"text": "that microservices are independently deployable.", "timestamp": "00:02:35,428", "timestamp_s": 155.0}, {"text": "So I should be able to deploy a single", "timestamp": "00:02:39,970", "timestamp_s": 159.0}, {"text": "service without others really noticing", "timestamp": "00:02:43,864", "timestamp_s": 163.0}, {"text": "that on top of that comes another", "timestamp": "00:02:47,406", "timestamp_s": 167.0}, {"text": "important functionality, autonomity.", "timestamp": "00:02:51,592", "timestamp_s": 171.0}, {"text": "So this means that a single feature often", "timestamp": "00:02:54,958", "timestamp_s": 174.0}, {"text": "is mapped to a single microservice.", "timestamp": "00:02:58,252", "timestamp_s": 178.0}, {"text": "So I\u0027m able to deliver a single feature, a single functional", "timestamp": "00:03:01,458", "timestamp_s": 181.0}, {"text": "change, by just modifying one microservice", "timestamp": "00:03:05,346", "timestamp_s": 185.0}, {"text": "and ending up in one deployment.", "timestamp": "00:03:09,542", "timestamp_s": 189.0}, {"text": "What\u0027s also important is that microservices are polyglot.", "timestamp": "00:03:13,470", "timestamp_s": 193.0}, {"text": "They\u0027re also polyglot in", "timestamp": "00:03:19,230", "timestamp_s": 199.0}, {"text": "terms of database usage or database technology choices,", "timestamp": "00:03:22,388", "timestamp_s": 202.0}, {"text": "also in language choices, but also in other technological aspects.", "timestamp": "00:03:26,042", "timestamp_s": 206.0}, {"text": "So let\u0027s say if I want to come up with a", "timestamp": "00:03:30,122", "timestamp_s": 210.0}, {"text": "JVM upgrade, I don\u0027t have to upgrade", "timestamp": "00:03:34,370", "timestamp_s": 214.0}, {"text": "all the services all at once and then deploy them in a", "timestamp": "00:03:38,458", "timestamp_s": 218.0}, {"text": "single coordinated, huge deployment.", "timestamp": "00:03:42,248", "timestamp_s": 222.0}, {"text": "I\u0027m free to go with a single service only. And then other", "timestamp": "00:03:45,166", "timestamp_s": 225.0}, {"text": "teams owning other services are also free to deploy", "timestamp": "00:03:48,876", "timestamp_s": 228.0}, {"text": "when they think that\u0027s suitable,", "timestamp": "00:03:52,930", "timestamp_s": 232.0}, {"text": "that goes on with other technologies. So for instance, if I", "timestamp": "00:03:56,530", "timestamp_s": 236.0}, {"text": "want to change from traditional rest based communication to", "timestamp": "00:03:59,648", "timestamp_s": 239.0}, {"text": "GRPC, same thing applies.", "timestamp": "00:04:03,632", "timestamp_s": 243.0}, {"text": "So for this to work perfectly, we need one", "timestamp": "00:04:08,830", "timestamp_s": 248.0}, {"text": "single ingredient, namely network calls.", "timestamp": "00:04:12,772", "timestamp_s": 252.0}, {"text": "So you can introduce dependencies between", "timestamp": "00:04:15,978", "timestamp_s": 255.0}, {"text": "services in many ways, but network call", "timestamp": "00:04:19,780", "timestamp_s": 259.0}, {"text": "is one of the most efficient way of doing that.", "timestamp": "00:04:24,184", "timestamp_s": 264.0}, {"text": "So if you have dependencies by", "timestamp": "00:04:27,928", "timestamp_s": 267.0}, {"text": "using libraries shared data or anything else, it won\u0027t", "timestamp": "00:04:31,416", "timestamp_s": 271.0}, {"text": "work. As well as network communication, a single network", "timestamp": "00:04:35,118", "timestamp_s": 275.0}, {"text": "communication. So that\u0027s why we need to deep dive", "timestamp": "00:04:38,402", "timestamp_s": 278.0}, {"text": "into these network calls and understand how we can", "timestamp": "00:04:42,466", "timestamp_s": 282.0}, {"text": "optimize them.", "timestamp": "00:04:46,332", "timestamp_s": 286.0}, {"text": "These network calls have driving forces, I think about driving", "timestamp": "00:04:50,190", "timestamp_s": 290.0}, {"text": "forces. I think we have around", "timestamp": "00:04:54,566", "timestamp_s": 294.0}, {"text": "five of the most important ones. These are,", "timestamp": "00:04:58,624", "timestamp_s": 298.0}, {"text": "namely latency, availability, reliability,", "timestamp": "00:05:01,632", "timestamp_s": 301.0}, {"text": "queuing theory, and Conway slow. So let\u0027s go through each them,", "timestamp": "00:05:05,078", "timestamp_s": 305.0}, {"text": "each by each, and let\u0027s understand, how do they affect microservice", "timestamp": "00:05:09,076", "timestamp_s": 309.0}, {"text": "communications. First one is latency.", "timestamp": "00:05:13,662", "timestamp_s": 313.0}, {"text": "Latency has a had limit. That\u0027s the", "timestamp": "00:05:17,990", "timestamp_s": 317.0}, {"text": "speed of light. So if you take the two costs of", "timestamp": "00:05:22,120", "timestamp_s": 322.0}, {"text": "the US,", "timestamp": "00:05:25,784", "timestamp_s": 325.0}, {"text": "the runtrip time for the light to travel through the", "timestamp": "00:05:31,110", "timestamp_s": 331.0}, {"text": "left side to the right side is 27", "timestamp": "00:05:35,484", "timestamp_s": 335.0}, {"text": "milliseconds. Okay? But in practice,", "timestamp": "00:05:40,848", "timestamp_s": 340.0}, {"text": "we cannot reach this number because the speed of", "timestamp": "00:05:45,470", "timestamp_s": 345.0}, {"text": "light is also affected by the density of the material it\u0027s", "timestamp": "00:05:49,168", "timestamp_s": 349.0}, {"text": "passing through. So, for instance, in fiber optics,", "timestamp": "00:05:52,634", "timestamp_s": 352.0}, {"text": "if this would be just a single cable, it\u0027s more like", "timestamp": "00:05:56,010", "timestamp_s": 356.0}, {"text": "41 milliseconds.", "timestamp": "00:06:00,084", "timestamp_s": 360.0}, {"text": "I got this from this website I", "timestamp": "00:06:03,010", "timestamp_s": 363.0}, {"text": "have on the references section, which simulates", "timestamp": "00:06:06,340", "timestamp_s": 366.0}, {"text": "the expected roundtips time in two parts", "timestamp": "00:06:10,862", "timestamp_s": 370.0}, {"text": "of the globe.", "timestamp": "00:06:14,638", "timestamp_s": 374.0}, {"text": "So that\u0027s where these numbers are coming from. But there are many other", "timestamp": "00:06:18,890", "timestamp_s": 378.0}, {"text": "pages where they are expecting you or explaining", "timestamp": "00:06:22,812", "timestamp_s": 382.0}, {"text": "you the expected round trip time in", "timestamp": "00:06:26,578", "timestamp_s": 386.0}, {"text": "different data centers for a cloud provider.", "timestamp": "00:06:30,176", "timestamp_s": 390.0}, {"text": "So, in case of AWS,", "timestamp": "00:06:33,302", "timestamp_s": 393.0}, {"text": "this is realistically more like 50", "timestamp": "00:06:36,854", "timestamp_s": 396.0}, {"text": "to 60 milliseconds, but it depends on which", "timestamp": "00:06:40,112", "timestamp_s": 400.0}, {"text": "region are you connecting to? Another region. But why is", "timestamp": "00:06:43,812", "timestamp_s": 403.0}, {"text": "this important for us? So this gives you the minimum latency", "timestamp": "00:06:47,124", "timestamp_s": 407.0}, {"text": "if you connect from one region to another.", "timestamp": "00:06:52,550", "timestamp_s": 412.0}, {"text": "So if you want to come up with", "timestamp": "00:06:55,670", "timestamp_s": 415.0}, {"text": "a multiregion deployment, because of various reasons,", "timestamp": "00:06:59,128", "timestamp_s": 419.0}, {"text": "you always have to think about the data synchronization between the regions.", "timestamp": "00:07:03,590", "timestamp_s": 423.0}, {"text": "And this will be the minimum latency until", "timestamp": "00:07:07,394", "timestamp_s": 427.0}, {"text": "data arrives to the other edge.", "timestamp": "00:07:11,276", "timestamp_s": 431.0}, {"text": "And I say this is the minimum latency,", "timestamp": "00:07:13,930", "timestamp_s": 433.0}, {"text": "because in reality, when you have more", "timestamp": "00:07:17,506", "timestamp_s": 437.0}, {"text": "pressure on your data layer, work will stockpile", "timestamp": "00:07:20,992", "timestamp_s": 440.0}, {"text": "up, and this will go up to 506 hundred", "timestamp": "00:07:24,966", "timestamp_s": 444.0}, {"text": "milliseconds. So 60 milliseconds", "timestamp": "00:07:28,656", "timestamp_s": 448.0}, {"text": "is the optimistic duration.", "timestamp": "00:07:32,986", "timestamp_s": 452.0}, {"text": "So if you want to have a synchronous write,", "timestamp": "00:07:37,018", "timestamp_s": 457.0}, {"text": "let\u0027s say each synchronous write will have at least 40 to", "timestamp": "00:07:40,580", "timestamp_s": 460.0}, {"text": "600 milliseconds of latency. This has to be considered", "timestamp": "00:07:44,776", "timestamp_s": 464.0}, {"text": "if you are planning to do something which has", "timestamp": "00:07:48,846", "timestamp_s": 468.0}, {"text": "its own low latency requirements.", "timestamp": "00:07:52,456", "timestamp_s": 472.0}, {"text": "Also, it\u0027s important to understand the correlation between latency", "timestamp": "00:07:56,410", "timestamp_s": 476.0}, {"text": "and throughput. This comes from another research. So they try to simulate", "timestamp": "00:08:00,530", "timestamp_s": 480.0}, {"text": "two things by creating fictional", "timestamp": "00:08:06,890", "timestamp_s": 486.0}, {"text": "website. They measured the", "timestamp": "00:08:10,902", "timestamp_s": 490.0}, {"text": "page load time and measured the bandwidth, and they were interested", "timestamp": "00:08:14,528", "timestamp_s": 494.0}, {"text": "in how the page load time varies based on", "timestamp": "00:08:18,352", "timestamp_s": 498.0}, {"text": "the bandwidth and based on the roundtree time. So if you increase", "timestamp": "00:08:22,000", "timestamp_s": 502.0}, {"text": "the bandwidth, if you increase the throughput, then you will see that the", "timestamp": "00:08:25,562", "timestamp_s": 505.0}, {"text": "benefits are diminishing. Very early page", "timestamp": "00:08:29,428", "timestamp_s": 509.0}, {"text": "load time actually maps to around hundreds of requests", "timestamp": "00:08:33,736", "timestamp_s": 513.0}, {"text": "of latency because roughly that\u0027s the amount of", "timestamp": "00:08:37,198", "timestamp_s": 517.0}, {"text": "independent requests required to load whole page.", "timestamp": "00:08:41,990", "timestamp_s": 521.0}, {"text": "Okay, but if you try to change the round trip", "timestamp": "00:08:45,510", "timestamp_s": 525.0}, {"text": "time, the page load time is really linearly decreasing.", "timestamp": "00:08:48,882", "timestamp_s": 528.0}, {"text": "So what does this tell us? First of all, there is", "timestamp": "00:08:52,290", "timestamp_s": 532.0}, {"text": "no direct correlation or direct", "timestamp": "00:08:55,836", "timestamp_s": 535.0}, {"text": "effect between page load time and bandwidth", "timestamp": "00:08:59,008", "timestamp_s": 539.0}, {"text": "or latency and throughput. So this means also that", "timestamp": "00:09:02,582", "timestamp_s": 542.0}, {"text": "if you rely on barely,", "timestamp": "00:09:06,320", "timestamp_s": 546.0}, {"text": "just on the scalability or auto scaling capacities on your", "timestamp": "00:09:10,006", "timestamp_s": 550.0}, {"text": "system, that is not going to have a positive effect on the latency.", "timestamp": "00:09:13,632", "timestamp_s": 553.0}, {"text": "So if you want to be efficient in terms of latency,", "timestamp": "00:09:18,770", "timestamp_s": 558.0}, {"text": "you have to think about other solutions, not just scaling", "timestamp": "00:09:22,330", "timestamp_s": 562.0}, {"text": "out or giving more juice for your instances.", "timestamp": "00:09:25,678", "timestamp_s": 565.0}, {"text": "Another thing is availability. Thinking about", "timestamp": "00:09:29,990", "timestamp_s": 569.0}, {"text": "availability, I always think in dependency graphs", "timestamp": "00:09:34,072", "timestamp_s": 574.0}, {"text": "because that\u0027s what determines the availability as a whole.", "timestamp": "00:09:37,746", "timestamp_s": 577.0}, {"text": "So let\u0027s see here a simplified machine", "timestamp": "00:09:41,548", "timestamp_s": 581.0}, {"text": "architecture. For a single transaction I need", "timestamp": "00:09:44,882", "timestamp_s": 584.0}, {"text": "all the components to", "timestamp": "00:09:48,336", "timestamp_s": 588.0}, {"text": "behave as expected. So I need both cpu,", "timestamp": "00:09:52,528", "timestamp_s": 592.0}, {"text": "the memory, the network and the disk component.", "timestamp": "00:09:55,862", "timestamp_s": 595.0}, {"text": "So what if we give it each by each availability", "timestamp": "00:09:59,702", "timestamp_s": 599.0}, {"text": "number? So let\u0027s say each component has now 99% of", "timestamp": "00:10:03,690", "timestamp_s": 603.0}, {"text": "availability. This will form dependency", "timestamp": "00:10:07,172", "timestamp_s": 607.0}, {"text": "graph and gives us the overall availability of 96%", "timestamp": "00:10:11,210", "timestamp_s": 611.0}, {"text": "just because the combination of the components had its own", "timestamp": "00:10:15,910", "timestamp_s": 615.0}, {"text": "probability of failure. If the individual", "timestamp": "00:10:20,870", "timestamp_s": 620.0}, {"text": "elements have their own probability of failure.", "timestamp": "00:10:25,208", "timestamp_s": 625.0}, {"text": "That\u0027s how maths work actually.", "timestamp": "00:10:28,446", "timestamp_s": 628.0}, {"text": "But if I scale this out to client server model,", "timestamp": "00:10:31,530", "timestamp_s": 631.0}, {"text": "you can see that now I have more dependencies between", "timestamp": "00:10:34,860", "timestamp_s": 634.0}, {"text": "components. Again, let\u0027s say that each component has now 99%", "timestamp": "00:10:38,156", "timestamp_s": 638.0}, {"text": "of chance of being successful. Now the", "timestamp": "00:10:42,320", "timestamp_s": 642.0}, {"text": "availability will drop to 88 5%.", "timestamp": "00:10:46,208", "timestamp_s": 646.0}, {"text": "So with each single dependency your", "timestamp": "00:10:50,690", "timestamp_s": 650.0}, {"text": "availability will decrease. But it also depends", "timestamp": "00:10:54,516", "timestamp_s": 654.0}, {"text": "on how the dependency is introduced and", "timestamp": "00:10:58,762", "timestamp_s": 658.0}, {"text": "in which part of your architecture is introduced.", "timestamp": "00:11:02,228", "timestamp_s": 662.0}, {"text": "It depends on your dependency graph as well.", "timestamp": "00:11:05,750", "timestamp_s": 665.0}, {"text": "You can do the maths by hand if you\u0027re interested in the availability numbers.", "timestamp": "00:11:10,710", "timestamp_s": 670.0}, {"text": "I use my own availability simulator which", "timestamp": "00:11:14,696", "timestamp_s": 674.0}, {"text": "is just running a couple of cycles and testing each", "timestamp": "00:11:19,372", "timestamp_s": 679.0}, {"text": "connection and failing them randomly based on these numbers given.", "timestamp": "00:11:23,196", "timestamp_s": 683.0}, {"text": "I also have this in references section.", "timestamp": "00:11:26,828", "timestamp_s": 686.0}, {"text": "Okay, now for", "timestamp": "00:11:30,990", "timestamp_s": 690.0}, {"text": "reliability, I think the most important", "timestamp": "00:11:34,912", "timestamp_s": 694.0}, {"text": "use case is a single client server communication.", "timestamp": "00:11:38,544", "timestamp_s": 698.0}, {"text": "So let\u0027s say that we have a transaction", "timestamp": "00:11:42,290", "timestamp_s": 702.0}, {"text": "that\u0027s changing the state of the whole system. So let\u0027s say this", "timestamp": "00:11:46,234", "timestamp_s": 706.0}, {"text": "is a bright operation. For instance, how things", "timestamp": "00:11:50,068", "timestamp_s": 710.0}, {"text": "can fail. Let\u0027s go through them", "timestamp": "00:11:55,080", "timestamp_s": 715.0}, {"text": "sequentially. First of all,", "timestamp": "00:11:59,432", "timestamp_s": 719.0}, {"text": "the write operation can fail when client", "timestamp": "00:12:03,110", "timestamp_s": 723.0}, {"text": "sends the request to the server. Then it", "timestamp": "00:12:06,706", "timestamp_s": 726.0}, {"text": "can also fail by being processed on the server itself.", "timestamp": "00:12:10,124", "timestamp_s": 730.0}, {"text": "But it can also fail when server successfully", "timestamp": "00:12:14,730", "timestamp_s": 734.0}, {"text": "processed the write operation and it responds back to", "timestamp": "00:12:19,058", "timestamp_s": 739.0}, {"text": "the client. Here comes the problem.", "timestamp": "00:12:22,224", "timestamp_s": 742.0}, {"text": "Client can just simply retry the", "timestamp": "00:12:25,070", "timestamp_s": 745.0}, {"text": "write operation on the first two cases,", "timestamp": "00:12:28,752", "timestamp_s": 748.0}, {"text": "but after server successfully process the write operation,", "timestamp": "00:12:31,926", "timestamp_s": 751.0}, {"text": "client does not know what to do. You may", "timestamp": "00:12:36,138", "timestamp_s": 756.0}, {"text": "come over this problem by using item potent operations", "timestamp": "00:12:40,244", "timestamp_s": 760.0}, {"text": "or something similar, but in other cases", "timestamp": "00:12:44,778", "timestamp_s": 764.0}, {"text": "devious solution is not so obvious. And also,", "timestamp": "00:12:48,890", "timestamp_s": 768.0}, {"text": "of course I can fail on client side", "timestamp": "00:12:52,470", "timestamp_s": 772.0}, {"text": "by the request is being processed. But why", "timestamp": "00:12:55,672", "timestamp_s": 775.0}, {"text": "it\u0027s important for us. So first of all, that\u0027s why for instance,", "timestamp": "00:12:59,368", "timestamp_s": 779.0}, {"text": "two phase commit was not really working on", "timestamp": "00:13:03,426", "timestamp_s": 783.0}, {"text": "a larger scale. Because if you arrive on a", "timestamp": "00:13:06,828", "timestamp_s": 786.0}, {"text": "commit phase, if one of the request is failing on a commit", "timestamp": "00:13:10,288", "timestamp_s": 790.0}, {"text": "phase, the client or the coordinator does not really know", "timestamp": "00:13:13,878", "timestamp_s": 793.0}, {"text": "how to proceed because other previous operations", "timestamp": "00:13:17,712", "timestamp_s": 797.0}, {"text": "are already committed. And I just got one single failure.", "timestamp": "00:13:21,786", "timestamp_s": 801.0}, {"text": "Should I just re request the failed node to", "timestamp": "00:13:25,562", "timestamp_s": 805.0}, {"text": "commit its changes again, risking duplicated", "timestamp": "00:13:29,572", "timestamp_s": 809.0}, {"text": "write or something similar? Or should they just abort the whole operation?", "timestamp": "00:13:35,386", "timestamp_s": 815.0}, {"text": "Another example is, let\u0027s say exactly", "timestamp": "00:13:40,070", "timestamp_s": 820.0}, {"text": "once message semantics. So the server always", "timestamp": "00:13:44,584", "timestamp_s": 824.0}, {"text": "have to acknowledge if one message is processed.", "timestamp": "00:13:48,540", "timestamp_s": 828.0}, {"text": "If I fail after the message is", "timestamp": "00:13:52,410", "timestamp_s": 832.0}, {"text": "processed, the client cannot do anything else,", "timestamp": "00:13:56,396", "timestamp_s": 836.0}, {"text": "just resend the message. And that\u0027s why we often", "timestamp": "00:13:59,776", "timestamp_s": 839.0}, {"text": "have at least once message semantics instead of", "timestamp": "00:14:03,136", "timestamp_s": 843.0}, {"text": "exactly once message semantics mapping", "timestamp": "00:14:06,736", "timestamp_s": 846.0}, {"text": "again. So mapping reliability statistically", "timestamp": "00:14:10,726", "timestamp_s": 850.0}, {"text": "to more requests. We were talking about roundtree time", "timestamp": "00:14:14,474", "timestamp_s": 854.0}, {"text": "and talking about that. Or, sorry, not page load time. Yeah, page load time", "timestamp": "00:14:17,812", "timestamp_s": 857.0}, {"text": "and talking about that. Page load time usually involves", "timestamp": "00:14:21,492", "timestamp_s": 861.0}, {"text": "hundreds of requests, and it needs hundreds of requests to succeed.", "timestamp": "00:14:24,842", "timestamp_s": 864.0}, {"text": "So let\u0027s say we have theoretically a single request that has", "timestamp": "00:14:28,470", "timestamp_s": 868.0}, {"text": "99% of the probability of being successful.", "timestamp": "00:14:31,672", "timestamp_s": 871.0}, {"text": "If we have hundreds of requests with the same characteristics,", "timestamp": "00:14:35,854", "timestamp_s": 875.0}, {"text": "we cannot just say that less than 40% of the", "timestamp": "00:14:39,458", "timestamp_s": 879.0}, {"text": "chance will be that the hundreds", "timestamp": "00:14:42,828", "timestamp_s": 882.0}, {"text": "of requests will succeed. All hundreds of requests.", "timestamp": "00:14:46,418", "timestamp_s": 886.0}, {"text": "Because we have so many permutations of these hundreds", "timestamp": "00:14:50,350", "timestamp_s": 890.0}, {"text": "of requests being failed,", "timestamp": "00:14:53,878", "timestamp_s": 893.0}, {"text": "this drops our probability with around", "timestamp": "00:14:57,550", "timestamp_s": 897.0}, {"text": "60%. Just for the statistical reasons,", "timestamp": "00:15:01,620", "timestamp_s": 901.0}, {"text": "we can\u0027t really fight maths here. These are the hard facts.", "timestamp": "00:15:05,642", "timestamp_s": 905.0}, {"text": "So this actually resulted", "timestamp": "00:15:10,930", "timestamp_s": 910.0}, {"text": "as a couple of artifacts that I", "timestamp": "00:15:16,026", "timestamp_s": 916.0}, {"text": "think are very popular in", "timestamp": "00:15:19,608", "timestamp_s": 919.0}, {"text": "the engineering world. One of them is these latency numbers.", "timestamp": "00:15:23,208", "timestamp_s": 923.0}, {"text": "Every programmer should know that", "timestamp": "00:15:27,164", "timestamp_s": 927.0}, {"text": "presented in many forms like this one. This is", "timestamp": "00:15:30,412", "timestamp_s": 930.0}, {"text": "coming again from a web page where you have a slider and can change", "timestamp": "00:15:33,708", "timestamp_s": 933.0}, {"text": "the year and see how these latency numbers", "timestamp": "00:15:37,644", "timestamp_s": 937.0}, {"text": "have changed these are the recent numbers. So, for instance,", "timestamp": "00:15:41,360", "timestamp_s": 941.0}, {"text": "if we investigate the main memory reference and the latency for", "timestamp": "00:15:45,286", "timestamp_s": 945.0}, {"text": "a typical main memory reference, that\u0027s around 100 nanoseconds,", "timestamp": "00:15:49,392", "timestamp_s": 949.0}, {"text": "and the runtrip time in same data center as in a cloud infrastructure", "timestamp": "00:15:53,126", "timestamp_s": 953.0}, {"text": "is around 500 milliseconds. Why should we", "timestamp": "00:15:56,778", "timestamp_s": 956.0}, {"text": "care, you might ask? Because these numbers are fast enough.", "timestamp": "00:15:59,972", "timestamp_s": 959.0}, {"text": "So they are very fast, and they improved a", "timestamp": "00:16:03,556", "timestamp_s": 963.0}, {"text": "lot in the recent years. 500 nanoseconds", "timestamp": "00:16:07,048", "timestamp_s": 967.0}, {"text": "is something I should not care about, right? So if", "timestamp": "00:16:11,118", "timestamp_s": 971.0}, {"text": "you. Let\u0027s say you want to introduce a caching strategy and you need to", "timestamp": "00:16:14,408", "timestamp_s": 974.0}, {"text": "choose between an in memory solution, or maybe a distributed", "timestamp": "00:16:18,120", "timestamp_s": 978.0}, {"text": "caching solution, because you want to share it with", "timestamp": "00:16:21,938", "timestamp_s": 981.0}, {"text": "multiple services and you want to offer it as a separate", "timestamp": "00:16:25,516", "timestamp_s": 985.0}, {"text": "service, think about the runtime time. So,", "timestamp": "00:16:28,982", "timestamp_s": 988.0}, {"text": "difference between the in memory and", "timestamp": "00:16:33,870", "timestamp_s": 993.0}, {"text": "the distributed solution caching solution is", "timestamp": "00:16:37,296", "timestamp_s": 997.0}, {"text": "in terms of latency, is around", "timestamp": "00:16:41,236", "timestamp_s": 1001.0}, {"text": "5000 more if you choose", "timestamp": "00:16:45,300", "timestamp_s": 1005.0}, {"text": "distributed cache than if you choose an in", "timestamp": "00:16:49,236", "timestamp_s": 1009.0}, {"text": "memory cache solution.", "timestamp": "00:16:52,708", "timestamp_s": 1012.0}, {"text": "Another one. Another paper is this fallacy of distributed computing.", "timestamp": "00:16:55,970", "timestamp_s": 1015.0}, {"text": "You can find it in Wikipedia. If you look at the top three", "timestamp": "00:17:00,734", "timestamp_s": 1020.0}, {"text": "of these policies, then I think it\u0027s clear that", "timestamp": "00:17:04,456", "timestamp_s": 1024.0}, {"text": "we covered plenty of aspects of those,", "timestamp": "00:17:08,412", "timestamp_s": 1028.0}, {"text": "but the others are also important.", "timestamp": "00:17:11,770", "timestamp_s": 1031.0}, {"text": "But now I want to talk about something else,", "timestamp": "00:17:15,930", "timestamp_s": 1035.0}, {"text": "talk about the queuing theory and spend a little bit more time.", "timestamp": "00:17:19,020", "timestamp_s": 1039.0}, {"text": "Because in my experience, how I saw people", "timestamp": "00:17:22,720", "timestamp_s": 1042.0}, {"text": "in the engineering area are not really familiar with queuing theory", "timestamp": "00:17:27,856", "timestamp_s": 1047.0}, {"text": "and not really thinking in queues. But in", "timestamp": "00:17:31,546", "timestamp_s": 1051.0}, {"text": "practice, I think queues are everywhere in a modern architecture", "timestamp": "00:17:35,508", "timestamp_s": 1055.0}, {"text": "at large scale, also in small scale. So it\u0027s very good to", "timestamp": "00:17:39,690", "timestamp_s": 1059.0}, {"text": "understand the basics. So here comes the basics of queuing theory.", "timestamp": "00:17:43,368", "timestamp_s": 1063.0}, {"text": "By talking about queues,", "timestamp": "00:17:47,510", "timestamp_s": 1067.0}, {"text": "I think you can think about the simplified model", "timestamp": "00:17:51,910", "timestamp_s": 1071.0}, {"text": "that you can see on the screen. So you have queues of these orange marbles,", "timestamp": "00:17:55,400", "timestamp_s": 1075.0}, {"text": "a single queue of these orange marbles that needs to be processed.", "timestamp": "00:17:59,186", "timestamp_s": 1079.0}, {"text": "Then you have something on the right side", "timestamp": "00:18:02,850", "timestamp_s": 1082.0}, {"text": "that is processing the marbles and", "timestamp": "00:18:06,572", "timestamp_s": 1086.0}, {"text": "producing these green marbles on the right side of the screen. And then", "timestamp": "00:18:10,432", "timestamp_s": 1090.0}, {"text": "you have these metrics around the queue that determines the queue", "timestamp": "00:18:14,192", "timestamp_s": 1094.0}, {"text": "performance. We are interested in these four,", "timestamp": "00:18:17,542", "timestamp_s": 1097.0}, {"text": "mainly so there is execution time needed for a", "timestamp": "00:18:21,796", "timestamp_s": 1101.0}, {"text": "single node to process an orange marble and", "timestamp": "00:18:25,108", "timestamp_s": 1105.0}, {"text": "create a green marble. Then there is this", "timestamp": "00:18:29,490", "timestamp_s": 1109.0}, {"text": "departure rate, meaning the rate of", "timestamp": "00:18:32,932", "timestamp_s": 1112.0}, {"text": "green marbles being processed. Then we", "timestamp": "00:18:37,560", "timestamp_s": 1117.0}, {"text": "have the latency that requires duration of", "timestamp": "00:18:41,208", "timestamp_s": 1121.0}, {"text": "either a single marble traversing cv", "timestamp": "00:18:45,112", "timestamp_s": 1125.0}, {"text": "up to the right side when it becomes to green marble,", "timestamp": "00:18:48,366", "timestamp_s": 1128.0}, {"text": "or the overall duration required for all the", "timestamp": "00:18:52,194", "timestamp_s": 1132.0}, {"text": "marbles being processed. And we have then the", "timestamp": "00:18:55,452", "timestamp_s": 1135.0}, {"text": "arrival rate on the very left side, which is", "timestamp": "00:18:58,656", "timestamp_s": 1138.0}, {"text": "the rate of the orange marbles arriving in the queue.", "timestamp": "00:19:02,064", "timestamp_s": 1142.0}, {"text": "So the most basic question is, what happens", "timestamp": "00:19:07,470", "timestamp_s": 1147.0}, {"text": "when the arrival rate is much larger than the departure", "timestamp": "00:19:11,730", "timestamp_s": 1151.0}, {"text": "rate? So, this happens with us all the time,", "timestamp": "00:19:15,562", "timestamp_s": 1155.0}, {"text": "actually. So if you have something that\u0027s publicly available on the", "timestamp": "00:19:18,580", "timestamp_s": 1158.0}, {"text": "web, you don\u0027t have control over the user base", "timestamp": "00:19:22,548", "timestamp_s": 1162.0}, {"text": "and their usage statistics", "timestamp": "00:19:26,152", "timestamp_s": 1166.0}, {"text": "and often have to operate in this area.", "timestamp": "00:19:29,982", "timestamp_s": 1169.0}, {"text": "So, in this case, if you just accept", "timestamp": "00:19:34,150", "timestamp_s": 1174.0}, {"text": "all the requests and try to process them, you are guaranteed", "timestamp": "00:19:37,586", "timestamp_s": 1177.0}, {"text": "to fail after a certain period. And you", "timestamp": "00:19:41,602", "timestamp_s": 1181.0}, {"text": "have to introduce something. Right. This something is called", "timestamp": "00:19:45,628", "timestamp_s": 1185.0}, {"text": "back pressure or rate limiting. Okay. So,", "timestamp": "00:19:48,880", "timestamp_s": 1188.0}, {"text": "often in the edge, you have rate limiter service that", "timestamp": "00:19:53,070", "timestamp_s": 1193.0}, {"text": "determines which endpoint is", "timestamp": "00:19:57,040", "timestamp_s": 1197.0}, {"text": "limited to what", "timestamp": "00:20:00,676", "timestamp_s": 1200.0}, {"text": "sort of throughput, and tries to keep the", "timestamp": "00:20:04,820", "timestamp_s": 1204.0}, {"text": "right side or protected from higher", "timestamp": "00:20:09,492", "timestamp_s": 1209.0}, {"text": "aid than usual, and tries to introduce some sort of", "timestamp": "00:20:14,120", "timestamp_s": 1214.0}, {"text": "a logic on limiting those", "timestamp": "00:20:17,768", "timestamp_s": 1217.0}, {"text": "clients who maybe misbehave or limiting", "timestamp": "00:20:21,672", "timestamp_s": 1221.0}, {"text": "requests to a specific service overall.", "timestamp": "00:20:26,230", "timestamp_s": 1226.0}, {"text": "So, this is an important topic, and we will talk about it a.", "timestamp": "00:20:30,330", "timestamp_s": 1230.0}, {"text": "But more in the second half.", "timestamp": "00:20:33,484", "timestamp_s": 1233.0}, {"text": "So, coming back to queuing theory, let\u0027s have a couple of practical", "timestamp": "00:20:36,970", "timestamp_s": 1236.0}, {"text": "examples. So, in this simplified scenario,", "timestamp": "00:20:40,582", "timestamp_s": 1240.0}, {"text": "the execution time is 100 milliseconds. So what", "timestamp": "00:20:44,070", "timestamp_s": 1244.0}, {"text": "is the throughput? In this case,", "timestamp": "00:20:47,984", "timestamp_s": 1247.0}, {"text": "we produce ten marbles per second. Because we produce", "timestamp": "00:20:51,188", "timestamp_s": 1251.0}, {"text": "a single marble each 100 millisecond. The overall latency", "timestamp": "00:20:55,290", "timestamp_s": 1255.0}, {"text": "for processing all these eight marbles is 800 milliseconds. It\u0027s eight", "timestamp": "00:20:59,578", "timestamp_s": 1259.0}, {"text": "times 100 milliseconds. Right. Very simple. Now, what if", "timestamp": "00:21:03,752", "timestamp_s": 1263.0}, {"text": "I try to parallelize now and have a singular queue,", "timestamp": "00:21:07,512", "timestamp_s": 1267.0}, {"text": "but have doubled deburkers? Now I", "timestamp": "00:21:11,214", "timestamp_s": 1271.0}, {"text": "produce two marbles in each 100 millisecond.", "timestamp": "00:21:15,496", "timestamp_s": 1275.0}, {"text": "So I have 20 marbles per second as my throughput", "timestamp": "00:21:19,698", "timestamp_s": 1279.0}, {"text": "or as my departure rate. The latency is also housed", "timestamp": "00:21:23,618", "timestamp_s": 1283.0}, {"text": "because now I can produce", "timestamp": "00:21:28,034", "timestamp_s": 1288.0}, {"text": "four times two marbles overall", "timestamp": "00:21:33,870", "timestamp_s": 1293.0}, {"text": "in four times 100 milliseconds. And that comes up to 400 milliseconds.", "timestamp": "00:21:39,970", "timestamp_s": 1299.0}, {"text": "Okay, but what if I divide", "timestamp": "00:21:45,570", "timestamp_s": 1305.0}, {"text": "the work like that? So what? Instead of", "timestamp": "00:21:49,178", "timestamp_s": 1309.0}, {"text": "having a single parallelized operation,", "timestamp": "00:21:53,704", "timestamp_s": 1313.0}, {"text": "I try to split the work in", "timestamp": "00:21:56,910", "timestamp_s": 1316.0}, {"text": "two halves, which can be finished in two times 50 millisecond.", "timestamp": "00:22:00,776", "timestamp_s": 1320.0}, {"text": "Let\u0027s see the numbers. Now, I can produce a single marble within", "timestamp": "00:22:05,998", "timestamp_s": 1325.0}, {"text": "50 millisecond that comes up with this throughput as before,", "timestamp": "00:22:10,156", "timestamp_s": 1330.0}, {"text": "as in the previous example, as 20 marbles per second.", "timestamp": "00:22:13,772", "timestamp_s": 1333.0}, {"text": "So I still improved, doubled my throughput. But how is my", "timestamp": "00:22:17,516", "timestamp_s": 1337.0}, {"text": "latency changed? My latency will be still 800", "timestamp": "00:22:21,248", "timestamp_s": 1341.0}, {"text": "milliseconds because I need a single marble,", "timestamp": "00:22:24,960", "timestamp_s": 1344.0}, {"text": "100 millisecond to travel through from the left side to the right", "timestamp": "00:22:29,390", "timestamp_s": 1349.0}, {"text": "side. Right. I need two times 50 milliseconds for a single marble", "timestamp": "00:22:32,612", "timestamp_s": 1352.0}, {"text": "to become an orange marble, to become green marble.", "timestamp": "00:22:36,618", "timestamp_s": 1356.0}, {"text": "So interestingly, latency did not change, but throughput", "timestamp": "00:22:40,930", "timestamp_s": 1360.0}, {"text": "increased. And that\u0027s the magic, I think, of these reactive", "timestamp": "00:22:44,398", "timestamp_s": 1364.0}, {"text": "libraries that are becoming very popular these days. So by simply", "timestamp": "00:22:49,582", "timestamp_s": 1369.0}, {"text": "declaring my work in a different way,", "timestamp": "00:22:53,854", "timestamp_s": 1373.0}, {"text": "it allows me to have higher level of parallelization.", "timestamp": "00:22:57,468", "timestamp_s": 1377.0}, {"text": "By splitting my workload into smaller chunks", "timestamp": "00:23:01,210", "timestamp_s": 1381.0}, {"text": "and introducing more queues and processing", "timestamp": "00:23:05,186", "timestamp_s": 1385.0}, {"text": "them in smaller units.", "timestamp": "00:23:09,222", "timestamp_s": 1389.0}, {"text": "Overall it increases my parallelization,", "timestamp": "00:23:13,062", "timestamp_s": 1393.0}, {"text": "even though I\u0027m not aware of that, because in the code", "timestamp": "00:23:16,646", "timestamp_s": 1396.0}, {"text": "everything seems sequential calls,", "timestamp": "00:23:20,384", "timestamp_s": 1400.0}, {"text": "right? So if I have a bottleneck, let\u0027s say, then the numbers are", "timestamp": "00:23:24,388", "timestamp_s": 1404.0}, {"text": "changed as following. So I still have ten marbles", "timestamp": "00:23:28,212", "timestamp_s": 1408.0}, {"text": "per second, because the bottleneck keeps me", "timestamp": "00:23:31,514", "timestamp_s": 1411.0}, {"text": "from processing a single marble within", "timestamp": "00:23:36,950", "timestamp_s": 1416.0}, {"text": "50 millisecond. And it", "timestamp": "00:23:40,760", "timestamp_s": 1420.0}, {"text": "just allows me to have a green marble in every", "timestamp": "00:23:44,168", "timestamp_s": 1424.0}, {"text": "100 millisecond, because that\u0027s where the bottleneck is.", "timestamp": "00:23:47,432", "timestamp_s": 1427.0}, {"text": "And in total, I need 100 plus 50 milliseconds", "timestamp": "00:23:50,748", "timestamp_s": 1430.0}, {"text": "for a single marble to go through. So my latency again is increased to", "timestamp": "00:23:54,194", "timestamp_s": 1434.0}, {"text": "1200 milliseconds. Were are many other scenarios,", "timestamp": "00:23:58,320", "timestamp_s": 1438.0}, {"text": "but I think you can do the math easily in", "timestamp": "00:24:01,830", "timestamp_s": 1441.0}, {"text": "your head. I have a few formulas", "timestamp": "00:24:05,328", "timestamp_s": 1445.0}, {"text": "that maybe not really precise,", "timestamp": "00:24:09,890", "timestamp_s": 1449.0}, {"text": "but it\u0027s enough to me to understand what\u0027s", "timestamp": "00:24:12,954", "timestamp_s": 1452.0}, {"text": "going on. What\u0027s really important, as you can see that the", "timestamp": "00:24:17,018", "timestamp_s": 1457.0}, {"text": "throughput is not really depending", "timestamp": "00:24:20,868", "timestamp_s": 1460.0}, {"text": "on the queue length.", "timestamp": "00:24:24,446", "timestamp_s": 1464.0}, {"text": "It\u0027s behaving a little bit differently than the latency.", "timestamp": "00:24:29,110", "timestamp_s": 1469.0}, {"text": "So as you\u0027ve seen before in the example were,", "timestamp": "00:24:33,534", "timestamp_s": 1473.0}, {"text": "I was talking about this research with", "timestamp": "00:24:38,252", "timestamp_s": 1478.0}, {"text": "page load time and bandwidth, throughput and latency", "timestamp": "00:24:42,010", "timestamp_s": 1482.0}, {"text": "does not really depend on each other.", "timestamp": "00:24:46,610", "timestamp_s": 1486.0}, {"text": "What other things can you do with queues? What\u0027s very important", "timestamp": "00:24:51,630", "timestamp_s": 1491.0}, {"text": "is that for each queue you can provide its own quality of", "timestamp": "00:24:55,136", "timestamp_s": 1495.0}, {"text": "service. So these numbers can be independently provided", "timestamp": "00:24:58,928", "timestamp_s": 1498.0}, {"text": "for each queue. So let\u0027s say if you have producer,", "timestamp": "00:25:02,634", "timestamp_s": 1502.0}, {"text": "a single producer like the one who is producing the", "timestamp": "00:25:07,810", "timestamp_s": 1507.0}, {"text": "orange marbles, which needs higher demand,", "timestamp": "00:25:11,460", "timestamp_s": 1511.0}, {"text": "you can separate it to its own dedicated channel and", "timestamp": "00:25:15,182", "timestamp_s": 1515.0}, {"text": "it won\u0027t affect those which are producing. Want to process", "timestamp": "00:25:18,552", "timestamp_s": 1518.0}, {"text": "blue marbles and yellow marbles and won\u0027t choke the system", "timestamp": "00:25:22,630", "timestamp_s": 1522.0}, {"text": "so easily with its own requests.", "timestamp": "00:25:26,460", "timestamp_s": 1526.0}, {"text": "So by separating them to different channels, you can", "timestamp": "00:25:29,610", "timestamp_s": 1529.0}, {"text": "offer a separate quality of service to each channel.", "timestamp": "00:25:33,148", "timestamp_s": 1533.0}, {"text": "And this can be done in a couple of ways in microservice.", "timestamp": "00:25:37,790", "timestamp_s": 1537.0}, {"text": "First of all, a single service concentrates on one specific", "timestamp": "00:25:41,702", "timestamp_s": 1541.0}, {"text": "workload. And that forms its own queue and", "timestamp": "00:25:46,912", "timestamp_s": 1546.0}, {"text": "its own special way of optimizing for that kind", "timestamp": "00:25:50,532", "timestamp_s": 1550.0}, {"text": "of workload. It\u0027s independent from other services, but can", "timestamp": "00:25:53,748", "timestamp_s": 1553.0}, {"text": "be introduced also in a single service.", "timestamp": "00:25:57,572", "timestamp_s": 1557.0}, {"text": "If you have messaging and use multiple channels for multiple", "timestamp": "00:26:01,890", "timestamp_s": 1561.0}, {"text": "clients. I will have again, a detailed, practical example", "timestamp": "00:26:06,222", "timestamp_s": 1566.0}, {"text": "of how we use this feature.", "timestamp": "00:26:09,832", "timestamp_s": 1569.0}, {"text": "So, about Conway\u0027s law. Just very quickly thinking", "timestamp": "00:26:14,890", "timestamp_s": 1574.0}, {"text": "about Conway\u0027s law, I always just consider how many", "timestamp": "00:26:21,436", "timestamp_s": 1581.0}, {"text": "scenarios we have with", "timestamp": "00:26:25,404", "timestamp_s": 1585.0}, {"text": "communication, with single communication,", "timestamp": "00:26:29,772", "timestamp_s": 1589.0}, {"text": "considering teams. Okay, so in this scenario,", "timestamp": "00:26:33,074", "timestamp_s": 1593.0}, {"text": "in the two part, these two", "timestamp": "00:26:38,090", "timestamp_s": 1598.0}, {"text": "sides of the communication, a single team is controlling", "timestamp": "00:26:42,356", "timestamp_s": 1602.0}, {"text": "the change for each side. This is the easiest scenario", "timestamp": "00:26:46,762", "timestamp_s": 1606.0}, {"text": "because you can do whatever you want. You can proceed", "timestamp": "00:26:51,890", "timestamp_s": 1611.0}, {"text": "as fast as you would like to. Now we have these scenarios", "timestamp": "00:26:56,698", "timestamp_s": 1616.0}, {"text": "when a shared responsibility is on one of the other side.", "timestamp": "00:27:00,142", "timestamp_s": 1620.0}, {"text": "This is some sort of an anti pattern. This is", "timestamp": "00:27:03,272", "timestamp_s": 1623.0}, {"text": "not frequently used. Only companies use on", "timestamp": "00:27:06,572", "timestamp_s": 1626.0}, {"text": "those occasions when they don\u0027t really need to", "timestamp": "00:27:10,508", "timestamp_s": 1630.0}, {"text": "change so many things in a legacy service,", "timestamp": "00:27:14,204", "timestamp_s": 1634.0}, {"text": "or they don\u0027t really know how to separate the ownership", "timestamp": "00:27:20,910", "timestamp_s": 1640.0}, {"text": "of maybe a bigger chunk of code.", "timestamp": "00:27:25,558", "timestamp_s": 1645.0}, {"text": "This slows things down radically. This is", "timestamp": "00:27:28,910", "timestamp_s": 1648.0}, {"text": "when you have to be very careful. This is when you need to introduce nonbreaking", "timestamp": "00:27:32,468", "timestamp_s": 1652.0}, {"text": "changes or have the legacy endpoint live", "timestamp": "00:27:36,026", "timestamp_s": 1656.0}, {"text": "for a very long time. Now,", "timestamp": "00:27:40,228", "timestamp_s": 1660.0}, {"text": "there is this more healthier scenario when you have multiple", "timestamp": "00:27:43,876", "timestamp_s": 1663.0}, {"text": "consumers and you are the producer side or you are the", "timestamp": "00:27:47,102", "timestamp_s": 1667.0}, {"text": "consumer and there are multiple producers. This can happen in", "timestamp": "00:27:50,168", "timestamp_s": 1670.0}, {"text": "many situations. What\u0027s important to understand, I think that", "timestamp": "00:27:53,848", "timestamp_s": 1673.0}, {"text": "the service ownership does not necessarily come", "timestamp": "00:27:58,524", "timestamp_s": 1678.0}, {"text": "with the schema ownership. You are free to", "timestamp": "00:28:02,060", "timestamp_s": 1682.0}, {"text": "move the scheme ownership to the other side,", "timestamp": "00:28:05,900", "timestamp_s": 1685.0}, {"text": "back and forth, however you feel it\u0027s more", "timestamp": "00:28:10,350", "timestamp_s": 1690.0}, {"text": "suitable. This comes had in a couple", "timestamp": "00:28:13,792", "timestamp_s": 1693.0}, {"text": "of situations. So let\u0027s say that this is an event", "timestamp": "00:28:17,072", "timestamp_s": 1697.0}, {"text": "based system and who should control, in this case", "timestamp": "00:28:20,932", "timestamp_s": 1700.0}, {"text": "the schema, the message producers who are producing", "timestamp": "00:28:25,172", "timestamp_s": 1705.0}, {"text": "the events themselves, should they tell for the other teams", "timestamp": "00:28:29,146", "timestamp_s": 1709.0}, {"text": "that, yeah, there\u0027s going to be a schema change and be aware of that,", "timestamp": "00:28:32,922", "timestamp_s": 1712.0}, {"text": "and then just contact all the other teams, see if they", "timestamp": "00:28:36,680", "timestamp_s": 1716.0}, {"text": "are ready for accepting the new event.", "timestamp": "00:28:40,312", "timestamp_s": 1720.0}, {"text": "Or should we do it in a different way? Should the consumers", "timestamp": "00:28:44,150", "timestamp_s": 1724.0}, {"text": "be controlling the scheme ownership and tell the producer that,", "timestamp": "00:28:47,666", "timestamp_s": 1727.0}, {"text": "okay, we are expecting these kind of messages.", "timestamp": "00:28:51,548", "timestamp_s": 1731.0}, {"text": "From now on, we are accepting this kind", "timestamp": "00:28:55,290", "timestamp_s": 1735.0}, {"text": "of change, but not ready for another change and so on.", "timestamp": "00:28:59,308", "timestamp_s": 1739.0}, {"text": "There are tools and techniques", "timestamp": "00:29:02,592", "timestamp_s": 1742.0}, {"text": "on how to do this, and it helps visibility.", "timestamp": "00:29:06,406", "timestamp_s": 1746.0}, {"text": "It had testability helps with many things.", "timestamp": "00:29:10,582", "timestamp_s": 1750.0}, {"text": "There are also schema registries that you can introduce. You can switch", "timestamp": "00:29:13,812", "timestamp_s": 1753.0}, {"text": "from something that\u0027s schema s", "timestamp": "00:29:17,786", "timestamp_s": 1757.0}, {"text": "like traditional rest based API,", "timestamp": "00:29:21,780", "timestamp_s": 1761.0}, {"text": "which is offering just simple JSON", "timestamp": "00:29:26,766", "timestamp_s": 1766.0}, {"text": "based communication to a more", "timestamp": "00:29:30,526", "timestamp_s": 1770.0}, {"text": "conservative way of communicating, using strongly typed", "timestamp": "00:29:35,590", "timestamp_s": 1775.0}, {"text": "APIs like graphQL, GrPC,", "timestamp": "00:29:40,146", "timestamp_s": 1780.0}, {"text": "or maybe introducing schemas into events or messages as well.", "timestamp": "00:29:43,474", "timestamp_s": 1783.0}, {"text": "And there is this more most complicated scenario, when there", "timestamp": "00:29:49,950", "timestamp_s": 1789.0}, {"text": "are multiple teams in each side,", "timestamp": "00:29:53,808", "timestamp_s": 1793.0}, {"text": "producer and consumer side, that\u0027s when you need something more", "timestamp": "00:29:56,910", "timestamp_s": 1796.0}, {"text": "advanced, or the most advanced things for controlling schemas.", "timestamp": "00:30:01,970", "timestamp_s": 1801.0}, {"text": "Something like schema Federation, that\u0027s storing", "timestamp": "00:30:05,642", "timestamp_s": 1805.0}, {"text": "different versions and kinds of schemas and schema changes in a", "timestamp": "00:30:10,474", "timestamp_s": 1810.0}, {"text": "controlled way, most preferably in a venture controlled", "timestamp": "00:30:14,408", "timestamp_s": 1814.0}, {"text": "way. Okay, so this", "timestamp": "00:30:18,238", "timestamp_s": 1818.0}, {"text": "is where the first part ends. Now I would like to just", "timestamp": "00:30:23,192", "timestamp_s": 1823.0}, {"text": "quickly introduce you the toolbox or the", "timestamp": "00:30:26,556", "timestamp_s": 1826.0}, {"text": "things that I consider and jump right to the next", "timestamp": "00:30:30,316", "timestamp_s": 1830.0}, {"text": "section. And we will talk about practical examples and", "timestamp": "00:30:34,810", "timestamp_s": 1834.0}, {"text": "situations that I faced. And I would like to guide", "timestamp": "00:30:38,752", "timestamp_s": 1838.0}, {"text": "you how we improved situations each by each.", "timestamp": "00:30:42,278", "timestamp_s": 1842.0}, {"text": "So the tools that I use, usually you", "timestamp": "00:30:46,830", "timestamp_s": 1846.0}, {"text": "can do something like cqrs,", "timestamp": "00:30:51,268", "timestamp_s": 1851.0}, {"text": "meaning that you can separate the write and read path.", "timestamp": "00:30:55,570", "timestamp_s": 1855.0}, {"text": "If you need something special on the read side, or maybe something", "timestamp": "00:30:58,794", "timestamp_s": 1858.0}, {"text": "special on the right side, then we", "timestamp": "00:31:02,036", "timestamp_s": 1862.0}, {"text": "talked a lot about schemas. You can introduce contract based testing.", "timestamp": "00:31:05,672", "timestamp_s": 1865.0}, {"text": "It helps to move the schema ownership to the other side.", "timestamp": "00:31:09,910", "timestamp_s": 1869.0}, {"text": "Then you can introduce caching. We saw in", "timestamp": "00:31:15,290", "timestamp_s": 1875.0}, {"text": "the latency part how caching can improve the latency.", "timestamp": "00:31:18,668", "timestamp_s": 1878.0}, {"text": "With caching, you have to think about data freshness", "timestamp": "00:31:23,530", "timestamp_s": 1883.0}, {"text": "and multi write helps. Here I", "timestamp": "00:31:27,282", "timestamp_s": 1887.0}, {"text": "call multi write something that keeps", "timestamp": "00:31:31,692", "timestamp_s": 1891.0}, {"text": "the cached values fresh in a proactive way.", "timestamp": "00:31:36,758", "timestamp_s": 1896.0}, {"text": "So if you grab a fresh value from", "timestamp": "00:31:40,644", "timestamp_s": 1900.0}, {"text": "one side of your system because one of the clients", "timestamp": "00:31:44,996", "timestamp_s": 1904.0}, {"text": "needs that, you need to proactively write it to other", "timestamp": "00:31:49,146", "timestamp_s": 1909.0}, {"text": "cache instances to keep the data fresh and", "timestamp": "00:31:52,792", "timestamp_s": 1912.0}, {"text": "reduce the number of cache misses.", "timestamp": "00:31:56,616", "timestamp_s": 1916.0}, {"text": "Then you can switch from synchronous to", "timestamp": "00:32:00,710", "timestamp_s": 1920.0}, {"text": "asynchronous communication by keeping the original", "timestamp": "00:32:04,136", "timestamp_s": 1924.0}, {"text": "API, by introducing polling, by introducing maybe", "timestamp": "00:32:07,522", "timestamp_s": 1927.0}, {"text": "synchronous API that sends forward the request", "timestamp": "00:32:11,370", "timestamp_s": 1931.0}, {"text": "to a message queue and then just", "timestamp": "00:32:15,586", "timestamp_s": 1935.0}, {"text": "send simple response back to the client. There are", "timestamp": "00:32:19,148", "timestamp_s": 1939.0}, {"text": "also design practices or design principles", "timestamp": "00:32:23,168", "timestamp_s": 1943.0}, {"text": "that you can rely on like cloud native and twelve factor.", "timestamp": "00:32:27,366", "timestamp_s": 1947.0}, {"text": "I won\u0027t cover these, just thought it\u0027s good to mention", "timestamp": "00:32:30,890", "timestamp_s": 1950.0}, {"text": "them. Auto scaling can be", "timestamp": "00:32:34,340", "timestamp_s": 1954.0}, {"text": "effective in many ways and", "timestamp": "00:32:38,212", "timestamp_s": 1958.0}, {"text": "auto scaling has a positive effect on throughput,", "timestamp": "00:32:43,300", "timestamp_s": 1963.0}, {"text": "but not on latency. As we discussed, I talked about", "timestamp": "00:32:46,862", "timestamp_s": 1966.0}, {"text": "back pressure back in this section when talking about queuing", "timestamp": "00:32:50,712", "timestamp_s": 1970.0}, {"text": "theory, when you have higher", "timestamp": "00:32:54,238", "timestamp_s": 1974.0}, {"text": "arrival rate than departure rate,", "timestamp": "00:32:57,404", "timestamp_s": 1977.0}, {"text": "if you need large scale transactions, then you can introduce", "timestamp": "00:33:01,050", "timestamp_s": 1981.0}, {"text": "sagas in a microservice architecture. You can do it in a", "timestamp": "00:33:05,122", "timestamp_s": 1985.0}, {"text": "couple of ways. You can control the transaction either", "timestamp": "00:33:08,128", "timestamp_s": 1988.0}, {"text": "by using orchestration or choreography.", "timestamp": "00:33:11,632", "timestamp_s": 1991.0}, {"text": "You can introduce a service mesh. I think service meshes are", "timestamp": "00:33:17,550", "timestamp_s": 1997.0}, {"text": "important because there are many ways to fine tune the communication", "timestamp": "00:33:21,172", "timestamp_s": 2001.0}, {"text": "channels inside service mesh.", "timestamp": "00:33:25,066", "timestamp_s": 2005.0}, {"text": "It improves your observability.", "timestamp": "00:33:28,370", "timestamp_s": 2008.0}, {"text": "It helps you with certain kind of security aspects", "timestamp": "00:33:32,130", "timestamp_s": 2012.0}, {"text": "and you can introduce many resiliency patterns,", "timestamp": "00:33:36,110", "timestamp_s": 2016.0}, {"text": "us configurations inside service meshes like", "timestamp": "00:33:39,518", "timestamp_s": 2019.0}, {"text": "security breakers, timeouts, retries and so on and", "timestamp": "00:33:43,848", "timestamp_s": 2023.0}, {"text": "so forth. You can be conscious", "timestamp": "00:33:47,932", "timestamp_s": 2027.0}, {"text": "about your technology choices. So for instance,", "timestamp": "00:33:51,826", "timestamp_s": 2031.0}, {"text": "if you choose GRPC over traditional", "timestamp": "00:33:55,074", "timestamp_s": 2035.0}, {"text": "rest based communication, you can expect lower latency", "timestamp": "00:33:58,722", "timestamp_s": 2038.0}, {"text": "because usually GRPC has less", "timestamp": "00:34:02,294", "timestamp_s": 2042.0}, {"text": "round trips during a communication,", "timestamp": "00:34:06,000", "timestamp_s": 2046.0}, {"text": "during a request reasons, and payload is smaller", "timestamp": "00:34:08,990", "timestamp_s": 2048.0}, {"text": "because it\u0027s binary based. So probably you have more throughput.", "timestamp": "00:34:13,162", "timestamp_s": 2053.0}, {"text": "Messaging has many patterns, so if asynchronous communication", "timestamp": "00:34:18,610", "timestamp_s": 2058.0}, {"text": "is not enough for you, then you can introduce messaging in one", "timestamp": "00:34:22,682", "timestamp_s": 2062.0}, {"text": "of the sides. Switch from synchronous to asynchronous communications", "timestamp": "00:34:25,928", "timestamp_s": 2065.0}, {"text": "and then you are free to use all those messaging patterns", "timestamp": "00:34:30,158", "timestamp_s": 2070.0}, {"text": "which will increase the robustness of the communication itself", "timestamp": "00:34:33,742", "timestamp_s": 2073.0}, {"text": "and maybe help in a specific situation.", "timestamp": "00:34:37,164", "timestamp_s": 2077.0}, {"text": "If you choose your concurrency model well,", "timestamp": "00:34:41,210", "timestamp_s": 2081.0}, {"text": "it will have higher throughput,", "timestamp": "00:34:44,892", "timestamp_s": 2084.0}, {"text": "probably won\u0027t have a positive effect on the latency, but have higher", "timestamp": "00:34:48,598", "timestamp_s": 2088.0}, {"text": "throughput with less resource. So it will introduce", "timestamp": "00:34:52,560", "timestamp_s": 2092.0}, {"text": "more channels, more queues, but not necessarily more threads.", "timestamp": "00:34:56,438", "timestamp_s": 2096.0}, {"text": "This is very well used", "timestamp": "00:35:01,170", "timestamp_s": 2101.0}, {"text": "and I think a well settled technology coming in with reactive", "timestamp": "00:35:04,996", "timestamp_s": 2104.0}, {"text": "programming or coroutines, so they are good", "timestamp": "00:35:08,842", "timestamp_s": 2108.0}, {"text": "choices if you want to save resources with", "timestamp": "00:35:12,916", "timestamp_s": 2112.0}, {"text": "your communication. Then there are", "timestamp": "00:35:16,788", "timestamp_s": 2116.0}, {"text": "these resiliency patterns I think many people know", "timestamp": "00:35:20,136", "timestamp_s": 2120.0}, {"text": "because they are widely used in", "timestamp": "00:35:23,928", "timestamp_s": 2123.0}, {"text": "the microservice world. Also, service meshes offer them", "timestamp": "00:35:27,900", "timestamp_s": 2127.0}, {"text": "by default. There are also libraries that are providing", "timestamp": "00:35:31,212", "timestamp_s": 2131.0}, {"text": "most of these. So there are circuit breakers, bulkheads,", "timestamp": "00:35:36,090", "timestamp_s": 2136.0}, {"text": "retries, timeouts, just to name the most important", "timestamp": "00:35:39,174", "timestamp_s": 2139.0}, {"text": "parts. Timeout comes with all the libraries which are communicating", "timestamp": "00:35:43,408", "timestamp_s": 2143.0}, {"text": "with the network. Then you have observability", "timestamp": "00:35:47,702", "timestamp_s": 2147.0}, {"text": "to just review the whole and understand if you", "timestamp": "00:35:52,130", "timestamp_s": 2152.0}, {"text": "are improved or not. Now let\u0027s jump to the example", "timestamp": "00:35:55,588", "timestamp_s": 2155.0}, {"text": "part. So I will pick a couple of practical examples", "timestamp": "00:35:59,444", "timestamp_s": 2159.0}, {"text": "that I\u0027ve met, and I will go through how", "timestamp": "00:36:03,146", "timestamp_s": 2163.0}, {"text": "in a specific situation things are improved", "timestamp": "00:36:08,472", "timestamp_s": 2168.0}, {"text": "with what kind of practices. Okay, so one", "timestamp": "00:36:12,870", "timestamp_s": 2172.0}, {"text": "of the examples I like is coming from the", "timestamp": "00:36:16,348", "timestamp_s": 2176.0}, {"text": "distributed database called Cassandra, and this is called", "timestamp": "00:36:20,524", "timestamp_s": 2180.0}, {"text": "this technique called rapid read protection. This is how it works.", "timestamp": "00:36:24,316", "timestamp_s": 2184.0}, {"text": "So let\u0027s say a client needs to read the data", "timestamp": "00:36:28,092", "timestamp_s": 2188.0}, {"text": "from the database, and it needs the data", "timestamp": "00:36:32,350", "timestamp_s": 2192.0}, {"text": "to be up to date. So then the client goes to this", "timestamp": "00:36:35,776", "timestamp_s": 2195.0}, {"text": "so called coordinator node that you can see on the top left,", "timestamp": "00:36:39,168", "timestamp_s": 2199.0}, {"text": "and the coordinator node then gets", "timestamp": "00:36:42,388", "timestamp_s": 2202.0}, {"text": "the data from each replicas, then aggregates the data", "timestamp": "00:36:47,492", "timestamp_s": 2207.0}, {"text": "based on its freshness, and then sends back the update data to the client.", "timestamp": "00:36:51,620", "timestamp_s": 2211.0}, {"text": "Now what happens if one of the requests is", "timestamp": "00:36:56,550", "timestamp_s": 2216.0}, {"text": "being slow? Instead of waiting for the request,", "timestamp": "00:36:59,896", "timestamp_s": 2219.0}, {"text": "the coordinator node is going to fire a so", "timestamp": "00:37:03,166", "timestamp_s": 2223.0}, {"text": "called backup request, hoping that this backup request", "timestamp": "00:37:06,892", "timestamp_s": 2226.0}, {"text": "will finish faster,", "timestamp": "00:37:10,706", "timestamp_s": 2230.0}, {"text": "and hoping that the coordinator node will", "timestamp": "00:37:14,130", "timestamp_s": 2234.0}, {"text": "send back the data to the client also faster.", "timestamp": "00:37:19,920", "timestamp_s": 2239.0}, {"text": "Why is this happening? Isn\u0027t this just a waste", "timestamp": "00:37:24,110", "timestamp_s": 2244.0}, {"text": "of effort and just a lot", "timestamp": "00:37:27,734", "timestamp_s": 2247.0}, {"text": "of complication? Shouldn\u0027t be more efficient", "timestamp": "00:37:31,348", "timestamp_s": 2251.0}, {"text": "if we just wait for", "timestamp": "00:37:35,546", "timestamp_s": 2255.0}, {"text": "that request to finish and just fail if there is no", "timestamp": "00:37:38,948", "timestamp_s": 2258.0}, {"text": "answer. So if you think about availability again,", "timestamp": "00:37:42,468", "timestamp_s": 2262.0}, {"text": "let\u0027s say we have 99% of availability for each node", "timestamp": "00:37:46,168", "timestamp_s": 2266.0}, {"text": "to be successful, successfully responding", "timestamp": "00:37:49,902", "timestamp_s": 2269.0}, {"text": "the payload to the coordinator node.", "timestamp": "00:37:55,030", "timestamp_s": 2275.0}, {"text": "In this case of 1% of a chance. When we have failure,", "timestamp": "00:38:00,150", "timestamp_s": 2280.0}, {"text": "we will still have 1% of chance, or 99% of the", "timestamp": "00:38:04,050", "timestamp_s": 2284.0}, {"text": "chance to be successful if we use a backup request.", "timestamp": "00:38:07,628", "timestamp_s": 2287.0}, {"text": "So the overall availability for this simplified scenario", "timestamp": "00:38:10,758", "timestamp_s": 2290.0}, {"text": "for the simplified query is increased. It\u0027s now around 99 99%.", "timestamp": "00:38:15,254", "timestamp_s": 2295.0}, {"text": "Okay? And if you would also investigate the P 99", "timestamp": "00:38:21,250", "timestamp_s": 2301.0}, {"text": "latency numbers, we will also see a decrease", "timestamp": "00:38:26,290", "timestamp_s": 2306.0}, {"text": "compared to the scenario if we would turn this off,", "timestamp": "00:38:30,690", "timestamp_s": 2310.0}, {"text": "because we won\u0027t see timeouts that often. It\u0027s true", "timestamp": "00:38:35,670", "timestamp_s": 2315.0}, {"text": "that overall we have some situations when", "timestamp": "00:38:39,720", "timestamp_s": 2319.0}, {"text": "it would have been better to wait for", "timestamp": "00:38:42,824", "timestamp_s": 2322.0}, {"text": "the answer to arrive instead of just going on with another", "timestamp": "00:38:47,770", "timestamp_s": 2327.0}, {"text": "backup request, because that can also fail. But overall at large scale,", "timestamp": "00:38:51,260", "timestamp_s": 2331.0}, {"text": "statistically we are still better, still performing better.", "timestamp": "00:38:55,122", "timestamp_s": 2335.0}, {"text": "Okay, now the other use case is coming from a scenario where", "timestamp": "00:38:59,950", "timestamp_s": 2339.0}, {"text": "I had to design a system with a read heavy workload,", "timestamp": "00:39:03,488", "timestamp_s": 2343.0}, {"text": "a very read heavy workload,", "timestamp": "00:39:06,966", "timestamp_s": 2346.0}, {"text": "and the writes were theoretically", "timestamp": "00:39:09,970", "timestamp_s": 2349.0}, {"text": "almost immutable. They were not changing at all.", "timestamp": "00:39:14,938", "timestamp_s": 2354.0}, {"text": "So we just created objects in this", "timestamp": "00:39:18,532", "timestamp_s": 2358.0}, {"text": "part of the architecture. We were hardly changing", "timestamp": "00:39:22,452", "timestamp_s": 2362.0}, {"text": "or updating them.", "timestamp": "00:39:25,742", "timestamp_s": 2365.0}, {"text": "I needed to reach a very low latency requirement.", "timestamp": "00:39:30,070", "timestamp_s": 2370.0}, {"text": "So this as", "timestamp": "00:39:33,486", "timestamp_s": 2373.0}, {"text": "the result meant that I had", "timestamp": "00:39:37,756", "timestamp_s": 2377.0}, {"text": "to prevent scenarios were I had to deal with cold cache with", "timestamp": "00:39:41,468", "timestamp_s": 2381.0}, {"text": "something that comes and reads", "timestamp": "00:39:44,972", "timestamp_s": 2384.0}, {"text": "up all the data from the database. And I", "timestamp": "00:39:48,374", "timestamp_s": 2388.0}, {"text": "couldn\u0027t use any distributed cache solution for this situation because", "timestamp": "00:39:52,048", "timestamp_s": 2392.0}, {"text": "it would have hurt latency so much that", "timestamp": "00:39:55,680", "timestamp_s": 2395.0}, {"text": "it would have been impossible to meet this low latency requirement.", "timestamp": "00:40:01,170", "timestamp_s": 2401.0}, {"text": "Were comes cqrs in the play. So instead of just trying", "timestamp": "00:40:06,690", "timestamp_s": 2406.0}, {"text": "to put everything in a single service and try to fine tune", "timestamp": "00:40:10,292", "timestamp_s": 2410.0}, {"text": "and optimize that and try to benchmark things and try", "timestamp": "00:40:14,014", "timestamp_s": 2414.0}, {"text": "to find a bottleneck and improve,", "timestamp": "00:40:17,848", "timestamp_s": 2417.0}, {"text": "you should think in a larger picture", "timestamp": "00:40:22,310", "timestamp_s": 2422.0}, {"text": "and you should use the techniques that I talked about. So we", "timestamp": "00:40:26,078", "timestamp_s": 2426.0}, {"text": "separated the bright path that you can see on the left because we are not", "timestamp": "00:40:29,612", "timestamp_s": 2429.0}, {"text": "really interested in the latency of the bright operations.", "timestamp": "00:40:32,812", "timestamp_s": 2432.0}, {"text": "It was not critical for these writes to", "timestamp": "00:40:36,250", "timestamp_s": 2436.0}, {"text": "happen immediately. So we went", "timestamp": "00:40:40,496", "timestamp_s": 2440.0}, {"text": "on to AWS queue and then continued the", "timestamp": "00:40:43,760", "timestamp_s": 2443.0}, {"text": "write to the database side. And it was", "timestamp": "00:40:47,808", "timestamp_s": 2447.0}, {"text": "true that we were having large write workloads, but because", "timestamp": "00:40:51,172", "timestamp_s": 2451.0}, {"text": "we were writing through a queue, it helps us to keep", "timestamp": "00:40:55,076", "timestamp_s": 2455.0}, {"text": "the write operations on the dynamodb lower and", "timestamp": "00:41:00,050", "timestamp_s": 2460.0}, {"text": "iron out these bursty", "timestamp": "00:41:05,336", "timestamp_s": 2465.0}, {"text": "operations and help to keep the write capacity unit", "timestamp": "00:41:09,006", "timestamp_s": 2469.0}, {"text": "lower than usual. Now for", "timestamp": "00:41:12,590", "timestamp_s": 2472.0}, {"text": "the reads, we introduced a distributed", "timestamp": "00:41:15,708", "timestamp_s": 2475.0}, {"text": "in memory cache solution with hazelcast that was also replicating", "timestamp": "00:41:18,962", "timestamp_s": 2478.0}, {"text": "between each read node. So this", "timestamp": "00:41:23,682", "timestamp_s": 2483.0}, {"text": "resulted us a couple of things. Like if I scale out", "timestamp": "00:41:27,324", "timestamp_s": 2487.0}, {"text": "and have fresh read node, it should not come up with", "timestamp": "00:41:31,088", "timestamp_s": 2491.0}, {"text": "an empty memory database. So immediately when a new read node", "timestamp": "00:41:34,592", "timestamp_s": 2494.0}, {"text": "comes in, it starts synchronizing with the other read", "timestamp": "00:41:38,438", "timestamp_s": 2498.0}, {"text": "nodes, which helps the data being fresh.", "timestamp": "00:41:41,792", "timestamp_s": 2501.0}, {"text": "Also, when we have a cache miss in one of the read nodes,", "timestamp": "00:41:45,130", "timestamp_s": 2505.0}, {"text": "it finds new data. By going to DynamoDB, it immediately", "timestamp": "00:41:48,410", "timestamp_s": 2508.0}, {"text": "proactively starts replicating this data and writing it", "timestamp": "00:41:52,682", "timestamp_s": 2512.0}, {"text": "to other need nodes. So that\u0027s how we solved", "timestamp": "00:41:56,088", "timestamp_s": 2516.0}, {"text": "keeping the cache warm with multi writes, with using replication", "timestamp": "00:42:00,790", "timestamp_s": 2520.0}, {"text": "as well. And the bright instance or", "timestamp": "00:42:05,006", "timestamp_s": 2525.0}, {"text": "the bright service responsible for the writes", "timestamp": "00:42:08,972", "timestamp_s": 2528.0}, {"text": "had nothing to do with Hazelcast was", "timestamp": "00:42:13,290", "timestamp_s": 2533.0}, {"text": "not aware of this complicated configuration", "timestamp": "00:42:16,812", "timestamp_s": 2536.0}, {"text": "of the memory cache solution.", "timestamp": "00:42:20,322", "timestamp_s": 2540.0}, {"text": "Also, the read instances did not", "timestamp": "00:42:22,750", "timestamp_s": 2542.0}, {"text": "have to have the SQS based libraries and did", "timestamp": "00:42:26,352", "timestamp_s": 2546.0}, {"text": "not have to do anything with SQS at", "timestamp": "00:42:30,308", "timestamp_s": 2550.0}, {"text": "all with that communication with the access and so on and so", "timestamp": "00:42:33,508", "timestamp_s": 2553.0}, {"text": "forth. So this simplifies your architecture overall even further.", "timestamp": "00:42:37,188", "timestamp_s": 2557.0}, {"text": "Okay, now with client libraries, I have a", "timestamp": "00:42:42,870", "timestamp_s": 2562.0}, {"text": "couple of stories. I treat client", "timestamp": "00:42:46,264", "timestamp_s": 2566.0}, {"text": "libraries as a double edged sword because I", "timestamp": "00:42:49,742", "timestamp_s": 2569.0}, {"text": "think it\u0027s very hard to design them in an effective way.", "timestamp": "00:42:53,208", "timestamp_s": 2573.0}, {"text": "They are not considering all these extensibility", "timestamp": "00:42:57,690", "timestamp_s": 2577.0}, {"text": "options and not all the features can be", "timestamp": "00:43:00,850", "timestamp_s": 2580.0}, {"text": "turned off in this specific", "timestamp": "00:43:04,124", "timestamp_s": 2584.0}, {"text": "client library.", "timestamp": "00:43:07,388", "timestamp_s": 2587.0}, {"text": "This library was included in many, many services as dependencies,", "timestamp": "00:43:10,750", "timestamp_s": 2590.0}, {"text": "as default dependencies, and this was the only way of use", "timestamp": "00:43:14,982", "timestamp_s": 2594.0}, {"text": "a specific shared feature of the whole architecture.", "timestamp": "00:43:18,656", "timestamp_s": 2598.0}, {"text": "This specific client library downloaded a", "timestamp": "00:43:21,978", "timestamp_s": 2601.0}, {"text": "zipped archive at startup from Amazon s three", "timestamp": "00:43:25,204", "timestamp_s": 2605.0}, {"text": "and started decompressing", "timestamp": "00:43:29,332", "timestamp_s": 2609.0}, {"text": "it during startup just to get the configuration data", "timestamp": "00:43:32,922", "timestamp_s": 2612.0}, {"text": "required for this library to work effectively. This was", "timestamp": "00:43:36,920", "timestamp_s": 2616.0}, {"text": "the single bottleneck for the startup of the whole service and", "timestamp": "00:43:40,808", "timestamp_s": 2620.0}, {"text": "unfortunately hidden behind", "timestamp": "00:43:45,128", "timestamp_s": 2625.0}, {"text": "the scenes, it messed up the startup for", "timestamp": "00:43:48,556", "timestamp_s": 2628.0}, {"text": "all the development environments.", "timestamp": "00:43:52,220", "timestamp_s": 2632.0}, {"text": "So all the developers suffered from the startup", "timestamp": "00:43:55,410", "timestamp_s": 2635.0}, {"text": "penalty introduced by this client library.", "timestamp": "00:43:58,982", "timestamp_s": 2638.0}, {"text": "Unfortunately, there was no way to turn off the library itself", "timestamp": "00:44:02,110", "timestamp_s": 2642.0}, {"text": "because the maintaining team wanted to keep it as", "timestamp": "00:44:06,350", "timestamp_s": 2646.0}, {"text": "simple as possible. They did not want", "timestamp": "00:44:09,712", "timestamp_s": 2649.0}, {"text": "anyone to change", "timestamp": "00:44:12,884", "timestamp_s": 2652.0}, {"text": "certain functionalities of the library and they thought that if", "timestamp": "00:44:17,972", "timestamp_s": 2657.0}, {"text": "they introduced too many features for the library for customization,", "timestamp": "00:44:21,268", "timestamp_s": 2661.0}, {"text": "then they will have hard times with the maintenance of the library", "timestamp": "00:44:25,110", "timestamp_s": 2665.0}, {"text": "because were could be plenty of ways teams are using", "timestamp": "00:44:28,686", "timestamp_s": 2668.0}, {"text": "that. So how", "timestamp": "00:44:32,536", "timestamp_s": 2672.0}, {"text": "we solve this problem.", "timestamp": "00:44:36,392", "timestamp_s": 2676.0}, {"text": "Okay, sorry. So another issue with client libraries", "timestamp": "00:44:39,910", "timestamp_s": 2679.0}, {"text": "was that it often messed up the deployments. So because", "timestamp": "00:44:43,362", "timestamp_s": 2683.0}, {"text": "health check was depending on the functionality", "timestamp": "00:44:47,790", "timestamp_s": 2687.0}, {"text": "of the library itself, midbed was failure during downloading the", "timestamp": "00:44:52,502", "timestamp_s": 2692.0}, {"text": "data. The service often just timed out and", "timestamp": "00:44:55,968", "timestamp_s": 2695.0}, {"text": "failed to meet the health check requirements. That messed up the deployment.", "timestamp": "00:44:59,888", "timestamp_s": 2699.0}, {"text": "So even deployments were negatively affected by the usage", "timestamp": "00:45:04,610", "timestamp_s": 2704.0}, {"text": "of this library. So how do we solve", "timestamp": "00:45:08,218", "timestamp_s": 2708.0}, {"text": "this issue? We just simply shift things on", "timestamp": "00:45:11,418", "timestamp_s": 2711.0}, {"text": "the left side during delivery, during deployment.", "timestamp": "00:45:15,464", "timestamp_s": 2715.0}, {"text": "What this meant for us, we just picked a mock", "timestamp": "00:45:19,198", "timestamp_s": 2719.0}, {"text": "s three. S three solution, containerized that using", "timestamp": "00:45:23,102", "timestamp_s": 2723.0}, {"text": "Docker and used it as a sidecar container. We prepared", "timestamp": "00:45:27,356", "timestamp_s": 2727.0}, {"text": "two mock libraries in", "timestamp": "00:45:32,034", "timestamp_s": 2732.0}, {"text": "this mock storage. One of them was empty, meaning that it", "timestamp": "00:45:37,628", "timestamp_s": 2737.0}, {"text": "was very fast to download and extract that, and the other one", "timestamp": "00:45:41,488", "timestamp_s": 2741.0}, {"text": "downloaded the same way,", "timestamp": "00:45:45,550", "timestamp_s": 2745.0}, {"text": "downloaded the data from the s three bucket, then extracted that and", "timestamp": "00:45:48,592", "timestamp_s": 2748.0}, {"text": "just had that certain configuration which", "timestamp": "00:45:51,812", "timestamp_s": 2751.0}, {"text": "was meaningful for us, and we put that into", "timestamp": "00:45:55,300", "timestamp_s": 2755.0}, {"text": "this mock container. The thing why this worked was very", "timestamp": "00:45:58,852", "timestamp_s": 2758.0}, {"text": "simple. It was because they", "timestamp": "00:46:02,856", "timestamp_s": 2762.0}, {"text": "did not have a feature switch or a queue switch for the library itself,", "timestamp": "00:46:07,112", "timestamp_s": 2767.0}, {"text": "but the URL was configurable because each specific", "timestamp": "00:46:10,888", "timestamp_s": 2770.0}, {"text": "environment had its own bucket configuration.", "timestamp": "00:46:15,304", "timestamp_s": 2775.0}, {"text": "Another way other teams solved this problem was introducing some kind of", "timestamp": "00:46:20,570", "timestamp_s": 2780.0}, {"text": "programmatic proxy, which was still trying", "timestamp": "00:46:24,684", "timestamp_s": 2784.0}, {"text": "to keep things on when this connection was not available.", "timestamp": "00:46:28,720", "timestamp_s": 2788.0}, {"text": "And behind the scenes try to kick in this client library", "timestamp": "00:46:33,040", "timestamp_s": 2793.0}, {"text": "to retry loading up the data and", "timestamp": "00:46:37,126", "timestamp_s": 2797.0}, {"text": "delaying the downloading of this data", "timestamp": "00:46:40,692", "timestamp_s": 2800.0}, {"text": "that was important for them. Okay. Rather thing,", "timestamp": "00:46:45,860", "timestamp_s": 2805.0}, {"text": "another technique that I want to talk about is what I call region", "timestamp": "00:46:49,604", "timestamp_s": 2809.0}, {"text": "pinning. This might be not necessarily the appropriate", "timestamp": "00:46:53,722", "timestamp_s": 2813.0}, {"text": "name for this technique, but this is how everybody was", "timestamp": "00:46:57,422", "timestamp_s": 2817.0}, {"text": "using that. So this is how I refer to that as well.", "timestamp": "00:47:00,712", "timestamp_s": 2820.0}, {"text": "Imagine the following scenario.", "timestamp": "00:47:04,070", "timestamp_s": 2824.0}, {"text": "We had to migrate to the cloud with our own whole architecture,", "timestamp": "00:47:07,690", "timestamp_s": 2827.0}, {"text": "the whole system, and then had to make it multiregion.", "timestamp": "00:47:12,082", "timestamp_s": 2832.0}, {"text": "And one of the problem", "timestamp": "00:47:16,090", "timestamp_s": 2836.0}, {"text": "was what we found was with shopping calls.", "timestamp": "00:47:19,840", "timestamp_s": 2839.0}, {"text": "So each service was, there was not", "timestamp": "00:47:23,558", "timestamp_s": 2843.0}, {"text": "a dedicated shopping cart service. Each service was managing", "timestamp": "00:47:26,912", "timestamp_s": 2846.0}, {"text": "the shopping cart through a shared database,", "timestamp": "00:47:31,178", "timestamp_s": 2851.0}, {"text": "namely a shared Cassandra cluster. So each service", "timestamp": "00:47:35,626", "timestamp_s": 2855.0}, {"text": "was reading the shopping cart from a shant Cassandra cluster through a library,", "timestamp": "00:47:39,508", "timestamp_s": 2859.0}, {"text": "and was updating the shopping cart when was necessary through a", "timestamp": "00:47:44,270", "timestamp_s": 2864.0}, {"text": "shared library, and then putting the", "timestamp": "00:47:48,008", "timestamp_s": 2868.0}, {"text": "data to this shared database. Okay. And we", "timestamp": "00:47:51,208", "timestamp_s": 2871.0}, {"text": "could not really rely on the replication lag because it", "timestamp": "00:47:57,132", "timestamp_s": 2877.0}, {"text": "was uncontrollable. So the minimum", "timestamp": "00:48:00,668", "timestamp_s": 2880.0}, {"text": "replication lag was around 60 milliseconds.", "timestamp": "00:48:04,002", "timestamp_s": 2884.0}, {"text": "But in practice, if you have more pressure on the database,", "timestamp": "00:48:07,202", "timestamp_s": 2887.0}, {"text": "this could have been easily increased to 500 600 milliseconds", "timestamp": "00:48:10,406", "timestamp_s": 2890.0}, {"text": "based on the load itself. Okay.", "timestamp": "00:48:14,902", "timestamp_s": 2894.0}, {"text": "And we were afraid that these shopping carts will disappear. So if one", "timestamp": "00:48:20,190", "timestamp_s": 2900.0}, {"text": "of the service or the traffic for one of the service is", "timestamp": "00:48:24,068", "timestamp_s": 2904.0}, {"text": "put to the right side during the user journey, we were afraid", "timestamp": "00:48:27,780", "timestamp_s": 2907.0}, {"text": "that when the shopping cart was being loaded because of", "timestamp": "00:48:31,658", "timestamp_s": 2911.0}, {"text": "the replication lag, it was not available, it was empty, or maybe", "timestamp": "00:48:34,868", "timestamp_s": 2914.0}, {"text": "it\u0027s not containing the up to date changes. So how do we", "timestamp": "00:48:38,392", "timestamp_s": 2918.0}, {"text": "distinguish between users that have up to date shopping cart", "timestamp": "00:48:41,976", "timestamp_s": 2921.0}, {"text": "in the left region from those that, let\u0027s say,", "timestamp": "00:48:45,982", "timestamp_s": 2925.0}, {"text": "don\u0027t have yet shopping cart or started their journey on", "timestamp": "00:48:49,692", "timestamp_s": 2929.0}, {"text": "the right region? That\u0027s where region pinning is coming into play.", "timestamp": "00:48:53,004", "timestamp_s": 2933.0}, {"text": "We flag the originating", "timestamp": "00:48:57,390", "timestamp_s": 2937.0}, {"text": "region for each user by using a cookie, and when", "timestamp": "00:49:01,510", "timestamp_s": 2941.0}, {"text": "moving those certain users who need their shopping", "timestamp": "00:49:05,136", "timestamp_s": 2945.0}, {"text": "cart, but started their user journey in the left", "timestamp": "00:49:08,438", "timestamp_s": 2948.0}, {"text": "region to the right side. Based on this cookie, we reach", "timestamp": "00:49:11,796", "timestamp_s": 2951.0}, {"text": "out through this white line,", "timestamp": "00:49:15,316", "timestamp_s": 2955.0}, {"text": "paying that 60 millisecond latency penalty,", "timestamp": "00:49:19,076", "timestamp_s": 2959.0}, {"text": "but loading the data consistently and having", "timestamp": "00:49:22,378", "timestamp_s": 2962.0}, {"text": "the shopping cart. So with this way we", "timestamp": "00:49:26,296", "timestamp_s": 2966.0}, {"text": "won\u0027t penalize calls the users because we are not going through", "timestamp": "00:49:29,528", "timestamp_s": 2969.0}, {"text": "this white line for all the users all the time.", "timestamp": "00:49:36,230", "timestamp_s": 2976.0}, {"text": "We just proactively select those users which needs", "timestamp": "00:49:39,500", "timestamp_s": 2979.0}, {"text": "more consistency but should be okay", "timestamp": "00:49:44,650", "timestamp_s": 2984.0}, {"text": "with this latest increase.", "timestamp": "00:49:48,384", "timestamp_s": 2988.0}, {"text": "In another example, I was using the same approach", "timestamp": "00:49:53,950", "timestamp_s": 2993.0}, {"text": "when I was designing an auction solution.", "timestamp": "00:49:58,070", "timestamp_s": 2998.0}, {"text": "So here we had to keep a strict", "timestamp": "00:50:02,050", "timestamp_s": 3002.0}, {"text": "ordering of the biddings. And no", "timestamp": "00:50:05,322", "timestamp_s": 3005.0}, {"text": "matter which message provider I looked at,", "timestamp": "00:50:10,356", "timestamp_s": 3010.0}, {"text": "I figured out that there is no way to keep", "timestamp": "00:50:13,592", "timestamp_s": 3013.0}, {"text": "the messages in order when I rely on synchronization", "timestamp": "00:50:17,432", "timestamp_s": 3017.0}, {"text": "between regions. Okay, so if the auction started", "timestamp": "00:50:21,678", "timestamp_s": 3021.0}, {"text": "on the left side in the", "timestamp": "00:50:25,944", "timestamp_s": 3025.0}, {"text": "kafka cluster on the left side, and somebody wanted", "timestamp": "00:50:29,192", "timestamp_s": 3029.0}, {"text": "to participate in that auction on", "timestamp": "00:50:33,260", "timestamp_s": 3033.0}, {"text": "the other region, they had to pay for this latency", "timestamp": "00:50:36,748", "timestamp_s": 3036.0}, {"text": "penalty proactively, but right to", "timestamp": "00:50:40,326", "timestamp_s": 3040.0}, {"text": "the kafka cluster still on the left side.", "timestamp": "00:50:44,128", "timestamp_s": 3044.0}, {"text": "And then for other services or participants", "timestamp": "00:50:47,472", "timestamp_s": 3047.0}, {"text": "which were not latency sensitive, the data was", "timestamp": "00:50:51,466", "timestamp_s": 3051.0}, {"text": "still available, maybe later in a synchronous way through the", "timestamp": "00:50:54,852", "timestamp_s": 3054.0}, {"text": "mirror maker on the right side. So they could have continued", "timestamp": "00:50:58,388", "timestamp_s": 3058.0}, {"text": "processing this data or maybe showing this data a bit", "timestamp": "00:51:04,150", "timestamp_s": 3064.0}, {"text": "later. But those who wanted to participate in bidding had", "timestamp": "00:51:07,832", "timestamp_s": 3067.0}, {"text": "to write consistently on the same region", "timestamp": "00:51:11,816", "timestamp_s": 3071.0}, {"text": "as the auction is originated. This is similar", "timestamp": "00:51:15,570", "timestamp_s": 3075.0}, {"text": "to the things we used to in the gaming", "timestamp": "00:51:19,292", "timestamp_s": 3079.0}, {"text": "world. So if there\u0027s a game started in", "timestamp": "00:51:22,722", "timestamp_s": 3082.0}, {"text": "one certain region, then when other players", "timestamp": "00:51:26,476", "timestamp_s": 3086.0}, {"text": "are participating, they have to consider the latency penalty", "timestamp": "00:51:30,022", "timestamp_s": 3090.0}, {"text": "and maybe not perform that well. But this keeps", "timestamp": "00:51:33,526", "timestamp_s": 3093.0}, {"text": "the game consistent. Okay, the last example is", "timestamp": "00:51:36,918", "timestamp_s": 3096.0}, {"text": "coming from the journey of optimizing", "timestamp": "00:51:40,468", "timestamp_s": 3100.0}, {"text": "a single service in a couple of iterations.", "timestamp": "00:51:44,314", "timestamp_s": 3104.0}, {"text": "It. So I would explain the", "timestamp": "00:51:52,870", "timestamp_s": 3112.0}, {"text": "behavior and the functionality of the service a bit more at first.", "timestamp": "00:51:56,632", "timestamp_s": 3116.0}, {"text": "Okay, so this service had two endpoints.", "timestamp": "00:52:00,296", "timestamp_s": 3120.0}, {"text": "First of all, it was a very simple key value store.", "timestamp": "00:52:03,806", "timestamp_s": 3123.0}, {"text": "You can see the value endpoint on the right side of the service,", "timestamp": "00:52:11,770", "timestamp_s": 3131.0}, {"text": "which reads the values for a certain key given.", "timestamp": "00:52:16,970", "timestamp_s": 3136.0}, {"text": "Okay? And that\u0027s coming to specific", "timestamp": "00:52:20,448", "timestamp_s": 3140.0}, {"text": "DynamoDB table, which has its own read capacity", "timestamp": "00:52:23,952", "timestamp_s": 3143.0}, {"text": "unit defined. But we have also another endpoint.", "timestamp": "00:52:27,862", "timestamp_s": 3147.0}, {"text": "This is how things were when we got the service,", "timestamp": "00:52:32,610", "timestamp_s": 3152.0}, {"text": "when we started maintaining that. So this is what we", "timestamp": "00:52:36,708", "timestamp_s": 3156.0}, {"text": "kept. This was the endpoint of the recent keys.", "timestamp": "00:52:40,788", "timestamp_s": 3160.0}, {"text": "What designpoint did was that it gave back to the clients", "timestamp": "00:52:45,110", "timestamp_s": 3165.0}, {"text": "the keys that were being accessed by that specific", "timestamp": "00:52:49,454", "timestamp_s": 3169.0}, {"text": "client in the last seven days. That went off", "timestamp": "00:52:53,528", "timestamp_s": 3173.0}, {"text": "to another table, which was statistical", "timestamp": "00:52:57,532", "timestamp_s": 3177.0}, {"text": "table that you can see on the left, which had their own read", "timestamp": "00:53:01,586", "timestamp_s": 3181.0}, {"text": "and write capacity unit defined. Now, both tables had", "timestamp": "00:53:05,292", "timestamp_s": 3185.0}, {"text": "auto scaling configured, but there was a problem. As you can see on", "timestamp": "00:53:09,036", "timestamp_s": 3189.0}, {"text": "the right side diagram, there were one specific client which was", "timestamp": "00:53:12,384", "timestamp_s": 3192.0}, {"text": "firing up 15,000 requests to this values", "timestamp": "00:53:15,936", "timestamp_s": 3195.0}, {"text": "endpoint, because it had 15,000 keys in", "timestamp": "00:53:19,318", "timestamp_s": 3199.0}, {"text": "this usually being used. And it", "timestamp": "00:53:23,028", "timestamp_s": 3203.0}, {"text": "went with one single request to this recent keys. Endpoint found", "timestamp": "00:53:26,772", "timestamp_s": 3206.0}, {"text": "that in the last seven days it was using these 15,000 keys,", "timestamp": "00:53:30,756", "timestamp_s": 3210.0}, {"text": "and iteratively it went through calls, the keys bashing", "timestamp": "00:53:34,638", "timestamp_s": 3214.0}, {"text": "sequentially, these values endpoint increasing traffic", "timestamp": "00:53:38,470", "timestamp_s": 3218.0}, {"text": "and the dynamodb auto scaling was not catching up.", "timestamp": "00:53:42,390", "timestamp_s": 3222.0}, {"text": "There was a time window defined by this", "timestamp": "00:53:48,010", "timestamp_s": 3228.0}, {"text": "red line that you can see on the screen when DynamoDB", "timestamp": "00:53:51,852", "timestamp_s": 3231.0}, {"text": "was throttling because it failed to meet the capacity needs", "timestamp": "00:53:55,858", "timestamp_s": 3235.0}, {"text": "for the traffic. And unfortunately,", "timestamp": "00:53:59,536", "timestamp_s": 3239.0}, {"text": "when the traffic burst was over,", "timestamp": "00:54:02,726", "timestamp_s": 3242.0}, {"text": "auto scaling increased for DynamoDB, but then the increase decreased", "timestamp": "00:54:05,760", "timestamp_s": 3245.0}, {"text": "immediately after that. Okay, now one of the problem was that", "timestamp": "00:54:10,230", "timestamp_s": 3250.0}, {"text": "the statistics table was written", "timestamp": "00:54:14,580", "timestamp_s": 3254.0}, {"text": "sequentially before the read happens, when getting the values,", "timestamp": "00:54:18,602", "timestamp_s": 3258.0}, {"text": "just to keep the statistics fresh. So in", "timestamp": "00:54:22,554", "timestamp_s": 3262.0}, {"text": "maintenance, during maintenance, we had to keep the write capacity units", "timestamp": "00:54:26,072", "timestamp_s": 3266.0}, {"text": "the same as the read capacity units for the two", "timestamp": "00:54:29,358", "timestamp_s": 3269.0}, {"text": "tables, because auto scaling was hard to", "timestamp": "00:54:32,776", "timestamp_s": 3272.0}, {"text": "fine tune, and because if", "timestamp": "00:54:36,252", "timestamp_s": 3276.0}, {"text": "the write capacity units are lower, we are just failing", "timestamp": "00:54:42,268", "timestamp_s": 3282.0}, {"text": "by writing the statistics table. And the reads were also failing.", "timestamp": "00:54:46,162", "timestamp_s": 3286.0}, {"text": "So what we did first was separating this critical path", "timestamp": "00:54:50,510", "timestamp_s": 3290.0}, {"text": "from the rest. So we put", "timestamp": "00:54:54,182", "timestamp_s": 3294.0}, {"text": "the update operation just in a different thread", "timestamp": "00:54:58,336", "timestamp_s": 3298.0}, {"text": "and just deployed solution. Things worked", "timestamp": "00:55:02,006", "timestamp_s": 3302.0}, {"text": "bit better than before, but after a short", "timestamp": "00:55:06,130", "timestamp_s": 3306.0}, {"text": "period of time, there were other problems were seen,", "timestamp": "00:55:09,716", "timestamp_s": 3309.0}, {"text": "that for some reason the", "timestamp": "00:55:13,528", "timestamp_s": 3313.0}, {"text": "memory consumption of the services were increasing", "timestamp": "00:55:17,272", "timestamp_s": 3317.0}, {"text": "and the container orchestrator started killing the services", "timestamp": "00:55:21,070", "timestamp_s": 3321.0}, {"text": "themselves. And the", "timestamp": "00:55:25,080", "timestamp_s": 3325.0}, {"text": "reason of that was in the flow of the implementation.", "timestamp": "00:55:30,188", "timestamp_s": 3330.0}, {"text": "This was the", "timestamp": "00:55:35,450", "timestamp_s": 3335.0}, {"text": "way we created the thread that", "timestamp": "00:55:39,308", "timestamp_s": 3339.0}, {"text": "was dealing with this change. This is in Java,", "timestamp": "00:55:43,180", "timestamp_s": 3343.0}, {"text": "but you don\u0027t necessarily have to understand Java to understand this use case.", "timestamp": "00:55:46,642", "timestamp_s": 3346.0}, {"text": "We just created a separate", "timestamp": "00:55:50,348", "timestamp_s": 3350.0}, {"text": "thread that had a separate queue that is", "timestamp": "00:55:54,218", "timestamp_s": 3354.0}, {"text": "processing these requests when they are coming in and updating the statistics", "timestamp": "00:55:58,292", "timestamp_s": 3358.0}, {"text": "table. Now where comes the problem?", "timestamp": "00:56:02,026", "timestamp_s": 3362.0}, {"text": "What do you think? Where is the problem with", "timestamp": "00:56:05,510", "timestamp_s": 3365.0}, {"text": "this implementation that\u0027s causing the memory increase and the killing", "timestamp": "00:56:09,992", "timestamp_s": 3369.0}, {"text": "of the instances from time to time and restarting them?", "timestamp": "00:56:13,838", "timestamp_s": 3373.0}, {"text": "Well, it\u0027s not obvious because calls the implementation details,", "timestamp": "00:56:18,410", "timestamp_s": 3378.0}, {"text": "but this is the single place when there is a problem.", "timestamp": "00:56:22,002", "timestamp_s": 3382.0}, {"text": "So very often Java", "timestamp": "00:56:24,924", "timestamp_s": 3384.0}, {"text": "old school threaded implementations are coming with this unbounded queue,", "timestamp": "00:56:29,790", "timestamp_s": 3389.0}, {"text": "meaning you have a limitless queue. And what happens when you have", "timestamp": "00:56:34,510", "timestamp_s": 3394.0}, {"text": "an increased arrival rate and your departure rate", "timestamp": "00:56:38,512", "timestamp_s": 3398.0}, {"text": "is much lower? You will have this unbounded queue filled up,", "timestamp": "00:56:41,988", "timestamp_s": 3401.0}, {"text": "your latency will increase up to infinity, and your", "timestamp": "00:56:46,020", "timestamp_s": 3406.0}, {"text": "memory consumption will also increase up", "timestamp": "00:56:49,828", "timestamp_s": 3409.0}, {"text": "to infinity, up to where", "timestamp": "00:56:53,464", "timestamp_s": 3413.0}, {"text": "you can hold this data, because you", "timestamp": "00:56:58,328", "timestamp_s": 3418.0}, {"text": "have a very huge queue sitting there for no reason.", "timestamp": "00:57:02,120", "timestamp_s": 3422.0}, {"text": "And actually this is not even true, because this is not an unbounded", "timestamp": "00:57:06,070", "timestamp_s": 3426.0}, {"text": "queue, it\u0027s not erasing data automatically, it gets", "timestamp": "00:57:09,858", "timestamp_s": 3429.0}, {"text": "full, it\u0027s just a very huge queue because it\u0027s", "timestamp": "00:57:13,116", "timestamp_s": 3433.0}, {"text": "implemented by using the maximum value.", "timestamp": "00:57:17,058", "timestamp_s": 3437.0}, {"text": "Java is declaring for integers,", "timestamp": "00:57:19,728", "timestamp_s": 3439.0}, {"text": "right? So we can do better than that. So we", "timestamp": "00:57:23,550", "timestamp_s": 3443.0}, {"text": "iterated with this implementation and we just simply", "timestamp": "00:57:27,280", "timestamp_s": 3447.0}, {"text": "challenges to a more sophisticated solution. We started using", "timestamp": "00:57:31,354", "timestamp_s": 3451.0}, {"text": "resiliency for J, which is a library", "timestamp": "00:57:35,236", "timestamp_s": 3455.0}, {"text": "for implementing resiliency patterns in Java,", "timestamp": "00:57:38,666", "timestamp_s": 3458.0}, {"text": "and we wrapped the call with a bulkhead which now", "timestamp": "00:57:42,906", "timestamp_s": 3462.0}, {"text": "had a limited queue capacity, up to 25 items.", "timestamp": "00:57:47,000", "timestamp_s": 3467.0}, {"text": "And when that queue was full, bulkhead was throwing", "timestamp": "00:57:52,390", "timestamp_s": 3472.0}, {"text": "an exception. Now we could have exception because", "timestamp": "00:57:56,018", "timestamp_s": 3476.0}, {"text": "of two cases. First of all, because DynamoDB is starting", "timestamp": "00:57:59,756", "timestamp_s": 3479.0}, {"text": "throttling, it\u0027s just rejecting our request. Or the", "timestamp": "00:58:03,532", "timestamp_s": 3483.0}, {"text": "bulkhead was full, so were wrapped this whole thing, this whole", "timestamp": "00:58:06,988", "timestamp_s": 3486.0}, {"text": "calls into a circuit breaker, and the circuit breaker", "timestamp": "00:58:10,592", "timestamp_s": 3490.0}, {"text": "just opened when it saw these two exceptions,", "timestamp": "00:58:14,694", "timestamp_s": 3494.0}, {"text": "give the whole thing a pause and then started again updating", "timestamp": "00:58:17,990", "timestamp_s": 3497.0}, {"text": "the statistics. And this", "timestamp": "00:58:21,482", "timestamp_s": 3501.0}, {"text": "helped us recover from the situation from", "timestamp": "00:58:25,636", "timestamp_s": 3505.0}, {"text": "before. We did not have these memory", "timestamp": "00:58:30,212", "timestamp_s": 3510.0}, {"text": "issues, the clients did not really notice anything", "timestamp": "00:58:33,514", "timestamp_s": 3513.0}, {"text": "at all. And we looked actually at the metrics of the circuit breaker,", "timestamp": "00:58:37,048", "timestamp_s": 3517.0}, {"text": "looked at the statistics of when they were opened", "timestamp": "00:58:41,598", "timestamp_s": 3521.0}, {"text": "up, and it was not a big number.", "timestamp": "00:58:46,482", "timestamp_s": 3526.0}, {"text": "So statistics not really suffered because of that.", "timestamp": "00:58:50,730", "timestamp_s": 3530.0}, {"text": "And normally behaving clients could just", "timestamp": "00:58:54,172", "timestamp_s": 3534.0}, {"text": "keep up with their normal operation, maybe having a couple", "timestamp": "00:58:57,676", "timestamp_s": 3537.0}, {"text": "of more cache misses than usual, but with", "timestamp": "00:59:01,376", "timestamp_s": 3541.0}, {"text": "metrics, and with this solution and investigating metrics,", "timestamp": "00:59:06,896", "timestamp_s": 3546.0}, {"text": "we thought we were fine, so we dropped the right copper units.", "timestamp": "00:59:10,886", "timestamp_s": 3550.0}, {"text": "Finally, for a statistics table that did not have to match", "timestamp": "00:59:14,378", "timestamp_s": 3554.0}, {"text": "with the read part of", "timestamp": "00:59:18,610", "timestamp_s": 3558.0}, {"text": "the values table. Okay.", "timestamp": "00:59:23,028", "timestamp_s": 3563.0}, {"text": "But we really wanted to reduce also the read capacity unit", "timestamp": "00:59:27,170", "timestamp_s": 3567.0}, {"text": "for the reads. So again, let\u0027s go back to the baseline", "timestamp": "00:59:30,766", "timestamp_s": 3570.0}, {"text": "and talk about what we have with a bit more detail. So we", "timestamp": "00:59:34,526", "timestamp_s": 3574.0}, {"text": "have 15,000 items coming in, 15,000 requests", "timestamp": "00:59:37,884", "timestamp_s": 3577.0}, {"text": "like almost instantly.", "timestamp": "00:59:42,242", "timestamp_s": 3582.0}, {"text": "And then we had these services packed in", "timestamp": "00:59:45,850", "timestamp_s": 3585.0}, {"text": "an auto scaling group that was cpu based.", "timestamp": "00:59:49,808", "timestamp_s": 3589.0}, {"text": "And then Dynamodb was throttling and giving us back HTTP", "timestamp": "00:59:53,310", "timestamp_s": 3593.0}, {"text": "400 errors on any case when we", "timestamp": "00:59:57,942", "timestamp_s": 3597.0}, {"text": "breached our read capacity unit.", "timestamp": "01:00:01,476", "timestamp_s": 3601.0}, {"text": "So what if we do retry? So what if we have this HTTP 400", "timestamp": "01:00:08,130", "timestamp_s": 3608.0}, {"text": "errors? We just retry the request and hope now that the read", "timestamp": "01:00:11,780", "timestamp_s": 3611.0}, {"text": "capacity unit was catching up,", "timestamp": "01:00:15,176", "timestamp_s": 3615.0}, {"text": "unfortunately, this was not introducing any fairness", "timestamp": "01:00:19,190", "timestamp_s": 3619.0}, {"text": "to this whole solution. So when this misbehaving", "timestamp": "01:00:22,606", "timestamp_s": 3622.0}, {"text": "client came with this 15,000 keys,", "timestamp": "01:00:27,106", "timestamp_s": 3627.0}, {"text": "it just choked the whole system with its own request.", "timestamp": "01:00:31,530", "timestamp_s": 3631.0}, {"text": "Other clients had to wait until DynamoDB", "timestamp": "01:00:35,394", "timestamp_s": 3635.0}, {"text": "was catching up to get their own answers. Okay,", "timestamp": "01:00:39,942", "timestamp_s": 3639.0}, {"text": "so it was not introducing any fairness for this whole scenario.", "timestamp": "01:00:43,616", "timestamp_s": 3643.0}, {"text": "We wanted to do better. We tried also built", "timestamp": "01:00:47,542", "timestamp_s": 3647.0}, {"text": "in rate limiting, but we did not go into production because for obvious reasons,", "timestamp": "01:00:51,876", "timestamp_s": 3651.0}, {"text": "it was not working very well. So you can introduce", "timestamp": "01:00:56,298", "timestamp_s": 3656.0}, {"text": "a quite okay ish implementation with resilience", "timestamp": "01:00:59,482", "timestamp_s": 3659.0}, {"text": "for j that also has rate limiting. It\u0027s quite precise.", "timestamp": "01:01:03,530", "timestamp_s": 3663.0}, {"text": "So you can have 40 operations per second for", "timestamp": "01:01:07,510", "timestamp_s": 3667.0}, {"text": "each instance. Now with two instances, you have in total of 80", "timestamp": "01:01:10,728", "timestamp_s": 3670.0}, {"text": "operations per second. Now, when the auto scaling", "timestamp": "01:01:14,462", "timestamp_s": 3674.0}, {"text": "kicks in immediately, you have another instance having", "timestamp": "01:01:18,206", "timestamp_s": 3678.0}, {"text": "40 operations per second, which now also increasing", "timestamp": "01:01:21,756", "timestamp_s": 3681.0}, {"text": "your rate limit and your capacity, which is not true, of course,", "timestamp": "01:01:25,378", "timestamp_s": 3685.0}, {"text": "because the original capacity is determined", "timestamp": "01:01:29,056", "timestamp_s": 3689.0}, {"text": "by the dynamoDb\u0027s current capacity and by its", "timestamp": "01:01:32,678", "timestamp_s": 3692.0}, {"text": "auto scaling characteristics. Okay,", "timestamp": "01:01:36,272", "timestamp_s": 3696.0}, {"text": "we tried many other things,", "timestamp": "01:01:40,770", "timestamp_s": 3700.0}, {"text": "but one of these was this one obviously did not", "timestamp": "01:01:43,490", "timestamp_s": 3703.0}, {"text": "work well. We could have put", "timestamp": "01:01:47,092", "timestamp_s": 3707.0}, {"text": "rate limiting into something that\u0027s used centrally,", "timestamp": "01:01:50,980", "timestamp_s": 3710.0}, {"text": "namely into the service mesh. So in this case,", "timestamp": "01:01:54,638", "timestamp_s": 3714.0}, {"text": "istio was also offering rate limiting. It was using redis", "timestamp": "01:01:57,688", "timestamp_s": 3717.0}, {"text": "to keep the states of each client", "timestamp": "01:02:02,622", "timestamp_s": 3722.0}, {"text": "to control the rate limits. Problem was that it", "timestamp": "01:02:06,810", "timestamp_s": 3726.0}, {"text": "introduced an API change. So instead of giving these requests a", "timestamp": "01:02:10,700", "timestamp_s": 3730.0}, {"text": "post or maybe slowing down the clients, which are misbehaving", "timestamp": "01:02:14,412", "timestamp_s": 3734.0}, {"text": "similarly to what back pressure does, it immediately", "timestamp": "01:02:18,498", "timestamp_s": 3738.0}, {"text": "gave them another kind of HTTP response. And we thought", "timestamp": "01:02:23,126", "timestamp_s": 3743.0}, {"text": "that something, that it caused more trouble", "timestamp": "01:02:26,880", "timestamp_s": 3746.0}, {"text": "than solve solutions. So we", "timestamp": "01:02:30,502", "timestamp_s": 3750.0}, {"text": "did not go on with this change. Instead of that, we went", "timestamp": "01:02:33,748", "timestamp_s": 3753.0}, {"text": "back to queuing theory. So if we simplify this into", "timestamp": "01:02:37,412", "timestamp_s": 3757.0}, {"text": "a simpler queue, this is what happens. So we have 15,000 items coming", "timestamp": "01:02:42,692", "timestamp_s": 3762.0}, {"text": "in, you have couple of executors that are on the right side,", "timestamp": "01:02:47,080", "timestamp_s": 3767.0}, {"text": "and the throughput, and to determine the throughput,", "timestamp": "01:02:51,272", "timestamp_s": 3771.0}, {"text": "you need the latency. So the overall latency was", "timestamp": "01:02:54,862", "timestamp_s": 3774.0}, {"text": "equal to the timeout configuration of the client,", "timestamp": "01:02:58,748", "timestamp_s": 3778.0}, {"text": "which was not easy to figure out, or not difficult to figure out,", "timestamp": "01:03:02,274", "timestamp_s": 3782.0}, {"text": "sorry. Because it was the default being used. I don\u0027t", "timestamp": "01:03:05,932", "timestamp_s": 3785.0}, {"text": "know where this number is coming from, but for every library", "timestamp": "01:03:09,248", "timestamp_s": 3789.0}, {"text": "it looks like timeout is 30 seconds. So we", "timestamp": "01:03:12,838", "timestamp_s": 3792.0}, {"text": "have 30 seconds to consume and", "timestamp": "01:03:16,912", "timestamp_s": 3796.0}, {"text": "problems these 15,000 items.", "timestamp": "01:03:20,372", "timestamp_s": 3800.0}, {"text": "This gives us the overall throughput of 500 operations", "timestamp": "01:03:23,242", "timestamp_s": 3803.0}, {"text": "per second, regardless of the number of executors.", "timestamp": "01:03:26,506", "timestamp_s": 3806.0}, {"text": "So with two instances working on", "timestamp": "01:03:30,122", "timestamp_s": 3810.0}, {"text": "that, we need to complete each operation in four milliseconds,", "timestamp": "01:03:33,492", "timestamp_s": 3813.0}, {"text": "which seems to be nearly impossible. We do not really want to bother", "timestamp": "01:03:37,406", "timestamp_s": 3817.0}, {"text": "with optimizing the whole thing, but if you scale this out", "timestamp": "01:03:41,278", "timestamp_s": 3821.0}, {"text": "to five nodes, to five workers, it\u0027s a more", "timestamp": "01:03:44,792", "timestamp_s": 3824.0}, {"text": "user friendly number. It\u0027s now ten milliseconds, which seem", "timestamp": "01:03:48,796", "timestamp_s": 3828.0}, {"text": "to be doable. So we think that if we can", "timestamp": "01:03:52,738", "timestamp_s": 3832.0}, {"text": "slow down these 15,000 requests", "timestamp": "01:03:56,316", "timestamp_s": 3836.0}, {"text": "not to be processed immediately,", "timestamp": "01:04:00,750", "timestamp_s": 3840.0}, {"text": "but instead of them to be processed within 30 seconds,", "timestamp": "01:04:04,486", "timestamp_s": 3844.0}, {"text": "close to a number that\u0027s 30 seconds with five worker,", "timestamp": "01:04:08,310", "timestamp_s": 3848.0}, {"text": "if we have the execution duration of ten milliseconds for", "timestamp": "01:04:13,330", "timestamp_s": 3853.0}, {"text": "each worker, we can do it in a sensible", "timestamp": "01:04:16,788", "timestamp_s": 3856.0}, {"text": "way. So we tried to", "timestamp": "01:04:20,826", "timestamp_s": 3860.0}, {"text": "find a technology that allows us to do that.", "timestamp": "01:04:24,152", "timestamp_s": 3864.0}, {"text": "We were looking towards RabbitMQ because a", "timestamp": "01:04:28,070", "timestamp_s": 3868.0}, {"text": "couple of very interesting features. RabbitMQ has", "timestamp": "01:04:32,088", "timestamp_s": 3872.0}, {"text": "this queue overflow behavior. So if you set this overflow", "timestamp": "01:04:35,548", "timestamp_s": 3875.0}, {"text": "setting for a queue, then it will immediately reject", "timestamp": "01:04:39,138", "timestamp_s": 3879.0}, {"text": "and not consume those requests that was put into the queue.", "timestamp": "01:04:44,930", "timestamp_s": 3884.0}, {"text": "So I think it will wait on the client side until", "timestamp": "01:04:48,662", "timestamp_s": 3888.0}, {"text": "the queue still has capacity and until the consumers are catching", "timestamp": "01:04:53,470", "timestamp_s": 3893.0}, {"text": "up instead of failing them. It\u0027s failing after a", "timestamp": "01:04:57,446", "timestamp_s": 3897.0}, {"text": "certain amount of time. It also have another rate limiting or", "timestamp": "01:05:00,848", "timestamp_s": 3900.0}, {"text": "flow rate behavior, but it\u0027s bound to", "timestamp": "01:05:05,348", "timestamp_s": 3905.0}, {"text": "the memory and cpusage to RabbitMQ that you can see on the", "timestamp": "01:05:08,708", "timestamp_s": 3908.0}, {"text": "middle left on the screen. That\u0027s very hard to control. This is", "timestamp": "01:05:12,468", "timestamp_s": 3912.0}, {"text": "not what we were looking for, but still was quite promising then.", "timestamp": "01:05:16,612", "timestamp_s": 3916.0}, {"text": "What\u0027s important is that you can define your prefetch settings", "timestamp": "01:05:21,412", "timestamp_s": 3921.0}, {"text": "by queues that you can see on the top right", "timestamp": "01:05:25,130", "timestamp_s": 3925.0}, {"text": "corner. So you can have a single channel of", "timestamp": "01:05:28,316", "timestamp_s": 3928.0}, {"text": "connection that\u0027s connecting through different consumers", "timestamp": "01:05:32,316", "timestamp_s": 3932.0}, {"text": "to different queues, and have different settings for", "timestamp": "01:05:36,034", "timestamp_s": 3936.0}, {"text": "each queue. So if you have one of", "timestamp": "01:05:39,488", "timestamp_s": 3939.0}, {"text": "the clients coming into one queue, you can have", "timestamp": "01:05:43,328", "timestamp_s": 3943.0}, {"text": "a specific configuration for that single queue.", "timestamp": "01:05:48,030", "timestamp_s": 3948.0}, {"text": "Now for another queue, you can either have the same or different configuration", "timestamp": "01:05:51,882", "timestamp_s": 3951.0}, {"text": "as well, and you have many options to acknowledge", "timestamp": "01:05:55,722", "timestamp_s": 3955.0}, {"text": "the request. So when DynamoDB starts throttling, so you just", "timestamp": "01:05:59,322", "timestamp_s": 3959.0}, {"text": "simply acknowledge or not negatively", "timestamp": "01:06:03,112", "timestamp_s": 3963.0}, {"text": "acknowledge or reject the", "timestamp": "01:06:06,782", "timestamp_s": 3966.0}, {"text": "message from the worker side and you can retry with", "timestamp": "01:06:11,032", "timestamp_s": 3971.0}, {"text": "the next interactions or with the next worker.", "timestamp": "01:06:14,472", "timestamp_s": 3974.0}, {"text": "So here was our setup. Basically we had a service or", "timestamp": "01:06:17,930", "timestamp_s": 3977.0}, {"text": "the array of services now having an auto scaling", "timestamp": "01:06:21,852", "timestamp_s": 3981.0}, {"text": "group. This was using so", "timestamp": "01:06:26,034", "timestamp_s": 3986.0}, {"text": "calls fake boundary. This is how I calls that. Maybe there\u0027s a", "timestamp": "01:06:30,844", "timestamp_s": 3990.0}, {"text": "better name. And instead of being synchronous, it was asynchronous,", "timestamp": "01:06:34,304", "timestamp_s": 3994.0}, {"text": "but it used request reply queues and we separated", "timestamp": "01:06:38,182", "timestamp_s": 3998.0}, {"text": "the request by each API key. So each client fortunately", "timestamp": "01:06:42,426", "timestamp_s": 4002.0}, {"text": "forwarded their own API key in the header and", "timestamp": "01:06:46,826", "timestamp_s": 4006.0}, {"text": "in each queue in each channel. We had", "timestamp": "01:06:50,788", "timestamp_s": 4010.0}, {"text": "the same configuration, the same configuration of overflow", "timestamp": "01:06:54,072", "timestamp_s": 4014.0}, {"text": "settings, the same configuration of prefetch rate, and each worker", "timestamp": "01:06:58,078", "timestamp_s": 4018.0}, {"text": "connected to all the queues at the same time", "timestamp": "01:07:02,334", "timestamp_s": 4022.0}, {"text": "and have their own auto scaling group.", "timestamp": "01:07:06,232", "timestamp_s": 4026.0}, {"text": "Creating a new queue if we saw a fresh API", "timestamp": "01:07:10,010", "timestamp_s": 4030.0}, {"text": "key was not a problem because the configuration was in service itself.", "timestamp": "01:07:13,218", "timestamp_s": 4033.0}, {"text": "So this had something like global configuration", "timestamp": "01:07:17,548", "timestamp_s": 4037.0}, {"text": "available and connecting to a new queue", "timestamp": "01:07:21,618", "timestamp_s": 4041.0}, {"text": "when we see a new queue from the worker side seemed to be a bit", "timestamp": "01:07:25,710", "timestamp_s": 4045.0}, {"text": "more complicated but doable, because RevitMQ is", "timestamp": "01:07:29,136", "timestamp_s": 4049.0}, {"text": "offering management API as well,", "timestamp": "01:07:32,628", "timestamp_s": 4052.0}, {"text": "which helps you discover if there\u0027s a fresh queue.", "timestamp": "01:07:35,604", "timestamp_s": 4055.0}, {"text": "And opening up a new connection seemed", "timestamp": "01:07:38,570", "timestamp_s": 4058.0}, {"text": "to be, and having a new unit", "timestamp": "01:07:42,122", "timestamp_s": 4062.0}, {"text": "which is consuming that connection seemed to be", "timestamp": "01:07:46,350", "timestamp_s": 4066.0}, {"text": "not a big deal. And then", "timestamp": "01:07:50,168", "timestamp_s": 4070.0}, {"text": "we could connect to Dynamodb and reject", "timestamp": "01:07:53,992", "timestamp_s": 4073.0}, {"text": "the request if there\u0027s throttling, but hope that this", "timestamp": "01:07:57,442", "timestamp_s": 4077.0}, {"text": "will have the effect what we desired for. Again, we need", "timestamp": "01:08:01,612", "timestamp_s": 4081.0}, {"text": "to slow down misbehaving clients. So if there\u0027s more orange marble coming", "timestamp": "01:08:05,308", "timestamp_s": 4085.0}, {"text": "in, we have to say after a while that orange marbles have to wait.", "timestamp": "01:08:09,536", "timestamp_s": 4089.0}, {"text": "Why? Processing the blue and the yellow marbles in their own pace,", "timestamp": "01:08:13,120", "timestamp_s": 4093.0}, {"text": "and this is the metrics that I\u0027ve got.", "timestamp": "01:08:18,350", "timestamp_s": 4098.0}, {"text": "Unfortunately this is not from the real scenario.", "timestamp": "01:08:21,828", "timestamp_s": 4101.0}, {"text": "I had to rebuild it in a sandbox for certain", "timestamp": "01:08:25,226", "timestamp_s": 4105.0}, {"text": "reasons, but it\u0027s available under my GitHub profile,", "timestamp": "01:08:28,964", "timestamp_s": 4108.0}, {"text": "so you can try it by your own if you want to. So this", "timestamp": "01:08:32,986", "timestamp_s": 4112.0}, {"text": "was the baseline of direct reads. Now directly", "timestamp": "01:08:36,232", "timestamp_s": 4116.0}, {"text": "coming from the service of the database, you can see that we have", "timestamp": "01:08:39,726", "timestamp_s": 4119.0}, {"text": "the throughput of 300 operations per second, and we", "timestamp": "01:08:42,888", "timestamp_s": 4122.0}, {"text": "complete all the reads in 15 seconds.", "timestamp": "01:08:46,312", "timestamp_s": 4126.0}, {"text": "And the response time is around ten milliseconds.", "timestamp": "01:08:50,810", "timestamp_s": 4130.0}, {"text": "This is quite good because it\u0027s quite", "timestamp": "01:08:54,410", "timestamp_s": 4134.0}, {"text": "close to this scenario with five workers that you want", "timestamp": "01:08:57,996", "timestamp_s": 4137.0}, {"text": "to reach, and the response time distribution is", "timestamp": "01:09:01,328", "timestamp_s": 4141.0}, {"text": "very tight. So it\u0027s everything from going", "timestamp": "01:09:04,752", "timestamp_s": 4144.0}, {"text": "to zero to one eight.", "timestamp": "01:09:08,736", "timestamp_s": 4148.0}, {"text": "I don\u0027t know what the unit is, maybe this graph", "timestamp": "01:09:12,516", "timestamp_s": 4152.0}, {"text": "is messed up, sorry for that. But for the other diagrams", "timestamp": "01:09:16,458", "timestamp_s": 4156.0}, {"text": "you can see that these numbers change at least. So with single worker", "timestamp": "01:09:20,362", "timestamp_s": 4160.0}, {"text": "what we can see now is that rate limiting is working", "timestamp": "01:09:24,398", "timestamp_s": 4164.0}, {"text": "as expected or the back pressure is working as expected. So now", "timestamp": "01:09:28,104", "timestamp_s": 4168.0}, {"text": "instead of doing", "timestamp": "01:09:31,432", "timestamp_s": 4171.0}, {"text": "all these reads within 15 seconds we", "timestamp": "01:09:35,096", "timestamp_s": 4175.0}, {"text": "just give it a pause and we do it instead of that", "timestamp": "01:09:39,068", "timestamp_s": 4179.0}, {"text": "in I think two minutes,", "timestamp": "01:09:42,780", "timestamp_s": 4182.0}, {"text": "yeah, in around two minutes. So it\u0027s not", "timestamp": "01:09:45,884", "timestamp_s": 4185.0}, {"text": "reaching that 30 seconds goal what we aimed for, but it\u0027s", "timestamp": "01:09:49,632", "timestamp_s": 4189.0}, {"text": "giving the request a pause when we have more process", "timestamp": "01:09:54,038", "timestamp_s": 4194.0}, {"text": "than what we want to. The reasons time average is a bit", "timestamp": "01:09:59,024", "timestamp_s": 4199.0}, {"text": "higher than expected,", "timestamp": "01:10:02,852", "timestamp_s": 4202.0}, {"text": "but at least", "timestamp": "01:10:06,850", "timestamp_s": 4206.0}, {"text": "we have, this is just a single worker, so at least", "timestamp": "01:10:10,372", "timestamp_s": 4210.0}, {"text": "we have this set up as we", "timestamp": "01:10:13,636", "timestamp_s": 4213.0}, {"text": "want to. And you can see this from the response time distribution.", "timestamp": "01:10:17,448", "timestamp_s": 4217.0}, {"text": "So the response time distribution now is going to up", "timestamp": "01:10:20,798", "timestamp_s": 4220.0}, {"text": "to 14 units. So it\u0027s better than", "timestamp": "01:10:24,152", "timestamp_s": 4224.0}, {"text": "it was before. And now with five workers we tried it out also with", "timestamp": "01:10:27,548", "timestamp_s": 4227.0}, {"text": "five workers we saw that with five workers", "timestamp": "01:10:31,132", "timestamp_s": 4231.0}, {"text": "we succeeded to reach our goals.", "timestamp": "01:10:35,850", "timestamp_s": 4235.0}, {"text": "So this exactly takes 30", "timestamp": "01:10:39,126", "timestamp_s": 4239.0}, {"text": "seconds as expected. And interestingly in", "timestamp": "01:10:43,248", "timestamp_s": 4243.0}, {"text": "one point we measured even a higher throughput. So the original", "timestamp": "01:10:47,184", "timestamp_s": 4247.0}, {"text": "throughput was very close to the", "timestamp": "01:10:50,682", "timestamp_s": 4250.0}, {"text": "baseline and in one case it was a", "timestamp": "01:10:54,260", "timestamp_s": 4254.0}, {"text": "bit even higher, even 500 operations per second.", "timestamp": "01:10:57,364", "timestamp_s": 4257.0}, {"text": "It also shows that throughput has nothing to", "timestamp": "01:11:01,140", "timestamp_s": 4261.0}, {"text": "do really with latency. It has some relations", "timestamp": "01:11:04,968", "timestamp_s": 4264.0}, {"text": "with latency, but we are able to meet even higher throughput", "timestamp": "01:11:08,344", "timestamp_s": 4268.0}, {"text": "even if we have higher latency for some of the clients,", "timestamp": "01:11:12,318", "timestamp_s": 4272.0}, {"text": "for some of the channels and the worker execution time and response", "timestamp": "01:11:15,838", "timestamp_s": 4275.0}, {"text": "time. Now, especially if you see the worker execution time,", "timestamp": "01:11:19,858", "timestamp_s": 4279.0}, {"text": "you can see that now it\u0027s getting short to getting to", "timestamp": "01:11:23,436", "timestamp_s": 4283.0}, {"text": "ten milliseconds at the end where we want it to be.", "timestamp": "01:11:27,980", "timestamp_s": 4287.0}, {"text": "And the response time distribution is again a bit", "timestamp": "01:11:32,510", "timestamp_s": 4292.0}, {"text": "better distributed than before. So this was", "timestamp": "01:11:35,968", "timestamp_s": 4295.0}, {"text": "quite promising for us. So that\u0027s all I wanted", "timestamp": "01:11:39,312", "timestamp_s": 4299.0}, {"text": "to say and present to you. Thank you so much", "timestamp": "01:11:43,236", "timestamp_s": 4303.0}, {"text": "for listening again. I was orthes Margaret and", "timestamp": "01:11:46,676", "timestamp_s": 4306.0}, {"text": "I work as associate chief software engineer at EPAm.", "timestamp": "01:11:50,372", "timestamp_s": 4310.0}, {"text": "If you have questions related to the things I talked about,", "timestamp": "01:11:54,870", "timestamp_s": 4314.0}, {"text": "just feel free to reach out to me by using either Twitter", "timestamp": "01:11:58,360", "timestamp_s": 4318.0}, {"text": "or LinkedIn or feel free to visit my GitHub", "timestamp": "01:12:02,542", "timestamp_s": 4322.0}, {"text": "profile. But I have an availability simulator", "timestamp": "01:12:06,162", "timestamp_s": 4326.0}, {"text": "that helps you to get these availability numbers that I was talking about", "timestamp": "01:12:10,034", "timestamp_s": 4330.0}, {"text": "and have this example sandbox of back pressure", "timestamp": "01:12:14,076", "timestamp_s": 4334.0}, {"text": "and ray limiting I was presenting to you and", "timestamp": "01:12:17,618", "timestamp_s": 4337.0}, {"text": "I get plenty of plenty of references. If you are interested in more just", "timestamp": "01:12:22,028", "timestamp_s": 4342.0}, {"text": "feel free to look at the end of the slides and discover", "timestamp": "01:12:25,452", "timestamp_s": 4345.0}, {"text": "a couple of the things I talked about even in more detail. Thank you", "timestamp": "01:12:29,442", "timestamp_s": 4349.0}, {"text": "very much again for listening. I hope you had a great time and", "timestamp": "01:12:33,284", "timestamp_s": 4353.0}, {"text": "learned something new.", "timestamp": "01:12:36,708", "timestamp_s": 4356.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'Y3exQA1iY7k',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Aspects of Microservice Interactions
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>What are the recent challenges of microservice interactions? Have you ever had any issues with service-to-service calls which were especially hard to manage? Were you wondering about the reasons? Unfortunately, there are some rules we can not bend. But luckily some problems are easily solvable.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Today I'm going to talk about the aspects of microservice interactions. This is coming from these complex interaction diagrams that were generated in well known microservice infrastructures. Having this huge amount of network communications has its own implications. It's very important to understand the upcoming challenges of a network communication at such a large scale.

              </li>
              
              <li>
                Microservices are independently deployable. A single feature often is mapped to a single microservice. For this to work perfectly, we need one single ingredient, namely network calls. We need to deep dive into these network calls and understand how we can optimize them.

              </li>
              
              <li>
                These network calls have driving forces, I think we have around five of the most important ones. These are, namely latency, availability, reliability, queuing theory, and Conway slow. Let's go through each them, each by each, and understand how they affect microservice communications.

              </li>
              
              <li>
                Most important use case is a single client server communication. Mapping reliability statistically to more requests. Two phase commit was not really working on a larger scale. Another paper is this fallacy of distributed computing.

              </li>
              
              <li>
                In practice, I think queues are everywhere in a modern architecture at large scale, also in small scale. What happens when the arrival rate is much larger than the departure rate? This something is called back pressure or rate limiting. We will talk about it more in the second half.

              </li>
              
              <li>
                For each queue you can provide its own quality of service. By splitting my workload into smaller chunks and introducing more queues, processing them in smaller units. latency did not change, but throughput increased. This can be done in a couple of ways in microservice.

              </li>
              
              <li>
                In the next section, we will talk about practical examples and situations that I faced. And I would like to guide you how we improved situations each by each. You can be conscious about your technology choices. Auto scaling has a positive effect on throughput, but not on latency.

              </li>
              
              <li>
                The technique called rapid read protection works in distributed databases. Instead of waiting for the request, the coordinator node fires a backup request. The overall availability for this simplified scenario for the simplified query is increased. Overall at large scale, statistically we are still better, still performing better.

              </li>
              
              <li>
                We introduced a distributed in memory cache solution with hazelcast that was also replicating between each read node. This helps keep the cache warm with multi writes, with using replication as well. This simplifies your architecture overall even further.

              </li>
              
              <li>
                Client libraries are a double edged sword. Not all the features can be turned off in this specific client library. Another issue with client libraries was that it often messed up the deployments. We just simply shift things on the left side during delivery, during deployment.

              </li>
              
              <li>
                Another technique that I want to talk about is what I call region pinning. We flag the originating region for each user by using a cookie. When moving certain users who need their shopping cart to the right side, we pay a latency penalty. This keeps the game consistent.

              </li>
              
              <li>
                Designpoint optimizing a single service in a couple of iterations. For some reason the memory consumption of the services were increasing and the container orchestrator started killing the services themselves. Solution was to create a separate thread that had a separate queue that is processing requests.

              </li>
              
              <li>
                You can define your prefetch settings by queues that you can see on the top right corner. Each worker connected to all the queues at the same time and have their own auto scaling group. With five workers, we were able to reach our goals with higher throughput.

              </li>
              
              <li>
                So that's all I wanted to say and present to you. If you have questions related to the things I talked about, just feel free to reach out to me by using either Twitter or LinkedIn. I hope you had a great time and learned something new.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/Y3exQA1iY7k.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:27,810'); seek(27.0)">
              Hello everyone. Today I'm going to talk about
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:31,532'); seek(31.0)">
              the aspects of microservice interactions,
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:34,890'); seek(34.0)">
              and you might wonder what does Devstar is
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:38,252'); seek(38.0)">
              doing on the screen. I put this
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:41,852'); seek(41.0)">
              image here because microservice architectures are
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:45,916'); seek(45.0)">
              also often referred to as
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:49,388'); seek(49.0)">
              Dev star architectures. This is coming from
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:53,140'); seek(53.0)">
              these very complex interaction diagrams
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:56,842'); seek(56.0)">
              that were generated in well known microservice infrastructures
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:01:02,470'); seek(62.0)">
              like you can see on the screen. Left one is from Netflix,
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:01:07,110'); seek(67.0)">
              the middle one you can see from one from Twitter,
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:11,110'); seek(71.0)">
              and the right one is from Amazon.
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:15,450'); seek(75.0)">
              So having this huge amount of
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:19,052'); seek(79.0)">
              network communications has its own implications.
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:23,370'); seek(83.0)">
              And that's why I think it's very important to understand the
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:29,132'); seek(89.0)">
              upcoming challenges of a network communication
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:33,026'); seek(93.0)">
              at such a large scale. So that drove
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:37,138'); seek(97.0)">
              me into this topic, into this area,
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:40,220'); seek(100.0)">
              and encouraged me to look behind
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:44,740'); seek(104.0)">
              the scenes and understand the details. And that's how this
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:48,612'); seek(108.0)">
              presentation was born. So the first section,
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:52,970'); seek(112.0)">
              let's talk about the
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:57,432'); seek(117.0)">
              reasons, the driving
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:02:01,646'); seek(121.0)">
              forces that are affecting these communications
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:02:05,650'); seek(125.0)">
              channels. But first, I would
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:02:08,972'); seek(128.0)">
              like to just have a little recap on
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:02:12,268'); seek(132.0)">
              what are the main characteristics of the microservice
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:16,402'); seek(136.0)">
              architecture style and why
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:20,752'); seek(140.0)">
              are we doing this overall, what's the benefit if we are doing it
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:24,592'); seek(144.0)">
              well? So one
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:29,728'); seek(149.0)">
              of the most important aspect of these things is
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:35,428'); seek(155.0)">
              that microservices are independently deployable.
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:39,970'); seek(159.0)">
              So I should be able to deploy a single
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:43,864'); seek(163.0)">
              service without others really noticing
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:47,406'); seek(167.0)">
              that on top of that comes another
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:51,592'); seek(171.0)">
              important functionality, autonomity.
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:54,958'); seek(174.0)">
              So this means that a single feature often
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:58,252'); seek(178.0)">
              is mapped to a single microservice.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:03:01,458'); seek(181.0)">
              So I'm able to deliver a single feature, a single functional
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:03:05,346'); seek(185.0)">
              change, by just modifying one microservice
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:03:09,542'); seek(189.0)">
              and ending up in one deployment.
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:03:13,470'); seek(193.0)">
              What's also important is that microservices are polyglot.
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:19,230'); seek(199.0)">
              They're also polyglot in
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:22,388'); seek(202.0)">
              terms of database usage or database technology choices,
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:26,042'); seek(206.0)">
              also in language choices, but also in other technological aspects.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:30,122'); seek(210.0)">
              So let's say if I want to come up with a
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:34,370'); seek(214.0)">
              JVM upgrade, I don't have to upgrade
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:38,458'); seek(218.0)">
              all the services all at once and then deploy them in a
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:42,248'); seek(222.0)">
              single coordinated, huge deployment.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:45,166'); seek(225.0)">
              I'm free to go with a single service only. And then other
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:48,876'); seek(228.0)">
              teams owning other services are also free to deploy
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:52,930'); seek(232.0)">
              when they think that's suitable,
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:56,530'); seek(236.0)">
              that goes on with other technologies. So for instance, if I
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:59,648'); seek(239.0)">
              want to change from traditional rest based communication to
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:04:03,632'); seek(243.0)">
              GRPC, same thing applies.
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:04:08,830'); seek(248.0)">
              So for this to work perfectly, we need one
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:04:12,772'); seek(252.0)">
              single ingredient, namely network calls.
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:04:15,978'); seek(255.0)">
              So you can introduce dependencies between
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:19,780'); seek(259.0)">
              services in many ways, but network call
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:24,184'); seek(264.0)">
              is one of the most efficient way of doing that.
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:27,928'); seek(267.0)">
              So if you have dependencies by
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:31,416'); seek(271.0)">
              using libraries shared data or anything else, it won't
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:35,118'); seek(275.0)">
              work. As well as network communication, a single network
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:38,402'); seek(278.0)">
              communication. So that's why we need to deep dive
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:42,466'); seek(282.0)">
              into these network calls and understand how we can
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:46,332'); seek(286.0)">
              optimize them.
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:50,190'); seek(290.0)">
              These network calls have driving forces, I think about driving
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:54,566'); seek(294.0)">
              forces. I think we have around
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:58,624'); seek(298.0)">
              five of the most important ones. These are,
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:05:01,632'); seek(301.0)">
              namely latency, availability, reliability,
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:05:05,078'); seek(305.0)">
              queuing theory, and Conway slow. So let's go through each them,
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:05:09,076'); seek(309.0)">
              each by each, and let's understand, how do they affect microservice
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:05:13,662'); seek(313.0)">
              communications. First one is latency.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:05:17,990'); seek(317.0)">
              Latency has a had limit. That's the
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:22,120'); seek(322.0)">
              speed of light. So if you take the two costs of
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:25,784'); seek(325.0)">
              the US,
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:31,110'); seek(331.0)">
              the runtrip time for the light to travel through the
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:35,484'); seek(335.0)">
              left side to the right side is 27
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:40,848'); seek(340.0)">
              milliseconds. Okay? But in practice,
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:45,470'); seek(345.0)">
              we cannot reach this number because the speed of
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:49,168'); seek(349.0)">
              light is also affected by the density of the material it's
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:52,634'); seek(352.0)">
              passing through. So, for instance, in fiber optics,
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:56,010'); seek(356.0)">
              if this would be just a single cable, it's more like
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:06:00,084'); seek(360.0)">
              41 milliseconds.
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:06:03,010'); seek(363.0)">
              I got this from this website I
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:06:06,340'); seek(366.0)">
              have on the references section, which simulates
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:06:10,862'); seek(370.0)">
              the expected roundtips time in two parts
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:06:14,638'); seek(374.0)">
              of the globe.
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:06:18,890'); seek(378.0)">
              So that's where these numbers are coming from. But there are many other
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:06:22,812'); seek(382.0)">
              pages where they are expecting you or explaining
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:26,578'); seek(386.0)">
              you the expected round trip time in
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:30,176'); seek(390.0)">
              different data centers for a cloud provider.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:33,302'); seek(393.0)">
              So, in case of AWS,
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:36,854'); seek(396.0)">
              this is realistically more like 50
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:40,112'); seek(400.0)">
              to 60 milliseconds, but it depends on which
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:43,812'); seek(403.0)">
              region are you connecting to? Another region. But why is
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:47,124'); seek(407.0)">
              this important for us? So this gives you the minimum latency
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:52,550'); seek(412.0)">
              if you connect from one region to another.
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:55,670'); seek(415.0)">
              So if you want to come up with
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:59,128'); seek(419.0)">
              a multiregion deployment, because of various reasons,
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:07:03,590'); seek(423.0)">
              you always have to think about the data synchronization between the regions.
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:07:07,394'); seek(427.0)">
              And this will be the minimum latency until
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:07:11,276'); seek(431.0)">
              data arrives to the other edge.
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:07:13,930'); seek(433.0)">
              And I say this is the minimum latency,
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:07:17,506'); seek(437.0)">
              because in reality, when you have more
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:20,992'); seek(440.0)">
              pressure on your data layer, work will stockpile
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:24,966'); seek(444.0)">
              up, and this will go up to 506 hundred
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:28,656'); seek(448.0)">
              milliseconds. So 60 milliseconds
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:32,986'); seek(452.0)">
              is the optimistic duration.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:37,018'); seek(457.0)">
              So if you want to have a synchronous write,
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:40,580'); seek(460.0)">
              let's say each synchronous write will have at least 40 to
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:44,776'); seek(464.0)">
              600 milliseconds of latency. This has to be considered
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:48,846'); seek(468.0)">
              if you are planning to do something which has
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:52,456'); seek(472.0)">
              its own low latency requirements.
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:56,410'); seek(476.0)">
              Also, it's important to understand the correlation between latency
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:08:00,530'); seek(480.0)">
              and throughput. This comes from another research. So they try to simulate
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:08:06,890'); seek(486.0)">
              two things by creating fictional
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:08:10,902'); seek(490.0)">
              website. They measured the
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:08:14,528'); seek(494.0)">
              page load time and measured the bandwidth, and they were interested
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:08:18,352'); seek(498.0)">
              in how the page load time varies based on
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:08:22,000'); seek(502.0)">
              the bandwidth and based on the roundtree time. So if you increase
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:25,562'); seek(505.0)">
              the bandwidth, if you increase the throughput, then you will see that the
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:29,428'); seek(509.0)">
              benefits are diminishing. Very early page
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:33,736'); seek(513.0)">
              load time actually maps to around hundreds of requests
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:37,198'); seek(517.0)">
              of latency because roughly that's the amount of
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:41,990'); seek(521.0)">
              independent requests required to load whole page.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:45,510'); seek(525.0)">
              Okay, but if you try to change the round trip
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:48,882'); seek(528.0)">
              time, the page load time is really linearly decreasing.
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:52,290'); seek(532.0)">
              So what does this tell us? First of all, there is
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:55,836'); seek(535.0)">
              no direct correlation or direct
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:59,008'); seek(539.0)">
              effect between page load time and bandwidth
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:09:02,582'); seek(542.0)">
              or latency and throughput. So this means also that
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:09:06,320'); seek(546.0)">
              if you rely on barely,
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:09:10,006'); seek(550.0)">
              just on the scalability or auto scaling capacities on your
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:09:13,632'); seek(553.0)">
              system, that is not going to have a positive effect on the latency.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:09:18,770'); seek(558.0)">
              So if you want to be efficient in terms of latency,
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:09:22,330'); seek(562.0)">
              you have to think about other solutions, not just scaling
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:25,678'); seek(565.0)">
              out or giving more juice for your instances.
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:29,990'); seek(569.0)">
              Another thing is availability. Thinking about
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:34,072'); seek(574.0)">
              availability, I always think in dependency graphs
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:37,746'); seek(577.0)">
              because that's what determines the availability as a whole.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:41,548'); seek(581.0)">
              So let's see here a simplified machine
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:44,882'); seek(584.0)">
              architecture. For a single transaction I need
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:48,336'); seek(588.0)">
              all the components to
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:52,528'); seek(592.0)">
              behave as expected. So I need both cpu,
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:55,862'); seek(595.0)">
              the memory, the network and the disk component.
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:59,702'); seek(599.0)">
              So what if we give it each by each availability
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:10:03,690'); seek(603.0)">
              number? So let's say each component has now 99% of
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:10:07,172'); seek(607.0)">
              availability. This will form dependency
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:10:11,210'); seek(611.0)">
              graph and gives us the overall availability of 96%
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:10:15,910'); seek(615.0)">
              just because the combination of the components had its own
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:10:20,870'); seek(620.0)">
              probability of failure. If the individual
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:10:25,208'); seek(625.0)">
              elements have their own probability of failure.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:28,446'); seek(628.0)">
              That's how maths work actually.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:31,530'); seek(631.0)">
              But if I scale this out to client server model,
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:34,860'); seek(634.0)">
              you can see that now I have more dependencies between
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:38,156'); seek(638.0)">
              components. Again, let's say that each component has now 99%
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:42,320'); seek(642.0)">
              of chance of being successful. Now the
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:46,208'); seek(646.0)">
              availability will drop to 88 5%.
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:50,690'); seek(650.0)">
              So with each single dependency your
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:54,516'); seek(654.0)">
              availability will decrease. But it also depends
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:58,762'); seek(658.0)">
              on how the dependency is introduced and
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:11:02,228'); seek(662.0)">
              in which part of your architecture is introduced.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:11:05,750'); seek(665.0)">
              It depends on your dependency graph as well.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:11:10,710'); seek(670.0)">
              You can do the maths by hand if you're interested in the availability numbers.
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:11:14,696'); seek(674.0)">
              I use my own availability simulator which
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:11:19,372'); seek(679.0)">
              is just running a couple of cycles and testing each
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:11:23,196'); seek(683.0)">
              connection and failing them randomly based on these numbers given.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:11:26,828'); seek(686.0)">
              I also have this in references section.
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:11:30,990'); seek(690.0)">
              Okay, now for
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:34,912'); seek(694.0)">
              reliability, I think the most important
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:38,544'); seek(698.0)">
              use case is a single client server communication.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:42,290'); seek(702.0)">
              So let's say that we have a transaction
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:46,234'); seek(706.0)">
              that's changing the state of the whole system. So let's say this
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:50,068'); seek(710.0)">
              is a bright operation. For instance, how things
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:55,080'); seek(715.0)">
              can fail. Let's go through them
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:59,432'); seek(719.0)">
              sequentially. First of all,
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:12:03,110'); seek(723.0)">
              the write operation can fail when client
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:12:06,706'); seek(726.0)">
              sends the request to the server. Then it
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:12:10,124'); seek(730.0)">
              can also fail by being processed on the server itself.
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:12:14,730'); seek(734.0)">
              But it can also fail when server successfully
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:12:19,058'); seek(739.0)">
              processed the write operation and it responds back to
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:12:22,224'); seek(742.0)">
              the client. Here comes the problem.
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:12:25,070'); seek(745.0)">
              Client can just simply retry the
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:12:28,752'); seek(748.0)">
              write operation on the first two cases,
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:31,926'); seek(751.0)">
              but after server successfully process the write operation,
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:36,138'); seek(756.0)">
              client does not know what to do. You may
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:40,244'); seek(760.0)">
              come over this problem by using item potent operations
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:44,778'); seek(764.0)">
              or something similar, but in other cases
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:48,890'); seek(768.0)">
              devious solution is not so obvious. And also,
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:52,470'); seek(772.0)">
              of course I can fail on client side
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:55,672'); seek(775.0)">
              by the request is being processed. But why
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:59,368'); seek(779.0)">
              it's important for us. So first of all, that's why for instance,
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:13:03,426'); seek(783.0)">
              two phase commit was not really working on
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:13:06,828'); seek(786.0)">
              a larger scale. Because if you arrive on a
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:13:10,288'); seek(790.0)">
              commit phase, if one of the request is failing on a commit
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:13:13,878'); seek(793.0)">
              phase, the client or the coordinator does not really know
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:13:17,712'); seek(797.0)">
              how to proceed because other previous operations
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:13:21,786'); seek(801.0)">
              are already committed. And I just got one single failure.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:13:25,562'); seek(805.0)">
              Should I just re request the failed node to
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:13:29,572'); seek(809.0)">
              commit its changes again, risking duplicated
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:13:35,386'); seek(815.0)">
              write or something similar? Or should they just abort the whole operation?
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:40,070'); seek(820.0)">
              Another example is, let's say exactly
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:44,584'); seek(824.0)">
              once message semantics. So the server always
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:48,540'); seek(828.0)">
              have to acknowledge if one message is processed.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:52,410'); seek(832.0)">
              If I fail after the message is
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:56,396'); seek(836.0)">
              processed, the client cannot do anything else,
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:59,776'); seek(839.0)">
              just resend the message. And that's why we often
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:14:03,136'); seek(843.0)">
              have at least once message semantics instead of
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:14:06,736'); seek(846.0)">
              exactly once message semantics mapping
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:14:10,726'); seek(850.0)">
              again. So mapping reliability statistically
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:14:14,474'); seek(854.0)">
              to more requests. We were talking about roundtree time
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:14:17,812'); seek(857.0)">
              and talking about that. Or, sorry, not page load time. Yeah, page load time
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:14:21,492'); seek(861.0)">
              and talking about that. Page load time usually involves
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:14:24,842'); seek(864.0)">
              hundreds of requests, and it needs hundreds of requests to succeed.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:14:28,470'); seek(868.0)">
              So let's say we have theoretically a single request that has
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:14:31,672'); seek(871.0)">
              99% of the probability of being successful.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:14:35,854'); seek(875.0)">
              If we have hundreds of requests with the same characteristics,
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:39,458'); seek(879.0)">
              we cannot just say that less than 40% of the
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:42,828'); seek(882.0)">
              chance will be that the hundreds
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:46,418'); seek(886.0)">
              of requests will succeed. All hundreds of requests.
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:50,350'); seek(890.0)">
              Because we have so many permutations of these hundreds
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:53,878'); seek(893.0)">
              of requests being failed,
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:57,550'); seek(897.0)">
              this drops our probability with around
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:15:01,620'); seek(901.0)">
              60%. Just for the statistical reasons,
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:15:05,642'); seek(905.0)">
              we can't really fight maths here. These are the hard facts.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:15:10,930'); seek(910.0)">
              So this actually resulted
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:15:16,026'); seek(916.0)">
              as a couple of artifacts that I
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:15:19,608'); seek(919.0)">
              think are very popular in
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:15:23,208'); seek(923.0)">
              the engineering world. One of them is these latency numbers.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:15:27,164'); seek(927.0)">
              Every programmer should know that
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:15:30,412'); seek(930.0)">
              presented in many forms like this one. This is
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:15:33,708'); seek(933.0)">
              coming again from a web page where you have a slider and can change
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:15:37,644'); seek(937.0)">
              the year and see how these latency numbers
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:41,360'); seek(941.0)">
              have changed these are the recent numbers. So, for instance,
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:45,286'); seek(945.0)">
              if we investigate the main memory reference and the latency for
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:49,392'); seek(949.0)">
              a typical main memory reference, that's around 100 nanoseconds,
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:53,126'); seek(953.0)">
              and the runtrip time in same data center as in a cloud infrastructure
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:56,778'); seek(956.0)">
              is around 500 milliseconds. Why should we
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:59,972'); seek(959.0)">
              care, you might ask? Because these numbers are fast enough.
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:16:03,556'); seek(963.0)">
              So they are very fast, and they improved a
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:16:07,048'); seek(967.0)">
              lot in the recent years. 500 nanoseconds
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:16:11,118'); seek(971.0)">
              is something I should not care about, right? So if
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:16:14,408'); seek(974.0)">
              you. Let's say you want to introduce a caching strategy and you need to
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:16:18,120'); seek(978.0)">
              choose between an in memory solution, or maybe a distributed
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:16:21,938'); seek(981.0)">
              caching solution, because you want to share it with
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:16:25,516'); seek(985.0)">
              multiple services and you want to offer it as a separate
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:16:28,982'); seek(988.0)">
              service, think about the runtime time. So,
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:16:33,870'); seek(993.0)">
              difference between the in memory and
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:16:37,296'); seek(997.0)">
              the distributed solution caching solution is
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:16:41,236'); seek(1001.0)">
              in terms of latency, is around
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:45,300'); seek(1005.0)">
              5000 more if you choose
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:49,236'); seek(1009.0)">
              distributed cache than if you choose an in
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:52,708'); seek(1012.0)">
              memory cache solution.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:55,970'); seek(1015.0)">
              Another one. Another paper is this fallacy of distributed computing.
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:17:00,734'); seek(1020.0)">
              You can find it in Wikipedia. If you look at the top three
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:17:04,456'); seek(1024.0)">
              of these policies, then I think it's clear that
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:17:08,412'); seek(1028.0)">
              we covered plenty of aspects of those,
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:17:11,770'); seek(1031.0)">
              but the others are also important.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:17:15,930'); seek(1035.0)">
              But now I want to talk about something else,
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:17:19,020'); seek(1039.0)">
              talk about the queuing theory and spend a little bit more time.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:17:22,720'); seek(1042.0)">
              Because in my experience, how I saw people
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:17:27,856'); seek(1047.0)">
              in the engineering area are not really familiar with queuing theory
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:17:31,546'); seek(1051.0)">
              and not really thinking in queues. But in
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:17:35,508'); seek(1055.0)">
              practice, I think queues are everywhere in a modern architecture
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:17:39,690'); seek(1059.0)">
              at large scale, also in small scale. So it's very good to
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:17:43,368'); seek(1063.0)">
              understand the basics. So here comes the basics of queuing theory.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:47,510'); seek(1067.0)">
              By talking about queues,
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:51,910'); seek(1071.0)">
              I think you can think about the simplified model
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:55,400'); seek(1075.0)">
              that you can see on the screen. So you have queues of these orange marbles,
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:59,186'); seek(1079.0)">
              a single queue of these orange marbles that needs to be processed.
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:18:02,850'); seek(1082.0)">
              Then you have something on the right side
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:18:06,572'); seek(1086.0)">
              that is processing the marbles and
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:18:10,432'); seek(1090.0)">
              producing these green marbles on the right side of the screen. And then
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:18:14,192'); seek(1094.0)">
              you have these metrics around the queue that determines the queue
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:18:17,542'); seek(1097.0)">
              performance. We are interested in these four,
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:18:21,796'); seek(1101.0)">
              mainly so there is execution time needed for a
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:18:25,108'); seek(1105.0)">
              single node to process an orange marble and
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:18:29,490'); seek(1109.0)">
              create a green marble. Then there is this
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:18:32,932'); seek(1112.0)">
              departure rate, meaning the rate of
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:18:37,560'); seek(1117.0)">
              green marbles being processed. Then we
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:18:41,208'); seek(1121.0)">
              have the latency that requires duration of
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:18:45,112'); seek(1125.0)">
              either a single marble traversing cv
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:48,366'); seek(1128.0)">
              up to the right side when it becomes to green marble,
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:52,194'); seek(1132.0)">
              or the overall duration required for all the
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:55,452'); seek(1135.0)">
              marbles being processed. And we have then the
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:58,656'); seek(1138.0)">
              arrival rate on the very left side, which is
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:19:02,064'); seek(1142.0)">
              the rate of the orange marbles arriving in the queue.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:19:07,470'); seek(1147.0)">
              So the most basic question is, what happens
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:19:11,730'); seek(1151.0)">
              when the arrival rate is much larger than the departure
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:19:15,562'); seek(1155.0)">
              rate? So, this happens with us all the time,
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:19:18,580'); seek(1158.0)">
              actually. So if you have something that's publicly available on the
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:19:22,548'); seek(1162.0)">
              web, you don't have control over the user base
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:19:26,152'); seek(1166.0)">
              and their usage statistics
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:19:29,982'); seek(1169.0)">
              and often have to operate in this area.
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:19:34,150'); seek(1174.0)">
              So, in this case, if you just accept
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:19:37,586'); seek(1177.0)">
              all the requests and try to process them, you are guaranteed
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:19:41,602'); seek(1181.0)">
              to fail after a certain period. And you
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:19:45,628'); seek(1185.0)">
              have to introduce something. Right. This something is called
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:48,880'); seek(1188.0)">
              back pressure or rate limiting. Okay. So,
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:53,070'); seek(1193.0)">
              often in the edge, you have rate limiter service that
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:57,040'); seek(1197.0)">
              determines which endpoint is
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:20:00,676'); seek(1200.0)">
              limited to what
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:20:04,820'); seek(1204.0)">
              sort of throughput, and tries to keep the
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:20:09,492'); seek(1209.0)">
              right side or protected from higher
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:20:14,120'); seek(1214.0)">
              aid than usual, and tries to introduce some sort of
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:20:17,768'); seek(1217.0)">
              a logic on limiting those
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:20:21,672'); seek(1221.0)">
              clients who maybe misbehave or limiting
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:20:26,230'); seek(1226.0)">
              requests to a specific service overall.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:20:30,330'); seek(1230.0)">
              So, this is an important topic, and we will talk about it a.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:20:33,484'); seek(1233.0)">
              But more in the second half.
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:20:36,970'); seek(1236.0)">
              So, coming back to queuing theory, let's have a couple of practical
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:20:40,582'); seek(1240.0)">
              examples. So, in this simplified scenario,
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:20:44,070'); seek(1244.0)">
              the execution time is 100 milliseconds. So what
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:20:47,984'); seek(1247.0)">
              is the throughput? In this case,
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:20:51,188'); seek(1251.0)">
              we produce ten marbles per second. Because we produce
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:55,290'); seek(1255.0)">
              a single marble each 100 millisecond. The overall latency
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:59,578'); seek(1259.0)">
              for processing all these eight marbles is 800 milliseconds. It's eight
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:21:03,752'); seek(1263.0)">
              times 100 milliseconds. Right. Very simple. Now, what if
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:21:07,512'); seek(1267.0)">
              I try to parallelize now and have a singular queue,
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:21:11,214'); seek(1271.0)">
              but have doubled deburkers? Now I
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:21:15,496'); seek(1275.0)">
              produce two marbles in each 100 millisecond.
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:21:19,698'); seek(1279.0)">
              So I have 20 marbles per second as my throughput
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:21:23,618'); seek(1283.0)">
              or as my departure rate. The latency is also housed
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:21:28,034'); seek(1288.0)">
              because now I can produce
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:21:33,870'); seek(1293.0)">
              four times two marbles overall
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:21:39,970'); seek(1299.0)">
              in four times 100 milliseconds. And that comes up to 400 milliseconds.
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:21:45,570'); seek(1305.0)">
              Okay, but what if I divide
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:21:49,178'); seek(1309.0)">
              the work like that? So what? Instead of
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:21:53,704'); seek(1313.0)">
              having a single parallelized operation,
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:21:56,910'); seek(1316.0)">
              I try to split the work in
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:22:00,776'); seek(1320.0)">
              two halves, which can be finished in two times 50 millisecond.
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:22:05,998'); seek(1325.0)">
              Let's see the numbers. Now, I can produce a single marble within
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:22:10,156'); seek(1330.0)">
              50 millisecond that comes up with this throughput as before,
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:22:13,772'); seek(1333.0)">
              as in the previous example, as 20 marbles per second.
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:22:17,516'); seek(1337.0)">
              So I still improved, doubled my throughput. But how is my
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:22:21,248'); seek(1341.0)">
              latency changed? My latency will be still 800
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:22:24,960'); seek(1344.0)">
              milliseconds because I need a single marble,
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:22:29,390'); seek(1349.0)">
              100 millisecond to travel through from the left side to the right
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:22:32,612'); seek(1352.0)">
              side. Right. I need two times 50 milliseconds for a single marble
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:22:36,618'); seek(1356.0)">
              to become an orange marble, to become green marble.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:22:40,930'); seek(1360.0)">
              So interestingly, latency did not change, but throughput
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:22:44,398'); seek(1364.0)">
              increased. And that's the magic, I think, of these reactive
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:22:49,582'); seek(1369.0)">
              libraries that are becoming very popular these days. So by simply
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:22:53,854'); seek(1373.0)">
              declaring my work in a different way,
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:22:57,468'); seek(1377.0)">
              it allows me to have higher level of parallelization.
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:23:01,210'); seek(1381.0)">
              By splitting my workload into smaller chunks
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:23:05,186'); seek(1385.0)">
              and introducing more queues and processing
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:23:09,222'); seek(1389.0)">
              them in smaller units.
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:23:13,062'); seek(1393.0)">
              Overall it increases my parallelization,
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:23:16,646'); seek(1396.0)">
              even though I'm not aware of that, because in the code
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:23:20,384'); seek(1400.0)">
              everything seems sequential calls,
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:23:24,388'); seek(1404.0)">
              right? So if I have a bottleneck, let's say, then the numbers are
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:23:28,212'); seek(1408.0)">
              changed as following. So I still have ten marbles
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:23:31,514'); seek(1411.0)">
              per second, because the bottleneck keeps me
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:23:36,950'); seek(1416.0)">
              from processing a single marble within
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:23:40,760'); seek(1420.0)">
              50 millisecond. And it
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:23:44,168'); seek(1424.0)">
              just allows me to have a green marble in every
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:23:47,432'); seek(1427.0)">
              100 millisecond, because that's where the bottleneck is.
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:23:50,748'); seek(1430.0)">
              And in total, I need 100 plus 50 milliseconds
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:23:54,194'); seek(1434.0)">
              for a single marble to go through. So my latency again is increased to
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:23:58,320'); seek(1438.0)">
              1200 milliseconds. Were are many other scenarios,
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:24:01,830'); seek(1441.0)">
              but I think you can do the math easily in
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:24:05,328'); seek(1445.0)">
              your head. I have a few formulas
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:24:09,890'); seek(1449.0)">
              that maybe not really precise,
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:24:12,954'); seek(1452.0)">
              but it's enough to me to understand what's
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:24:17,018'); seek(1457.0)">
              going on. What's really important, as you can see that the
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:24:20,868'); seek(1460.0)">
              throughput is not really depending
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:24:24,446'); seek(1464.0)">
              on the queue length.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:24:29,110'); seek(1469.0)">
              It's behaving a little bit differently than the latency.
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:24:33,534'); seek(1473.0)">
              So as you've seen before in the example were,
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:24:38,252'); seek(1478.0)">
              I was talking about this research with
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:24:42,010'); seek(1482.0)">
              page load time and bandwidth, throughput and latency
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:24:46,610'); seek(1486.0)">
              does not really depend on each other.
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:24:51,630'); seek(1491.0)">
              What other things can you do with queues? What's very important
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:24:55,136'); seek(1495.0)">
              is that for each queue you can provide its own quality of
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:24:58,928'); seek(1498.0)">
              service. So these numbers can be independently provided
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:25:02,634'); seek(1502.0)">
              for each queue. So let's say if you have producer,
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:25:07,810'); seek(1507.0)">
              a single producer like the one who is producing the
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:25:11,460'); seek(1511.0)">
              orange marbles, which needs higher demand,
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:25:15,182'); seek(1515.0)">
              you can separate it to its own dedicated channel and
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:25:18,552'); seek(1518.0)">
              it won't affect those which are producing. Want to process
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:25:22,630'); seek(1522.0)">
              blue marbles and yellow marbles and won't choke the system
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:25:26,460'); seek(1526.0)">
              so easily with its own requests.
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:25:29,610'); seek(1529.0)">
              So by separating them to different channels, you can
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:25:33,148'); seek(1533.0)">
              offer a separate quality of service to each channel.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:25:37,790'); seek(1537.0)">
              And this can be done in a couple of ways in microservice.
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:25:41,702'); seek(1541.0)">
              First of all, a single service concentrates on one specific
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:25:46,912'); seek(1546.0)">
              workload. And that forms its own queue and
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:25:50,532'); seek(1550.0)">
              its own special way of optimizing for that kind
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:25:53,748'); seek(1553.0)">
              of workload. It's independent from other services, but can
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:25:57,572'); seek(1557.0)">
              be introduced also in a single service.
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:26:01,890'); seek(1561.0)">
              If you have messaging and use multiple channels for multiple
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:26:06,222'); seek(1566.0)">
              clients. I will have again, a detailed, practical example
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:26:09,832'); seek(1569.0)">
              of how we use this feature.
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:26:14,890'); seek(1574.0)">
              So, about Conway's law. Just very quickly thinking
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:26:21,436'); seek(1581.0)">
              about Conway's law, I always just consider how many
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:26:25,404'); seek(1585.0)">
              scenarios we have with
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:26:29,772'); seek(1589.0)">
              communication, with single communication,
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:26:33,074'); seek(1593.0)">
              considering teams. Okay, so in this scenario,
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:26:38,090'); seek(1598.0)">
              in the two part, these two
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:26:42,356'); seek(1602.0)">
              sides of the communication, a single team is controlling
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:26:46,762'); seek(1606.0)">
              the change for each side. This is the easiest scenario
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:26:51,890'); seek(1611.0)">
              because you can do whatever you want. You can proceed
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:26:56,698'); seek(1616.0)">
              as fast as you would like to. Now we have these scenarios
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:27:00,142'); seek(1620.0)">
              when a shared responsibility is on one of the other side.
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:27:03,272'); seek(1623.0)">
              This is some sort of an anti pattern. This is
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:27:06,572'); seek(1626.0)">
              not frequently used. Only companies use on
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:27:10,508'); seek(1630.0)">
              those occasions when they don't really need to
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:27:14,204'); seek(1634.0)">
              change so many things in a legacy service,
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:27:20,910'); seek(1640.0)">
              or they don't really know how to separate the ownership
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:27:25,558'); seek(1645.0)">
              of maybe a bigger chunk of code.
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:27:28,910'); seek(1648.0)">
              This slows things down radically. This is
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:27:32,468'); seek(1652.0)">
              when you have to be very careful. This is when you need to introduce nonbreaking
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:27:36,026'); seek(1656.0)">
              changes or have the legacy endpoint live
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:27:40,228'); seek(1660.0)">
              for a very long time. Now,
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:27:43,876'); seek(1663.0)">
              there is this more healthier scenario when you have multiple
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:27:47,102'); seek(1667.0)">
              consumers and you are the producer side or you are the
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:27:50,168'); seek(1670.0)">
              consumer and there are multiple producers. This can happen in
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:27:53,848'); seek(1673.0)">
              many situations. What's important to understand, I think that
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:27:58,524'); seek(1678.0)">
              the service ownership does not necessarily come
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:28:02,060'); seek(1682.0)">
              with the schema ownership. You are free to
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:28:05,900'); seek(1685.0)">
              move the scheme ownership to the other side,
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:28:10,350'); seek(1690.0)">
              back and forth, however you feel it's more
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:28:13,792'); seek(1693.0)">
              suitable. This comes had in a couple
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:28:17,072'); seek(1697.0)">
              of situations. So let's say that this is an event
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:28:20,932'); seek(1700.0)">
              based system and who should control, in this case
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:28:25,172'); seek(1705.0)">
              the schema, the message producers who are producing
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:28:29,146'); seek(1709.0)">
              the events themselves, should they tell for the other teams
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:28:32,922'); seek(1712.0)">
              that, yeah, there's going to be a schema change and be aware of that,
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:28:36,680'); seek(1716.0)">
              and then just contact all the other teams, see if they
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:28:40,312'); seek(1720.0)">
              are ready for accepting the new event.
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:28:44,150'); seek(1724.0)">
              Or should we do it in a different way? Should the consumers
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:28:47,666'); seek(1727.0)">
              be controlling the scheme ownership and tell the producer that,
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:28:51,548'); seek(1731.0)">
              okay, we are expecting these kind of messages.
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:28:55,290'); seek(1735.0)">
              From now on, we are accepting this kind
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:28:59,308'); seek(1739.0)">
              of change, but not ready for another change and so on.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:29:02,592'); seek(1742.0)">
              There are tools and techniques
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:29:06,406'); seek(1746.0)">
              on how to do this, and it helps visibility.
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:29:10,582'); seek(1750.0)">
              It had testability helps with many things.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:29:13,812'); seek(1753.0)">
              There are also schema registries that you can introduce. You can switch
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:29:17,786'); seek(1757.0)">
              from something that's schema s
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:29:21,780'); seek(1761.0)">
              like traditional rest based API,
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:29:26,766'); seek(1766.0)">
              which is offering just simple JSON
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:29:30,526'); seek(1770.0)">
              based communication to a more
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:29:35,590'); seek(1775.0)">
              conservative way of communicating, using strongly typed
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:29:40,146'); seek(1780.0)">
              APIs like graphQL, GrPC,
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:29:43,474'); seek(1783.0)">
              or maybe introducing schemas into events or messages as well.
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:29:49,950'); seek(1789.0)">
              And there is this more most complicated scenario, when there
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:29:53,808'); seek(1793.0)">
              are multiple teams in each side,
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:29:56,910'); seek(1796.0)">
              producer and consumer side, that's when you need something more
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:30:01,970'); seek(1801.0)">
              advanced, or the most advanced things for controlling schemas.
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:30:05,642'); seek(1805.0)">
              Something like schema Federation, that's storing
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:30:10,474'); seek(1810.0)">
              different versions and kinds of schemas and schema changes in a
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:30:14,408'); seek(1814.0)">
              controlled way, most preferably in a venture controlled
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:30:18,238'); seek(1818.0)">
              way. Okay, so this
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:30:23,192'); seek(1823.0)">
              is where the first part ends. Now I would like to just
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:30:26,556'); seek(1826.0)">
              quickly introduce you the toolbox or the
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:30:30,316'); seek(1830.0)">
              things that I consider and jump right to the next
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:30:34,810'); seek(1834.0)">
              section. And we will talk about practical examples and
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:30:38,752'); seek(1838.0)">
              situations that I faced. And I would like to guide
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:30:42,278'); seek(1842.0)">
              you how we improved situations each by each.
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:30:46,830'); seek(1846.0)">
              So the tools that I use, usually you
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:30:51,268'); seek(1851.0)">
              can do something like cqrs,
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:30:55,570'); seek(1855.0)">
              meaning that you can separate the write and read path.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:30:58,794'); seek(1858.0)">
              If you need something special on the read side, or maybe something
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:31:02,036'); seek(1862.0)">
              special on the right side, then we
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:31:05,672'); seek(1865.0)">
              talked a lot about schemas. You can introduce contract based testing.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:31:09,910'); seek(1869.0)">
              It helps to move the schema ownership to the other side.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:31:15,290'); seek(1875.0)">
              Then you can introduce caching. We saw in
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:31:18,668'); seek(1878.0)">
              the latency part how caching can improve the latency.
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:31:23,530'); seek(1883.0)">
              With caching, you have to think about data freshness
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:31:27,282'); seek(1887.0)">
              and multi write helps. Here I
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:31:31,692'); seek(1891.0)">
              call multi write something that keeps
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:31:36,758'); seek(1896.0)">
              the cached values fresh in a proactive way.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:31:40,644'); seek(1900.0)">
              So if you grab a fresh value from
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:31:44,996'); seek(1904.0)">
              one side of your system because one of the clients
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:31:49,146'); seek(1909.0)">
              needs that, you need to proactively write it to other
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:31:52,792'); seek(1912.0)">
              cache instances to keep the data fresh and
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:31:56,616'); seek(1916.0)">
              reduce the number of cache misses.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:32:00,710'); seek(1920.0)">
              Then you can switch from synchronous to
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:32:04,136'); seek(1924.0)">
              asynchronous communication by keeping the original
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:32:07,522'); seek(1927.0)">
              API, by introducing polling, by introducing maybe
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:32:11,370'); seek(1931.0)">
              synchronous API that sends forward the request
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:32:15,586'); seek(1935.0)">
              to a message queue and then just
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:32:19,148'); seek(1939.0)">
              send simple response back to the client. There are
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:32:23,168'); seek(1943.0)">
              also design practices or design principles
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:32:27,366'); seek(1947.0)">
              that you can rely on like cloud native and twelve factor.
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:32:30,890'); seek(1950.0)">
              I won't cover these, just thought it's good to mention
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:32:34,340'); seek(1954.0)">
              them. Auto scaling can be
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:32:38,212'); seek(1958.0)">
              effective in many ways and
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:32:43,300'); seek(1963.0)">
              auto scaling has a positive effect on throughput,
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:32:46,862'); seek(1966.0)">
              but not on latency. As we discussed, I talked about
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:32:50,712'); seek(1970.0)">
              back pressure back in this section when talking about queuing
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:32:54,238'); seek(1974.0)">
              theory, when you have higher
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:32:57,404'); seek(1977.0)">
              arrival rate than departure rate,
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:33:01,050'); seek(1981.0)">
              if you need large scale transactions, then you can introduce
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:33:05,122'); seek(1985.0)">
              sagas in a microservice architecture. You can do it in a
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:33:08,128'); seek(1988.0)">
              couple of ways. You can control the transaction either
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:33:11,632'); seek(1991.0)">
              by using orchestration or choreography.
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:33:17,550'); seek(1997.0)">
              You can introduce a service mesh. I think service meshes are
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:33:21,172'); seek(2001.0)">
              important because there are many ways to fine tune the communication
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:33:25,066'); seek(2005.0)">
              channels inside service mesh.
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:33:28,370'); seek(2008.0)">
              It improves your observability.
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:33:32,130'); seek(2012.0)">
              It helps you with certain kind of security aspects
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:33:36,110'); seek(2016.0)">
              and you can introduce many resiliency patterns,
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:33:39,518'); seek(2019.0)">
              us configurations inside service meshes like
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:33:43,848'); seek(2023.0)">
              security breakers, timeouts, retries and so on and
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:33:47,932'); seek(2027.0)">
              so forth. You can be conscious
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:33:51,826'); seek(2031.0)">
              about your technology choices. So for instance,
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:33:55,074'); seek(2035.0)">
              if you choose GRPC over traditional
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:33:58,722'); seek(2038.0)">
              rest based communication, you can expect lower latency
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:34:02,294'); seek(2042.0)">
              because usually GRPC has less
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:34:06,000'); seek(2046.0)">
              round trips during a communication,
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:34:08,990'); seek(2048.0)">
              during a request reasons, and payload is smaller
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:34:13,162'); seek(2053.0)">
              because it's binary based. So probably you have more throughput.
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:34:18,610'); seek(2058.0)">
              Messaging has many patterns, so if asynchronous communication
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:34:22,682'); seek(2062.0)">
              is not enough for you, then you can introduce messaging in one
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:34:25,928'); seek(2065.0)">
              of the sides. Switch from synchronous to asynchronous communications
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:34:30,158'); seek(2070.0)">
              and then you are free to use all those messaging patterns
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:34:33,742'); seek(2073.0)">
              which will increase the robustness of the communication itself
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:34:37,164'); seek(2077.0)">
              and maybe help in a specific situation.
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:34:41,210'); seek(2081.0)">
              If you choose your concurrency model well,
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:34:44,892'); seek(2084.0)">
              it will have higher throughput,
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:34:48,598'); seek(2088.0)">
              probably won't have a positive effect on the latency, but have higher
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:34:52,560'); seek(2092.0)">
              throughput with less resource. So it will introduce
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:34:56,438'); seek(2096.0)">
              more channels, more queues, but not necessarily more threads.
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:35:01,170'); seek(2101.0)">
              This is very well used
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:35:04,996'); seek(2104.0)">
              and I think a well settled technology coming in with reactive
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:35:08,842'); seek(2108.0)">
              programming or coroutines, so they are good
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:35:12,916'); seek(2112.0)">
              choices if you want to save resources with
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:35:16,788'); seek(2116.0)">
              your communication. Then there are
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:35:20,136'); seek(2120.0)">
              these resiliency patterns I think many people know
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:35:23,928'); seek(2123.0)">
              because they are widely used in
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:35:27,900'); seek(2127.0)">
              the microservice world. Also, service meshes offer them
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:35:31,212'); seek(2131.0)">
              by default. There are also libraries that are providing
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:35:36,090'); seek(2136.0)">
              most of these. So there are circuit breakers, bulkheads,
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:35:39,174'); seek(2139.0)">
              retries, timeouts, just to name the most important
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:35:43,408'); seek(2143.0)">
              parts. Timeout comes with all the libraries which are communicating
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:35:47,702'); seek(2147.0)">
              with the network. Then you have observability
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:35:52,130'); seek(2152.0)">
              to just review the whole and understand if you
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:35:55,588'); seek(2155.0)">
              are improved or not. Now let's jump to the example
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:35:59,444'); seek(2159.0)">
              part. So I will pick a couple of practical examples
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:36:03,146'); seek(2163.0)">
              that I've met, and I will go through how
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:36:08,472'); seek(2168.0)">
              in a specific situation things are improved
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:36:12,870'); seek(2172.0)">
              with what kind of practices. Okay, so one
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:36:16,348'); seek(2176.0)">
              of the examples I like is coming from the
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:36:20,524'); seek(2180.0)">
              distributed database called Cassandra, and this is called
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:36:24,316'); seek(2184.0)">
              this technique called rapid read protection. This is how it works.
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:36:28,092'); seek(2188.0)">
              So let's say a client needs to read the data
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:36:32,350'); seek(2192.0)">
              from the database, and it needs the data
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:36:35,776'); seek(2195.0)">
              to be up to date. So then the client goes to this
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:36:39,168'); seek(2199.0)">
              so called coordinator node that you can see on the top left,
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:36:42,388'); seek(2202.0)">
              and the coordinator node then gets
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:36:47,492'); seek(2207.0)">
              the data from each replicas, then aggregates the data
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:36:51,620'); seek(2211.0)">
              based on its freshness, and then sends back the update data to the client.
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:36:56,550'); seek(2216.0)">
              Now what happens if one of the requests is
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:36:59,896'); seek(2219.0)">
              being slow? Instead of waiting for the request,
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:37:03,166'); seek(2223.0)">
              the coordinator node is going to fire a so
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:37:06,892'); seek(2226.0)">
              called backup request, hoping that this backup request
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:37:10,706'); seek(2230.0)">
              will finish faster,
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:37:14,130'); seek(2234.0)">
              and hoping that the coordinator node will
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:37:19,920'); seek(2239.0)">
              send back the data to the client also faster.
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:37:24,110'); seek(2244.0)">
              Why is this happening? Isn't this just a waste
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:37:27,734'); seek(2247.0)">
              of effort and just a lot
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:37:31,348'); seek(2251.0)">
              of complication? Shouldn't be more efficient
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:37:35,546'); seek(2255.0)">
              if we just wait for
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:37:38,948'); seek(2258.0)">
              that request to finish and just fail if there is no
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:37:42,468'); seek(2262.0)">
              answer. So if you think about availability again,
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:37:46,168'); seek(2266.0)">
              let's say we have 99% of availability for each node
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:37:49,902'); seek(2269.0)">
              to be successful, successfully responding
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:37:55,030'); seek(2275.0)">
              the payload to the coordinator node.
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:38:00,150'); seek(2280.0)">
              In this case of 1% of a chance. When we have failure,
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:38:04,050'); seek(2284.0)">
              we will still have 1% of chance, or 99% of the
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:38:07,628'); seek(2287.0)">
              chance to be successful if we use a backup request.
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:38:10,758'); seek(2290.0)">
              So the overall availability for this simplified scenario
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:38:15,254'); seek(2295.0)">
              for the simplified query is increased. It's now around 99 99%.
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:38:21,250'); seek(2301.0)">
              Okay? And if you would also investigate the P 99
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:38:26,290'); seek(2306.0)">
              latency numbers, we will also see a decrease
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:38:30,690'); seek(2310.0)">
              compared to the scenario if we would turn this off,
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:38:35,670'); seek(2315.0)">
              because we won't see timeouts that often. It's true
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:38:39,720'); seek(2319.0)">
              that overall we have some situations when
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:38:42,824'); seek(2322.0)">
              it would have been better to wait for
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:38:47,770'); seek(2327.0)">
              the answer to arrive instead of just going on with another
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:38:51,260'); seek(2331.0)">
              backup request, because that can also fail. But overall at large scale,
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:38:55,122'); seek(2335.0)">
              statistically we are still better, still performing better.
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:38:59,950'); seek(2339.0)">
              Okay, now the other use case is coming from a scenario where
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:39:03,488'); seek(2343.0)">
              I had to design a system with a read heavy workload,
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:39:06,966'); seek(2346.0)">
              a very read heavy workload,
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:39:09,970'); seek(2349.0)">
              and the writes were theoretically
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:39:14,938'); seek(2354.0)">
              almost immutable. They were not changing at all.
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:39:18,532'); seek(2358.0)">
              So we just created objects in this
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:39:22,452'); seek(2362.0)">
              part of the architecture. We were hardly changing
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:39:25,742'); seek(2365.0)">
              or updating them.
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:39:30,070'); seek(2370.0)">
              I needed to reach a very low latency requirement.
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:39:33,486'); seek(2373.0)">
              So this as
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:39:37,756'); seek(2377.0)">
              the result meant that I had
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:39:41,468'); seek(2381.0)">
              to prevent scenarios were I had to deal with cold cache with
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:39:44,972'); seek(2384.0)">
              something that comes and reads
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:39:48,374'); seek(2388.0)">
              up all the data from the database. And I
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:39:52,048'); seek(2392.0)">
              couldn't use any distributed cache solution for this situation because
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:39:55,680'); seek(2395.0)">
              it would have hurt latency so much that
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:40:01,170'); seek(2401.0)">
              it would have been impossible to meet this low latency requirement.
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:40:06,690'); seek(2406.0)">
              Were comes cqrs in the play. So instead of just trying
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:40:10,292'); seek(2410.0)">
              to put everything in a single service and try to fine tune
            </span>
            
            <span id="chunk-605" class="transcript-chunks" onclick="console.log('00:40:14,014'); seek(2414.0)">
              and optimize that and try to benchmark things and try
            </span>
            
            <span id="chunk-606" class="transcript-chunks" onclick="console.log('00:40:17,848'); seek(2417.0)">
              to find a bottleneck and improve,
            </span>
            
            <span id="chunk-607" class="transcript-chunks" onclick="console.log('00:40:22,310'); seek(2422.0)">
              you should think in a larger picture
            </span>
            
            <span id="chunk-608" class="transcript-chunks" onclick="console.log('00:40:26,078'); seek(2426.0)">
              and you should use the techniques that I talked about. So we
            </span>
            
            <span id="chunk-609" class="transcript-chunks" onclick="console.log('00:40:29,612'); seek(2429.0)">
              separated the bright path that you can see on the left because we are not
            </span>
            
            <span id="chunk-610" class="transcript-chunks" onclick="console.log('00:40:32,812'); seek(2432.0)">
              really interested in the latency of the bright operations.
            </span>
            
            <span id="chunk-611" class="transcript-chunks" onclick="console.log('00:40:36,250'); seek(2436.0)">
              It was not critical for these writes to
            </span>
            
            <span id="chunk-612" class="transcript-chunks" onclick="console.log('00:40:40,496'); seek(2440.0)">
              happen immediately. So we went
            </span>
            
            <span id="chunk-613" class="transcript-chunks" onclick="console.log('00:40:43,760'); seek(2443.0)">
              on to AWS queue and then continued the
            </span>
            
            <span id="chunk-614" class="transcript-chunks" onclick="console.log('00:40:47,808'); seek(2447.0)">
              write to the database side. And it was
            </span>
            
            <span id="chunk-615" class="transcript-chunks" onclick="console.log('00:40:51,172'); seek(2451.0)">
              true that we were having large write workloads, but because
            </span>
            
            <span id="chunk-616" class="transcript-chunks" onclick="console.log('00:40:55,076'); seek(2455.0)">
              we were writing through a queue, it helps us to keep
            </span>
            
            <span id="chunk-617" class="transcript-chunks" onclick="console.log('00:41:00,050'); seek(2460.0)">
              the write operations on the dynamodb lower and
            </span>
            
            <span id="chunk-618" class="transcript-chunks" onclick="console.log('00:41:05,336'); seek(2465.0)">
              iron out these bursty
            </span>
            
            <span id="chunk-619" class="transcript-chunks" onclick="console.log('00:41:09,006'); seek(2469.0)">
              operations and help to keep the write capacity unit
            </span>
            
            <span id="chunk-620" class="transcript-chunks" onclick="console.log('00:41:12,590'); seek(2472.0)">
              lower than usual. Now for
            </span>
            
            <span id="chunk-621" class="transcript-chunks" onclick="console.log('00:41:15,708'); seek(2475.0)">
              the reads, we introduced a distributed
            </span>
            
            <span id="chunk-622" class="transcript-chunks" onclick="console.log('00:41:18,962'); seek(2478.0)">
              in memory cache solution with hazelcast that was also replicating
            </span>
            
            <span id="chunk-623" class="transcript-chunks" onclick="console.log('00:41:23,682'); seek(2483.0)">
              between each read node. So this
            </span>
            
            <span id="chunk-624" class="transcript-chunks" onclick="console.log('00:41:27,324'); seek(2487.0)">
              resulted us a couple of things. Like if I scale out
            </span>
            
            <span id="chunk-625" class="transcript-chunks" onclick="console.log('00:41:31,088'); seek(2491.0)">
              and have fresh read node, it should not come up with
            </span>
            
            <span id="chunk-626" class="transcript-chunks" onclick="console.log('00:41:34,592'); seek(2494.0)">
              an empty memory database. So immediately when a new read node
            </span>
            
            <span id="chunk-627" class="transcript-chunks" onclick="console.log('00:41:38,438'); seek(2498.0)">
              comes in, it starts synchronizing with the other read
            </span>
            
            <span id="chunk-628" class="transcript-chunks" onclick="console.log('00:41:41,792'); seek(2501.0)">
              nodes, which helps the data being fresh.
            </span>
            
            <span id="chunk-629" class="transcript-chunks" onclick="console.log('00:41:45,130'); seek(2505.0)">
              Also, when we have a cache miss in one of the read nodes,
            </span>
            
            <span id="chunk-630" class="transcript-chunks" onclick="console.log('00:41:48,410'); seek(2508.0)">
              it finds new data. By going to DynamoDB, it immediately
            </span>
            
            <span id="chunk-631" class="transcript-chunks" onclick="console.log('00:41:52,682'); seek(2512.0)">
              proactively starts replicating this data and writing it
            </span>
            
            <span id="chunk-632" class="transcript-chunks" onclick="console.log('00:41:56,088'); seek(2516.0)">
              to other need nodes. So that's how we solved
            </span>
            
            <span id="chunk-633" class="transcript-chunks" onclick="console.log('00:42:00,790'); seek(2520.0)">
              keeping the cache warm with multi writes, with using replication
            </span>
            
            <span id="chunk-634" class="transcript-chunks" onclick="console.log('00:42:05,006'); seek(2525.0)">
              as well. And the bright instance or
            </span>
            
            <span id="chunk-635" class="transcript-chunks" onclick="console.log('00:42:08,972'); seek(2528.0)">
              the bright service responsible for the writes
            </span>
            
            <span id="chunk-636" class="transcript-chunks" onclick="console.log('00:42:13,290'); seek(2533.0)">
              had nothing to do with Hazelcast was
            </span>
            
            <span id="chunk-637" class="transcript-chunks" onclick="console.log('00:42:16,812'); seek(2536.0)">
              not aware of this complicated configuration
            </span>
            
            <span id="chunk-638" class="transcript-chunks" onclick="console.log('00:42:20,322'); seek(2540.0)">
              of the memory cache solution.
            </span>
            
            <span id="chunk-639" class="transcript-chunks" onclick="console.log('00:42:22,750'); seek(2542.0)">
              Also, the read instances did not
            </span>
            
            <span id="chunk-640" class="transcript-chunks" onclick="console.log('00:42:26,352'); seek(2546.0)">
              have to have the SQS based libraries and did
            </span>
            
            <span id="chunk-641" class="transcript-chunks" onclick="console.log('00:42:30,308'); seek(2550.0)">
              not have to do anything with SQS at
            </span>
            
            <span id="chunk-642" class="transcript-chunks" onclick="console.log('00:42:33,508'); seek(2553.0)">
              all with that communication with the access and so on and so
            </span>
            
            <span id="chunk-643" class="transcript-chunks" onclick="console.log('00:42:37,188'); seek(2557.0)">
              forth. So this simplifies your architecture overall even further.
            </span>
            
            <span id="chunk-644" class="transcript-chunks" onclick="console.log('00:42:42,870'); seek(2562.0)">
              Okay, now with client libraries, I have a
            </span>
            
            <span id="chunk-645" class="transcript-chunks" onclick="console.log('00:42:46,264'); seek(2566.0)">
              couple of stories. I treat client
            </span>
            
            <span id="chunk-646" class="transcript-chunks" onclick="console.log('00:42:49,742'); seek(2569.0)">
              libraries as a double edged sword because I
            </span>
            
            <span id="chunk-647" class="transcript-chunks" onclick="console.log('00:42:53,208'); seek(2573.0)">
              think it's very hard to design them in an effective way.
            </span>
            
            <span id="chunk-648" class="transcript-chunks" onclick="console.log('00:42:57,690'); seek(2577.0)">
              They are not considering all these extensibility
            </span>
            
            <span id="chunk-649" class="transcript-chunks" onclick="console.log('00:43:00,850'); seek(2580.0)">
              options and not all the features can be
            </span>
            
            <span id="chunk-650" class="transcript-chunks" onclick="console.log('00:43:04,124'); seek(2584.0)">
              turned off in this specific
            </span>
            
            <span id="chunk-651" class="transcript-chunks" onclick="console.log('00:43:07,388'); seek(2587.0)">
              client library.
            </span>
            
            <span id="chunk-652" class="transcript-chunks" onclick="console.log('00:43:10,750'); seek(2590.0)">
              This library was included in many, many services as dependencies,
            </span>
            
            <span id="chunk-653" class="transcript-chunks" onclick="console.log('00:43:14,982'); seek(2594.0)">
              as default dependencies, and this was the only way of use
            </span>
            
            <span id="chunk-654" class="transcript-chunks" onclick="console.log('00:43:18,656'); seek(2598.0)">
              a specific shared feature of the whole architecture.
            </span>
            
            <span id="chunk-655" class="transcript-chunks" onclick="console.log('00:43:21,978'); seek(2601.0)">
              This specific client library downloaded a
            </span>
            
            <span id="chunk-656" class="transcript-chunks" onclick="console.log('00:43:25,204'); seek(2605.0)">
              zipped archive at startup from Amazon s three
            </span>
            
            <span id="chunk-657" class="transcript-chunks" onclick="console.log('00:43:29,332'); seek(2609.0)">
              and started decompressing
            </span>
            
            <span id="chunk-658" class="transcript-chunks" onclick="console.log('00:43:32,922'); seek(2612.0)">
              it during startup just to get the configuration data
            </span>
            
            <span id="chunk-659" class="transcript-chunks" onclick="console.log('00:43:36,920'); seek(2616.0)">
              required for this library to work effectively. This was
            </span>
            
            <span id="chunk-660" class="transcript-chunks" onclick="console.log('00:43:40,808'); seek(2620.0)">
              the single bottleneck for the startup of the whole service and
            </span>
            
            <span id="chunk-661" class="transcript-chunks" onclick="console.log('00:43:45,128'); seek(2625.0)">
              unfortunately hidden behind
            </span>
            
            <span id="chunk-662" class="transcript-chunks" onclick="console.log('00:43:48,556'); seek(2628.0)">
              the scenes, it messed up the startup for
            </span>
            
            <span id="chunk-663" class="transcript-chunks" onclick="console.log('00:43:52,220'); seek(2632.0)">
              all the development environments.
            </span>
            
            <span id="chunk-664" class="transcript-chunks" onclick="console.log('00:43:55,410'); seek(2635.0)">
              So all the developers suffered from the startup
            </span>
            
            <span id="chunk-665" class="transcript-chunks" onclick="console.log('00:43:58,982'); seek(2638.0)">
              penalty introduced by this client library.
            </span>
            
            <span id="chunk-666" class="transcript-chunks" onclick="console.log('00:44:02,110'); seek(2642.0)">
              Unfortunately, there was no way to turn off the library itself
            </span>
            
            <span id="chunk-667" class="transcript-chunks" onclick="console.log('00:44:06,350'); seek(2646.0)">
              because the maintaining team wanted to keep it as
            </span>
            
            <span id="chunk-668" class="transcript-chunks" onclick="console.log('00:44:09,712'); seek(2649.0)">
              simple as possible. They did not want
            </span>
            
            <span id="chunk-669" class="transcript-chunks" onclick="console.log('00:44:12,884'); seek(2652.0)">
              anyone to change
            </span>
            
            <span id="chunk-670" class="transcript-chunks" onclick="console.log('00:44:17,972'); seek(2657.0)">
              certain functionalities of the library and they thought that if
            </span>
            
            <span id="chunk-671" class="transcript-chunks" onclick="console.log('00:44:21,268'); seek(2661.0)">
              they introduced too many features for the library for customization,
            </span>
            
            <span id="chunk-672" class="transcript-chunks" onclick="console.log('00:44:25,110'); seek(2665.0)">
              then they will have hard times with the maintenance of the library
            </span>
            
            <span id="chunk-673" class="transcript-chunks" onclick="console.log('00:44:28,686'); seek(2668.0)">
              because were could be plenty of ways teams are using
            </span>
            
            <span id="chunk-674" class="transcript-chunks" onclick="console.log('00:44:32,536'); seek(2672.0)">
              that. So how
            </span>
            
            <span id="chunk-675" class="transcript-chunks" onclick="console.log('00:44:36,392'); seek(2676.0)">
              we solve this problem.
            </span>
            
            <span id="chunk-676" class="transcript-chunks" onclick="console.log('00:44:39,910'); seek(2679.0)">
              Okay, sorry. So another issue with client libraries
            </span>
            
            <span id="chunk-677" class="transcript-chunks" onclick="console.log('00:44:43,362'); seek(2683.0)">
              was that it often messed up the deployments. So because
            </span>
            
            <span id="chunk-678" class="transcript-chunks" onclick="console.log('00:44:47,790'); seek(2687.0)">
              health check was depending on the functionality
            </span>
            
            <span id="chunk-679" class="transcript-chunks" onclick="console.log('00:44:52,502'); seek(2692.0)">
              of the library itself, midbed was failure during downloading the
            </span>
            
            <span id="chunk-680" class="transcript-chunks" onclick="console.log('00:44:55,968'); seek(2695.0)">
              data. The service often just timed out and
            </span>
            
            <span id="chunk-681" class="transcript-chunks" onclick="console.log('00:44:59,888'); seek(2699.0)">
              failed to meet the health check requirements. That messed up the deployment.
            </span>
            
            <span id="chunk-682" class="transcript-chunks" onclick="console.log('00:45:04,610'); seek(2704.0)">
              So even deployments were negatively affected by the usage
            </span>
            
            <span id="chunk-683" class="transcript-chunks" onclick="console.log('00:45:08,218'); seek(2708.0)">
              of this library. So how do we solve
            </span>
            
            <span id="chunk-684" class="transcript-chunks" onclick="console.log('00:45:11,418'); seek(2711.0)">
              this issue? We just simply shift things on
            </span>
            
            <span id="chunk-685" class="transcript-chunks" onclick="console.log('00:45:15,464'); seek(2715.0)">
              the left side during delivery, during deployment.
            </span>
            
            <span id="chunk-686" class="transcript-chunks" onclick="console.log('00:45:19,198'); seek(2719.0)">
              What this meant for us, we just picked a mock
            </span>
            
            <span id="chunk-687" class="transcript-chunks" onclick="console.log('00:45:23,102'); seek(2723.0)">
              s three. S three solution, containerized that using
            </span>
            
            <span id="chunk-688" class="transcript-chunks" onclick="console.log('00:45:27,356'); seek(2727.0)">
              Docker and used it as a sidecar container. We prepared
            </span>
            
            <span id="chunk-689" class="transcript-chunks" onclick="console.log('00:45:32,034'); seek(2732.0)">
              two mock libraries in
            </span>
            
            <span id="chunk-690" class="transcript-chunks" onclick="console.log('00:45:37,628'); seek(2737.0)">
              this mock storage. One of them was empty, meaning that it
            </span>
            
            <span id="chunk-691" class="transcript-chunks" onclick="console.log('00:45:41,488'); seek(2741.0)">
              was very fast to download and extract that, and the other one
            </span>
            
            <span id="chunk-692" class="transcript-chunks" onclick="console.log('00:45:45,550'); seek(2745.0)">
              downloaded the same way,
            </span>
            
            <span id="chunk-693" class="transcript-chunks" onclick="console.log('00:45:48,592'); seek(2748.0)">
              downloaded the data from the s three bucket, then extracted that and
            </span>
            
            <span id="chunk-694" class="transcript-chunks" onclick="console.log('00:45:51,812'); seek(2751.0)">
              just had that certain configuration which
            </span>
            
            <span id="chunk-695" class="transcript-chunks" onclick="console.log('00:45:55,300'); seek(2755.0)">
              was meaningful for us, and we put that into
            </span>
            
            <span id="chunk-696" class="transcript-chunks" onclick="console.log('00:45:58,852'); seek(2758.0)">
              this mock container. The thing why this worked was very
            </span>
            
            <span id="chunk-697" class="transcript-chunks" onclick="console.log('00:46:02,856'); seek(2762.0)">
              simple. It was because they
            </span>
            
            <span id="chunk-698" class="transcript-chunks" onclick="console.log('00:46:07,112'); seek(2767.0)">
              did not have a feature switch or a queue switch for the library itself,
            </span>
            
            <span id="chunk-699" class="transcript-chunks" onclick="console.log('00:46:10,888'); seek(2770.0)">
              but the URL was configurable because each specific
            </span>
            
            <span id="chunk-700" class="transcript-chunks" onclick="console.log('00:46:15,304'); seek(2775.0)">
              environment had its own bucket configuration.
            </span>
            
            <span id="chunk-701" class="transcript-chunks" onclick="console.log('00:46:20,570'); seek(2780.0)">
              Another way other teams solved this problem was introducing some kind of
            </span>
            
            <span id="chunk-702" class="transcript-chunks" onclick="console.log('00:46:24,684'); seek(2784.0)">
              programmatic proxy, which was still trying
            </span>
            
            <span id="chunk-703" class="transcript-chunks" onclick="console.log('00:46:28,720'); seek(2788.0)">
              to keep things on when this connection was not available.
            </span>
            
            <span id="chunk-704" class="transcript-chunks" onclick="console.log('00:46:33,040'); seek(2793.0)">
              And behind the scenes try to kick in this client library
            </span>
            
            <span id="chunk-705" class="transcript-chunks" onclick="console.log('00:46:37,126'); seek(2797.0)">
              to retry loading up the data and
            </span>
            
            <span id="chunk-706" class="transcript-chunks" onclick="console.log('00:46:40,692'); seek(2800.0)">
              delaying the downloading of this data
            </span>
            
            <span id="chunk-707" class="transcript-chunks" onclick="console.log('00:46:45,860'); seek(2805.0)">
              that was important for them. Okay. Rather thing,
            </span>
            
            <span id="chunk-708" class="transcript-chunks" onclick="console.log('00:46:49,604'); seek(2809.0)">
              another technique that I want to talk about is what I call region
            </span>
            
            <span id="chunk-709" class="transcript-chunks" onclick="console.log('00:46:53,722'); seek(2813.0)">
              pinning. This might be not necessarily the appropriate
            </span>
            
            <span id="chunk-710" class="transcript-chunks" onclick="console.log('00:46:57,422'); seek(2817.0)">
              name for this technique, but this is how everybody was
            </span>
            
            <span id="chunk-711" class="transcript-chunks" onclick="console.log('00:47:00,712'); seek(2820.0)">
              using that. So this is how I refer to that as well.
            </span>
            
            <span id="chunk-712" class="transcript-chunks" onclick="console.log('00:47:04,070'); seek(2824.0)">
              Imagine the following scenario.
            </span>
            
            <span id="chunk-713" class="transcript-chunks" onclick="console.log('00:47:07,690'); seek(2827.0)">
              We had to migrate to the cloud with our own whole architecture,
            </span>
            
            <span id="chunk-714" class="transcript-chunks" onclick="console.log('00:47:12,082'); seek(2832.0)">
              the whole system, and then had to make it multiregion.
            </span>
            
            <span id="chunk-715" class="transcript-chunks" onclick="console.log('00:47:16,090'); seek(2836.0)">
              And one of the problem
            </span>
            
            <span id="chunk-716" class="transcript-chunks" onclick="console.log('00:47:19,840'); seek(2839.0)">
              was what we found was with shopping calls.
            </span>
            
            <span id="chunk-717" class="transcript-chunks" onclick="console.log('00:47:23,558'); seek(2843.0)">
              So each service was, there was not
            </span>
            
            <span id="chunk-718" class="transcript-chunks" onclick="console.log('00:47:26,912'); seek(2846.0)">
              a dedicated shopping cart service. Each service was managing
            </span>
            
            <span id="chunk-719" class="transcript-chunks" onclick="console.log('00:47:31,178'); seek(2851.0)">
              the shopping cart through a shared database,
            </span>
            
            <span id="chunk-720" class="transcript-chunks" onclick="console.log('00:47:35,626'); seek(2855.0)">
              namely a shared Cassandra cluster. So each service
            </span>
            
            <span id="chunk-721" class="transcript-chunks" onclick="console.log('00:47:39,508'); seek(2859.0)">
              was reading the shopping cart from a shant Cassandra cluster through a library,
            </span>
            
            <span id="chunk-722" class="transcript-chunks" onclick="console.log('00:47:44,270'); seek(2864.0)">
              and was updating the shopping cart when was necessary through a
            </span>
            
            <span id="chunk-723" class="transcript-chunks" onclick="console.log('00:47:48,008'); seek(2868.0)">
              shared library, and then putting the
            </span>
            
            <span id="chunk-724" class="transcript-chunks" onclick="console.log('00:47:51,208'); seek(2871.0)">
              data to this shared database. Okay. And we
            </span>
            
            <span id="chunk-725" class="transcript-chunks" onclick="console.log('00:47:57,132'); seek(2877.0)">
              could not really rely on the replication lag because it
            </span>
            
            <span id="chunk-726" class="transcript-chunks" onclick="console.log('00:48:00,668'); seek(2880.0)">
              was uncontrollable. So the minimum
            </span>
            
            <span id="chunk-727" class="transcript-chunks" onclick="console.log('00:48:04,002'); seek(2884.0)">
              replication lag was around 60 milliseconds.
            </span>
            
            <span id="chunk-728" class="transcript-chunks" onclick="console.log('00:48:07,202'); seek(2887.0)">
              But in practice, if you have more pressure on the database,
            </span>
            
            <span id="chunk-729" class="transcript-chunks" onclick="console.log('00:48:10,406'); seek(2890.0)">
              this could have been easily increased to 500 600 milliseconds
            </span>
            
            <span id="chunk-730" class="transcript-chunks" onclick="console.log('00:48:14,902'); seek(2894.0)">
              based on the load itself. Okay.
            </span>
            
            <span id="chunk-731" class="transcript-chunks" onclick="console.log('00:48:20,190'); seek(2900.0)">
              And we were afraid that these shopping carts will disappear. So if one
            </span>
            
            <span id="chunk-732" class="transcript-chunks" onclick="console.log('00:48:24,068'); seek(2904.0)">
              of the service or the traffic for one of the service is
            </span>
            
            <span id="chunk-733" class="transcript-chunks" onclick="console.log('00:48:27,780'); seek(2907.0)">
              put to the right side during the user journey, we were afraid
            </span>
            
            <span id="chunk-734" class="transcript-chunks" onclick="console.log('00:48:31,658'); seek(2911.0)">
              that when the shopping cart was being loaded because of
            </span>
            
            <span id="chunk-735" class="transcript-chunks" onclick="console.log('00:48:34,868'); seek(2914.0)">
              the replication lag, it was not available, it was empty, or maybe
            </span>
            
            <span id="chunk-736" class="transcript-chunks" onclick="console.log('00:48:38,392'); seek(2918.0)">
              it's not containing the up to date changes. So how do we
            </span>
            
            <span id="chunk-737" class="transcript-chunks" onclick="console.log('00:48:41,976'); seek(2921.0)">
              distinguish between users that have up to date shopping cart
            </span>
            
            <span id="chunk-738" class="transcript-chunks" onclick="console.log('00:48:45,982'); seek(2925.0)">
              in the left region from those that, let's say,
            </span>
            
            <span id="chunk-739" class="transcript-chunks" onclick="console.log('00:48:49,692'); seek(2929.0)">
              don't have yet shopping cart or started their journey on
            </span>
            
            <span id="chunk-740" class="transcript-chunks" onclick="console.log('00:48:53,004'); seek(2933.0)">
              the right region? That's where region pinning is coming into play.
            </span>
            
            <span id="chunk-741" class="transcript-chunks" onclick="console.log('00:48:57,390'); seek(2937.0)">
              We flag the originating
            </span>
            
            <span id="chunk-742" class="transcript-chunks" onclick="console.log('00:49:01,510'); seek(2941.0)">
              region for each user by using a cookie, and when
            </span>
            
            <span id="chunk-743" class="transcript-chunks" onclick="console.log('00:49:05,136'); seek(2945.0)">
              moving those certain users who need their shopping
            </span>
            
            <span id="chunk-744" class="transcript-chunks" onclick="console.log('00:49:08,438'); seek(2948.0)">
              cart, but started their user journey in the left
            </span>
            
            <span id="chunk-745" class="transcript-chunks" onclick="console.log('00:49:11,796'); seek(2951.0)">
              region to the right side. Based on this cookie, we reach
            </span>
            
            <span id="chunk-746" class="transcript-chunks" onclick="console.log('00:49:15,316'); seek(2955.0)">
              out through this white line,
            </span>
            
            <span id="chunk-747" class="transcript-chunks" onclick="console.log('00:49:19,076'); seek(2959.0)">
              paying that 60 millisecond latency penalty,
            </span>
            
            <span id="chunk-748" class="transcript-chunks" onclick="console.log('00:49:22,378'); seek(2962.0)">
              but loading the data consistently and having
            </span>
            
            <span id="chunk-749" class="transcript-chunks" onclick="console.log('00:49:26,296'); seek(2966.0)">
              the shopping cart. So with this way we
            </span>
            
            <span id="chunk-750" class="transcript-chunks" onclick="console.log('00:49:29,528'); seek(2969.0)">
              won't penalize calls the users because we are not going through
            </span>
            
            <span id="chunk-751" class="transcript-chunks" onclick="console.log('00:49:36,230'); seek(2976.0)">
              this white line for all the users all the time.
            </span>
            
            <span id="chunk-752" class="transcript-chunks" onclick="console.log('00:49:39,500'); seek(2979.0)">
              We just proactively select those users which needs
            </span>
            
            <span id="chunk-753" class="transcript-chunks" onclick="console.log('00:49:44,650'); seek(2984.0)">
              more consistency but should be okay
            </span>
            
            <span id="chunk-754" class="transcript-chunks" onclick="console.log('00:49:48,384'); seek(2988.0)">
              with this latest increase.
            </span>
            
            <span id="chunk-755" class="transcript-chunks" onclick="console.log('00:49:53,950'); seek(2993.0)">
              In another example, I was using the same approach
            </span>
            
            <span id="chunk-756" class="transcript-chunks" onclick="console.log('00:49:58,070'); seek(2998.0)">
              when I was designing an auction solution.
            </span>
            
            <span id="chunk-757" class="transcript-chunks" onclick="console.log('00:50:02,050'); seek(3002.0)">
              So here we had to keep a strict
            </span>
            
            <span id="chunk-758" class="transcript-chunks" onclick="console.log('00:50:05,322'); seek(3005.0)">
              ordering of the biddings. And no
            </span>
            
            <span id="chunk-759" class="transcript-chunks" onclick="console.log('00:50:10,356'); seek(3010.0)">
              matter which message provider I looked at,
            </span>
            
            <span id="chunk-760" class="transcript-chunks" onclick="console.log('00:50:13,592'); seek(3013.0)">
              I figured out that there is no way to keep
            </span>
            
            <span id="chunk-761" class="transcript-chunks" onclick="console.log('00:50:17,432'); seek(3017.0)">
              the messages in order when I rely on synchronization
            </span>
            
            <span id="chunk-762" class="transcript-chunks" onclick="console.log('00:50:21,678'); seek(3021.0)">
              between regions. Okay, so if the auction started
            </span>
            
            <span id="chunk-763" class="transcript-chunks" onclick="console.log('00:50:25,944'); seek(3025.0)">
              on the left side in the
            </span>
            
            <span id="chunk-764" class="transcript-chunks" onclick="console.log('00:50:29,192'); seek(3029.0)">
              kafka cluster on the left side, and somebody wanted
            </span>
            
            <span id="chunk-765" class="transcript-chunks" onclick="console.log('00:50:33,260'); seek(3033.0)">
              to participate in that auction on
            </span>
            
            <span id="chunk-766" class="transcript-chunks" onclick="console.log('00:50:36,748'); seek(3036.0)">
              the other region, they had to pay for this latency
            </span>
            
            <span id="chunk-767" class="transcript-chunks" onclick="console.log('00:50:40,326'); seek(3040.0)">
              penalty proactively, but right to
            </span>
            
            <span id="chunk-768" class="transcript-chunks" onclick="console.log('00:50:44,128'); seek(3044.0)">
              the kafka cluster still on the left side.
            </span>
            
            <span id="chunk-769" class="transcript-chunks" onclick="console.log('00:50:47,472'); seek(3047.0)">
              And then for other services or participants
            </span>
            
            <span id="chunk-770" class="transcript-chunks" onclick="console.log('00:50:51,466'); seek(3051.0)">
              which were not latency sensitive, the data was
            </span>
            
            <span id="chunk-771" class="transcript-chunks" onclick="console.log('00:50:54,852'); seek(3054.0)">
              still available, maybe later in a synchronous way through the
            </span>
            
            <span id="chunk-772" class="transcript-chunks" onclick="console.log('00:50:58,388'); seek(3058.0)">
              mirror maker on the right side. So they could have continued
            </span>
            
            <span id="chunk-773" class="transcript-chunks" onclick="console.log('00:51:04,150'); seek(3064.0)">
              processing this data or maybe showing this data a bit
            </span>
            
            <span id="chunk-774" class="transcript-chunks" onclick="console.log('00:51:07,832'); seek(3067.0)">
              later. But those who wanted to participate in bidding had
            </span>
            
            <span id="chunk-775" class="transcript-chunks" onclick="console.log('00:51:11,816'); seek(3071.0)">
              to write consistently on the same region
            </span>
            
            <span id="chunk-776" class="transcript-chunks" onclick="console.log('00:51:15,570'); seek(3075.0)">
              as the auction is originated. This is similar
            </span>
            
            <span id="chunk-777" class="transcript-chunks" onclick="console.log('00:51:19,292'); seek(3079.0)">
              to the things we used to in the gaming
            </span>
            
            <span id="chunk-778" class="transcript-chunks" onclick="console.log('00:51:22,722'); seek(3082.0)">
              world. So if there's a game started in
            </span>
            
            <span id="chunk-779" class="transcript-chunks" onclick="console.log('00:51:26,476'); seek(3086.0)">
              one certain region, then when other players
            </span>
            
            <span id="chunk-780" class="transcript-chunks" onclick="console.log('00:51:30,022'); seek(3090.0)">
              are participating, they have to consider the latency penalty
            </span>
            
            <span id="chunk-781" class="transcript-chunks" onclick="console.log('00:51:33,526'); seek(3093.0)">
              and maybe not perform that well. But this keeps
            </span>
            
            <span id="chunk-782" class="transcript-chunks" onclick="console.log('00:51:36,918'); seek(3096.0)">
              the game consistent. Okay, the last example is
            </span>
            
            <span id="chunk-783" class="transcript-chunks" onclick="console.log('00:51:40,468'); seek(3100.0)">
              coming from the journey of optimizing
            </span>
            
            <span id="chunk-784" class="transcript-chunks" onclick="console.log('00:51:44,314'); seek(3104.0)">
              a single service in a couple of iterations.
            </span>
            
            <span id="chunk-785" class="transcript-chunks" onclick="console.log('00:51:52,870'); seek(3112.0)">
              It. So I would explain the
            </span>
            
            <span id="chunk-786" class="transcript-chunks" onclick="console.log('00:51:56,632'); seek(3116.0)">
              behavior and the functionality of the service a bit more at first.
            </span>
            
            <span id="chunk-787" class="transcript-chunks" onclick="console.log('00:52:00,296'); seek(3120.0)">
              Okay, so this service had two endpoints.
            </span>
            
            <span id="chunk-788" class="transcript-chunks" onclick="console.log('00:52:03,806'); seek(3123.0)">
              First of all, it was a very simple key value store.
            </span>
            
            <span id="chunk-789" class="transcript-chunks" onclick="console.log('00:52:11,770'); seek(3131.0)">
              You can see the value endpoint on the right side of the service,
            </span>
            
            <span id="chunk-790" class="transcript-chunks" onclick="console.log('00:52:16,970'); seek(3136.0)">
              which reads the values for a certain key given.
            </span>
            
            <span id="chunk-791" class="transcript-chunks" onclick="console.log('00:52:20,448'); seek(3140.0)">
              Okay? And that's coming to specific
            </span>
            
            <span id="chunk-792" class="transcript-chunks" onclick="console.log('00:52:23,952'); seek(3143.0)">
              DynamoDB table, which has its own read capacity
            </span>
            
            <span id="chunk-793" class="transcript-chunks" onclick="console.log('00:52:27,862'); seek(3147.0)">
              unit defined. But we have also another endpoint.
            </span>
            
            <span id="chunk-794" class="transcript-chunks" onclick="console.log('00:52:32,610'); seek(3152.0)">
              This is how things were when we got the service,
            </span>
            
            <span id="chunk-795" class="transcript-chunks" onclick="console.log('00:52:36,708'); seek(3156.0)">
              when we started maintaining that. So this is what we
            </span>
            
            <span id="chunk-796" class="transcript-chunks" onclick="console.log('00:52:40,788'); seek(3160.0)">
              kept. This was the endpoint of the recent keys.
            </span>
            
            <span id="chunk-797" class="transcript-chunks" onclick="console.log('00:52:45,110'); seek(3165.0)">
              What designpoint did was that it gave back to the clients
            </span>
            
            <span id="chunk-798" class="transcript-chunks" onclick="console.log('00:52:49,454'); seek(3169.0)">
              the keys that were being accessed by that specific
            </span>
            
            <span id="chunk-799" class="transcript-chunks" onclick="console.log('00:52:53,528'); seek(3173.0)">
              client in the last seven days. That went off
            </span>
            
            <span id="chunk-800" class="transcript-chunks" onclick="console.log('00:52:57,532'); seek(3177.0)">
              to another table, which was statistical
            </span>
            
            <span id="chunk-801" class="transcript-chunks" onclick="console.log('00:53:01,586'); seek(3181.0)">
              table that you can see on the left, which had their own read
            </span>
            
            <span id="chunk-802" class="transcript-chunks" onclick="console.log('00:53:05,292'); seek(3185.0)">
              and write capacity unit defined. Now, both tables had
            </span>
            
            <span id="chunk-803" class="transcript-chunks" onclick="console.log('00:53:09,036'); seek(3189.0)">
              auto scaling configured, but there was a problem. As you can see on
            </span>
            
            <span id="chunk-804" class="transcript-chunks" onclick="console.log('00:53:12,384'); seek(3192.0)">
              the right side diagram, there were one specific client which was
            </span>
            
            <span id="chunk-805" class="transcript-chunks" onclick="console.log('00:53:15,936'); seek(3195.0)">
              firing up 15,000 requests to this values
            </span>
            
            <span id="chunk-806" class="transcript-chunks" onclick="console.log('00:53:19,318'); seek(3199.0)">
              endpoint, because it had 15,000 keys in
            </span>
            
            <span id="chunk-807" class="transcript-chunks" onclick="console.log('00:53:23,028'); seek(3203.0)">
              this usually being used. And it
            </span>
            
            <span id="chunk-808" class="transcript-chunks" onclick="console.log('00:53:26,772'); seek(3206.0)">
              went with one single request to this recent keys. Endpoint found
            </span>
            
            <span id="chunk-809" class="transcript-chunks" onclick="console.log('00:53:30,756'); seek(3210.0)">
              that in the last seven days it was using these 15,000 keys,
            </span>
            
            <span id="chunk-810" class="transcript-chunks" onclick="console.log('00:53:34,638'); seek(3214.0)">
              and iteratively it went through calls, the keys bashing
            </span>
            
            <span id="chunk-811" class="transcript-chunks" onclick="console.log('00:53:38,470'); seek(3218.0)">
              sequentially, these values endpoint increasing traffic
            </span>
            
            <span id="chunk-812" class="transcript-chunks" onclick="console.log('00:53:42,390'); seek(3222.0)">
              and the dynamodb auto scaling was not catching up.
            </span>
            
            <span id="chunk-813" class="transcript-chunks" onclick="console.log('00:53:48,010'); seek(3228.0)">
              There was a time window defined by this
            </span>
            
            <span id="chunk-814" class="transcript-chunks" onclick="console.log('00:53:51,852'); seek(3231.0)">
              red line that you can see on the screen when DynamoDB
            </span>
            
            <span id="chunk-815" class="transcript-chunks" onclick="console.log('00:53:55,858'); seek(3235.0)">
              was throttling because it failed to meet the capacity needs
            </span>
            
            <span id="chunk-816" class="transcript-chunks" onclick="console.log('00:53:59,536'); seek(3239.0)">
              for the traffic. And unfortunately,
            </span>
            
            <span id="chunk-817" class="transcript-chunks" onclick="console.log('00:54:02,726'); seek(3242.0)">
              when the traffic burst was over,
            </span>
            
            <span id="chunk-818" class="transcript-chunks" onclick="console.log('00:54:05,760'); seek(3245.0)">
              auto scaling increased for DynamoDB, but then the increase decreased
            </span>
            
            <span id="chunk-819" class="transcript-chunks" onclick="console.log('00:54:10,230'); seek(3250.0)">
              immediately after that. Okay, now one of the problem was that
            </span>
            
            <span id="chunk-820" class="transcript-chunks" onclick="console.log('00:54:14,580'); seek(3254.0)">
              the statistics table was written
            </span>
            
            <span id="chunk-821" class="transcript-chunks" onclick="console.log('00:54:18,602'); seek(3258.0)">
              sequentially before the read happens, when getting the values,
            </span>
            
            <span id="chunk-822" class="transcript-chunks" onclick="console.log('00:54:22,554'); seek(3262.0)">
              just to keep the statistics fresh. So in
            </span>
            
            <span id="chunk-823" class="transcript-chunks" onclick="console.log('00:54:26,072'); seek(3266.0)">
              maintenance, during maintenance, we had to keep the write capacity units
            </span>
            
            <span id="chunk-824" class="transcript-chunks" onclick="console.log('00:54:29,358'); seek(3269.0)">
              the same as the read capacity units for the two
            </span>
            
            <span id="chunk-825" class="transcript-chunks" onclick="console.log('00:54:32,776'); seek(3272.0)">
              tables, because auto scaling was hard to
            </span>
            
            <span id="chunk-826" class="transcript-chunks" onclick="console.log('00:54:36,252'); seek(3276.0)">
              fine tune, and because if
            </span>
            
            <span id="chunk-827" class="transcript-chunks" onclick="console.log('00:54:42,268'); seek(3282.0)">
              the write capacity units are lower, we are just failing
            </span>
            
            <span id="chunk-828" class="transcript-chunks" onclick="console.log('00:54:46,162'); seek(3286.0)">
              by writing the statistics table. And the reads were also failing.
            </span>
            
            <span id="chunk-829" class="transcript-chunks" onclick="console.log('00:54:50,510'); seek(3290.0)">
              So what we did first was separating this critical path
            </span>
            
            <span id="chunk-830" class="transcript-chunks" onclick="console.log('00:54:54,182'); seek(3294.0)">
              from the rest. So we put
            </span>
            
            <span id="chunk-831" class="transcript-chunks" onclick="console.log('00:54:58,336'); seek(3298.0)">
              the update operation just in a different thread
            </span>
            
            <span id="chunk-832" class="transcript-chunks" onclick="console.log('00:55:02,006'); seek(3302.0)">
              and just deployed solution. Things worked
            </span>
            
            <span id="chunk-833" class="transcript-chunks" onclick="console.log('00:55:06,130'); seek(3306.0)">
              bit better than before, but after a short
            </span>
            
            <span id="chunk-834" class="transcript-chunks" onclick="console.log('00:55:09,716'); seek(3309.0)">
              period of time, there were other problems were seen,
            </span>
            
            <span id="chunk-835" class="transcript-chunks" onclick="console.log('00:55:13,528'); seek(3313.0)">
              that for some reason the
            </span>
            
            <span id="chunk-836" class="transcript-chunks" onclick="console.log('00:55:17,272'); seek(3317.0)">
              memory consumption of the services were increasing
            </span>
            
            <span id="chunk-837" class="transcript-chunks" onclick="console.log('00:55:21,070'); seek(3321.0)">
              and the container orchestrator started killing the services
            </span>
            
            <span id="chunk-838" class="transcript-chunks" onclick="console.log('00:55:25,080'); seek(3325.0)">
              themselves. And the
            </span>
            
            <span id="chunk-839" class="transcript-chunks" onclick="console.log('00:55:30,188'); seek(3330.0)">
              reason of that was in the flow of the implementation.
            </span>
            
            <span id="chunk-840" class="transcript-chunks" onclick="console.log('00:55:35,450'); seek(3335.0)">
              This was the
            </span>
            
            <span id="chunk-841" class="transcript-chunks" onclick="console.log('00:55:39,308'); seek(3339.0)">
              way we created the thread that
            </span>
            
            <span id="chunk-842" class="transcript-chunks" onclick="console.log('00:55:43,180'); seek(3343.0)">
              was dealing with this change. This is in Java,
            </span>
            
            <span id="chunk-843" class="transcript-chunks" onclick="console.log('00:55:46,642'); seek(3346.0)">
              but you don't necessarily have to understand Java to understand this use case.
            </span>
            
            <span id="chunk-844" class="transcript-chunks" onclick="console.log('00:55:50,348'); seek(3350.0)">
              We just created a separate
            </span>
            
            <span id="chunk-845" class="transcript-chunks" onclick="console.log('00:55:54,218'); seek(3354.0)">
              thread that had a separate queue that is
            </span>
            
            <span id="chunk-846" class="transcript-chunks" onclick="console.log('00:55:58,292'); seek(3358.0)">
              processing these requests when they are coming in and updating the statistics
            </span>
            
            <span id="chunk-847" class="transcript-chunks" onclick="console.log('00:56:02,026'); seek(3362.0)">
              table. Now where comes the problem?
            </span>
            
            <span id="chunk-848" class="transcript-chunks" onclick="console.log('00:56:05,510'); seek(3365.0)">
              What do you think? Where is the problem with
            </span>
            
            <span id="chunk-849" class="transcript-chunks" onclick="console.log('00:56:09,992'); seek(3369.0)">
              this implementation that's causing the memory increase and the killing
            </span>
            
            <span id="chunk-850" class="transcript-chunks" onclick="console.log('00:56:13,838'); seek(3373.0)">
              of the instances from time to time and restarting them?
            </span>
            
            <span id="chunk-851" class="transcript-chunks" onclick="console.log('00:56:18,410'); seek(3378.0)">
              Well, it's not obvious because calls the implementation details,
            </span>
            
            <span id="chunk-852" class="transcript-chunks" onclick="console.log('00:56:22,002'); seek(3382.0)">
              but this is the single place when there is a problem.
            </span>
            
            <span id="chunk-853" class="transcript-chunks" onclick="console.log('00:56:24,924'); seek(3384.0)">
              So very often Java
            </span>
            
            <span id="chunk-854" class="transcript-chunks" onclick="console.log('00:56:29,790'); seek(3389.0)">
              old school threaded implementations are coming with this unbounded queue,
            </span>
            
            <span id="chunk-855" class="transcript-chunks" onclick="console.log('00:56:34,510'); seek(3394.0)">
              meaning you have a limitless queue. And what happens when you have
            </span>
            
            <span id="chunk-856" class="transcript-chunks" onclick="console.log('00:56:38,512'); seek(3398.0)">
              an increased arrival rate and your departure rate
            </span>
            
            <span id="chunk-857" class="transcript-chunks" onclick="console.log('00:56:41,988'); seek(3401.0)">
              is much lower? You will have this unbounded queue filled up,
            </span>
            
            <span id="chunk-858" class="transcript-chunks" onclick="console.log('00:56:46,020'); seek(3406.0)">
              your latency will increase up to infinity, and your
            </span>
            
            <span id="chunk-859" class="transcript-chunks" onclick="console.log('00:56:49,828'); seek(3409.0)">
              memory consumption will also increase up
            </span>
            
            <span id="chunk-860" class="transcript-chunks" onclick="console.log('00:56:53,464'); seek(3413.0)">
              to infinity, up to where
            </span>
            
            <span id="chunk-861" class="transcript-chunks" onclick="console.log('00:56:58,328'); seek(3418.0)">
              you can hold this data, because you
            </span>
            
            <span id="chunk-862" class="transcript-chunks" onclick="console.log('00:57:02,120'); seek(3422.0)">
              have a very huge queue sitting there for no reason.
            </span>
            
            <span id="chunk-863" class="transcript-chunks" onclick="console.log('00:57:06,070'); seek(3426.0)">
              And actually this is not even true, because this is not an unbounded
            </span>
            
            <span id="chunk-864" class="transcript-chunks" onclick="console.log('00:57:09,858'); seek(3429.0)">
              queue, it's not erasing data automatically, it gets
            </span>
            
            <span id="chunk-865" class="transcript-chunks" onclick="console.log('00:57:13,116'); seek(3433.0)">
              full, it's just a very huge queue because it's
            </span>
            
            <span id="chunk-866" class="transcript-chunks" onclick="console.log('00:57:17,058'); seek(3437.0)">
              implemented by using the maximum value.
            </span>
            
            <span id="chunk-867" class="transcript-chunks" onclick="console.log('00:57:19,728'); seek(3439.0)">
              Java is declaring for integers,
            </span>
            
            <span id="chunk-868" class="transcript-chunks" onclick="console.log('00:57:23,550'); seek(3443.0)">
              right? So we can do better than that. So we
            </span>
            
            <span id="chunk-869" class="transcript-chunks" onclick="console.log('00:57:27,280'); seek(3447.0)">
              iterated with this implementation and we just simply
            </span>
            
            <span id="chunk-870" class="transcript-chunks" onclick="console.log('00:57:31,354'); seek(3451.0)">
              challenges to a more sophisticated solution. We started using
            </span>
            
            <span id="chunk-871" class="transcript-chunks" onclick="console.log('00:57:35,236'); seek(3455.0)">
              resiliency for J, which is a library
            </span>
            
            <span id="chunk-872" class="transcript-chunks" onclick="console.log('00:57:38,666'); seek(3458.0)">
              for implementing resiliency patterns in Java,
            </span>
            
            <span id="chunk-873" class="transcript-chunks" onclick="console.log('00:57:42,906'); seek(3462.0)">
              and we wrapped the call with a bulkhead which now
            </span>
            
            <span id="chunk-874" class="transcript-chunks" onclick="console.log('00:57:47,000'); seek(3467.0)">
              had a limited queue capacity, up to 25 items.
            </span>
            
            <span id="chunk-875" class="transcript-chunks" onclick="console.log('00:57:52,390'); seek(3472.0)">
              And when that queue was full, bulkhead was throwing
            </span>
            
            <span id="chunk-876" class="transcript-chunks" onclick="console.log('00:57:56,018'); seek(3476.0)">
              an exception. Now we could have exception because
            </span>
            
            <span id="chunk-877" class="transcript-chunks" onclick="console.log('00:57:59,756'); seek(3479.0)">
              of two cases. First of all, because DynamoDB is starting
            </span>
            
            <span id="chunk-878" class="transcript-chunks" onclick="console.log('00:58:03,532'); seek(3483.0)">
              throttling, it's just rejecting our request. Or the
            </span>
            
            <span id="chunk-879" class="transcript-chunks" onclick="console.log('00:58:06,988'); seek(3486.0)">
              bulkhead was full, so were wrapped this whole thing, this whole
            </span>
            
            <span id="chunk-880" class="transcript-chunks" onclick="console.log('00:58:10,592'); seek(3490.0)">
              calls into a circuit breaker, and the circuit breaker
            </span>
            
            <span id="chunk-881" class="transcript-chunks" onclick="console.log('00:58:14,694'); seek(3494.0)">
              just opened when it saw these two exceptions,
            </span>
            
            <span id="chunk-882" class="transcript-chunks" onclick="console.log('00:58:17,990'); seek(3497.0)">
              give the whole thing a pause and then started again updating
            </span>
            
            <span id="chunk-883" class="transcript-chunks" onclick="console.log('00:58:21,482'); seek(3501.0)">
              the statistics. And this
            </span>
            
            <span id="chunk-884" class="transcript-chunks" onclick="console.log('00:58:25,636'); seek(3505.0)">
              helped us recover from the situation from
            </span>
            
            <span id="chunk-885" class="transcript-chunks" onclick="console.log('00:58:30,212'); seek(3510.0)">
              before. We did not have these memory
            </span>
            
            <span id="chunk-886" class="transcript-chunks" onclick="console.log('00:58:33,514'); seek(3513.0)">
              issues, the clients did not really notice anything
            </span>
            
            <span id="chunk-887" class="transcript-chunks" onclick="console.log('00:58:37,048'); seek(3517.0)">
              at all. And we looked actually at the metrics of the circuit breaker,
            </span>
            
            <span id="chunk-888" class="transcript-chunks" onclick="console.log('00:58:41,598'); seek(3521.0)">
              looked at the statistics of when they were opened
            </span>
            
            <span id="chunk-889" class="transcript-chunks" onclick="console.log('00:58:46,482'); seek(3526.0)">
              up, and it was not a big number.
            </span>
            
            <span id="chunk-890" class="transcript-chunks" onclick="console.log('00:58:50,730'); seek(3530.0)">
              So statistics not really suffered because of that.
            </span>
            
            <span id="chunk-891" class="transcript-chunks" onclick="console.log('00:58:54,172'); seek(3534.0)">
              And normally behaving clients could just
            </span>
            
            <span id="chunk-892" class="transcript-chunks" onclick="console.log('00:58:57,676'); seek(3537.0)">
              keep up with their normal operation, maybe having a couple
            </span>
            
            <span id="chunk-893" class="transcript-chunks" onclick="console.log('00:59:01,376'); seek(3541.0)">
              of more cache misses than usual, but with
            </span>
            
            <span id="chunk-894" class="transcript-chunks" onclick="console.log('00:59:06,896'); seek(3546.0)">
              metrics, and with this solution and investigating metrics,
            </span>
            
            <span id="chunk-895" class="transcript-chunks" onclick="console.log('00:59:10,886'); seek(3550.0)">
              we thought we were fine, so we dropped the right copper units.
            </span>
            
            <span id="chunk-896" class="transcript-chunks" onclick="console.log('00:59:14,378'); seek(3554.0)">
              Finally, for a statistics table that did not have to match
            </span>
            
            <span id="chunk-897" class="transcript-chunks" onclick="console.log('00:59:18,610'); seek(3558.0)">
              with the read part of
            </span>
            
            <span id="chunk-898" class="transcript-chunks" onclick="console.log('00:59:23,028'); seek(3563.0)">
              the values table. Okay.
            </span>
            
            <span id="chunk-899" class="transcript-chunks" onclick="console.log('00:59:27,170'); seek(3567.0)">
              But we really wanted to reduce also the read capacity unit
            </span>
            
            <span id="chunk-900" class="transcript-chunks" onclick="console.log('00:59:30,766'); seek(3570.0)">
              for the reads. So again, let's go back to the baseline
            </span>
            
            <span id="chunk-901" class="transcript-chunks" onclick="console.log('00:59:34,526'); seek(3574.0)">
              and talk about what we have with a bit more detail. So we
            </span>
            
            <span id="chunk-902" class="transcript-chunks" onclick="console.log('00:59:37,884'); seek(3577.0)">
              have 15,000 items coming in, 15,000 requests
            </span>
            
            <span id="chunk-903" class="transcript-chunks" onclick="console.log('00:59:42,242'); seek(3582.0)">
              like almost instantly.
            </span>
            
            <span id="chunk-904" class="transcript-chunks" onclick="console.log('00:59:45,850'); seek(3585.0)">
              And then we had these services packed in
            </span>
            
            <span id="chunk-905" class="transcript-chunks" onclick="console.log('00:59:49,808'); seek(3589.0)">
              an auto scaling group that was cpu based.
            </span>
            
            <span id="chunk-906" class="transcript-chunks" onclick="console.log('00:59:53,310'); seek(3593.0)">
              And then Dynamodb was throttling and giving us back HTTP
            </span>
            
            <span id="chunk-907" class="transcript-chunks" onclick="console.log('00:59:57,942'); seek(3597.0)">
              400 errors on any case when we
            </span>
            
            <span id="chunk-908" class="transcript-chunks" onclick="console.log('01:00:01,476'); seek(3601.0)">
              breached our read capacity unit.
            </span>
            
            <span id="chunk-909" class="transcript-chunks" onclick="console.log('01:00:08,130'); seek(3608.0)">
              So what if we do retry? So what if we have this HTTP 400
            </span>
            
            <span id="chunk-910" class="transcript-chunks" onclick="console.log('01:00:11,780'); seek(3611.0)">
              errors? We just retry the request and hope now that the read
            </span>
            
            <span id="chunk-911" class="transcript-chunks" onclick="console.log('01:00:15,176'); seek(3615.0)">
              capacity unit was catching up,
            </span>
            
            <span id="chunk-912" class="transcript-chunks" onclick="console.log('01:00:19,190'); seek(3619.0)">
              unfortunately, this was not introducing any fairness
            </span>
            
            <span id="chunk-913" class="transcript-chunks" onclick="console.log('01:00:22,606'); seek(3622.0)">
              to this whole solution. So when this misbehaving
            </span>
            
            <span id="chunk-914" class="transcript-chunks" onclick="console.log('01:00:27,106'); seek(3627.0)">
              client came with this 15,000 keys,
            </span>
            
            <span id="chunk-915" class="transcript-chunks" onclick="console.log('01:00:31,530'); seek(3631.0)">
              it just choked the whole system with its own request.
            </span>
            
            <span id="chunk-916" class="transcript-chunks" onclick="console.log('01:00:35,394'); seek(3635.0)">
              Other clients had to wait until DynamoDB
            </span>
            
            <span id="chunk-917" class="transcript-chunks" onclick="console.log('01:00:39,942'); seek(3639.0)">
              was catching up to get their own answers. Okay,
            </span>
            
            <span id="chunk-918" class="transcript-chunks" onclick="console.log('01:00:43,616'); seek(3643.0)">
              so it was not introducing any fairness for this whole scenario.
            </span>
            
            <span id="chunk-919" class="transcript-chunks" onclick="console.log('01:00:47,542'); seek(3647.0)">
              We wanted to do better. We tried also built
            </span>
            
            <span id="chunk-920" class="transcript-chunks" onclick="console.log('01:00:51,876'); seek(3651.0)">
              in rate limiting, but we did not go into production because for obvious reasons,
            </span>
            
            <span id="chunk-921" class="transcript-chunks" onclick="console.log('01:00:56,298'); seek(3656.0)">
              it was not working very well. So you can introduce
            </span>
            
            <span id="chunk-922" class="transcript-chunks" onclick="console.log('01:00:59,482'); seek(3659.0)">
              a quite okay ish implementation with resilience
            </span>
            
            <span id="chunk-923" class="transcript-chunks" onclick="console.log('01:01:03,530'); seek(3663.0)">
              for j that also has rate limiting. It's quite precise.
            </span>
            
            <span id="chunk-924" class="transcript-chunks" onclick="console.log('01:01:07,510'); seek(3667.0)">
              So you can have 40 operations per second for
            </span>
            
            <span id="chunk-925" class="transcript-chunks" onclick="console.log('01:01:10,728'); seek(3670.0)">
              each instance. Now with two instances, you have in total of 80
            </span>
            
            <span id="chunk-926" class="transcript-chunks" onclick="console.log('01:01:14,462'); seek(3674.0)">
              operations per second. Now, when the auto scaling
            </span>
            
            <span id="chunk-927" class="transcript-chunks" onclick="console.log('01:01:18,206'); seek(3678.0)">
              kicks in immediately, you have another instance having
            </span>
            
            <span id="chunk-928" class="transcript-chunks" onclick="console.log('01:01:21,756'); seek(3681.0)">
              40 operations per second, which now also increasing
            </span>
            
            <span id="chunk-929" class="transcript-chunks" onclick="console.log('01:01:25,378'); seek(3685.0)">
              your rate limit and your capacity, which is not true, of course,
            </span>
            
            <span id="chunk-930" class="transcript-chunks" onclick="console.log('01:01:29,056'); seek(3689.0)">
              because the original capacity is determined
            </span>
            
            <span id="chunk-931" class="transcript-chunks" onclick="console.log('01:01:32,678'); seek(3692.0)">
              by the dynamoDb's current capacity and by its
            </span>
            
            <span id="chunk-932" class="transcript-chunks" onclick="console.log('01:01:36,272'); seek(3696.0)">
              auto scaling characteristics. Okay,
            </span>
            
            <span id="chunk-933" class="transcript-chunks" onclick="console.log('01:01:40,770'); seek(3700.0)">
              we tried many other things,
            </span>
            
            <span id="chunk-934" class="transcript-chunks" onclick="console.log('01:01:43,490'); seek(3703.0)">
              but one of these was this one obviously did not
            </span>
            
            <span id="chunk-935" class="transcript-chunks" onclick="console.log('01:01:47,092'); seek(3707.0)">
              work well. We could have put
            </span>
            
            <span id="chunk-936" class="transcript-chunks" onclick="console.log('01:01:50,980'); seek(3710.0)">
              rate limiting into something that's used centrally,
            </span>
            
            <span id="chunk-937" class="transcript-chunks" onclick="console.log('01:01:54,638'); seek(3714.0)">
              namely into the service mesh. So in this case,
            </span>
            
            <span id="chunk-938" class="transcript-chunks" onclick="console.log('01:01:57,688'); seek(3717.0)">
              istio was also offering rate limiting. It was using redis
            </span>
            
            <span id="chunk-939" class="transcript-chunks" onclick="console.log('01:02:02,622'); seek(3722.0)">
              to keep the states of each client
            </span>
            
            <span id="chunk-940" class="transcript-chunks" onclick="console.log('01:02:06,810'); seek(3726.0)">
              to control the rate limits. Problem was that it
            </span>
            
            <span id="chunk-941" class="transcript-chunks" onclick="console.log('01:02:10,700'); seek(3730.0)">
              introduced an API change. So instead of giving these requests a
            </span>
            
            <span id="chunk-942" class="transcript-chunks" onclick="console.log('01:02:14,412'); seek(3734.0)">
              post or maybe slowing down the clients, which are misbehaving
            </span>
            
            <span id="chunk-943" class="transcript-chunks" onclick="console.log('01:02:18,498'); seek(3738.0)">
              similarly to what back pressure does, it immediately
            </span>
            
            <span id="chunk-944" class="transcript-chunks" onclick="console.log('01:02:23,126'); seek(3743.0)">
              gave them another kind of HTTP response. And we thought
            </span>
            
            <span id="chunk-945" class="transcript-chunks" onclick="console.log('01:02:26,880'); seek(3746.0)">
              that something, that it caused more trouble
            </span>
            
            <span id="chunk-946" class="transcript-chunks" onclick="console.log('01:02:30,502'); seek(3750.0)">
              than solve solutions. So we
            </span>
            
            <span id="chunk-947" class="transcript-chunks" onclick="console.log('01:02:33,748'); seek(3753.0)">
              did not go on with this change. Instead of that, we went
            </span>
            
            <span id="chunk-948" class="transcript-chunks" onclick="console.log('01:02:37,412'); seek(3757.0)">
              back to queuing theory. So if we simplify this into
            </span>
            
            <span id="chunk-949" class="transcript-chunks" onclick="console.log('01:02:42,692'); seek(3762.0)">
              a simpler queue, this is what happens. So we have 15,000 items coming
            </span>
            
            <span id="chunk-950" class="transcript-chunks" onclick="console.log('01:02:47,080'); seek(3767.0)">
              in, you have couple of executors that are on the right side,
            </span>
            
            <span id="chunk-951" class="transcript-chunks" onclick="console.log('01:02:51,272'); seek(3771.0)">
              and the throughput, and to determine the throughput,
            </span>
            
            <span id="chunk-952" class="transcript-chunks" onclick="console.log('01:02:54,862'); seek(3774.0)">
              you need the latency. So the overall latency was
            </span>
            
            <span id="chunk-953" class="transcript-chunks" onclick="console.log('01:02:58,748'); seek(3778.0)">
              equal to the timeout configuration of the client,
            </span>
            
            <span id="chunk-954" class="transcript-chunks" onclick="console.log('01:03:02,274'); seek(3782.0)">
              which was not easy to figure out, or not difficult to figure out,
            </span>
            
            <span id="chunk-955" class="transcript-chunks" onclick="console.log('01:03:05,932'); seek(3785.0)">
              sorry. Because it was the default being used. I don't
            </span>
            
            <span id="chunk-956" class="transcript-chunks" onclick="console.log('01:03:09,248'); seek(3789.0)">
              know where this number is coming from, but for every library
            </span>
            
            <span id="chunk-957" class="transcript-chunks" onclick="console.log('01:03:12,838'); seek(3792.0)">
              it looks like timeout is 30 seconds. So we
            </span>
            
            <span id="chunk-958" class="transcript-chunks" onclick="console.log('01:03:16,912'); seek(3796.0)">
              have 30 seconds to consume and
            </span>
            
            <span id="chunk-959" class="transcript-chunks" onclick="console.log('01:03:20,372'); seek(3800.0)">
              problems these 15,000 items.
            </span>
            
            <span id="chunk-960" class="transcript-chunks" onclick="console.log('01:03:23,242'); seek(3803.0)">
              This gives us the overall throughput of 500 operations
            </span>
            
            <span id="chunk-961" class="transcript-chunks" onclick="console.log('01:03:26,506'); seek(3806.0)">
              per second, regardless of the number of executors.
            </span>
            
            <span id="chunk-962" class="transcript-chunks" onclick="console.log('01:03:30,122'); seek(3810.0)">
              So with two instances working on
            </span>
            
            <span id="chunk-963" class="transcript-chunks" onclick="console.log('01:03:33,492'); seek(3813.0)">
              that, we need to complete each operation in four milliseconds,
            </span>
            
            <span id="chunk-964" class="transcript-chunks" onclick="console.log('01:03:37,406'); seek(3817.0)">
              which seems to be nearly impossible. We do not really want to bother
            </span>
            
            <span id="chunk-965" class="transcript-chunks" onclick="console.log('01:03:41,278'); seek(3821.0)">
              with optimizing the whole thing, but if you scale this out
            </span>
            
            <span id="chunk-966" class="transcript-chunks" onclick="console.log('01:03:44,792'); seek(3824.0)">
              to five nodes, to five workers, it's a more
            </span>
            
            <span id="chunk-967" class="transcript-chunks" onclick="console.log('01:03:48,796'); seek(3828.0)">
              user friendly number. It's now ten milliseconds, which seem
            </span>
            
            <span id="chunk-968" class="transcript-chunks" onclick="console.log('01:03:52,738'); seek(3832.0)">
              to be doable. So we think that if we can
            </span>
            
            <span id="chunk-969" class="transcript-chunks" onclick="console.log('01:03:56,316'); seek(3836.0)">
              slow down these 15,000 requests
            </span>
            
            <span id="chunk-970" class="transcript-chunks" onclick="console.log('01:04:00,750'); seek(3840.0)">
              not to be processed immediately,
            </span>
            
            <span id="chunk-971" class="transcript-chunks" onclick="console.log('01:04:04,486'); seek(3844.0)">
              but instead of them to be processed within 30 seconds,
            </span>
            
            <span id="chunk-972" class="transcript-chunks" onclick="console.log('01:04:08,310'); seek(3848.0)">
              close to a number that's 30 seconds with five worker,
            </span>
            
            <span id="chunk-973" class="transcript-chunks" onclick="console.log('01:04:13,330'); seek(3853.0)">
              if we have the execution duration of ten milliseconds for
            </span>
            
            <span id="chunk-974" class="transcript-chunks" onclick="console.log('01:04:16,788'); seek(3856.0)">
              each worker, we can do it in a sensible
            </span>
            
            <span id="chunk-975" class="transcript-chunks" onclick="console.log('01:04:20,826'); seek(3860.0)">
              way. So we tried to
            </span>
            
            <span id="chunk-976" class="transcript-chunks" onclick="console.log('01:04:24,152'); seek(3864.0)">
              find a technology that allows us to do that.
            </span>
            
            <span id="chunk-977" class="transcript-chunks" onclick="console.log('01:04:28,070'); seek(3868.0)">
              We were looking towards RabbitMQ because a
            </span>
            
            <span id="chunk-978" class="transcript-chunks" onclick="console.log('01:04:32,088'); seek(3872.0)">
              couple of very interesting features. RabbitMQ has
            </span>
            
            <span id="chunk-979" class="transcript-chunks" onclick="console.log('01:04:35,548'); seek(3875.0)">
              this queue overflow behavior. So if you set this overflow
            </span>
            
            <span id="chunk-980" class="transcript-chunks" onclick="console.log('01:04:39,138'); seek(3879.0)">
              setting for a queue, then it will immediately reject
            </span>
            
            <span id="chunk-981" class="transcript-chunks" onclick="console.log('01:04:44,930'); seek(3884.0)">
              and not consume those requests that was put into the queue.
            </span>
            
            <span id="chunk-982" class="transcript-chunks" onclick="console.log('01:04:48,662'); seek(3888.0)">
              So I think it will wait on the client side until
            </span>
            
            <span id="chunk-983" class="transcript-chunks" onclick="console.log('01:04:53,470'); seek(3893.0)">
              the queue still has capacity and until the consumers are catching
            </span>
            
            <span id="chunk-984" class="transcript-chunks" onclick="console.log('01:04:57,446'); seek(3897.0)">
              up instead of failing them. It's failing after a
            </span>
            
            <span id="chunk-985" class="transcript-chunks" onclick="console.log('01:05:00,848'); seek(3900.0)">
              certain amount of time. It also have another rate limiting or
            </span>
            
            <span id="chunk-986" class="transcript-chunks" onclick="console.log('01:05:05,348'); seek(3905.0)">
              flow rate behavior, but it's bound to
            </span>
            
            <span id="chunk-987" class="transcript-chunks" onclick="console.log('01:05:08,708'); seek(3908.0)">
              the memory and cpusage to RabbitMQ that you can see on the
            </span>
            
            <span id="chunk-988" class="transcript-chunks" onclick="console.log('01:05:12,468'); seek(3912.0)">
              middle left on the screen. That's very hard to control. This is
            </span>
            
            <span id="chunk-989" class="transcript-chunks" onclick="console.log('01:05:16,612'); seek(3916.0)">
              not what we were looking for, but still was quite promising then.
            </span>
            
            <span id="chunk-990" class="transcript-chunks" onclick="console.log('01:05:21,412'); seek(3921.0)">
              What's important is that you can define your prefetch settings
            </span>
            
            <span id="chunk-991" class="transcript-chunks" onclick="console.log('01:05:25,130'); seek(3925.0)">
              by queues that you can see on the top right
            </span>
            
            <span id="chunk-992" class="transcript-chunks" onclick="console.log('01:05:28,316'); seek(3928.0)">
              corner. So you can have a single channel of
            </span>
            
            <span id="chunk-993" class="transcript-chunks" onclick="console.log('01:05:32,316'); seek(3932.0)">
              connection that's connecting through different consumers
            </span>
            
            <span id="chunk-994" class="transcript-chunks" onclick="console.log('01:05:36,034'); seek(3936.0)">
              to different queues, and have different settings for
            </span>
            
            <span id="chunk-995" class="transcript-chunks" onclick="console.log('01:05:39,488'); seek(3939.0)">
              each queue. So if you have one of
            </span>
            
            <span id="chunk-996" class="transcript-chunks" onclick="console.log('01:05:43,328'); seek(3943.0)">
              the clients coming into one queue, you can have
            </span>
            
            <span id="chunk-997" class="transcript-chunks" onclick="console.log('01:05:48,030'); seek(3948.0)">
              a specific configuration for that single queue.
            </span>
            
            <span id="chunk-998" class="transcript-chunks" onclick="console.log('01:05:51,882'); seek(3951.0)">
              Now for another queue, you can either have the same or different configuration
            </span>
            
            <span id="chunk-999" class="transcript-chunks" onclick="console.log('01:05:55,722'); seek(3955.0)">
              as well, and you have many options to acknowledge
            </span>
            
            <span id="chunk-1000" class="transcript-chunks" onclick="console.log('01:05:59,322'); seek(3959.0)">
              the request. So when DynamoDB starts throttling, so you just
            </span>
            
            <span id="chunk-1001" class="transcript-chunks" onclick="console.log('01:06:03,112'); seek(3963.0)">
              simply acknowledge or not negatively
            </span>
            
            <span id="chunk-1002" class="transcript-chunks" onclick="console.log('01:06:06,782'); seek(3966.0)">
              acknowledge or reject the
            </span>
            
            <span id="chunk-1003" class="transcript-chunks" onclick="console.log('01:06:11,032'); seek(3971.0)">
              message from the worker side and you can retry with
            </span>
            
            <span id="chunk-1004" class="transcript-chunks" onclick="console.log('01:06:14,472'); seek(3974.0)">
              the next interactions or with the next worker.
            </span>
            
            <span id="chunk-1005" class="transcript-chunks" onclick="console.log('01:06:17,930'); seek(3977.0)">
              So here was our setup. Basically we had a service or
            </span>
            
            <span id="chunk-1006" class="transcript-chunks" onclick="console.log('01:06:21,852'); seek(3981.0)">
              the array of services now having an auto scaling
            </span>
            
            <span id="chunk-1007" class="transcript-chunks" onclick="console.log('01:06:26,034'); seek(3986.0)">
              group. This was using so
            </span>
            
            <span id="chunk-1008" class="transcript-chunks" onclick="console.log('01:06:30,844'); seek(3990.0)">
              calls fake boundary. This is how I calls that. Maybe there's a
            </span>
            
            <span id="chunk-1009" class="transcript-chunks" onclick="console.log('01:06:34,304'); seek(3994.0)">
              better name. And instead of being synchronous, it was asynchronous,
            </span>
            
            <span id="chunk-1010" class="transcript-chunks" onclick="console.log('01:06:38,182'); seek(3998.0)">
              but it used request reply queues and we separated
            </span>
            
            <span id="chunk-1011" class="transcript-chunks" onclick="console.log('01:06:42,426'); seek(4002.0)">
              the request by each API key. So each client fortunately
            </span>
            
            <span id="chunk-1012" class="transcript-chunks" onclick="console.log('01:06:46,826'); seek(4006.0)">
              forwarded their own API key in the header and
            </span>
            
            <span id="chunk-1013" class="transcript-chunks" onclick="console.log('01:06:50,788'); seek(4010.0)">
              in each queue in each channel. We had
            </span>
            
            <span id="chunk-1014" class="transcript-chunks" onclick="console.log('01:06:54,072'); seek(4014.0)">
              the same configuration, the same configuration of overflow
            </span>
            
            <span id="chunk-1015" class="transcript-chunks" onclick="console.log('01:06:58,078'); seek(4018.0)">
              settings, the same configuration of prefetch rate, and each worker
            </span>
            
            <span id="chunk-1016" class="transcript-chunks" onclick="console.log('01:07:02,334'); seek(4022.0)">
              connected to all the queues at the same time
            </span>
            
            <span id="chunk-1017" class="transcript-chunks" onclick="console.log('01:07:06,232'); seek(4026.0)">
              and have their own auto scaling group.
            </span>
            
            <span id="chunk-1018" class="transcript-chunks" onclick="console.log('01:07:10,010'); seek(4030.0)">
              Creating a new queue if we saw a fresh API
            </span>
            
            <span id="chunk-1019" class="transcript-chunks" onclick="console.log('01:07:13,218'); seek(4033.0)">
              key was not a problem because the configuration was in service itself.
            </span>
            
            <span id="chunk-1020" class="transcript-chunks" onclick="console.log('01:07:17,548'); seek(4037.0)">
              So this had something like global configuration
            </span>
            
            <span id="chunk-1021" class="transcript-chunks" onclick="console.log('01:07:21,618'); seek(4041.0)">
              available and connecting to a new queue
            </span>
            
            <span id="chunk-1022" class="transcript-chunks" onclick="console.log('01:07:25,710'); seek(4045.0)">
              when we see a new queue from the worker side seemed to be a bit
            </span>
            
            <span id="chunk-1023" class="transcript-chunks" onclick="console.log('01:07:29,136'); seek(4049.0)">
              more complicated but doable, because RevitMQ is
            </span>
            
            <span id="chunk-1024" class="transcript-chunks" onclick="console.log('01:07:32,628'); seek(4052.0)">
              offering management API as well,
            </span>
            
            <span id="chunk-1025" class="transcript-chunks" onclick="console.log('01:07:35,604'); seek(4055.0)">
              which helps you discover if there's a fresh queue.
            </span>
            
            <span id="chunk-1026" class="transcript-chunks" onclick="console.log('01:07:38,570'); seek(4058.0)">
              And opening up a new connection seemed
            </span>
            
            <span id="chunk-1027" class="transcript-chunks" onclick="console.log('01:07:42,122'); seek(4062.0)">
              to be, and having a new unit
            </span>
            
            <span id="chunk-1028" class="transcript-chunks" onclick="console.log('01:07:46,350'); seek(4066.0)">
              which is consuming that connection seemed to be
            </span>
            
            <span id="chunk-1029" class="transcript-chunks" onclick="console.log('01:07:50,168'); seek(4070.0)">
              not a big deal. And then
            </span>
            
            <span id="chunk-1030" class="transcript-chunks" onclick="console.log('01:07:53,992'); seek(4073.0)">
              we could connect to Dynamodb and reject
            </span>
            
            <span id="chunk-1031" class="transcript-chunks" onclick="console.log('01:07:57,442'); seek(4077.0)">
              the request if there's throttling, but hope that this
            </span>
            
            <span id="chunk-1032" class="transcript-chunks" onclick="console.log('01:08:01,612'); seek(4081.0)">
              will have the effect what we desired for. Again, we need
            </span>
            
            <span id="chunk-1033" class="transcript-chunks" onclick="console.log('01:08:05,308'); seek(4085.0)">
              to slow down misbehaving clients. So if there's more orange marble coming
            </span>
            
            <span id="chunk-1034" class="transcript-chunks" onclick="console.log('01:08:09,536'); seek(4089.0)">
              in, we have to say after a while that orange marbles have to wait.
            </span>
            
            <span id="chunk-1035" class="transcript-chunks" onclick="console.log('01:08:13,120'); seek(4093.0)">
              Why? Processing the blue and the yellow marbles in their own pace,
            </span>
            
            <span id="chunk-1036" class="transcript-chunks" onclick="console.log('01:08:18,350'); seek(4098.0)">
              and this is the metrics that I've got.
            </span>
            
            <span id="chunk-1037" class="transcript-chunks" onclick="console.log('01:08:21,828'); seek(4101.0)">
              Unfortunately this is not from the real scenario.
            </span>
            
            <span id="chunk-1038" class="transcript-chunks" onclick="console.log('01:08:25,226'); seek(4105.0)">
              I had to rebuild it in a sandbox for certain
            </span>
            
            <span id="chunk-1039" class="transcript-chunks" onclick="console.log('01:08:28,964'); seek(4108.0)">
              reasons, but it's available under my GitHub profile,
            </span>
            
            <span id="chunk-1040" class="transcript-chunks" onclick="console.log('01:08:32,986'); seek(4112.0)">
              so you can try it by your own if you want to. So this
            </span>
            
            <span id="chunk-1041" class="transcript-chunks" onclick="console.log('01:08:36,232'); seek(4116.0)">
              was the baseline of direct reads. Now directly
            </span>
            
            <span id="chunk-1042" class="transcript-chunks" onclick="console.log('01:08:39,726'); seek(4119.0)">
              coming from the service of the database, you can see that we have
            </span>
            
            <span id="chunk-1043" class="transcript-chunks" onclick="console.log('01:08:42,888'); seek(4122.0)">
              the throughput of 300 operations per second, and we
            </span>
            
            <span id="chunk-1044" class="transcript-chunks" onclick="console.log('01:08:46,312'); seek(4126.0)">
              complete all the reads in 15 seconds.
            </span>
            
            <span id="chunk-1045" class="transcript-chunks" onclick="console.log('01:08:50,810'); seek(4130.0)">
              And the response time is around ten milliseconds.
            </span>
            
            <span id="chunk-1046" class="transcript-chunks" onclick="console.log('01:08:54,410'); seek(4134.0)">
              This is quite good because it's quite
            </span>
            
            <span id="chunk-1047" class="transcript-chunks" onclick="console.log('01:08:57,996'); seek(4137.0)">
              close to this scenario with five workers that you want
            </span>
            
            <span id="chunk-1048" class="transcript-chunks" onclick="console.log('01:09:01,328'); seek(4141.0)">
              to reach, and the response time distribution is
            </span>
            
            <span id="chunk-1049" class="transcript-chunks" onclick="console.log('01:09:04,752'); seek(4144.0)">
              very tight. So it's everything from going
            </span>
            
            <span id="chunk-1050" class="transcript-chunks" onclick="console.log('01:09:08,736'); seek(4148.0)">
              to zero to one eight.
            </span>
            
            <span id="chunk-1051" class="transcript-chunks" onclick="console.log('01:09:12,516'); seek(4152.0)">
              I don't know what the unit is, maybe this graph
            </span>
            
            <span id="chunk-1052" class="transcript-chunks" onclick="console.log('01:09:16,458'); seek(4156.0)">
              is messed up, sorry for that. But for the other diagrams
            </span>
            
            <span id="chunk-1053" class="transcript-chunks" onclick="console.log('01:09:20,362'); seek(4160.0)">
              you can see that these numbers change at least. So with single worker
            </span>
            
            <span id="chunk-1054" class="transcript-chunks" onclick="console.log('01:09:24,398'); seek(4164.0)">
              what we can see now is that rate limiting is working
            </span>
            
            <span id="chunk-1055" class="transcript-chunks" onclick="console.log('01:09:28,104'); seek(4168.0)">
              as expected or the back pressure is working as expected. So now
            </span>
            
            <span id="chunk-1056" class="transcript-chunks" onclick="console.log('01:09:31,432'); seek(4171.0)">
              instead of doing
            </span>
            
            <span id="chunk-1057" class="transcript-chunks" onclick="console.log('01:09:35,096'); seek(4175.0)">
              all these reads within 15 seconds we
            </span>
            
            <span id="chunk-1058" class="transcript-chunks" onclick="console.log('01:09:39,068'); seek(4179.0)">
              just give it a pause and we do it instead of that
            </span>
            
            <span id="chunk-1059" class="transcript-chunks" onclick="console.log('01:09:42,780'); seek(4182.0)">
              in I think two minutes,
            </span>
            
            <span id="chunk-1060" class="transcript-chunks" onclick="console.log('01:09:45,884'); seek(4185.0)">
              yeah, in around two minutes. So it's not
            </span>
            
            <span id="chunk-1061" class="transcript-chunks" onclick="console.log('01:09:49,632'); seek(4189.0)">
              reaching that 30 seconds goal what we aimed for, but it's
            </span>
            
            <span id="chunk-1062" class="transcript-chunks" onclick="console.log('01:09:54,038'); seek(4194.0)">
              giving the request a pause when we have more process
            </span>
            
            <span id="chunk-1063" class="transcript-chunks" onclick="console.log('01:09:59,024'); seek(4199.0)">
              than what we want to. The reasons time average is a bit
            </span>
            
            <span id="chunk-1064" class="transcript-chunks" onclick="console.log('01:10:02,852'); seek(4202.0)">
              higher than expected,
            </span>
            
            <span id="chunk-1065" class="transcript-chunks" onclick="console.log('01:10:06,850'); seek(4206.0)">
              but at least
            </span>
            
            <span id="chunk-1066" class="transcript-chunks" onclick="console.log('01:10:10,372'); seek(4210.0)">
              we have, this is just a single worker, so at least
            </span>
            
            <span id="chunk-1067" class="transcript-chunks" onclick="console.log('01:10:13,636'); seek(4213.0)">
              we have this set up as we
            </span>
            
            <span id="chunk-1068" class="transcript-chunks" onclick="console.log('01:10:17,448'); seek(4217.0)">
              want to. And you can see this from the response time distribution.
            </span>
            
            <span id="chunk-1069" class="transcript-chunks" onclick="console.log('01:10:20,798'); seek(4220.0)">
              So the response time distribution now is going to up
            </span>
            
            <span id="chunk-1070" class="transcript-chunks" onclick="console.log('01:10:24,152'); seek(4224.0)">
              to 14 units. So it's better than
            </span>
            
            <span id="chunk-1071" class="transcript-chunks" onclick="console.log('01:10:27,548'); seek(4227.0)">
              it was before. And now with five workers we tried it out also with
            </span>
            
            <span id="chunk-1072" class="transcript-chunks" onclick="console.log('01:10:31,132'); seek(4231.0)">
              five workers we saw that with five workers
            </span>
            
            <span id="chunk-1073" class="transcript-chunks" onclick="console.log('01:10:35,850'); seek(4235.0)">
              we succeeded to reach our goals.
            </span>
            
            <span id="chunk-1074" class="transcript-chunks" onclick="console.log('01:10:39,126'); seek(4239.0)">
              So this exactly takes 30
            </span>
            
            <span id="chunk-1075" class="transcript-chunks" onclick="console.log('01:10:43,248'); seek(4243.0)">
              seconds as expected. And interestingly in
            </span>
            
            <span id="chunk-1076" class="transcript-chunks" onclick="console.log('01:10:47,184'); seek(4247.0)">
              one point we measured even a higher throughput. So the original
            </span>
            
            <span id="chunk-1077" class="transcript-chunks" onclick="console.log('01:10:50,682'); seek(4250.0)">
              throughput was very close to the
            </span>
            
            <span id="chunk-1078" class="transcript-chunks" onclick="console.log('01:10:54,260'); seek(4254.0)">
              baseline and in one case it was a
            </span>
            
            <span id="chunk-1079" class="transcript-chunks" onclick="console.log('01:10:57,364'); seek(4257.0)">
              bit even higher, even 500 operations per second.
            </span>
            
            <span id="chunk-1080" class="transcript-chunks" onclick="console.log('01:11:01,140'); seek(4261.0)">
              It also shows that throughput has nothing to
            </span>
            
            <span id="chunk-1081" class="transcript-chunks" onclick="console.log('01:11:04,968'); seek(4264.0)">
              do really with latency. It has some relations
            </span>
            
            <span id="chunk-1082" class="transcript-chunks" onclick="console.log('01:11:08,344'); seek(4268.0)">
              with latency, but we are able to meet even higher throughput
            </span>
            
            <span id="chunk-1083" class="transcript-chunks" onclick="console.log('01:11:12,318'); seek(4272.0)">
              even if we have higher latency for some of the clients,
            </span>
            
            <span id="chunk-1084" class="transcript-chunks" onclick="console.log('01:11:15,838'); seek(4275.0)">
              for some of the channels and the worker execution time and response
            </span>
            
            <span id="chunk-1085" class="transcript-chunks" onclick="console.log('01:11:19,858'); seek(4279.0)">
              time. Now, especially if you see the worker execution time,
            </span>
            
            <span id="chunk-1086" class="transcript-chunks" onclick="console.log('01:11:23,436'); seek(4283.0)">
              you can see that now it's getting short to getting to
            </span>
            
            <span id="chunk-1087" class="transcript-chunks" onclick="console.log('01:11:27,980'); seek(4287.0)">
              ten milliseconds at the end where we want it to be.
            </span>
            
            <span id="chunk-1088" class="transcript-chunks" onclick="console.log('01:11:32,510'); seek(4292.0)">
              And the response time distribution is again a bit
            </span>
            
            <span id="chunk-1089" class="transcript-chunks" onclick="console.log('01:11:35,968'); seek(4295.0)">
              better distributed than before. So this was
            </span>
            
            <span id="chunk-1090" class="transcript-chunks" onclick="console.log('01:11:39,312'); seek(4299.0)">
              quite promising for us. So that's all I wanted
            </span>
            
            <span id="chunk-1091" class="transcript-chunks" onclick="console.log('01:11:43,236'); seek(4303.0)">
              to say and present to you. Thank you so much
            </span>
            
            <span id="chunk-1092" class="transcript-chunks" onclick="console.log('01:11:46,676'); seek(4306.0)">
              for listening again. I was orthes Margaret and
            </span>
            
            <span id="chunk-1093" class="transcript-chunks" onclick="console.log('01:11:50,372'); seek(4310.0)">
              I work as associate chief software engineer at EPAm.
            </span>
            
            <span id="chunk-1094" class="transcript-chunks" onclick="console.log('01:11:54,870'); seek(4314.0)">
              If you have questions related to the things I talked about,
            </span>
            
            <span id="chunk-1095" class="transcript-chunks" onclick="console.log('01:11:58,360'); seek(4318.0)">
              just feel free to reach out to me by using either Twitter
            </span>
            
            <span id="chunk-1096" class="transcript-chunks" onclick="console.log('01:12:02,542'); seek(4322.0)">
              or LinkedIn or feel free to visit my GitHub
            </span>
            
            <span id="chunk-1097" class="transcript-chunks" onclick="console.log('01:12:06,162'); seek(4326.0)">
              profile. But I have an availability simulator
            </span>
            
            <span id="chunk-1098" class="transcript-chunks" onclick="console.log('01:12:10,034'); seek(4330.0)">
              that helps you to get these availability numbers that I was talking about
            </span>
            
            <span id="chunk-1099" class="transcript-chunks" onclick="console.log('01:12:14,076'); seek(4334.0)">
              and have this example sandbox of back pressure
            </span>
            
            <span id="chunk-1100" class="transcript-chunks" onclick="console.log('01:12:17,618'); seek(4337.0)">
              and ray limiting I was presenting to you and
            </span>
            
            <span id="chunk-1101" class="transcript-chunks" onclick="console.log('01:12:22,028'); seek(4342.0)">
              I get plenty of plenty of references. If you are interested in more just
            </span>
            
            <span id="chunk-1102" class="transcript-chunks" onclick="console.log('01:12:25,452'); seek(4345.0)">
              feel free to look at the end of the slides and discover
            </span>
            
            <span id="chunk-1103" class="transcript-chunks" onclick="console.log('01:12:29,442'); seek(4349.0)">
              a couple of the things I talked about even in more detail. Thank you
            </span>
            
            <span id="chunk-1104" class="transcript-chunks" onclick="console.log('01:12:33,284'); seek(4353.0)">
              very much again for listening. I hope you had a great time and
            </span>
            
            <span id="chunk-1105" class="transcript-chunks" onclick="console.log('01:12:36,708'); seek(4356.0)">
              learned something new.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Conf42%20Platform%20Engineering%202023%20-%20Oresztesz%20Margaritisz.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Conf42%20Platform%20Engineering%202023%20-%20Oresztesz%20Margaritisz.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #6F9471;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/platform2023" class="btn btn-sm btn-danger shadow lift" style="background-color: #6F9471;">
                <i class="fe fe-grid me-2"></i>
                See all 12 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Oresztesz%20Margaritisz_platform.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Oresztesz Margaritisz
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Chief Software Engineer @ EPAM
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/oresztesz/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Oresztesz Margaritisz's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/@gitaroktato" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Oresztesz Margaritisz's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @gitaroktato"
                  data-url="https://www.conf42.com/platform2023"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/platform2023"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Platform Engineering"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>