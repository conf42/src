<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: From DevOps to MLOps: Scaling Machine Learning Models to 2 Million+ Requests per Day</title>
    <meta name="description" content="Defend your systems against intergalactic invaders!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Chinmay%20Naik_sre.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="From DevOps to MLOps: Scaling Machine Learning Models to 2 Million+ Requests per Day | Conf42"/>
    <meta property="og:description" content="Learn how to deploy and scale Machine Learning models to 2 Million+ requests/day using MLOps best practices. In this talk, you'll learn how to go from data preparation to deployment to scaling ML models that can run at large scale - all without breaking the bank."/>
    <meta property="og:url" content="https://conf42.com/Site_Reliability_Engineering_SRE_2024_Chinmay_Naik_devops_mlops_scaling"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/DEVSECOPS2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        DevSecOps 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-12-05
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/devsecops2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #E36414;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Site Reliability Engineering (SRE) 2024 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2024-05-09">May 09 2024</time>
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Defend your systems against intergalactic invaders!
 -->
              <script>
                const event_date = new Date("2024-05-09T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-05-09T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "Au73jDVCoNg"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "IkrDqLkrVF4"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBTHIrhimpYy2ysdU8Fr3Zk" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Today I am going to talk about, from DevOps to Mlops,", "timestamp": "00:00:27,040", "timestamp_s": 27.0}, {"text": "a journey of scaling machine learning models to 2 million", "timestamp": "00:00:31,236", "timestamp_s": 31.0}, {"text": "API requests per day. So before we dive in", "timestamp": "00:00:34,924", "timestamp_s": 34.0}, {"text": "a brief about me, I am Chinmay. You can find me on", "timestamp": "00:00:38,692", "timestamp_s": 38.0}, {"text": "Twitter LinkedIn etcetera via Chinmay 185.", "timestamp": "00:00:42,492", "timestamp_s": 42.0}, {"text": "I am a founder at company called 120 n where we help startup", "timestamp": "00:00:46,364", "timestamp_s": 46.0}, {"text": "and enterprises with backend and site reliability engineering.", "timestamp": "00:00:49,908", "timestamp_s": 49.0}, {"text": "I write stories of our work in", "timestamp": "00:00:54,244", "timestamp_s": 54.0}, {"text": "what is called pragmatic software engineering. These stories, I published", "timestamp": "00:00:58,124", "timestamp_s": 58.0}, {"text": "them on Twitter LinkedIn, etcetera. I love engineering,", "timestamp": "00:01:01,604", "timestamp_s": 61.0}, {"text": "psychology, percussion, and I am a huge fan of", "timestamp": "00:01:04,996", "timestamp_s": 64.0}, {"text": "a game called Age of Empires. All right, so let\u0027s start.", "timestamp": "00:01:09,068", "timestamp_s": 69.0}, {"text": "So what\u0027s, what are we covering today? We are covering three", "timestamp": "00:01:12,860", "timestamp_s": 72.0}, {"text": "things fundamentally. One is what is mlops?", "timestamp": "00:01:16,604", "timestamp_s": 76.0}, {"text": "How do you think mlops for DevOps practitioners?", "timestamp": "00:01:20,324", "timestamp_s": 80.0}, {"text": "Fundamentally, I want to talk more about, and spend more time in talking about", "timestamp": "00:01:24,114", "timestamp_s": 84.0}, {"text": "a real world production case study that we worked on, which will", "timestamp": "00:01:28,090", "timestamp_s": 88.0}, {"text": "talk about all the learnings that we had into", "timestamp": "00:01:31,914", "timestamp_s": 91.0}, {"text": "in a case study kind of walkthrough. So what", "timestamp": "00:01:36,034", "timestamp_s": 96.0}, {"text": "is mlops fundamentally? Mlops is operationalizing", "timestamp": "00:01:39,170", "timestamp_s": 99.0}, {"text": "data science. We all know what DevOps is. DevOps is operationalizing", "timestamp": "00:01:43,274", "timestamp_s": 103.0}, {"text": "software delivery, software engineering.", "timestamp": "00:01:47,978", "timestamp_s": 107.0}, {"text": "Similarly, MlOps is equivalent to DevOps", "timestamp": "00:01:50,566", "timestamp_s": 110.0}, {"text": "in a sense. It talks about operationalizing data science", "timestamp": "00:01:54,814", "timestamp_s": 114.0}, {"text": "workloads. Think of machine learning AI ML", "timestamp": "00:01:58,350", "timestamp_s": 118.0}, {"text": "workloads essentially, right? So that means it is all", "timestamp": "00:02:02,022", "timestamp_s": 122.0}, {"text": "about moving machine learning workloads to production.", "timestamp": "00:02:05,982", "timestamp_s": 125.0}, {"text": "Just like we have DevOps phases, we have various phases in mlops.", "timestamp": "00:02:10,454", "timestamp_s": 130.0}, {"text": "Fundamentally, it\u0027s like build, where you build the models,", "timestamp": "00:02:14,470", "timestamp_s": 134.0}, {"text": "you manage various versions of the models. For example,", "timestamp": "00:02:17,954", "timestamp_s": 137.0}, {"text": "you deploy these models on production. You monitor, you take feedback,", "timestamp": "00:02:21,442", "timestamp_s": 141.0}, {"text": "you continuously improve the models, etcetera. So these are various four steps", "timestamp": "00:02:25,506", "timestamp_s": 145.0}, {"text": "of mlops. Let\u0027s look at some of them in more", "timestamp": "00:02:29,018", "timestamp_s": 149.0}, {"text": "detail. Right? So fundamentally, just like", "timestamp": "00:02:32,482", "timestamp_s": 152.0}, {"text": "software engineering is about building and shipping code to production,", "timestamp": "00:02:35,882", "timestamp_s": 155.0}, {"text": "MLOps is, is about building machine learning models.", "timestamp": "00:02:39,594", "timestamp_s": 159.0}, {"text": "Now, what do you need for building machine learning models? You need data.", "timestamp": "00:02:43,288", "timestamp_s": 163.0}, {"text": "Data. You need to extract this data in various forms.", "timestamp": "00:02:47,544", "timestamp_s": 167.0}, {"text": "You will need to analyze it. You will need to sort of prune some parts", "timestamp": "00:02:51,192", "timestamp_s": 171.0}, {"text": "of data. Essentially, you are doing data preparation and gathering.", "timestamp": "00:02:54,168", "timestamp_s": 174.0}, {"text": "Then you feed in this data to your ML model. You will", "timestamp": "00:02:57,704", "timestamp_s": 177.0}, {"text": "train the model. You will evaluate models response, you will", "timestamp": "00:03:01,360", "timestamp_s": 181.0}, {"text": "test and validate whether the model works correctly or not.", "timestamp": "00:03:04,784", "timestamp_s": 184.0}, {"text": "You will fine tune this process over time. You know, you have", "timestamp": "00:03:08,032", "timestamp_s": 188.0}, {"text": "test data segregation, you will have production data, stuff like that.", "timestamp": "00:03:11,344", "timestamp_s": 191.0}, {"text": "This is all the machine learning part of it, which is what data scientists", "timestamp": "00:03:15,784", "timestamp_s": 195.0}, {"text": "work on. Now, the operational parts of it are model", "timestamp": "00:03:19,344", "timestamp_s": 199.0}, {"text": "serving, how do you serve this model to production? How do", "timestamp": "00:03:23,072", "timestamp_s": 203.0}, {"text": "you run this on GPU\u0027s? Do you run this on cpu\u0027s? Which cloud provider do", "timestamp": "00:03:26,168", "timestamp_s": 206.0}, {"text": "you want to use? How do you monitor the model, whether it\u0027s performing as", "timestamp": "00:03:30,016", "timestamp_s": 210.0}, {"text": "per expectations or not? How do you manage scale up and scale down of that", "timestamp": "00:03:33,832", "timestamp_s": 213.0}, {"text": "model? All of that is the operational concern, which is the ops part of it.", "timestamp": "00:03:37,172", "timestamp_s": 217.0}, {"text": "Fundamentally, this is around a feedback loop,", "timestamp": "00:03:41,884", "timestamp_s": 221.0}, {"text": "just like in software. We have Ci CD continuous integration and", "timestamp": "00:03:44,836", "timestamp_s": 224.0}, {"text": "continuous delivery deployment. We have a third parameter,", "timestamp": "00:03:48,476", "timestamp_s": 228.0}, {"text": "or third item in mlops called continuous", "timestamp": "00:03:51,660", "timestamp_s": 231.0}, {"text": "testing and training, where you are going to continuously", "timestamp": "00:03:54,996", "timestamp_s": 234.0}, {"text": "monitor and train the model and improve", "timestamp": "00:03:58,820", "timestamp_s": 238.0}, {"text": "the model over the period of time. So here is", "timestamp": "00:04:02,818", "timestamp_s": 242.0}, {"text": "what simplest mlops workflow looks", "timestamp": "00:04:06,770", "timestamp_s": 246.0}, {"text": "like. This diagram is from sort of", "timestamp": "00:04:09,810", "timestamp_s": 249.0}, {"text": "Google\u0027s mlops guide. You can find the link in the description.", "timestamp": "00:04:12,850", "timestamp_s": 252.0}, {"text": "Fundamentally, again, it starts with getting data. So we are trying to map all these", "timestamp": "00:04:16,874", "timestamp_s": 256.0}, {"text": "previous steps and phases that we looked at into this model.", "timestamp": "00:04:20,954", "timestamp_s": 260.0}, {"text": "So we\u0027re going to get some data from various sources.", "timestamp": "00:04:24,490", "timestamp_s": 264.0}, {"text": "It could be offline data, it could be real time data, things like that.", "timestamp": "00:04:28,106", "timestamp_s": 268.0}, {"text": "For now, we\u0027re keeping the diagram very simple and just looking at some", "timestamp": "00:04:31,428", "timestamp_s": 271.0}, {"text": "offline data. For example, we are going to extract this data,", "timestamp": "00:04:35,340", "timestamp_s": 275.0}, {"text": "analyze it, prepare that data essentially.", "timestamp": "00:04:38,724", "timestamp_s": 278.0}, {"text": "And there\u0027s a second step. Then the whole model", "timestamp": "00:04:41,604", "timestamp_s": 281.0}, {"text": "training step appears where you are going to train the model, you\u0027re going to evaluate", "timestamp": "00:04:45,396", "timestamp_s": 285.0}, {"text": "the model, you\u0027re going to check the performance, you\u0027re going to manage various", "timestamp": "00:04:48,972", "timestamp_s": 288.0}, {"text": "versions, validations, etcetera. Finally, you have a train", "timestamp": "00:04:52,276", "timestamp_s": 292.0}, {"text": "model which you put into model registry.", "timestamp": "00:04:55,494", "timestamp_s": 295.0}, {"text": "Now once that model registry has", "timestamp": "00:04:58,598", "timestamp_s": 298.0}, {"text": "the model, the operational part of the mlops comes", "timestamp": "00:05:01,702", "timestamp_s": 301.0}, {"text": "into picture, which is serving the model, and then which is", "timestamp": "00:05:05,102", "timestamp_s": 305.0}, {"text": "where you have, for example, a prediction service which you can", "timestamp": "00:05:08,358", "timestamp_s": 308.0}, {"text": "run on production. Then you have to monitor, scale that service,", "timestamp": "00:05:12,062", "timestamp_s": 312.0}, {"text": "run this on GPU\u0027s, figure out cloud cost optimizations,", "timestamp": "00:05:15,774", "timestamp_s": 315.0}, {"text": "etcetera, around all of that. So that\u0027s operationalizing data", "timestamp": "00:05:19,062", "timestamp_s": 319.0}, {"text": "science. That\u0027s the simplest mlops", "timestamp": "00:05:22,542", "timestamp_s": 322.0}, {"text": "flow that you can think of. You can also map this into", "timestamp": "00:05:26,526", "timestamp_s": 326.0}, {"text": "a classic DevOps Infinity loop. So the typical", "timestamp": "00:05:30,446", "timestamp_s": 330.0}, {"text": "DevOps Infinity loop talks about your code, build, test,", "timestamp": "00:05:34,134", "timestamp_s": 334.0}, {"text": "plan, then release, deploy, operate, monitor,", "timestamp": "00:05:37,646", "timestamp_s": 337.0}, {"text": "and doing this in a loop consistently over long periods of time.", "timestamp": "00:05:40,590", "timestamp_s": 340.0}, {"text": "So similarly for mlops, it kind of starts with", "timestamp": "00:05:44,662", "timestamp_s": 344.0}, {"text": "having data preparation. You\u0027re going to prepare the data,", "timestamp": "00:05:48,950", "timestamp_s": 348.0}, {"text": "you\u0027re going to train the model. Again, that\u0027s a build and a test part of", "timestamp": "00:05:52,428", "timestamp_s": 352.0}, {"text": "it. Then the release will go into model registry.", "timestamp": "00:05:55,260", "timestamp_s": 355.0}, {"text": "You\u0027re going to then monitor the model performance.", "timestamp": "00:05:59,524", "timestamp_s": 359.0}, {"text": "You\u0027re going to deploy the model, monitor the performance,", "timestamp": "00:06:02,724", "timestamp_s": 362.0}, {"text": "and all of this altogether would be continuous training", "timestamp": "00:06:05,404", "timestamp_s": 365.0}, {"text": "and testing of the model. Right? Enough about theory.", "timestamp": "00:06:09,436", "timestamp_s": 369.0}, {"text": "What I want to talk more about is a", "timestamp": "00:06:13,212", "timestamp_s": 373.0}, {"text": "use case that we worked on. So this is a production work that we worked", "timestamp": "00:06:17,352", "timestamp_s": 377.0}, {"text": "on. I\u0027m going to cover the use case at", "timestamp": "00:06:20,640", "timestamp_s": 380.0}, {"text": "a high level. I\u0027m going to talk about what work we did,", "timestamp": "00:06:23,728", "timestamp_s": 383.0}, {"text": "how we applied the DevOps and mlops practices,", "timestamp": "00:06:27,120", "timestamp_s": 387.0}, {"text": "best practices in production, and the kind of issues", "timestamp": "00:06:30,272", "timestamp_s": 390.0}, {"text": "that we faced during the production journey.", "timestamp": "00:06:33,976", "timestamp_s": 393.0}, {"text": "Right. So let\u0027s start with the case study that we", "timestamp": "00:06:37,048", "timestamp_s": 397.0}, {"text": "had in mind. We were working on.", "timestamp": "00:06:40,408", "timestamp_s": 400.0}, {"text": "We were working with a company which was building Ekyc SaaS", "timestamp": "00:06:44,484", "timestamp_s": 404.0}, {"text": "APIs, which was accessible to B two", "timestamp": "00:06:48,244", "timestamp_s": 408.0}, {"text": "B and B two C customers. This needed to scale", "timestamp": "00:06:52,020", "timestamp_s": 412.0}, {"text": "up to 2 million API requests per day to the model. Now,", "timestamp": "00:06:55,436", "timestamp_s": 415.0}, {"text": "the SaaS APIs, these three or four APIs", "timestamp": "00:06:59,428", "timestamp_s": 419.0}, {"text": "that we had, one was face matching API. Imagine you", "timestamp": "00:07:02,884", "timestamp_s": 422.0}, {"text": "provide two images to the model. You\u0027re going to have to match", "timestamp": "00:07:06,884", "timestamp_s": 426.0}, {"text": "the face between two images and model outputs", "timestamp": "00:07:11,274", "timestamp_s": 431.0}, {"text": "a score between say zero and one. Based on the", "timestamp": "00:07:14,970", "timestamp_s": 434.0}, {"text": "matching score, you can decide if the two images, if the two people", "timestamp": "00:07:18,402", "timestamp_s": 438.0}, {"text": "in this images are same or not, and that\u0027s a face matching API,", "timestamp": "00:07:21,666", "timestamp_s": 441.0}, {"text": "then we have face liveness detection, which is if you", "timestamp": "00:07:26,018", "timestamp_s": 446.0}, {"text": "have an image of a face, is this face of a", "timestamp": "00:07:29,474", "timestamp_s": 449.0}, {"text": "live person, or is this a face of a non alive person?", "timestamp": "00:07:33,050", "timestamp_s": 453.0}, {"text": "Then we had an OCR or optical character recognition", "timestamp": "00:07:37,094", "timestamp_s": 457.0}, {"text": "from an image. For example, you upload a photo of a passport or any", "timestamp": "00:07:40,414", "timestamp_s": 460.0}, {"text": "identity card, you would be able to extract the text", "timestamp": "00:07:44,630", "timestamp_s": 464.0}, {"text": "information from it. So that imagine this Kyc", "timestamp": "00:07:48,430", "timestamp_s": 468.0}, {"text": "use case for an insurance or a telecom or any other domain.", "timestamp": "00:07:52,254", "timestamp_s": 472.0}, {"text": "People would have to manually enter a lot of information for the user, like their", "timestamp": "00:07:55,830", "timestamp_s": 475.0}, {"text": "name, their date of birth, their address, etcetera.", "timestamp": "00:07:59,806", "timestamp_s": 479.0}, {"text": "All of this information gets captured via the OCR", "timestamp": "00:08:02,854", "timestamp_s": 482.0}, {"text": "API and you get that information returned via response in a structured fashion.", "timestamp": "00:08:07,032", "timestamp_s": 487.0}, {"text": "So that eliminates having to type and mistype information.", "timestamp": "00:08:11,328", "timestamp_s": 491.0}, {"text": "So similarly, we had some other small APIs as well, which I\u0027ll ignore for now.", "timestamp": "00:08:16,344", "timestamp_s": 496.0}, {"text": "So fundamentally, we had this ML system,", "timestamp": "00:08:20,272", "timestamp_s": 500.0}, {"text": "and the architecture of that system, along with other components was something", "timestamp": "00:08:24,200", "timestamp_s": 504.0}, {"text": "like this. So it was served mainly for", "timestamp": "00:08:27,616", "timestamp_s": 507.0}, {"text": "b two b use cases. We also had some things for b, two c,", "timestamp": "00:08:31,172", "timestamp_s": 511.0}, {"text": "but again, we\u0027ll ignore that for now. So imagine from a b two b use", "timestamp": "00:08:34,164", "timestamp_s": 514.0}, {"text": "case. I\u0027m an insurance or a telecom company.", "timestamp": "00:08:37,412", "timestamp_s": 517.0}, {"text": "I have my own app, and there is a client SDK in that app", "timestamp": "00:08:40,820", "timestamp_s": 520.0}, {"text": "that I need to install and run. I need to package the client SDK", "timestamp": "00:08:44,604", "timestamp_s": 524.0}, {"text": "as part of my app, now this SDK talks to our backend", "timestamp": "00:08:48,284", "timestamp_s": 528.0}, {"text": "APIs, which are these ML APIs exposed via an", "timestamp": "00:08:51,804", "timestamp_s": 531.0}, {"text": "HTTP API, for example. So it connects to for example load balancer.", "timestamp": "00:08:56,028", "timestamp_s": 536.0}, {"text": "We have an API layer then to", "timestamp": "00:09:00,356", "timestamp_s": 540.0}, {"text": "be able to serve these models. We had a RabbitMQ as another like a", "timestamp": "00:09:04,340", "timestamp_s": 544.0}, {"text": "queue mechanism where we would push messages.", "timestamp": "00:09:08,148", "timestamp_s": 548.0}, {"text": "For example, we want to map match two images,", "timestamp": "00:09:12,012", "timestamp_s": 552.0}, {"text": "right? Face recognition or face matching across two images.", "timestamp": "00:09:15,660", "timestamp_s": 555.0}, {"text": "We would create a message in RabbitMQ, push that message on the", "timestamp": "00:09:18,988", "timestamp_s": 558.0}, {"text": "rabbitmQ. And there is these background workers, which are these ML", "timestamp": "00:09:22,756", "timestamp_s": 562.0}, {"text": "workers, which would run, which would accept message from the RabbitMQ.", "timestamp": "00:09:26,268", "timestamp_s": 566.0}, {"text": "They would do the processing, they would update the results in database.", "timestamp": "00:09:30,356", "timestamp_s": 570.0}, {"text": "Maybe they would even save the results in a cache", "timestamp": "00:09:33,812", "timestamp_s": 573.0}, {"text": "like redis, which we had. And the images themselves", "timestamp": "00:09:36,972", "timestamp_s": 576.0}, {"text": "can be stored in a distributed file store. Could be minio,", "timestamp": "00:09:40,524", "timestamp_s": 580.0}, {"text": "could be s, three things like that.", "timestamp": "00:09:44,108", "timestamp_s": 584.0}, {"text": "Essentially these workers would perform the bulk of the task and", "timestamp": "00:09:47,504", "timestamp_s": 587.0}, {"text": "then the API would return the results to the user.", "timestamp": "00:09:50,632", "timestamp_s": 590.0}, {"text": "So this is the kind of architecture we had. Now what were", "timestamp": "00:09:53,864", "timestamp_s": 593.0}, {"text": "the requirements from a slo point of view?", "timestamp": "00:09:57,432", "timestamp_s": 597.0}, {"text": "So we set out for achieving at least", "timestamp": "00:10:00,400", "timestamp_s": 600.0}, {"text": "like two nines of availability, that is, two nines of uptime", "timestamp": "00:10:03,600", "timestamp_s": 603.0}, {"text": "during peak hours or during our business hours. Because we were", "timestamp": "00:10:07,264", "timestamp_s": 607.0}, {"text": "dealing with a b two b company, there typically would be", "timestamp": "00:10:10,520", "timestamp_s": 610.0}, {"text": "business hours. Typically stores would open at 09:00 a.m. In the morning and", "timestamp": "00:10:13,920", "timestamp_s": 613.0}, {"text": "would go on till like 10:00 a.m. 10:00 p.m. In the night, for example.", "timestamp": "00:10:17,712", "timestamp_s": 617.0}, {"text": "Right, the local time. So we had promised like two lines of uptime", "timestamp": "00:10:20,792", "timestamp_s": 620.0}, {"text": "during that. In terms of SLO, we obviously", "timestamp": "00:10:24,936", "timestamp_s": 624.0}, {"text": "had to worry about costs and optimizing the costs as", "timestamp": "00:10:29,072", "timestamp_s": 629.0}, {"text": "an important requirement. From SLo point of view, we hadn\u0027t defined specific", "timestamp": "00:10:32,192", "timestamp_s": 632.0}, {"text": "metrics, but we\u0027ll get to that later. And then from an API point of view,", "timestamp": "00:10:36,296", "timestamp_s": 636.0}, {"text": "these were synchronous APIs as far as the user and the SDK is", "timestamp": "00:10:40,488", "timestamp_s": 640.0}, {"text": "concerned. So less than three second API latency for", "timestamp": "00:10:43,800", "timestamp_s": 643.0}, {"text": "95th percentile. That was our goal that we had set out.", "timestamp": "00:10:47,176", "timestamp_s": 647.0}, {"text": "Now, given this, let\u0027s think about our architecture", "timestamp": "00:10:51,344", "timestamp_s": 651.0}, {"text": "and we set out to build a cloud agnostic architecture.", "timestamp": "00:10:55,272", "timestamp_s": 655.0}, {"text": "I\u0027ll cover more of that soon. And why that is.", "timestamp": "00:10:58,552", "timestamp_s": 658.0}, {"text": "So, for example, for the storing of actual", "timestamp": "00:11:02,104", "timestamp_s": 662.0}, {"text": "images, which were ephemeral for short time and whatnot, because again we\u0027re", "timestamp": "00:11:05,904", "timestamp_s": 665.0}, {"text": "dealing with sensitive data. We were using s three", "timestamp": "00:11:09,408", "timestamp_s": 669.0}, {"text": "if we are deployed on AWS, we were using gcs if we were deployed", "timestamp": "00:11:13,612", "timestamp_s": 673.0}, {"text": "on GCP, and Minio if we were deployed on on", "timestamp": "00:11:17,180", "timestamp_s": 677.0}, {"text": "premise. One of the reason was that we wanted to create,", "timestamp": "00:11:20,540", "timestamp_s": 680.0}, {"text": "we wanted to have same code, could use different type", "timestamp": "00:11:25,404", "timestamp_s": 685.0}, {"text": "of image store without having to change the code a lot", "timestamp": "00:11:29,180", "timestamp_s": 689.0}, {"text": "or without no change to the code at all. Why? Because then", "timestamp": "00:11:32,372", "timestamp_s": 692.0}, {"text": "we could deploy this entire stack on any cloud.", "timestamp": "00:11:35,796", "timestamp_s": 695.0}, {"text": "We could run it on on premise. We could even run it on customers premise.", "timestamp": "00:11:39,452", "timestamp_s": 699.0}, {"text": "We could run it on AWS, GCP or any other cloud for that matter.", "timestamp": "00:11:42,940", "timestamp_s": 702.0}, {"text": "This was the main point that we wanted to achieve. That\u0027s why", "timestamp": "00:11:46,884", "timestamp_s": 706.0}, {"text": "we set out to have a cloud agnostic architecture where we don\u0027t", "timestamp": "00:11:50,660", "timestamp_s": 710.0}, {"text": "use a very cloud specific component and then we are tied to that particular cloud", "timestamp": "00:11:54,004", "timestamp_s": 714.0}, {"text": "as a code dependency. Now that\u0027s", "timestamp": "00:11:58,540", "timestamp_s": 718.0}, {"text": "for Binayo. For Redis we could either go with", "timestamp": "00:12:02,020", "timestamp_s": 722.0}, {"text": "self hosted redis which is basically a cache for storing", "timestamp": "00:12:05,548", "timestamp_s": 725.0}, {"text": "bunch of latest computation. That API can quickly return the", "timestamp": "00:12:09,636", "timestamp_s": 729.0}, {"text": "results to the users. We could do this as a self hosted", "timestamp": "00:12:13,068", "timestamp_s": 733.0}, {"text": "redis or elasticache if you are on one of the cloud providers.", "timestamp": "00:12:16,948", "timestamp_s": 736.0}, {"text": "Now for RabbitMQ, we again choose RabbitMQ", "timestamp": "00:12:21,124", "timestamp_s": 741.0}, {"text": "purely so that we could run this on premise easily.", "timestamp": "00:12:24,684", "timestamp_s": 744.0}, {"text": "And if you were on the cloud, we could use something like", "timestamp": "00:12:27,812", "timestamp_s": 747.0}, {"text": "sqs or like equivalent in India cloud for", "timestamp": "00:12:31,192", "timestamp_s": 751.0}, {"text": "GPU\u0027s and workers, which were predominantly GPU workload,", "timestamp": "00:12:35,680", "timestamp_s": 755.0}, {"text": "we would use them on one of the cloud providers,", "timestamp": "00:12:39,872", "timestamp_s": 759.0}, {"text": "or we could get our own custom GPU\u0027s, et cetera. For this", "timestamp": "00:12:43,384", "timestamp_s": 763.0}, {"text": "production use case we were on one of the cloud providers and so we", "timestamp": "00:12:46,904", "timestamp_s": 766.0}, {"text": "use most of the cloud components, but our code was such that it was", "timestamp": "00:12:50,368", "timestamp_s": 770.0}, {"text": "not tightly coupled to cloud at all. And for database, again,", "timestamp": "00:12:54,368", "timestamp_s": 774.0}, {"text": "we could use postgres. It could be RDS or cloud", "timestamp": "00:12:58,664", "timestamp_s": 778.0}, {"text": "SQL or something else depending on the cloud provider.", "timestamp": "00:13:02,144", "timestamp_s": 782.0}, {"text": "We fundamentally had Nomad as the orchestrator which would orchestrate", "timestamp": "00:13:07,944", "timestamp_s": 787.0}, {"text": "all the deployments and scaling", "timestamp": "00:13:11,808", "timestamp_s": 791.0}, {"text": "of components. Back then we were not using kubernetes purely,", "timestamp": "00:13:15,616", "timestamp_s": 795.0}, {"text": "again from a simplicity point of view that we wanted to deploy this whole", "timestamp": "00:13:19,616", "timestamp_s": 799.0}, {"text": "stack with the orchestrator on premise and", "timestamp": "00:13:23,504", "timestamp_s": 803.0}, {"text": "we didn\u0027t want to in the team. We did not have a lot", "timestamp": "00:13:27,244", "timestamp_s": 807.0}, {"text": "of Kubernetes expertise to be able to manage self hosted kubernetes", "timestamp": "00:13:30,420", "timestamp_s": 810.0}, {"text": "on premise ourselves. So that\u0027s where we chose Hashicorp stack,", "timestamp": "00:13:34,700", "timestamp_s": 814.0}, {"text": "which is fairly single binary, easy to manage and", "timestamp": "00:13:38,484", "timestamp_s": 818.0}, {"text": "easy to run, and we already had expertise in the team for that.", "timestamp": "00:13:41,748", "timestamp_s": 821.0}, {"text": "Why cloud agnostic? I think it\u0027s a very important point that I want to highlight", "timestamp": "00:13:45,884", "timestamp_s": 825.0}, {"text": "because we were cloud agnostic, we could package the same wine", "timestamp": "00:13:49,492", "timestamp_s": 829.0}, {"text": "in a different bottle, for example, so we", "timestamp": "00:13:53,030", "timestamp_s": 833.0}, {"text": "could package the same stack and run it on any environment", "timestamp": "00:13:56,950", "timestamp_s": 836.0}, {"text": "that we wanted to. We could do air gap environments if we had to,", "timestamp": "00:14:00,430", "timestamp_s": 840.0}, {"text": "things like that. So this was the main reason why we", "timestamp": "00:14:04,390", "timestamp_s": 844.0}, {"text": "went cloud agnostic. And I think one of the lessons to learn", "timestamp": "00:14:07,550", "timestamp_s": 847.0}, {"text": "is to build more cloud agnostic systems. That way", "timestamp": "00:14:11,246", "timestamp_s": 851.0}, {"text": "you\u0027re not tied to any of the cloud providers, although using", "timestamp": "00:14:15,174", "timestamp_s": 855.0}, {"text": "cloud providers obviously simplifies a lot of things for you. But you would want", "timestamp": "00:14:18,842", "timestamp_s": 858.0}, {"text": "to have your architecture and code not coupled", "timestamp": "00:14:22,362", "timestamp_s": 862.0}, {"text": "with the cloud provider so that you can change freely and", "timestamp": "00:14:25,994", "timestamp_s": 865.0}, {"text": "migrate to a different cloud if you want to without having to redo", "timestamp": "00:14:30,106", "timestamp_s": 870.0}, {"text": "a lot of effort. What did our scaling journey and", "timestamp": "00:14:33,370", "timestamp_s": 873.0}, {"text": "how did we go from zero requests to 2 million API requests", "timestamp": "00:14:37,778", "timestamp_s": 877.0}, {"text": "per day? Let\u0027s talk about that. Obviously it wasn\u0027t zero", "timestamp": "00:14:41,450", "timestamp_s": 881.0}, {"text": "request one day and 2 million the next day. It was a gradual", "timestamp": "00:14:44,924", "timestamp_s": 884.0}, {"text": "scaling journey, something like this.", "timestamp": "00:14:48,644", "timestamp_s": 888.0}, {"text": "So we would roll out on few regions", "timestamp": "00:14:51,524", "timestamp_s": 891.0}, {"text": "or few stores, and then we would slowly", "timestamp": "00:14:54,732", "timestamp_s": 894.0}, {"text": "increment the traffic, we would observe the traffic and so on, so forth.", "timestamp": "00:14:58,492", "timestamp_s": 898.0}, {"text": "So fundamentally from our scaling journey, I want to talk about", "timestamp": "00:15:02,492", "timestamp_s": 902.0}, {"text": "four or five important points and then drill down on each one of them", "timestamp": "00:15:06,476", "timestamp_s": 906.0}, {"text": "as we go through the talk. One is the elimination of single", "timestamp": "00:15:09,764", "timestamp_s": 909.0}, {"text": "points of failure to be able to scale. We want to have", "timestamp": "00:15:13,624", "timestamp_s": 913.0}, {"text": "zero or no single point of failure so that your system is more", "timestamp": "00:15:17,680", "timestamp_s": 917.0}, {"text": "resilient to changes, resilient to failures. We also", "timestamp": "00:15:21,600", "timestamp_s": 921.0}, {"text": "need to do good capacity planning so that you are able", "timestamp": "00:15:25,264", "timestamp_s": 925.0}, {"text": "to scale up and down very easily and you can save on cloud", "timestamp": "00:15:28,912", "timestamp_s": 928.0}, {"text": "costs. Otherwise, if it requires you to scale and you are", "timestamp": "00:15:32,896", "timestamp_s": 932.0}, {"text": "having to do a changes to architecture, it causes problems.", "timestamp": "00:15:36,808", "timestamp_s": 936.0}, {"text": "So having good capacity planning and how we went about that,", "timestamp": "00:15:40,552", "timestamp_s": 940.0}, {"text": "I\u0027ll also cover that. Obviously, cost optimization and auto scaling goes", "timestamp": "00:15:43,696", "timestamp_s": 943.0}, {"text": "hand in hand with capacity planning. So I\u0027ll cover that.", "timestamp": "00:15:47,520", "timestamp_s": 947.0}, {"text": "Then comes around a lot of operational aspects about", "timestamp": "00:15:50,976", "timestamp_s": 950.0}, {"text": "deployments, observability, being able to debug something,", "timestamp": "00:15:54,272", "timestamp_s": 954.0}, {"text": "dealing with production issues, stuff like that. And obviously all", "timestamp": "00:15:58,136", "timestamp_s": 958.0}, {"text": "of this journey wasn\u0027t very straightforward.", "timestamp": "00:16:01,840", "timestamp_s": 961.0}, {"text": "It was fraught with some challenges that we encountered. So I\u0027m going", "timestamp": "00:16:04,920", "timestamp_s": 964.0}, {"text": "to cover like two interesting challenges that we encountered along the way. So hopefully", "timestamp": "00:16:08,056", "timestamp_s": 968.0}, {"text": "you all can learn from it. So in the next part", "timestamp": "00:16:11,640", "timestamp_s": 971.0}, {"text": "of the talk, I\u0027m going to take each one of these points and then go", "timestamp": "00:16:15,008", "timestamp_s": 975.0}, {"text": "drill down on each one of them. So let\u0027s talk about eliminating", "timestamp": "00:16:18,024", "timestamp_s": 978.0}, {"text": "single points of failure. We had", "timestamp": "00:16:22,392", "timestamp_s": 982.0}, {"text": "this architecture and just showing the architecture here as, as a, in the background.", "timestamp": "00:16:25,608", "timestamp_s": 985.0}, {"text": "So one of the things we did is we added high availability mode", "timestamp": "00:16:29,972", "timestamp_s": 989.0}, {"text": "for RabbitMQ. What does that mean? It means we have queue", "timestamp": "00:16:33,628", "timestamp_s": 993.0}, {"text": "replication. So whichever queue is there on one machine, it gets", "timestamp": "00:16:37,484", "timestamp_s": 997.0}, {"text": "replicated or mirrored onto the other machine. We were running RabbitmQ in", "timestamp": "00:16:41,012", "timestamp_s": 1001.0}, {"text": "a three node cluster instead of a single single node, for example.", "timestamp": "00:16:45,036", "timestamp_s": 1005.0}, {"text": "We also had cross AZ deployment for Rabbitmq. So the", "timestamp": "00:16:48,812", "timestamp_s": 1008.0}, {"text": "three nodes of RapidMQ, each one would run its own easy,", "timestamp": "00:16:52,444", "timestamp_s": 1012.0}, {"text": "for example. Obviously this was", "timestamp": "00:16:55,676", "timestamp_s": 1015.0}, {"text": "on premise or a setup where we wanted to host and", "timestamp": "00:16:59,142", "timestamp_s": 1019.0}, {"text": "manage RabbitMQ ourselves. But if it were a cloud", "timestamp": "00:17:02,870", "timestamp_s": 1022.0}, {"text": "managed service that we would use, we would use something like sqs, for example.", "timestamp": "00:17:06,774", "timestamp_s": 1026.0}, {"text": "One of the other things that we did to eliminate single points of failure", "timestamp": "00:17:11,222", "timestamp_s": 1031.0}, {"text": "is to run ML workloads in multiple azs.", "timestamp": "00:17:15,118", "timestamp_s": 1035.0}, {"text": "Now back then we had only two azs where we", "timestamp": "00:17:19,046", "timestamp_s": 1039.0}, {"text": "could have ML GPU\u0027s available. The third zone", "timestamp": "00:17:23,010", "timestamp_s": 1043.0}, {"text": "from the cloud provider did not yet provide the GPU\u0027s.", "timestamp": "00:17:27,098", "timestamp_s": 1047.0}, {"text": "So we had to tweak our logic and deployment and", "timestamp": "00:17:30,458", "timestamp_s": 1050.0}, {"text": "automation to be able to spin up and load balance between these two acs.", "timestamp": "00:17:34,178", "timestamp_s": 1054.0}, {"text": "So we would have to fix auto scaling, we would have to fix deployment", "timestamp": "00:17:38,266", "timestamp_s": 1058.0}, {"text": "automation to be able to run workloads", "timestamp": "00:17:42,114", "timestamp_s": 1062.0}, {"text": "only on two zones instead of three. For most of the other two cloud", "timestamp": "00:17:45,490", "timestamp_s": 1065.0}, {"text": "components or most of the other components in the architecture, we would have workloads", "timestamp": "00:17:49,064", "timestamp_s": 1069.0}, {"text": "run on all three acs. Other thing that", "timestamp": "00:17:53,112", "timestamp_s": 1073.0}, {"text": "we did is wherever possible, we used SaaS offering for", "timestamp": "00:17:56,992", "timestamp_s": 1076.0}, {"text": "some of the important stateful systems, like databases,", "timestamp": "00:18:00,704", "timestamp_s": 1080.0}, {"text": "for example, redis and postgres,", "timestamp": "00:18:04,120", "timestamp_s": 1084.0}, {"text": "just to make sure that we don\u0027t have to manage and", "timestamp": "00:18:07,104", "timestamp_s": 1087.0}, {"text": "scale those components. Also. And managing and scaling ML", "timestamp": "00:18:10,352", "timestamp_s": 1090.0}, {"text": "was one of the bigger challenges. So we wanted to offload some of the lower", "timestamp": "00:18:13,816", "timestamp_s": 1093.0}, {"text": "hanging fruits to the cloud providers. Fundamentally, the idea again is that", "timestamp": "00:18:17,752", "timestamp_s": 1097.0}, {"text": "scaling and managing stateful components is bit hard and", "timestamp": "00:18:21,982", "timestamp_s": 1101.0}, {"text": "stateless is much more easier. So wherever possible,", "timestamp": "00:18:25,966", "timestamp_s": 1105.0}, {"text": "it\u0027s easy to automate stateless application scaling, component scaling", "timestamp": "00:18:29,454", "timestamp_s": 1109.0}, {"text": "and stateful becomes difficult. So that\u0027s", "timestamp": "00:18:33,438", "timestamp_s": 1113.0}, {"text": "on the eliminating single point of failure.", "timestamp": "00:18:36,950", "timestamp_s": 1116.0}, {"text": "Let\u0027s talk about capacity planning. So always when", "timestamp": "00:18:40,030", "timestamp_s": 1120.0}, {"text": "you think about capacity planning, you think of the bottleneck,", "timestamp": "00:18:43,918", "timestamp_s": 1123.0}, {"text": "because if the strength of the link is", "timestamp": "00:18:47,046", "timestamp_s": 1127.0}, {"text": "the strength of the weakest component in the link, for example,", "timestamp": "00:18:50,338", "timestamp_s": 1130.0}, {"text": "strength of the chain is the strength of the weakest component in the chain.", "timestamp": "00:18:53,890", "timestamp_s": 1133.0}, {"text": "So you want to find out what\u0027s the weakest component and improve", "timestamp": "00:18:57,050", "timestamp_s": 1137.0}, {"text": "the strength of that component. So for example, if you think about various components", "timestamp": "00:19:00,706", "timestamp_s": 1140.0}, {"text": "from the architecture, we have API, which is simple", "timestamp": "00:19:04,722", "timestamp_s": 1144.0}, {"text": "API which is does talk to database and get the results", "timestamp": "00:19:08,650", "timestamp_s": 1148.0}, {"text": "from database. For example, there is database, which is stateful", "timestamp": "00:19:11,922", "timestamp_s": 1151.0}, {"text": "component. It could be redis, rabbit, postgres, etcetera,", "timestamp": "00:19:15,306", "timestamp_s": 1155.0}, {"text": "then mlworkers and something else.", "timestamp": "00:19:18,782", "timestamp_s": 1158.0}, {"text": "So where do we think is the bottleneck.", "timestamp": "00:19:22,166", "timestamp_s": 1162.0}, {"text": "Obviously it was on the mlworker side, because that\u0027s the component", "timestamp": "00:19:25,318", "timestamp_s": 1165.0}, {"text": "which takes most time in the request path. So we set out", "timestamp": "00:19:28,910", "timestamp_s": 1168.0}, {"text": "to figure out, for example, how many ML model requests", "timestamp": "00:19:32,782", "timestamp_s": 1172.0}, {"text": "can a single node handle. So for example,", "timestamp": "00:19:36,654", "timestamp_s": 1176.0}, {"text": "if you have a single node with say one gpu with 16 gigs of", "timestamp": "00:19:39,718", "timestamp_s": 1179.0}, {"text": "GPU memory and a GPU with whatever", "timestamp": "00:19:43,526", "timestamp_s": 1183.0}, {"text": "few cores, how many workers can I run on that? And how many", "timestamp": "00:19:46,866", "timestamp_s": 1186.0}, {"text": "requests per second or per hour can I get out of that?", "timestamp": "00:19:50,282", "timestamp_s": 1190.0}, {"text": "Now, each model, ML model may give", "timestamp": "00:19:54,754", "timestamp_s": 1194.0}, {"text": "us different results. So for example, face matching may be faster than OCR,", "timestamp": "00:19:58,170", "timestamp_s": 1198.0}, {"text": "or face liveness detection could be faster than face matching,", "timestamp": "00:20:02,162", "timestamp_s": 1202.0}, {"text": "for example. So we would run load test and", "timestamp": "00:20:06,162", "timestamp_s": 1206.0}, {"text": "we would run each of these models, each of the", "timestamp": "00:20:10,162", "timestamp_s": 1210.0}, {"text": "nodes, and we would run them via a load test to be able to find", "timestamp": "00:20:13,650", "timestamp_s": 1213.0}, {"text": "the maximum throughput that we can get over a long period of time,", "timestamp": "00:20:17,090", "timestamp_s": 1217.0}, {"text": "say an hour or two, for example. So again,", "timestamp": "00:20:20,474", "timestamp_s": 1220.0}, {"text": "I\u0027ve broken this down into more detail and even more generic format", "timestamp": "00:20:24,170", "timestamp_s": 1224.0}, {"text": "in another talk that I gave, which is optimizing application performance.", "timestamp": "00:20:27,786", "timestamp_s": 1227.0}, {"text": "How do you go about it from first principle? So if you\u0027re interested, you can", "timestamp": "00:20:31,330", "timestamp_s": 1231.0}, {"text": "check that out. I\u0027ll provide the link in the description, hopefully cost", "timestamp": "00:20:34,730", "timestamp_s": 1234.0}, {"text": "optimization auto scaling, that is one of the pet peeves given the", "timestamp": "00:20:39,176", "timestamp_s": 1239.0}, {"text": "current market scenario. So mostly you will have seen, if you", "timestamp": "00:20:42,960", "timestamp_s": 1242.0}, {"text": "use GPU in cloud, it costs a lot. So how do you go", "timestamp": "00:20:46,736", "timestamp_s": 1246.0}, {"text": "about optimizing and auto scaling? So you", "timestamp": "00:20:50,312", "timestamp_s": 1250.0}, {"text": "have to think about what is the parameter on which you can auto scale like,", "timestamp": "00:20:54,288", "timestamp_s": 1254.0}, {"text": "is it the utilization of CPU or GPU? Could it", "timestamp": "00:20:57,552", "timestamp_s": 1257.0}, {"text": "be based on memory utilization or number of incoming requests,", "timestamp": "00:21:00,928", "timestamp_s": 1260.0}, {"text": "for example? Or it could be depth of the queue in case", "timestamp": "00:21:04,592", "timestamp_s": 1264.0}, {"text": "we were using rapid MQ. So could it be queue depth or", "timestamp": "00:21:08,116", "timestamp_s": 1268.0}, {"text": "something else? Now we kind of used a combination", "timestamp": "00:21:11,628", "timestamp_s": 1271.0}, {"text": "of some of these components. So I\u0027ll talk about how we went about.", "timestamp": "00:21:15,532", "timestamp_s": 1275.0}, {"text": "So this is the cost optimization auto scaling,", "timestamp": "00:21:19,396", "timestamp_s": 1279.0}, {"text": "like our auto scaler, how it works, right? So on the left you", "timestamp": "00:21:22,796", "timestamp_s": 1282.0}, {"text": "have top left you have a current request rate. This is", "timestamp": "00:21:25,980", "timestamp_s": 1285.0}, {"text": "the graph where we would have, what\u0027s the number of requests we are getting over", "timestamp": "00:21:29,988", "timestamp_s": 1289.0}, {"text": "last 20 or 30 minutes interval? And we", "timestamp": "00:21:34,784", "timestamp_s": 1294.0}, {"text": "would have a capacity predictor component which would run every 20 minutes,", "timestamp": "00:21:38,768", "timestamp_s": 1298.0}, {"text": "which would fetch this request rate, or it would have this information", "timestamp": "00:21:43,000", "timestamp_s": 1303.0}, {"text": "and it would also get the current node count or current worker", "timestamp": "00:21:46,744", "timestamp_s": 1306.0}, {"text": "count. So again, imagine we are running these gpu\u0027s on workers.", "timestamp": "00:21:50,432", "timestamp_s": 1310.0}, {"text": "What\u0027s the current number of workers that we have currently? So you would", "timestamp": "00:21:54,112", "timestamp_s": 1314.0}, {"text": "get the current workload, you would get the current", "timestamp": "00:21:57,630", "timestamp_s": 1317.0}, {"text": "request count. And based on that, based on", "timestamp": "00:22:01,518", "timestamp_s": 1321.0}, {"text": "the request number of requests and the growth rate of that,", "timestamp": "00:22:05,062", "timestamp_s": 1325.0}, {"text": "the capacitor, the capacity predictor would kind", "timestamp": "00:22:09,094", "timestamp_s": 1329.0}, {"text": "of predict, using just simple linear regression, the desired", "timestamp": "00:22:12,678", "timestamp_s": 1332.0}, {"text": "node count. So for example, if we, for example, open the stores at", "timestamp": "00:22:16,358", "timestamp_s": 1336.0}, {"text": "09:00 a.m. And we know that we at least need like", "timestamp": "00:22:19,950", "timestamp_s": 1339.0}, {"text": "50 machines at that point, so we would have time based auto scaling", "timestamp": "00:22:23,302", "timestamp_s": 1343.0}, {"text": "and we should just spin up 50 machines. You have them ready", "timestamp": "00:22:26,730", "timestamp_s": 1346.0}, {"text": "at like before ten minutes. The stores open, right?", "timestamp": "00:22:30,082", "timestamp_s": 1350.0}, {"text": "But the stores open and you continue to see increase in traffic,", "timestamp": "00:22:33,098", "timestamp_s": 1353.0}, {"text": "you would want to spin up more nodes. If you see decrease in traffic,", "timestamp": "00:22:36,762", "timestamp_s": 1356.0}, {"text": "typically during lunchtime, you would want to spin down a couple of nodes,", "timestamp": "00:22:40,114", "timestamp_s": 1360.0}, {"text": "for example, right? So we had this capacity predictor component", "timestamp": "00:22:43,634", "timestamp_s": 1363.0}, {"text": "which would take this request rate of growth or", "timestamp": "00:22:47,314", "timestamp_s": 1367.0}, {"text": "rate of decline, you would take the current node count, and based", "timestamp": "00:22:50,698", "timestamp_s": 1370.0}, {"text": "on the linear regression math and some other parameters that we talked about,", "timestamp": "00:22:54,916", "timestamp_s": 1374.0}, {"text": "it would predict the desired node count. Now this is", "timestamp": "00:22:58,916", "timestamp_s": 1378.0}, {"text": "the count that would go as an input to auto scalar. The autoscaler", "timestamp": "00:23:02,660", "timestamp_s": 1382.0}, {"text": "would then do couple of things. It would update Nomad.", "timestamp": "00:23:06,252", "timestamp_s": 1386.0}, {"text": "It would tell Nomad, hey, can you please spin up those many number", "timestamp": "00:23:09,924", "timestamp_s": 1389.0}, {"text": "of components or those many number of workers? So the Nomad", "timestamp": "00:23:13,132", "timestamp_s": 1393.0}, {"text": "update cluster configuration would run and the Nomad would correspondingly", "timestamp": "00:23:17,076", "timestamp_s": 1397.0}, {"text": "spin up more nodes. It will deploy the model on those nodes,", "timestamp": "00:23:20,880", "timestamp_s": 1400.0}, {"text": "etcetera, and it will also update, for example, if you\u0027re using slack.", "timestamp": "00:23:24,224", "timestamp_s": 1404.0}, {"text": "So it will also update us on slack. That, yeah, we\u0027ve got two more", "timestamp": "00:23:27,264", "timestamp_s": 1407.0}, {"text": "nodes added, or we\u0027ve got two nodes destroyed because it was less traffic", "timestamp": "00:23:30,632", "timestamp_s": 1410.0}, {"text": "time. For example, one of the reason we built", "timestamp": "00:23:34,704", "timestamp_s": 1414.0}, {"text": "this kind of a system is so that we have a manual override", "timestamp": "00:23:38,056", "timestamp_s": 1418.0}, {"text": "at any point. If we knew that there is a big campaign going on,", "timestamp": "00:23:42,416", "timestamp_s": 1422.0}, {"text": "are we going to scale, we are going to have to scale the machines", "timestamp": "00:23:45,600", "timestamp_s": 1425.0}, {"text": "at a particular time or due to some other kind of business", "timestamp": "00:23:49,752", "timestamp_s": 1429.0}, {"text": "constraints, we would be able to manually overwrite that value and", "timestamp": "00:23:53,584", "timestamp_s": 1433.0}, {"text": "change the configuration to be able to spin up those many", "timestamp": "00:23:57,416", "timestamp_s": 1437.0}, {"text": "number of nodes. This gave us a lot of control and", "timestamp": "00:24:00,728", "timestamp_s": 1440.0}, {"text": "we\u0027ve been using this autoscaler for years now and it\u0027s just been", "timestamp": "00:24:04,064", "timestamp_s": 1444.0}, {"text": "working very well. It\u0027s a very simple, less effort work,", "timestamp": "00:24:07,832", "timestamp_s": 1447.0}, {"text": "but it just works flawlessly for us.", "timestamp": "00:24:11,352", "timestamp_s": 1451.0}, {"text": "So we\u0027ve never had issues with auto scalar as such.", "timestamp": "00:24:15,204", "timestamp_s": 1455.0}, {"text": "There\u0027s more that we\u0027ve written about in our, one of the recent blog posts", "timestamp": "00:24:18,844", "timestamp_s": 1458.0}, {"text": "and case studies. You should check it out if you\u0027re interested in this kind of", "timestamp": "00:24:23,044", "timestamp_s": 1463.0}, {"text": "stuff. Now, our journey wasn\u0027t", "timestamp": "00:24:26,380", "timestamp_s": 1466.0}, {"text": "smooth, right. It was fraught with some issues and errors.", "timestamp": "00:24:30,108", "timestamp_s": 1470.0}, {"text": "So I\u0027m going to talk about some of the issues we encountered and how we", "timestamp": "00:24:33,516", "timestamp_s": 1473.0}, {"text": "navigated those, and what kind of impact it had on downtime,", "timestamp": "00:24:36,836", "timestamp_s": 1476.0}, {"text": "slo, etcetera. So one of the issue that we encountered was", "timestamp": "00:24:40,300", "timestamp_s": 1480.0}, {"text": "GPU utilization in Nomad. For example, imagine this", "timestamp": "00:24:44,692", "timestamp_s": 1484.0}, {"text": "top box to be a GPU. It has GPU cache memory", "timestamp": "00:24:49,236", "timestamp_s": 1489.0}, {"text": "and cores. We would be able to run a ML worker", "timestamp": "00:24:52,948", "timestamp_s": 1492.0}, {"text": "using Nomad, using Docker driver. So we would run one", "timestamp": "00:24:56,692", "timestamp_s": 1496.0}, {"text": "ML worker per GPU. What we notice is that the", "timestamp": "00:24:59,892", "timestamp_s": 1499.0}, {"text": "GPU wasn\u0027t utilized fully. It was with one worker, it was just 20%", "timestamp": "00:25:03,460", "timestamp_s": 1503.0}, {"text": "utilized, and lot of other", "timestamp": "00:25:07,456", "timestamp_s": 1507.0}, {"text": "resources of that machine were just left unused. We did load", "timestamp": "00:25:11,224", "timestamp_s": 1511.0}, {"text": "tests to be able to figure out how much throughput we can get out of", "timestamp": "00:25:15,264", "timestamp_s": 1515.0}, {"text": "a single machine running single ML worker. It\u0027s a", "timestamp": "00:25:18,448", "timestamp_s": 1518.0}, {"text": "docker container, and it wasn\u0027t impressive with", "timestamp": "00:25:21,688", "timestamp_s": 1521.0}, {"text": "this. If we just ran with this kind of", "timestamp": "00:25:25,456", "timestamp_s": 1525.0}, {"text": "hardware, our cost was going through the roof, and we", "timestamp": "00:25:28,800", "timestamp_s": 1528.0}, {"text": "had to really figure out how do we fix this. So we", "timestamp": "00:25:32,488", "timestamp_s": 1532.0}, {"text": "dug deep into Nomad. GitHub issues, some pull requests,", "timestamp": "00:25:37,040", "timestamp_s": 1537.0}, {"text": "some parts into reading obscure documentation and figuring out.", "timestamp": "00:25:41,504", "timestamp_s": 1541.0}, {"text": "And fundamentally later, what we discovered is one", "timestamp": "00:25:45,104", "timestamp_s": 1545.0}, {"text": "workaround which we can use, which is instead of using docker", "timestamp": "00:25:48,912", "timestamp_s": 1548.0}, {"text": "driver, if we use raw exec driver, which allows you", "timestamp": "00:25:52,520", "timestamp_s": 1552.0}, {"text": "to run any kind of component, it doesn\u0027t", "timestamp": "00:25:56,248", "timestamp_s": 1556.0}, {"text": "guarantee any. Like you", "timestamp": "00:25:59,560", "timestamp_s": 1559.0}, {"text": "have to worry about a lot of scheduling yourself when you use raw exec driver.", "timestamp": "00:26:02,618", "timestamp_s": 1562.0}, {"text": "But with raw exec driver, you could run docker compose. And we", "timestamp": "00:26:06,826", "timestamp_s": 1566.0}, {"text": "spin up multiple docker containers using docker", "timestamp": "00:26:10,370", "timestamp_s": 1570.0}, {"text": "compose via raw exec driver in Nomad. So what that allowed", "timestamp": "00:26:13,642", "timestamp_s": 1573.0}, {"text": "us to do is the same GPU machine,", "timestamp": "00:26:17,178", "timestamp_s": 1577.0}, {"text": "we could run not one, but four workers,", "timestamp": "00:26:20,274", "timestamp_s": 1580.0}, {"text": "right? Using docker composer. Now that led us", "timestamp": "00:26:23,946", "timestamp_s": 1583.0}, {"text": "to having about 80% utilization.", "timestamp": "00:26:27,788", "timestamp_s": 1587.0}, {"text": "It straight away brought our cost down by four x. So imagine if you", "timestamp": "00:26:31,444", "timestamp_s": 1591.0}, {"text": "had to have $100,000 per month on just on GPU,", "timestamp": "00:26:34,916", "timestamp_s": 1594.0}, {"text": "we would slash it by one fourth directly and have like 400%", "timestamp": "00:26:39,476", "timestamp_s": 1599.0}, {"text": "impact, essentially. So this was one way we solved it.", "timestamp": "00:26:43,572", "timestamp_s": 1603.0}, {"text": "This is as of today, last I checked, it is still open,", "timestamp": "00:26:47,260", "timestamp_s": 1607.0}, {"text": "this issue, and you would see this pull request, or this", "timestamp": "00:26:50,620", "timestamp_s": 1610.0}, {"text": "issue is still open. And one of the solutions or", "timestamp": "00:26:53,892", "timestamp_s": 1613.0}, {"text": "workarounds that we\u0027ve used, I\u0027ve highlighted that here for", "timestamp": "00:26:57,122", "timestamp_s": 1617.0}, {"text": "you to look at. One thing that we noticed, if you use raw exec", "timestamp": "00:27:01,218", "timestamp_s": 1621.0}, {"text": "driver, you\u0027re going to have to worry about", "timestamp": "00:27:05,034", "timestamp_s": 1625.0}, {"text": "the shutdown part of it yourself.", "timestamp": "00:27:08,426", "timestamp_s": 1628.0}, {"text": "So otherwise, nomad generally handles sick", "timestamp": "00:27:11,442", "timestamp_s": 1631.0}, {"text": "term, and it handles the graceful termination of resources", "timestamp": "00:27:15,186", "timestamp_s": 1635.0}, {"text": "or components. In this case you will have to have", "timestamp": "00:27:19,866", "timestamp_s": 1639.0}, {"text": "waits and timeouts and you have to do some magic", "timestamp": "00:27:23,138", "timestamp_s": 1643.0}, {"text": "and work. You have to put in some work to be able to tear", "timestamp": "00:27:26,714", "timestamp_s": 1646.0}, {"text": "down the components correctly. So we invested in that and we wrote", "timestamp": "00:27:30,034", "timestamp_s": 1650.0}, {"text": "some bash script to be able to run,", "timestamp": "00:27:33,530", "timestamp_s": 1653.0}, {"text": "which could run the components and also tear them down easily when we", "timestamp": "00:27:37,114", "timestamp_s": 1657.0}, {"text": "wanted to. One other issue that we encountered is", "timestamp": "00:27:40,850", "timestamp_s": 1660.0}, {"text": "high latency. So again we have", "timestamp": "00:27:44,722", "timestamp_s": 1664.0}, {"text": "steady traffic, new regions or new stores are opening up and", "timestamp": "00:27:48,194", "timestamp_s": 1668.0}, {"text": "we are getting them migrated to use our APIs. And the rollout", "timestamp": "00:27:52,122", "timestamp_s": 1672.0}, {"text": "is happening suddenly on one of the days. What we see is", "timestamp": "00:27:55,770", "timestamp_s": 1675.0}, {"text": "that more than 25 2nd response time.", "timestamp": "00:27:59,010", "timestamp_s": 1679.0}, {"text": "Now imagine like our sla is less than 3 seconds response", "timestamp": "00:28:02,506", "timestamp_s": 1682.0}, {"text": "time for 90th percentile or 95th percentile.", "timestamp": "00:28:05,682", "timestamp_s": 1685.0}, {"text": "Now we suddenly get 25 seconds of response time.", "timestamp": "00:28:09,122", "timestamp_s": 1689.0}, {"text": "That\u0027s unacceptable. So we debug", "timestamp": "00:28:12,490", "timestamp_s": 1692.0}, {"text": "this issue. We try to find out, oh, there must be some problem with Mlworker,", "timestamp": "00:28:16,722", "timestamp_s": 1696.0}, {"text": "right? Because that\u0027s the slowest component in the chain. But we realize", "timestamp": "00:28:20,194", "timestamp_s": 1700.0}, {"text": "that there is no queue depth. The workers are doing their job,", "timestamp": "00:28:24,154", "timestamp_s": 1704.0}, {"text": "there is no extra jobs for them to be processed.", "timestamp": "00:28:27,682", "timestamp_s": 1707.0}, {"text": "So the worker scaling or auto scaling is not a problem. But then", "timestamp": "00:28:31,010", "timestamp_s": 1711.0}, {"text": "why do we have this much latency on the API side?", "timestamp": "00:28:35,194", "timestamp_s": 1715.0}, {"text": "There is no processing lag on the worker. Why do we have that?", "timestamp": "00:28:39,546", "timestamp_s": 1719.0}, {"text": "We spent a lot of time debugging this issue and ultimately", "timestamp": "00:28:43,504", "timestamp_s": 1723.0}, {"text": "we discovered that the number of go routines that we had on", "timestamp": "00:28:47,320", "timestamp_s": 1727.0}, {"text": "the API side which would process the results from RabbitMQ.", "timestamp": "00:28:51,088", "timestamp_s": 1731.0}, {"text": "So for example, our flow was that", "timestamp": "00:28:54,616", "timestamp_s": 1734.0}, {"text": "we would have the client SDK call our NLB or AlB.", "timestamp": "00:28:58,384", "timestamp_s": 1738.0}, {"text": "The API would send a message to Rabbitmq. The workers would", "timestamp": "00:29:02,584", "timestamp_s": 1742.0}, {"text": "then consume that message, produce the result.", "timestamp": "00:29:06,192", "timestamp_s": 1746.0}, {"text": "It would be updated on database and it will also send another", "timestamp": "00:29:09,756", "timestamp_s": 1749.0}, {"text": "message on to RabbitMQ that the processing is done and some other metadata.", "timestamp": "00:29:13,140", "timestamp_s": 1753.0}, {"text": "Now there is this workers that were running on the API which", "timestamp": "00:29:17,436", "timestamp_s": 1757.0}, {"text": "would then consume this metadata via the RabbitMQ message", "timestamp": "00:29:21,532", "timestamp_s": 1761.0}, {"text": "and then it would return the response to the user or do something else.", "timestamp": "00:29:25,020", "timestamp_s": 1765.0}, {"text": "Right now that\u0027s the part. We were just running five coroutines.", "timestamp": "00:29:28,132", "timestamp_s": 1768.0}, {"text": "Now what we realize is that we were also running three nodes or three", "timestamp": "00:29:32,212", "timestamp_s": 1772.0}, {"text": "docker containers for workers. And we never faced any issue with worker,", "timestamp": "00:29:36,276", "timestamp_s": 1776.0}, {"text": "the cpu utilized API layer. Sorry, we never faced", "timestamp": "00:29:40,548", "timestamp_s": 1780.0}, {"text": "any cpu utilization or any issue with API layer.", "timestamp": "00:29:44,460", "timestamp_s": 1784.0}, {"text": "But when the request count increased,", "timestamp": "00:29:47,252", "timestamp_s": 1787.0}, {"text": "what we realized is that these five go routines were not enough and", "timestamp": "00:29:50,900", "timestamp_s": 1790.0}, {"text": "which is where we\u0027re actually seeing queue depth on the API side.", "timestamp": "00:29:54,364", "timestamp_s": 1794.0}, {"text": "The API go routines that we\u0027re trying to consume from Arabic MQ.", "timestamp": "00:29:57,860", "timestamp_s": 1797.0}, {"text": "That\u0027s where we saw the queue depth. And it took us a while to figure", "timestamp": "00:30:01,620", "timestamp_s": 1801.0}, {"text": "this out and to fix this, because our preconceived notion and the first response", "timestamp": "00:30:04,934", "timestamp_s": 1804.0}, {"text": "was to look at workers as a problem. Now we", "timestamp": "00:30:09,174", "timestamp_s": 1809.0}, {"text": "change the go routines to 30 and suddenly the flip,", "timestamp": "00:30:12,758", "timestamp_s": 1812.0}, {"text": "the traffic goes normal and we have less than 3 seconds response", "timestamp": "00:30:16,118", "timestamp_s": 1816.0}, {"text": "time. So that really shows how you want to,", "timestamp": "00:30:19,510", "timestamp_s": 1819.0}, {"text": "how you should understand the components and how data pipeline works,", "timestamp": "00:30:23,310", "timestamp_s": 1823.0}, {"text": "and you need to really think about where the latency is being introduced.", "timestamp": "00:30:26,878", "timestamp_s": 1826.0}, {"text": "After that, we also added a lot of other observability", "timestamp": "00:30:31,072", "timestamp_s": 1831.0}, {"text": "signals and metrics to be able to track this issue and further any", "timestamp": "00:30:35,184", "timestamp_s": 1835.0}, {"text": "other issues even even further. So we invested a lot in", "timestamp": "00:30:38,760", "timestamp_s": 1838.0}, {"text": "tracking cross service latencies", "timestamp": "00:30:42,944", "timestamp_s": 1842.0}, {"text": "and stuff like that. So I\u0027ve written about this as", "timestamp": "00:30:47,544", "timestamp_s": 1847.0}, {"text": "a form of pragmatic engineering story.", "timestamp": "00:30:50,976", "timestamp_s": 1850.0}, {"text": "You can check it out on Twitter if you follow me there. I want to", "timestamp": "00:30:54,244", "timestamp_s": 1854.0}, {"text": "conclude this session by talking about some of the lessons that we learned along the", "timestamp": "00:30:58,148", "timestamp_s": 1858.0}, {"text": "way. So one is for ML workloads,", "timestamp": "00:31:01,780", "timestamp_s": 1861.0}, {"text": "it goes without saying, but it\u0027s worth repeating that.", "timestamp": "00:31:05,340", "timestamp_s": 1865.0}, {"text": "Data quality and training the model is super important. Like a lot", "timestamp": "00:31:08,732", "timestamp_s": 1868.0}, {"text": "of it depends on the quality of data and the volume of the", "timestamp": "00:31:12,532", "timestamp_s": 1872.0}, {"text": "data and how you train your models, how you carve out", "timestamp": "00:31:15,748", "timestamp_s": 1875.0}, {"text": "the test data versus the data that you", "timestamp": "00:31:19,332", "timestamp_s": 1879.0}, {"text": "run the model on and you want to check and how you label and", "timestamp": "00:31:23,102", "timestamp_s": 1883.0}, {"text": "data. So a lot of it depends on data quality and training in", "timestamp": "00:31:27,310", "timestamp_s": 1887.0}, {"text": "our case, and I would highly recommend to use cloud agnostic architecture to", "timestamp": "00:31:31,950", "timestamp_s": 1891.0}, {"text": "be able to build scalable systems that can", "timestamp": "00:31:35,870", "timestamp_s": 1895.0}, {"text": "give you that flexibility to deploy them on any type of workload or any type", "timestamp": "00:31:39,934", "timestamp_s": 1899.0}, {"text": "of underlying cloud. For us, that has been the massive win.", "timestamp": "00:31:43,718", "timestamp_s": 1903.0}, {"text": "And I would highly encourage you to think about building cloud agnostic systems.", "timestamp": "00:31:47,446", "timestamp_s": 1907.0}, {"text": "One of the things that I\u0027ve seen majorly is people do not", "timestamp": "00:31:52,104", "timestamp_s": 1912.0}, {"text": "treat operational workloads as first class citizens. They just slap on", "timestamp": "00:31:55,416", "timestamp_s": 1915.0}, {"text": "top of the existing software and just say that yeah,", "timestamp": "00:31:59,384", "timestamp_s": 1919.0}, {"text": "somebody will manage this. But treating operational work as", "timestamp": "00:32:02,904", "timestamp_s": 1922.0}, {"text": "first class citizen really helps in automation of a lot", "timestamp": "00:32:06,304", "timestamp_s": 1926.0}, {"text": "of your day to day tasks, and it gives you,", "timestamp": "00:32:09,688", "timestamp_s": 1929.0}, {"text": "especially when you are first launching, you should really treat production,", "timestamp": "00:32:13,368", "timestamp_s": 1933.0}, {"text": "maintenance and operations as a first class citizen in", "timestamp": "00:32:17,580", "timestamp_s": 1937.0}, {"text": "software building and delivery process.", "timestamp": "00:32:21,420", "timestamp_s": 1941.0}, {"text": "Lastly, from a team point of view, we had really good collaboration", "timestamp": "00:32:24,084", "timestamp_s": 1944.0}, {"text": "with various teams, data scientists, the mobile", "timestamp": "00:32:28,692", "timestamp_s": 1948.0}, {"text": "SDK team, the backend engineering team, sres,", "timestamp": "00:32:32,444", "timestamp_s": 1952.0}, {"text": "and even the business folks. So for example, whenever there is a new campaign", "timestamp": "00:32:35,868", "timestamp_s": 1955.0}, {"text": "or we would get some info from business team that a new region or", "timestamp": "00:32:39,810", "timestamp_s": 1959.0}, {"text": "new bunch of stores are being onboarded,", "timestamp": "00:32:43,442", "timestamp_s": 1963.0}, {"text": "preemptively scaled the nodes to be able to handle that traffic,", "timestamp": "00:32:46,634", "timestamp_s": 1966.0}, {"text": "for example. So being able to closely collaborate with data scientists,", "timestamp": "00:32:50,434", "timestamp_s": 1970.0}, {"text": "we also optimize, for example, one of the case, we also optimize", "timestamp": "00:32:54,386", "timestamp_s": 1974.0}, {"text": "the docker image size for the models. Earlier, we would have all", "timestamp": "00:32:57,746", "timestamp_s": 1977.0}, {"text": "the versions of the models in our final", "timestamp": "00:33:01,330", "timestamp_s": 1981.0}, {"text": "Docker image, which would mean the Docker image itself would be like tens", "timestamp": "00:33:05,166", "timestamp_s": 1985.0}, {"text": "of gb. So we\u0027ll have ten GB Docker image later. We optimize that", "timestamp": "00:33:08,590", "timestamp_s": 1988.0}, {"text": "with close collaboration with the data scientists to less than like three gb", "timestamp": "00:33:12,686", "timestamp_s": 1992.0}, {"text": "of model. So that just speeds up a lot of", "timestamp": "00:33:16,734", "timestamp_s": 1996.0}, {"text": "warming up of nodes. It just speeds up the deployment process and", "timestamp": "00:33:20,598", "timestamp_s": 2000.0}, {"text": "the time it takes for nodes to be ready to serve traffic. So ensuring", "timestamp": "00:33:25,046", "timestamp_s": 2005.0}, {"text": "good collaboration between teams is super, super important.", "timestamp": "00:33:29,590", "timestamp_s": 2009.0}, {"text": "I think that\u0027s it from my side, what I would say is connect with", "timestamp": "00:33:33,314", "timestamp_s": 2013.0}, {"text": "me on LinkedIn, Twitter, et cetera. You should check out our go", "timestamp": "00:33:36,842", "timestamp_s": 2016.0}, {"text": "or SRE bootcamp that we have built at 120 n, along with", "timestamp": "00:33:40,570", "timestamp_s": 2020.0}, {"text": "the software engineering stories that are right. Here\u0027s the QR code. You can", "timestamp": "00:33:44,570", "timestamp_s": 2024.0}, {"text": "scan this and you can check this out.", "timestamp": "00:33:48,090", "timestamp_s": 2028.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'Au73jDVCoNg',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              From DevOps to MLOps: Scaling Machine Learning Models to 2 Million+ Requests per Day
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Learn how to deploy and scale Machine Learning models to 2 Million+ requests/day using MLOps best practices. In this talk, you&rsquo;ll learn how to go from data preparation to deployment to scaling ML models that can run at large scale - all without breaking the bank.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Today I am going to talk about, from DevOps to Mlops, a journey of scaling machine learning models to 2 million API requests per day. I am a founder at company called 120 n where we help startup and enterprises with backend and site reliability engineering.

              </li>
              
              <li>
                Mlops is operationalizing data science. It is all about moving machine learning workloads to production. Fundamentally, this is around a feedback loop, just like in software. Here is what simplest mlops workflow looks like.

              </li>
              
              <li>
                We were working with a company which was building Ekyc SaaS APIs. This needed to scale up to 2 million API requests per day to the model. We applied DevOps and mlops practices, best practices in production. We set out for two nines of availability during peak hours or during business hours.

              </li>
              
              <li>
                What did our scaling journey and how did we go from zero requests to 2 million API requests per day? I want to talk about four or five important points and then drill down on each one of them. So hopefully you all can learn from it.

              </li>
              
              <li>
                One of the things we did is we added high availability mode for RabbitMQ. We also had cross AZ deployment for Rabbitmq. Where possible, we used SaaS offering for some of the important stateful systems. Managing and scaling ML was one of the bigger challenges.

              </li>
              
              <li>
                Let's talk about capacity planning. Where do we think is the bottleneck. Obviously it was on the mlworker side, because that's the component which takes most time in the request path. We set out to figure out how many ML model requests can a single node handle.

              </li>
              
              <li>
                How do you go about optimizing and auto scaling? What is the parameter on which you can auto scale? Could it be based on memory utilization or number of incoming requests. We've been using this autoscaler for years now and it's just been working very well.

              </li>
              
              <li>
                One of the issues that we encountered was GPU utilization in Nomad. Another issue was high latency on the API side. One workaround was using raw exec driver instead of docker. That allowed us to run not one, but four workers. It straight away brought our cost down by four x.

              </li>
              
              <li>
                Data quality and training the model is super important. I would highly recommend to use cloud agnostic architecture to build scalable systems. Treat production, maintenance and operations as a first class citizen in software building and delivery process.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/Au73jDVCoNg.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:27,040'); seek(27.0)">
              Today I am going to talk about, from DevOps to Mlops,
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:31,236'); seek(31.0)">
              a journey of scaling machine learning models to 2 million
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:34,924'); seek(34.0)">
              API requests per day. So before we dive in
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:38,692'); seek(38.0)">
              a brief about me, I am Chinmay. You can find me on
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:42,492'); seek(42.0)">
              Twitter LinkedIn etcetera via Chinmay 185.
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:46,364'); seek(46.0)">
              I am a founder at company called 120 n where we help startup
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:49,908'); seek(49.0)">
              and enterprises with backend and site reliability engineering.
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:54,244'); seek(54.0)">
              I write stories of our work in
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:58,124'); seek(58.0)">
              what is called pragmatic software engineering. These stories, I published
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:01:01,604'); seek(61.0)">
              them on Twitter LinkedIn, etcetera. I love engineering,
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:01:04,996'); seek(64.0)">
              psychology, percussion, and I am a huge fan of
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:09,068'); seek(69.0)">
              a game called Age of Empires. All right, so let's start.
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:12,860'); seek(72.0)">
              So what's, what are we covering today? We are covering three
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:16,604'); seek(76.0)">
              things fundamentally. One is what is mlops?
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:20,324'); seek(80.0)">
              How do you think mlops for DevOps practitioners?
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:24,114'); seek(84.0)">
              Fundamentally, I want to talk more about, and spend more time in talking about
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:28,090'); seek(88.0)">
              a real world production case study that we worked on, which will
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:31,914'); seek(91.0)">
              talk about all the learnings that we had into
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:36,034'); seek(96.0)">
              in a case study kind of walkthrough. So what
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:39,170'); seek(99.0)">
              is mlops fundamentally? Mlops is operationalizing
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:43,274'); seek(103.0)">
              data science. We all know what DevOps is. DevOps is operationalizing
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:47,978'); seek(107.0)">
              software delivery, software engineering.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:50,566'); seek(110.0)">
              Similarly, MlOps is equivalent to DevOps
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:54,814'); seek(114.0)">
              in a sense. It talks about operationalizing data science
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:58,350'); seek(118.0)">
              workloads. Think of machine learning AI ML
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:02:02,022'); seek(122.0)">
              workloads essentially, right? So that means it is all
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:02:05,982'); seek(125.0)">
              about moving machine learning workloads to production.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:10,454'); seek(130.0)">
              Just like we have DevOps phases, we have various phases in mlops.
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:14,470'); seek(134.0)">
              Fundamentally, it's like build, where you build the models,
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:17,954'); seek(137.0)">
              you manage various versions of the models. For example,
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:21,442'); seek(141.0)">
              you deploy these models on production. You monitor, you take feedback,
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:25,506'); seek(145.0)">
              you continuously improve the models, etcetera. So these are various four steps
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:29,018'); seek(149.0)">
              of mlops. Let's look at some of them in more
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:32,482'); seek(152.0)">
              detail. Right? So fundamentally, just like
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:35,882'); seek(155.0)">
              software engineering is about building and shipping code to production,
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:39,594'); seek(159.0)">
              MLOps is, is about building machine learning models.
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:43,288'); seek(163.0)">
              Now, what do you need for building machine learning models? You need data.
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:47,544'); seek(167.0)">
              Data. You need to extract this data in various forms.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:51,192'); seek(171.0)">
              You will need to analyze it. You will need to sort of prune some parts
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:54,168'); seek(174.0)">
              of data. Essentially, you are doing data preparation and gathering.
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:57,704'); seek(177.0)">
              Then you feed in this data to your ML model. You will
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:03:01,360'); seek(181.0)">
              train the model. You will evaluate models response, you will
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:04,784'); seek(184.0)">
              test and validate whether the model works correctly or not.
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:08,032'); seek(188.0)">
              You will fine tune this process over time. You know, you have
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:11,344'); seek(191.0)">
              test data segregation, you will have production data, stuff like that.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:15,784'); seek(195.0)">
              This is all the machine learning part of it, which is what data scientists
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:19,344'); seek(199.0)">
              work on. Now, the operational parts of it are model
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:23,072'); seek(203.0)">
              serving, how do you serve this model to production? How do
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:26,168'); seek(206.0)">
              you run this on GPU's? Do you run this on cpu's? Which cloud provider do
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:30,016'); seek(210.0)">
              you want to use? How do you monitor the model, whether it's performing as
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:33,832'); seek(213.0)">
              per expectations or not? How do you manage scale up and scale down of that
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:37,172'); seek(217.0)">
              model? All of that is the operational concern, which is the ops part of it.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:41,884'); seek(221.0)">
              Fundamentally, this is around a feedback loop,
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:44,836'); seek(224.0)">
              just like in software. We have Ci CD continuous integration and
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:48,476'); seek(228.0)">
              continuous delivery deployment. We have a third parameter,
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:51,660'); seek(231.0)">
              or third item in mlops called continuous
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:54,996'); seek(234.0)">
              testing and training, where you are going to continuously
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:58,820'); seek(238.0)">
              monitor and train the model and improve
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:02,818'); seek(242.0)">
              the model over the period of time. So here is
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:06,770'); seek(246.0)">
              what simplest mlops workflow looks
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:09,810'); seek(249.0)">
              like. This diagram is from sort of
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:12,850'); seek(252.0)">
              Google's mlops guide. You can find the link in the description.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:16,874'); seek(256.0)">
              Fundamentally, again, it starts with getting data. So we are trying to map all these
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:20,954'); seek(260.0)">
              previous steps and phases that we looked at into this model.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:24,490'); seek(264.0)">
              So we're going to get some data from various sources.
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:28,106'); seek(268.0)">
              It could be offline data, it could be real time data, things like that.
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:31,428'); seek(271.0)">
              For now, we're keeping the diagram very simple and just looking at some
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:35,340'); seek(275.0)">
              offline data. For example, we are going to extract this data,
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:38,724'); seek(278.0)">
              analyze it, prepare that data essentially.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:41,604'); seek(281.0)">
              And there's a second step. Then the whole model
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:45,396'); seek(285.0)">
              training step appears where you are going to train the model, you're going to evaluate
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:48,972'); seek(288.0)">
              the model, you're going to check the performance, you're going to manage various
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:52,276'); seek(292.0)">
              versions, validations, etcetera. Finally, you have a train
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:55,494'); seek(295.0)">
              model which you put into model registry.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:58,598'); seek(298.0)">
              Now once that model registry has
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:01,702'); seek(301.0)">
              the model, the operational part of the mlops comes
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:05,102'); seek(305.0)">
              into picture, which is serving the model, and then which is
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:08,358'); seek(308.0)">
              where you have, for example, a prediction service which you can
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:12,062'); seek(312.0)">
              run on production. Then you have to monitor, scale that service,
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:15,774'); seek(315.0)">
              run this on GPU's, figure out cloud cost optimizations,
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:19,062'); seek(319.0)">
              etcetera, around all of that. So that's operationalizing data
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:22,542'); seek(322.0)">
              science. That's the simplest mlops
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:26,526'); seek(326.0)">
              flow that you can think of. You can also map this into
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:30,446'); seek(330.0)">
              a classic DevOps Infinity loop. So the typical
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:34,134'); seek(334.0)">
              DevOps Infinity loop talks about your code, build, test,
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:37,646'); seek(337.0)">
              plan, then release, deploy, operate, monitor,
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:40,590'); seek(340.0)">
              and doing this in a loop consistently over long periods of time.
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:44,662'); seek(344.0)">
              So similarly for mlops, it kind of starts with
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:48,950'); seek(348.0)">
              having data preparation. You're going to prepare the data,
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:52,428'); seek(352.0)">
              you're going to train the model. Again, that's a build and a test part of
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:55,260'); seek(355.0)">
              it. Then the release will go into model registry.
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:59,524'); seek(359.0)">
              You're going to then monitor the model performance.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:02,724'); seek(362.0)">
              You're going to deploy the model, monitor the performance,
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:05,404'); seek(365.0)">
              and all of this altogether would be continuous training
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:09,436'); seek(369.0)">
              and testing of the model. Right? Enough about theory.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:13,212'); seek(373.0)">
              What I want to talk more about is a
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:17,352'); seek(377.0)">
              use case that we worked on. So this is a production work that we worked
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:20,640'); seek(380.0)">
              on. I'm going to cover the use case at
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:23,728'); seek(383.0)">
              a high level. I'm going to talk about what work we did,
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:27,120'); seek(387.0)">
              how we applied the DevOps and mlops practices,
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:30,272'); seek(390.0)">
              best practices in production, and the kind of issues
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:33,976'); seek(393.0)">
              that we faced during the production journey.
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:37,048'); seek(397.0)">
              Right. So let's start with the case study that we
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:40,408'); seek(400.0)">
              had in mind. We were working on.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:44,484'); seek(404.0)">
              We were working with a company which was building Ekyc SaaS
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:48,244'); seek(408.0)">
              APIs, which was accessible to B two
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:52,020'); seek(412.0)">
              B and B two C customers. This needed to scale
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:55,436'); seek(415.0)">
              up to 2 million API requests per day to the model. Now,
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:06:59,428'); seek(419.0)">
              the SaaS APIs, these three or four APIs
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:02,884'); seek(422.0)">
              that we had, one was face matching API. Imagine you
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:06,884'); seek(426.0)">
              provide two images to the model. You're going to have to match
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:11,274'); seek(431.0)">
              the face between two images and model outputs
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:14,970'); seek(434.0)">
              a score between say zero and one. Based on the
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:18,402'); seek(438.0)">
              matching score, you can decide if the two images, if the two people
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:21,666'); seek(441.0)">
              in this images are same or not, and that's a face matching API,
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:26,018'); seek(446.0)">
              then we have face liveness detection, which is if you
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:29,474'); seek(449.0)">
              have an image of a face, is this face of a
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:33,050'); seek(453.0)">
              live person, or is this a face of a non alive person?
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:37,094'); seek(457.0)">
              Then we had an OCR or optical character recognition
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:40,414'); seek(460.0)">
              from an image. For example, you upload a photo of a passport or any
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:44,630'); seek(464.0)">
              identity card, you would be able to extract the text
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:48,430'); seek(468.0)">
              information from it. So that imagine this Kyc
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:52,254'); seek(472.0)">
              use case for an insurance or a telecom or any other domain.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:55,830'); seek(475.0)">
              People would have to manually enter a lot of information for the user, like their
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:07:59,806'); seek(479.0)">
              name, their date of birth, their address, etcetera.
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:02,854'); seek(482.0)">
              All of this information gets captured via the OCR
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:07,032'); seek(487.0)">
              API and you get that information returned via response in a structured fashion.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:11,328'); seek(491.0)">
              So that eliminates having to type and mistype information.
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:16,344'); seek(496.0)">
              So similarly, we had some other small APIs as well, which I'll ignore for now.
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:20,272'); seek(500.0)">
              So fundamentally, we had this ML system,
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:24,200'); seek(504.0)">
              and the architecture of that system, along with other components was something
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:27,616'); seek(507.0)">
              like this. So it was served mainly for
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:31,172'); seek(511.0)">
              b two b use cases. We also had some things for b, two c,
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:34,164'); seek(514.0)">
              but again, we'll ignore that for now. So imagine from a b two b use
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:37,412'); seek(517.0)">
              case. I'm an insurance or a telecom company.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:40,820'); seek(520.0)">
              I have my own app, and there is a client SDK in that app
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:44,604'); seek(524.0)">
              that I need to install and run. I need to package the client SDK
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:48,284'); seek(528.0)">
              as part of my app, now this SDK talks to our backend
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:51,804'); seek(531.0)">
              APIs, which are these ML APIs exposed via an
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:56,028'); seek(536.0)">
              HTTP API, for example. So it connects to for example load balancer.
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:00,356'); seek(540.0)">
              We have an API layer then to
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:04,340'); seek(544.0)">
              be able to serve these models. We had a RabbitMQ as another like a
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:08,148'); seek(548.0)">
              queue mechanism where we would push messages.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:12,012'); seek(552.0)">
              For example, we want to map match two images,
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:15,660'); seek(555.0)">
              right? Face recognition or face matching across two images.
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:18,988'); seek(558.0)">
              We would create a message in RabbitMQ, push that message on the
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:22,756'); seek(562.0)">
              rabbitmQ. And there is these background workers, which are these ML
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:26,268'); seek(566.0)">
              workers, which would run, which would accept message from the RabbitMQ.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:30,356'); seek(570.0)">
              They would do the processing, they would update the results in database.
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:33,812'); seek(573.0)">
              Maybe they would even save the results in a cache
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:36,972'); seek(576.0)">
              like redis, which we had. And the images themselves
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:40,524'); seek(580.0)">
              can be stored in a distributed file store. Could be minio,
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:44,108'); seek(584.0)">
              could be s, three things like that.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:47,504'); seek(587.0)">
              Essentially these workers would perform the bulk of the task and
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:50,632'); seek(590.0)">
              then the API would return the results to the user.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:53,864'); seek(593.0)">
              So this is the kind of architecture we had. Now what were
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:09:57,432'); seek(597.0)">
              the requirements from a slo point of view?
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:00,400'); seek(600.0)">
              So we set out for achieving at least
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:03,600'); seek(603.0)">
              like two nines of availability, that is, two nines of uptime
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:07,264'); seek(607.0)">
              during peak hours or during our business hours. Because we were
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:10,520'); seek(610.0)">
              dealing with a b two b company, there typically would be
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:13,920'); seek(613.0)">
              business hours. Typically stores would open at 09:00 a.m. In the morning and
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:17,712'); seek(617.0)">
              would go on till like 10:00 a.m. 10:00 p.m. In the night, for example.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:20,792'); seek(620.0)">
              Right, the local time. So we had promised like two lines of uptime
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:24,936'); seek(624.0)">
              during that. In terms of SLO, we obviously
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:29,072'); seek(629.0)">
              had to worry about costs and optimizing the costs as
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:32,192'); seek(632.0)">
              an important requirement. From SLo point of view, we hadn't defined specific
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:36,296'); seek(636.0)">
              metrics, but we'll get to that later. And then from an API point of view,
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:40,488'); seek(640.0)">
              these were synchronous APIs as far as the user and the SDK is
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:43,800'); seek(643.0)">
              concerned. So less than three second API latency for
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:47,176'); seek(647.0)">
              95th percentile. That was our goal that we had set out.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:51,344'); seek(651.0)">
              Now, given this, let's think about our architecture
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:10:55,272'); seek(655.0)">
              and we set out to build a cloud agnostic architecture.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:10:58,552'); seek(658.0)">
              I'll cover more of that soon. And why that is.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:02,104'); seek(662.0)">
              So, for example, for the storing of actual
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:05,904'); seek(665.0)">
              images, which were ephemeral for short time and whatnot, because again we're
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:09,408'); seek(669.0)">
              dealing with sensitive data. We were using s three
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:13,612'); seek(673.0)">
              if we are deployed on AWS, we were using gcs if we were deployed
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:17,180'); seek(677.0)">
              on GCP, and Minio if we were deployed on on
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:20,540'); seek(680.0)">
              premise. One of the reason was that we wanted to create,
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:25,404'); seek(685.0)">
              we wanted to have same code, could use different type
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:29,180'); seek(689.0)">
              of image store without having to change the code a lot
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:32,372'); seek(692.0)">
              or without no change to the code at all. Why? Because then
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:35,796'); seek(695.0)">
              we could deploy this entire stack on any cloud.
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:39,452'); seek(699.0)">
              We could run it on on premise. We could even run it on customers premise.
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:42,940'); seek(702.0)">
              We could run it on AWS, GCP or any other cloud for that matter.
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:46,884'); seek(706.0)">
              This was the main point that we wanted to achieve. That's why
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:50,660'); seek(710.0)">
              we set out to have a cloud agnostic architecture where we don't
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:54,004'); seek(714.0)">
              use a very cloud specific component and then we are tied to that particular cloud
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:11:58,540'); seek(718.0)">
              as a code dependency. Now that's
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:02,020'); seek(722.0)">
              for Binayo. For Redis we could either go with
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:05,548'); seek(725.0)">
              self hosted redis which is basically a cache for storing
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:09,636'); seek(729.0)">
              bunch of latest computation. That API can quickly return the
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:13,068'); seek(733.0)">
              results to the users. We could do this as a self hosted
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:16,948'); seek(736.0)">
              redis or elasticache if you are on one of the cloud providers.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:21,124'); seek(741.0)">
              Now for RabbitMQ, we again choose RabbitMQ
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:24,684'); seek(744.0)">
              purely so that we could run this on premise easily.
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:27,812'); seek(747.0)">
              And if you were on the cloud, we could use something like
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:31,192'); seek(751.0)">
              sqs or like equivalent in India cloud for
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:35,680'); seek(755.0)">
              GPU's and workers, which were predominantly GPU workload,
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:39,872'); seek(759.0)">
              we would use them on one of the cloud providers,
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:43,384'); seek(763.0)">
              or we could get our own custom GPU's, et cetera. For this
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:46,904'); seek(766.0)">
              production use case we were on one of the cloud providers and so we
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:50,368'); seek(770.0)">
              use most of the cloud components, but our code was such that it was
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:54,368'); seek(774.0)">
              not tightly coupled to cloud at all. And for database, again,
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:12:58,664'); seek(778.0)">
              we could use postgres. It could be RDS or cloud
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:02,144'); seek(782.0)">
              SQL or something else depending on the cloud provider.
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:07,944'); seek(787.0)">
              We fundamentally had Nomad as the orchestrator which would orchestrate
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:11,808'); seek(791.0)">
              all the deployments and scaling
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:15,616'); seek(795.0)">
              of components. Back then we were not using kubernetes purely,
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:19,616'); seek(799.0)">
              again from a simplicity point of view that we wanted to deploy this whole
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:23,504'); seek(803.0)">
              stack with the orchestrator on premise and
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:27,244'); seek(807.0)">
              we didn't want to in the team. We did not have a lot
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:30,420'); seek(810.0)">
              of Kubernetes expertise to be able to manage self hosted kubernetes
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:34,700'); seek(814.0)">
              on premise ourselves. So that's where we chose Hashicorp stack,
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:38,484'); seek(818.0)">
              which is fairly single binary, easy to manage and
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:41,748'); seek(821.0)">
              easy to run, and we already had expertise in the team for that.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:45,884'); seek(825.0)">
              Why cloud agnostic? I think it's a very important point that I want to highlight
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:49,492'); seek(829.0)">
              because we were cloud agnostic, we could package the same wine
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:53,030'); seek(833.0)">
              in a different bottle, for example, so we
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:56,950'); seek(836.0)">
              could package the same stack and run it on any environment
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:00,430'); seek(840.0)">
              that we wanted to. We could do air gap environments if we had to,
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:04,390'); seek(844.0)">
              things like that. So this was the main reason why we
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:07,550'); seek(847.0)">
              went cloud agnostic. And I think one of the lessons to learn
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:11,246'); seek(851.0)">
              is to build more cloud agnostic systems. That way
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:15,174'); seek(855.0)">
              you're not tied to any of the cloud providers, although using
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:18,842'); seek(858.0)">
              cloud providers obviously simplifies a lot of things for you. But you would want
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:22,362'); seek(862.0)">
              to have your architecture and code not coupled
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:25,994'); seek(865.0)">
              with the cloud provider so that you can change freely and
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:30,106'); seek(870.0)">
              migrate to a different cloud if you want to without having to redo
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:33,370'); seek(873.0)">
              a lot of effort. What did our scaling journey and
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:37,778'); seek(877.0)">
              how did we go from zero requests to 2 million API requests
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:41,450'); seek(881.0)">
              per day? Let's talk about that. Obviously it wasn't zero
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:44,924'); seek(884.0)">
              request one day and 2 million the next day. It was a gradual
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:48,644'); seek(888.0)">
              scaling journey, something like this.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:51,524'); seek(891.0)">
              So we would roll out on few regions
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:54,732'); seek(894.0)">
              or few stores, and then we would slowly
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:14:58,492'); seek(898.0)">
              increment the traffic, we would observe the traffic and so on, so forth.
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:02,492'); seek(902.0)">
              So fundamentally from our scaling journey, I want to talk about
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:06,476'); seek(906.0)">
              four or five important points and then drill down on each one of them
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:09,764'); seek(909.0)">
              as we go through the talk. One is the elimination of single
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:13,624'); seek(913.0)">
              points of failure to be able to scale. We want to have
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:17,680'); seek(917.0)">
              zero or no single point of failure so that your system is more
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:21,600'); seek(921.0)">
              resilient to changes, resilient to failures. We also
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:25,264'); seek(925.0)">
              need to do good capacity planning so that you are able
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:28,912'); seek(928.0)">
              to scale up and down very easily and you can save on cloud
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:32,896'); seek(932.0)">
              costs. Otherwise, if it requires you to scale and you are
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:36,808'); seek(936.0)">
              having to do a changes to architecture, it causes problems.
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:40,552'); seek(940.0)">
              So having good capacity planning and how we went about that,
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:43,696'); seek(943.0)">
              I'll also cover that. Obviously, cost optimization and auto scaling goes
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:47,520'); seek(947.0)">
              hand in hand with capacity planning. So I'll cover that.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:50,976'); seek(950.0)">
              Then comes around a lot of operational aspects about
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:54,272'); seek(954.0)">
              deployments, observability, being able to debug something,
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:58,136'); seek(958.0)">
              dealing with production issues, stuff like that. And obviously all
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:01,840'); seek(961.0)">
              of this journey wasn't very straightforward.
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:04,920'); seek(964.0)">
              It was fraught with some challenges that we encountered. So I'm going
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:08,056'); seek(968.0)">
              to cover like two interesting challenges that we encountered along the way. So hopefully
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:11,640'); seek(971.0)">
              you all can learn from it. So in the next part
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:15,008'); seek(975.0)">
              of the talk, I'm going to take each one of these points and then go
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:18,024'); seek(978.0)">
              drill down on each one of them. So let's talk about eliminating
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:22,392'); seek(982.0)">
              single points of failure. We had
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:25,608'); seek(985.0)">
              this architecture and just showing the architecture here as, as a, in the background.
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:29,972'); seek(989.0)">
              So one of the things we did is we added high availability mode
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:33,628'); seek(993.0)">
              for RabbitMQ. What does that mean? It means we have queue
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:37,484'); seek(997.0)">
              replication. So whichever queue is there on one machine, it gets
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:41,012'); seek(1001.0)">
              replicated or mirrored onto the other machine. We were running RabbitmQ in
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:45,036'); seek(1005.0)">
              a three node cluster instead of a single single node, for example.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:48,812'); seek(1008.0)">
              We also had cross AZ deployment for Rabbitmq. So the
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:52,444'); seek(1012.0)">
              three nodes of RapidMQ, each one would run its own easy,
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:16:55,676'); seek(1015.0)">
              for example. Obviously this was
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:16:59,142'); seek(1019.0)">
              on premise or a setup where we wanted to host and
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:02,870'); seek(1022.0)">
              manage RabbitMQ ourselves. But if it were a cloud
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:06,774'); seek(1026.0)">
              managed service that we would use, we would use something like sqs, for example.
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:11,222'); seek(1031.0)">
              One of the other things that we did to eliminate single points of failure
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:15,118'); seek(1035.0)">
              is to run ML workloads in multiple azs.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:19,046'); seek(1039.0)">
              Now back then we had only two azs where we
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:23,010'); seek(1043.0)">
              could have ML GPU's available. The third zone
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:27,098'); seek(1047.0)">
              from the cloud provider did not yet provide the GPU's.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:30,458'); seek(1050.0)">
              So we had to tweak our logic and deployment and
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:34,178'); seek(1054.0)">
              automation to be able to spin up and load balance between these two acs.
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:38,266'); seek(1058.0)">
              So we would have to fix auto scaling, we would have to fix deployment
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:42,114'); seek(1062.0)">
              automation to be able to run workloads
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:45,490'); seek(1065.0)">
              only on two zones instead of three. For most of the other two cloud
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:17:49,064'); seek(1069.0)">
              components or most of the other components in the architecture, we would have workloads
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:17:53,112'); seek(1073.0)">
              run on all three acs. Other thing that
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:17:56,992'); seek(1076.0)">
              we did is wherever possible, we used SaaS offering for
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:00,704'); seek(1080.0)">
              some of the important stateful systems, like databases,
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:04,120'); seek(1084.0)">
              for example, redis and postgres,
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:07,104'); seek(1087.0)">
              just to make sure that we don't have to manage and
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:10,352'); seek(1090.0)">
              scale those components. Also. And managing and scaling ML
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:13,816'); seek(1093.0)">
              was one of the bigger challenges. So we wanted to offload some of the lower
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:17,752'); seek(1097.0)">
              hanging fruits to the cloud providers. Fundamentally, the idea again is that
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:21,982'); seek(1101.0)">
              scaling and managing stateful components is bit hard and
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:25,966'); seek(1105.0)">
              stateless is much more easier. So wherever possible,
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:29,454'); seek(1109.0)">
              it's easy to automate stateless application scaling, component scaling
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:33,438'); seek(1113.0)">
              and stateful becomes difficult. So that's
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:36,950'); seek(1116.0)">
              on the eliminating single point of failure.
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:40,030'); seek(1120.0)">
              Let's talk about capacity planning. So always when
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:43,918'); seek(1123.0)">
              you think about capacity planning, you think of the bottleneck,
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:47,046'); seek(1127.0)">
              because if the strength of the link is
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:18:50,338'); seek(1130.0)">
              the strength of the weakest component in the link, for example,
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:18:53,890'); seek(1133.0)">
              strength of the chain is the strength of the weakest component in the chain.
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:18:57,050'); seek(1137.0)">
              So you want to find out what's the weakest component and improve
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:00,706'); seek(1140.0)">
              the strength of that component. So for example, if you think about various components
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:04,722'); seek(1144.0)">
              from the architecture, we have API, which is simple
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:08,650'); seek(1148.0)">
              API which is does talk to database and get the results
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:11,922'); seek(1151.0)">
              from database. For example, there is database, which is stateful
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:15,306'); seek(1155.0)">
              component. It could be redis, rabbit, postgres, etcetera,
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:18,782'); seek(1158.0)">
              then mlworkers and something else.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:22,166'); seek(1162.0)">
              So where do we think is the bottleneck.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:25,318'); seek(1165.0)">
              Obviously it was on the mlworker side, because that's the component
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:28,910'); seek(1168.0)">
              which takes most time in the request path. So we set out
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:32,782'); seek(1172.0)">
              to figure out, for example, how many ML model requests
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:36,654'); seek(1176.0)">
              can a single node handle. So for example,
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:39,718'); seek(1179.0)">
              if you have a single node with say one gpu with 16 gigs of
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:43,526'); seek(1183.0)">
              GPU memory and a GPU with whatever
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:19:46,866'); seek(1186.0)">
              few cores, how many workers can I run on that? And how many
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:19:50,282'); seek(1190.0)">
              requests per second or per hour can I get out of that?
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:19:54,754'); seek(1194.0)">
              Now, each model, ML model may give
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:19:58,170'); seek(1198.0)">
              us different results. So for example, face matching may be faster than OCR,
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:02,162'); seek(1202.0)">
              or face liveness detection could be faster than face matching,
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:06,162'); seek(1206.0)">
              for example. So we would run load test and
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:10,162'); seek(1210.0)">
              we would run each of these models, each of the
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:13,650'); seek(1213.0)">
              nodes, and we would run them via a load test to be able to find
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:17,090'); seek(1217.0)">
              the maximum throughput that we can get over a long period of time,
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:20,474'); seek(1220.0)">
              say an hour or two, for example. So again,
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:24,170'); seek(1224.0)">
              I've broken this down into more detail and even more generic format
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:27,786'); seek(1227.0)">
              in another talk that I gave, which is optimizing application performance.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:31,330'); seek(1231.0)">
              How do you go about it from first principle? So if you're interested, you can
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:34,730'); seek(1234.0)">
              check that out. I'll provide the link in the description, hopefully cost
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:39,176'); seek(1239.0)">
              optimization auto scaling, that is one of the pet peeves given the
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:20:42,960'); seek(1242.0)">
              current market scenario. So mostly you will have seen, if you
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:20:46,736'); seek(1246.0)">
              use GPU in cloud, it costs a lot. So how do you go
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:20:50,312'); seek(1250.0)">
              about optimizing and auto scaling? So you
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:20:54,288'); seek(1254.0)">
              have to think about what is the parameter on which you can auto scale like,
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:20:57,552'); seek(1257.0)">
              is it the utilization of CPU or GPU? Could it
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:00,928'); seek(1260.0)">
              be based on memory utilization or number of incoming requests,
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:04,592'); seek(1264.0)">
              for example? Or it could be depth of the queue in case
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:08,116'); seek(1268.0)">
              we were using rapid MQ. So could it be queue depth or
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:11,628'); seek(1271.0)">
              something else? Now we kind of used a combination
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:15,532'); seek(1275.0)">
              of some of these components. So I'll talk about how we went about.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:19,396'); seek(1279.0)">
              So this is the cost optimization auto scaling,
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:22,796'); seek(1282.0)">
              like our auto scaler, how it works, right? So on the left you
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:25,980'); seek(1285.0)">
              have top left you have a current request rate. This is
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:29,988'); seek(1289.0)">
              the graph where we would have, what's the number of requests we are getting over
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:34,784'); seek(1294.0)">
              last 20 or 30 minutes interval? And we
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:38,768'); seek(1298.0)">
              would have a capacity predictor component which would run every 20 minutes,
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:21:43,000'); seek(1303.0)">
              which would fetch this request rate, or it would have this information
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:21:46,744'); seek(1306.0)">
              and it would also get the current node count or current worker
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:21:50,432'); seek(1310.0)">
              count. So again, imagine we are running these gpu's on workers.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:21:54,112'); seek(1314.0)">
              What's the current number of workers that we have currently? So you would
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:21:57,630'); seek(1317.0)">
              get the current workload, you would get the current
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:01,518'); seek(1321.0)">
              request count. And based on that, based on
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:05,062'); seek(1325.0)">
              the request number of requests and the growth rate of that,
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:09,094'); seek(1329.0)">
              the capacitor, the capacity predictor would kind
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:12,678'); seek(1332.0)">
              of predict, using just simple linear regression, the desired
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:16,358'); seek(1336.0)">
              node count. So for example, if we, for example, open the stores at
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:19,950'); seek(1339.0)">
              09:00 a.m. And we know that we at least need like
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:23,302'); seek(1343.0)">
              50 machines at that point, so we would have time based auto scaling
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:26,730'); seek(1346.0)">
              and we should just spin up 50 machines. You have them ready
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:22:30,082'); seek(1350.0)">
              at like before ten minutes. The stores open, right?
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:33,098'); seek(1353.0)">
              But the stores open and you continue to see increase in traffic,
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:22:36,762'); seek(1356.0)">
              you would want to spin up more nodes. If you see decrease in traffic,
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:40,114'); seek(1360.0)">
              typically during lunchtime, you would want to spin down a couple of nodes,
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:22:43,634'); seek(1363.0)">
              for example, right? So we had this capacity predictor component
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:22:47,314'); seek(1367.0)">
              which would take this request rate of growth or
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:22:50,698'); seek(1370.0)">
              rate of decline, you would take the current node count, and based
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:22:54,916'); seek(1374.0)">
              on the linear regression math and some other parameters that we talked about,
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:22:58,916'); seek(1378.0)">
              it would predict the desired node count. Now this is
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:02,660'); seek(1382.0)">
              the count that would go as an input to auto scalar. The autoscaler
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:06,252'); seek(1386.0)">
              would then do couple of things. It would update Nomad.
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:09,924'); seek(1389.0)">
              It would tell Nomad, hey, can you please spin up those many number
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:13,132'); seek(1393.0)">
              of components or those many number of workers? So the Nomad
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:17,076'); seek(1397.0)">
              update cluster configuration would run and the Nomad would correspondingly
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:20,880'); seek(1400.0)">
              spin up more nodes. It will deploy the model on those nodes,
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:23:24,224'); seek(1404.0)">
              etcetera, and it will also update, for example, if you're using slack.
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:27,264'); seek(1407.0)">
              So it will also update us on slack. That, yeah, we've got two more
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:30,632'); seek(1410.0)">
              nodes added, or we've got two nodes destroyed because it was less traffic
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:23:34,704'); seek(1414.0)">
              time. For example, one of the reason we built
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:23:38,056'); seek(1418.0)">
              this kind of a system is so that we have a manual override
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:23:42,416'); seek(1422.0)">
              at any point. If we knew that there is a big campaign going on,
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:23:45,600'); seek(1425.0)">
              are we going to scale, we are going to have to scale the machines
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:23:49,752'); seek(1429.0)">
              at a particular time or due to some other kind of business
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:23:53,584'); seek(1433.0)">
              constraints, we would be able to manually overwrite that value and
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:23:57,416'); seek(1437.0)">
              change the configuration to be able to spin up those many
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:00,728'); seek(1440.0)">
              number of nodes. This gave us a lot of control and
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:04,064'); seek(1444.0)">
              we've been using this autoscaler for years now and it's just been
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:07,832'); seek(1447.0)">
              working very well. It's a very simple, less effort work,
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:11,352'); seek(1451.0)">
              but it just works flawlessly for us.
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:15,204'); seek(1455.0)">
              So we've never had issues with auto scalar as such.
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:24:18,844'); seek(1458.0)">
              There's more that we've written about in our, one of the recent blog posts
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:24:23,044'); seek(1463.0)">
              and case studies. You should check it out if you're interested in this kind of
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:24:26,380'); seek(1466.0)">
              stuff. Now, our journey wasn't
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:24:30,108'); seek(1470.0)">
              smooth, right. It was fraught with some issues and errors.
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:24:33,516'); seek(1473.0)">
              So I'm going to talk about some of the issues we encountered and how we
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:24:36,836'); seek(1476.0)">
              navigated those, and what kind of impact it had on downtime,
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:24:40,300'); seek(1480.0)">
              slo, etcetera. So one of the issue that we encountered was
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:24:44,692'); seek(1484.0)">
              GPU utilization in Nomad. For example, imagine this
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:24:49,236'); seek(1489.0)">
              top box to be a GPU. It has GPU cache memory
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:24:52,948'); seek(1492.0)">
              and cores. We would be able to run a ML worker
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:24:56,692'); seek(1496.0)">
              using Nomad, using Docker driver. So we would run one
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:24:59,892'); seek(1499.0)">
              ML worker per GPU. What we notice is that the
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:03,460'); seek(1503.0)">
              GPU wasn't utilized fully. It was with one worker, it was just 20%
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:07,456'); seek(1507.0)">
              utilized, and lot of other
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:11,224'); seek(1511.0)">
              resources of that machine were just left unused. We did load
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:25:15,264'); seek(1515.0)">
              tests to be able to figure out how much throughput we can get out of
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:25:18,448'); seek(1518.0)">
              a single machine running single ML worker. It's a
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:25:21,688'); seek(1521.0)">
              docker container, and it wasn't impressive with
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:25:25,456'); seek(1525.0)">
              this. If we just ran with this kind of
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:25:28,800'); seek(1528.0)">
              hardware, our cost was going through the roof, and we
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:25:32,488'); seek(1532.0)">
              had to really figure out how do we fix this. So we
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:25:37,040'); seek(1537.0)">
              dug deep into Nomad. GitHub issues, some pull requests,
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:25:41,504'); seek(1541.0)">
              some parts into reading obscure documentation and figuring out.
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:25:45,104'); seek(1545.0)">
              And fundamentally later, what we discovered is one
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:25:48,912'); seek(1548.0)">
              workaround which we can use, which is instead of using docker
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:25:52,520'); seek(1552.0)">
              driver, if we use raw exec driver, which allows you
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:25:56,248'); seek(1556.0)">
              to run any kind of component, it doesn't
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:25:59,560'); seek(1559.0)">
              guarantee any. Like you
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:02,618'); seek(1562.0)">
              have to worry about a lot of scheduling yourself when you use raw exec driver.
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:06,826'); seek(1566.0)">
              But with raw exec driver, you could run docker compose. And we
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:10,370'); seek(1570.0)">
              spin up multiple docker containers using docker
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:26:13,642'); seek(1573.0)">
              compose via raw exec driver in Nomad. So what that allowed
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:26:17,178'); seek(1577.0)">
              us to do is the same GPU machine,
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:26:20,274'); seek(1580.0)">
              we could run not one, but four workers,
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:26:23,946'); seek(1583.0)">
              right? Using docker composer. Now that led us
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:26:27,788'); seek(1587.0)">
              to having about 80% utilization.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:26:31,444'); seek(1591.0)">
              It straight away brought our cost down by four x. So imagine if you
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:26:34,916'); seek(1594.0)">
              had to have $100,000 per month on just on GPU,
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:26:39,476'); seek(1599.0)">
              we would slash it by one fourth directly and have like 400%
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:26:43,572'); seek(1603.0)">
              impact, essentially. So this was one way we solved it.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:26:47,260'); seek(1607.0)">
              This is as of today, last I checked, it is still open,
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:26:50,620'); seek(1610.0)">
              this issue, and you would see this pull request, or this
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:26:53,892'); seek(1613.0)">
              issue is still open. And one of the solutions or
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:26:57,122'); seek(1617.0)">
              workarounds that we've used, I've highlighted that here for
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:27:01,218'); seek(1621.0)">
              you to look at. One thing that we noticed, if you use raw exec
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:05,034'); seek(1625.0)">
              driver, you're going to have to worry about
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:27:08,426'); seek(1628.0)">
              the shutdown part of it yourself.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:27:11,442'); seek(1631.0)">
              So otherwise, nomad generally handles sick
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:27:15,186'); seek(1635.0)">
              term, and it handles the graceful termination of resources
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:27:19,866'); seek(1639.0)">
              or components. In this case you will have to have
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:27:23,138'); seek(1643.0)">
              waits and timeouts and you have to do some magic
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:27:26,714'); seek(1646.0)">
              and work. You have to put in some work to be able to tear
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:27:30,034'); seek(1650.0)">
              down the components correctly. So we invested in that and we wrote
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:27:33,530'); seek(1653.0)">
              some bash script to be able to run,
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:27:37,114'); seek(1657.0)">
              which could run the components and also tear them down easily when we
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:27:40,850'); seek(1660.0)">
              wanted to. One other issue that we encountered is
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:27:44,722'); seek(1664.0)">
              high latency. So again we have
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:27:48,194'); seek(1668.0)">
              steady traffic, new regions or new stores are opening up and
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:27:52,122'); seek(1672.0)">
              we are getting them migrated to use our APIs. And the rollout
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:27:55,770'); seek(1675.0)">
              is happening suddenly on one of the days. What we see is
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:27:59,010'); seek(1679.0)">
              that more than 25 2nd response time.
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:02,506'); seek(1682.0)">
              Now imagine like our sla is less than 3 seconds response
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:28:05,682'); seek(1685.0)">
              time for 90th percentile or 95th percentile.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:28:09,122'); seek(1689.0)">
              Now we suddenly get 25 seconds of response time.
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:28:12,490'); seek(1692.0)">
              That's unacceptable. So we debug
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:28:16,722'); seek(1696.0)">
              this issue. We try to find out, oh, there must be some problem with Mlworker,
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:28:20,194'); seek(1700.0)">
              right? Because that's the slowest component in the chain. But we realize
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:28:24,154'); seek(1704.0)">
              that there is no queue depth. The workers are doing their job,
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:28:27,682'); seek(1707.0)">
              there is no extra jobs for them to be processed.
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:28:31,010'); seek(1711.0)">
              So the worker scaling or auto scaling is not a problem. But then
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:28:35,194'); seek(1715.0)">
              why do we have this much latency on the API side?
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:28:39,546'); seek(1719.0)">
              There is no processing lag on the worker. Why do we have that?
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:28:43,504'); seek(1723.0)">
              We spent a lot of time debugging this issue and ultimately
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:28:47,320'); seek(1727.0)">
              we discovered that the number of go routines that we had on
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:28:51,088'); seek(1731.0)">
              the API side which would process the results from RabbitMQ.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:28:54,616'); seek(1734.0)">
              So for example, our flow was that
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:28:58,384'); seek(1738.0)">
              we would have the client SDK call our NLB or AlB.
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:02,584'); seek(1742.0)">
              The API would send a message to Rabbitmq. The workers would
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:29:06,192'); seek(1746.0)">
              then consume that message, produce the result.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:29:09,756'); seek(1749.0)">
              It would be updated on database and it will also send another
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:29:13,140'); seek(1753.0)">
              message on to RabbitMQ that the processing is done and some other metadata.
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:29:17,436'); seek(1757.0)">
              Now there is this workers that were running on the API which
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:29:21,532'); seek(1761.0)">
              would then consume this metadata via the RabbitMQ message
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:29:25,020'); seek(1765.0)">
              and then it would return the response to the user or do something else.
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:29:28,132'); seek(1768.0)">
              Right now that's the part. We were just running five coroutines.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:29:32,212'); seek(1772.0)">
              Now what we realize is that we were also running three nodes or three
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:29:36,276'); seek(1776.0)">
              docker containers for workers. And we never faced any issue with worker,
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:29:40,548'); seek(1780.0)">
              the cpu utilized API layer. Sorry, we never faced
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:29:44,460'); seek(1784.0)">
              any cpu utilization or any issue with API layer.
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:29:47,252'); seek(1787.0)">
              But when the request count increased,
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:29:50,900'); seek(1790.0)">
              what we realized is that these five go routines were not enough and
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:29:54,364'); seek(1794.0)">
              which is where we're actually seeing queue depth on the API side.
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:29:57,860'); seek(1797.0)">
              The API go routines that we're trying to consume from Arabic MQ.
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:01,620'); seek(1801.0)">
              That's where we saw the queue depth. And it took us a while to figure
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:30:04,934'); seek(1804.0)">
              this out and to fix this, because our preconceived notion and the first response
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:30:09,174'); seek(1809.0)">
              was to look at workers as a problem. Now we
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:30:12,758'); seek(1812.0)">
              change the go routines to 30 and suddenly the flip,
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:30:16,118'); seek(1816.0)">
              the traffic goes normal and we have less than 3 seconds response
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:30:19,510'); seek(1819.0)">
              time. So that really shows how you want to,
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:30:23,310'); seek(1823.0)">
              how you should understand the components and how data pipeline works,
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:30:26,878'); seek(1826.0)">
              and you need to really think about where the latency is being introduced.
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:30:31,072'); seek(1831.0)">
              After that, we also added a lot of other observability
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:30:35,184'); seek(1835.0)">
              signals and metrics to be able to track this issue and further any
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:30:38,760'); seek(1838.0)">
              other issues even even further. So we invested a lot in
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:30:42,944'); seek(1842.0)">
              tracking cross service latencies
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:30:47,544'); seek(1847.0)">
              and stuff like that. So I've written about this as
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:30:50,976'); seek(1850.0)">
              a form of pragmatic engineering story.
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:30:54,244'); seek(1854.0)">
              You can check it out on Twitter if you follow me there. I want to
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:30:58,148'); seek(1858.0)">
              conclude this session by talking about some of the lessons that we learned along the
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:31:01,780'); seek(1861.0)">
              way. So one is for ML workloads,
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:31:05,340'); seek(1865.0)">
              it goes without saying, but it's worth repeating that.
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:31:08,732'); seek(1868.0)">
              Data quality and training the model is super important. Like a lot
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:31:12,532'); seek(1872.0)">
              of it depends on the quality of data and the volume of the
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:31:15,748'); seek(1875.0)">
              data and how you train your models, how you carve out
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:31:19,332'); seek(1879.0)">
              the test data versus the data that you
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:31:23,102'); seek(1883.0)">
              run the model on and you want to check and how you label and
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:31:27,310'); seek(1887.0)">
              data. So a lot of it depends on data quality and training in
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:31:31,950'); seek(1891.0)">
              our case, and I would highly recommend to use cloud agnostic architecture to
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:31:35,870'); seek(1895.0)">
              be able to build scalable systems that can
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:31:39,934'); seek(1899.0)">
              give you that flexibility to deploy them on any type of workload or any type
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:31:43,718'); seek(1903.0)">
              of underlying cloud. For us, that has been the massive win.
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:31:47,446'); seek(1907.0)">
              And I would highly encourage you to think about building cloud agnostic systems.
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:31:52,104'); seek(1912.0)">
              One of the things that I've seen majorly is people do not
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:31:55,416'); seek(1915.0)">
              treat operational workloads as first class citizens. They just slap on
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:31:59,384'); seek(1919.0)">
              top of the existing software and just say that yeah,
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:32:02,904'); seek(1922.0)">
              somebody will manage this. But treating operational work as
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:32:06,304'); seek(1926.0)">
              first class citizen really helps in automation of a lot
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:32:09,688'); seek(1929.0)">
              of your day to day tasks, and it gives you,
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:32:13,368'); seek(1933.0)">
              especially when you are first launching, you should really treat production,
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:32:17,580'); seek(1937.0)">
              maintenance and operations as a first class citizen in
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:32:21,420'); seek(1941.0)">
              software building and delivery process.
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:32:24,084'); seek(1944.0)">
              Lastly, from a team point of view, we had really good collaboration
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:32:28,692'); seek(1948.0)">
              with various teams, data scientists, the mobile
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:32:32,444'); seek(1952.0)">
              SDK team, the backend engineering team, sres,
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:32:35,868'); seek(1955.0)">
              and even the business folks. So for example, whenever there is a new campaign
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:32:39,810'); seek(1959.0)">
              or we would get some info from business team that a new region or
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:32:43,442'); seek(1963.0)">
              new bunch of stores are being onboarded,
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:32:46,634'); seek(1966.0)">
              preemptively scaled the nodes to be able to handle that traffic,
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:32:50,434'); seek(1970.0)">
              for example. So being able to closely collaborate with data scientists,
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:32:54,386'); seek(1974.0)">
              we also optimize, for example, one of the case, we also optimize
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:32:57,746'); seek(1977.0)">
              the docker image size for the models. Earlier, we would have all
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:33:01,330'); seek(1981.0)">
              the versions of the models in our final
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:33:05,166'); seek(1985.0)">
              Docker image, which would mean the Docker image itself would be like tens
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:33:08,590'); seek(1988.0)">
              of gb. So we'll have ten GB Docker image later. We optimize that
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:33:12,686'); seek(1992.0)">
              with close collaboration with the data scientists to less than like three gb
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:33:16,734'); seek(1996.0)">
              of model. So that just speeds up a lot of
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:33:20,598'); seek(2000.0)">
              warming up of nodes. It just speeds up the deployment process and
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:33:25,046'); seek(2005.0)">
              the time it takes for nodes to be ready to serve traffic. So ensuring
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:33:29,590'); seek(2009.0)">
              good collaboration between teams is super, super important.
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:33:33,314'); seek(2013.0)">
              I think that's it from my side, what I would say is connect with
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:33:36,842'); seek(2016.0)">
              me on LinkedIn, Twitter, et cetera. You should check out our go
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:33:40,570'); seek(2020.0)">
              or SRE bootcamp that we have built at 120 n, along with
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:33:44,570'); seek(2024.0)">
              the software engineering stories that are right. Here's the QR code. You can
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:33:48,090'); seek(2028.0)">
              scan this and you can check this out.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Chinmay%20Naik%20-%20Conf42%20Site%20Reliability%20Engineering%20%28SRE%29%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Chinmay%20Naik%20-%20Conf42%20Site%20Reliability%20Engineering%20%28SRE%29%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #E36414;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/sre2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #E36414;">
                <i class="fe fe-grid me-2"></i>
                See all 26 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Chinmay%20Naik_sre.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Chinmay Naik
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Founder @ One2N
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/chinmay185/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Chinmay Naik's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/@chinmay185" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Chinmay Naik's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @chinmay185"
                  data-url="https://www.conf42.com/sre2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/sre2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-primary" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a cappucino</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Keynotes
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Weekly newsletter
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Site Reliability Engineering"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Keynotes
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Priority access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Weekly newsletter
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Live events</b>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community events weekly
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Free trial
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community <i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>