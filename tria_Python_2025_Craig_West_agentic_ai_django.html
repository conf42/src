<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Implementing Agentic AI Solutions in Python from scratch</title>
    <meta name="description" content="Get inspired by fellow Pythonistas, Snakes and Pandas united!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Craig%20West_python.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Implementing Agentic AI Solutions in Python from scratch | Conf42"/>
    <meta property="og:description" content="The use of AI and AI Agents in everyday Django can be viewed as `AI as API` where we can create on the Django side powerful agentic APIs."/>
    <meta property="og:url" content="https://conf42.com/Python_2025_Craig_West_agentic_ai_django"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/PLATFORM2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Platform Engineering 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-09-04
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/platform2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #69811f;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Python 2025 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Get inspired by fellow Pythonistas, Snakes and Pandas united!
 -->
              <script>
                const event_date = new Date("2025-02-06T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2025-02-06T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "BaWTWtKP7nE"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBo176Is4wP2F6UCB0yEkWO" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello, everyone, and welcome to the implementing agentic AI", "timestamp": "00:00:00,200", "timestamp_s": 0.0}, {"text": "solutions in Python from scratch.", "timestamp": "00:00:03,540", "timestamp_s": 3.0}, {"text": "Here is the repo, and you have full access to it.", "timestamp": "00:00:06,090", "timestamp_s": 6.0}, {"text": "We have all the code samples.", "timestamp": "00:00:09,589", "timestamp_s": 9.0}, {"text": "I\u0027m using my notes here, which I\u0027ll have access to, as well as an", "timestamp": "00:00:11,550", "timestamp_s": 11.0}, {"text": "HTML version, if you so need it.", "timestamp": "00:00:14,360", "timestamp_s": 14.0}, {"text": "So who am I?", "timestamp": "00:00:16,910", "timestamp_s": 16.0}, {"text": "I\u0027m one of us, a regular Pythonista.", "timestamp": "00:00:18,000", "timestamp_s": 18.0}, {"text": "I was in tech in the early 2000s as a business information architect", "timestamp": "00:00:20,400", "timestamp_s": 20.0}, {"text": "and certified Microsoft SQL Server DBA for about four years.", "timestamp": "00:00:24,250", "timestamp_s": 24.0}, {"text": "And then I returned in 2017 via WordPress and JavaScript.", "timestamp": "00:00:28,289", "timestamp_s": 28.0}, {"text": "Frameworks.", "timestamp": "00:00:31,795", "timestamp_s": 31.0}, {"text": "Moving to Python and machine learning in 2021.", "timestamp": "00:00:32,735", "timestamp_s": 32.0}, {"text": "Currently, I\u0027m working on a project, AI powered knowledge systems, building", "timestamp": "00:00:36,544", "timestamp_s": 36.0}, {"text": "a book framework similar to my Python.", "timestamp": "00:00:39,824", "timestamp_s": 39.0}, {"text": "test full stack, which is here at pytestcookbook.", "timestamp": "00:00:42,769", "timestamp_s": 42.0}, {"text": "com.", "timestamp": "00:00:45,339", "timestamp_s": 45.0}, {"text": "I\u0027ve also got some useful notes on Django full stack testing, and here", "timestamp": "00:00:46,110", "timestamp_s": 46.0}, {"text": "is my main project at the moment.", "timestamp": "00:00:50,950", "timestamp_s": 50.0}, {"text": "I\u0027m based in Brighton in the UK, down on the south coast,", "timestamp": "00:00:53,809", "timestamp_s": 53.0}, {"text": "and here is our lovely beach.", "timestamp": "00:00:56,370", "timestamp_s": 56.0}, {"text": "And I\u0027m a volunteer coach at Cobar.", "timestamp": "00:00:58,509", "timestamp_s": 58.0}, {"text": "io, which I find very rewarding.", "timestamp": "00:01:01,319", "timestamp_s": 61.0}, {"text": "We meet every two weeks.", "timestamp": "00:01:02,490", "timestamp_s": 62.0}, {"text": "And I\u0027ve just got myself a new Red Fox Labrador pup, Leo,", "timestamp": "00:01:04,970", "timestamp_s": 64.0}, {"text": "much earlier than planned.", "timestamp": "00:01:08,430", "timestamp_s": 68.0}, {"text": "And locally, we have a red fox that is quite tame, and seems to check", "timestamp": "00:01:10,205", "timestamp_s": 70.0}, {"text": "Leo out, and they both stare at each other, wondering who\u0027s who.", "timestamp": "00:01:15,525", "timestamp_s": 75.0}, {"text": "My first computer was in 1979, and it was a paper tape reader, with", "timestamp": "00:01:18,995", "timestamp_s": 78.0}, {"text": "a teletype printer for output, and cut and paste was cut and paste.", "timestamp": "00:01:22,795", "timestamp_s": 82.0}, {"text": "So what are AI agents?", "timestamp": "00:01:27,825", "timestamp_s": 87.0}, {"text": "The word agent is a subject of much discussion in academic circles, but", "timestamp": "00:01:30,205", "timestamp_s": 90.0}, {"text": "if we\u0027re looking at AI agents, we can see that Pydantic has its version, a", "timestamp": "00:01:35,565", "timestamp_s": 95.0}, {"text": "primary interface for interacting with LLMs, and an Anthropic also defines", "timestamp": "00:01:40,325", "timestamp_s": 100.0}, {"text": "workflow and agents, and Hugging Face says AI agents are programs where", "timestamp": "00:01:44,885", "timestamp_s": 104.0}, {"text": "LLM outputs control to workflow.", "timestamp": "00:01:49,105", "timestamp_s": 109.0}, {"text": "What we\u0027re going to do is we\u0027re going to look at examples of code to see what", "timestamp": "00:01:51,625", "timestamp_s": 111.0}, {"text": "AI agents are and what they can do.", "timestamp": "00:01:54,325", "timestamp_s": 114.0}, {"text": "Now, for example, if we look at this link here, we will see The range of", "timestamp": "00:01:57,235", "timestamp_s": 117.0}, {"text": "AI agents that are being created.", "timestamp": "00:02:02,504", "timestamp_s": 122.0}, {"text": "There\u0027s 46 character categories, 825, all different versions in different areas,", "timestamp": "00:02:04,364", "timestamp_s": 124.0}, {"text": "so it can be quite overwhelming to know which framework to use and what they are.", "timestamp": "00:02:11,604", "timestamp_s": 131.0}, {"text": "What\u0027s the aim of my talk?", "timestamp": "00:02:16,244", "timestamp_s": 136.0}, {"text": "Well, my talk is to a chain to achieve that, to demystify AI agents", "timestamp": "00:02:18,484", "timestamp_s": 138.0}, {"text": "and AI programming because it can seem like it\u0027s another different", "timestamp": "00:02:23,494", "timestamp_s": 143.0}, {"text": "world of development for us, ISTs.", "timestamp": "00:02:27,064", "timestamp_s": 147.0}, {"text": "And what I\u0027d like to propose is what if AI agents adjust Python code with a REST API", "timestamp": "00:02:29,454", "timestamp_s": 149.0}, {"text": "call, admittedly to a very magical API?", "timestamp": "00:02:35,104", "timestamp_s": 155.0}, {"text": "Then we could use day to day Python design patterns to handle the", "timestamp": "00:02:37,834", "timestamp_s": 157.0}, {"text": "responses we get back from these API calls to the AI, to the LLMs.", "timestamp": "00:02:40,944", "timestamp_s": 160.0}, {"text": "And so the main focus of this talk is to demystify and simplify, and not to", "timestamp": "00:02:45,944", "timestamp_s": 165.0}, {"text": "focus on actual real world applications.", "timestamp": "00:02:50,684", "timestamp_s": 170.0}, {"text": "And with that in mind, we don\u0027t need to fully graph the code this time round.", "timestamp": "00:02:54,114", "timestamp_s": 174.0}, {"text": "It may take a few minutes.", "timestamp": "00:02:57,734", "timestamp_s": 177.0}, {"text": "iterations to fully grasp it, so perhaps to look at the high level view", "timestamp": "00:02:59,574", "timestamp_s": 179.0}, {"text": "and to see how it is different from just regular Python and to realize", "timestamp": "00:03:04,114", "timestamp_s": 184.0}, {"text": "that actually AI agents are Python code with REST API calls to an LLM,", "timestamp": "00:03:08,904", "timestamp_s": 188.0}, {"text": "admittedly a very magical REST API.", "timestamp": "00:03:17,404", "timestamp_s": 197.0}, {"text": "In fact, what I\u0027d like to propose is that actually there\u0027s no real", "timestamp": "00:03:19,744", "timestamp_s": 199.0}, {"text": "difference between doing our regular day to day Python and Python code.", "timestamp": "00:03:22,444", "timestamp_s": 202.0}, {"text": "It is very much like a mouse that we turn around 180 degrees.", "timestamp": "00:03:26,119", "timestamp_s": 206.0}, {"text": "It\u0027s still the same actions, up, down, left, right, but in a different", "timestamp": "00:03:29,979", "timestamp_s": 209.0}, {"text": "way, in a different paradigm.", "timestamp": "00:03:34,419", "timestamp_s": 214.0}, {"text": "And that can be a little bit tricky to get to grips with initially.", "timestamp": "00:03:36,069", "timestamp_s": 216.0}, {"text": "And in this regard, there are three areas that I consider to be part of agentic AI.", "timestamp": "00:03:39,599", "timestamp_s": 219.0}, {"text": "First, it seems that we can almost create on the client side the", "timestamp": "00:03:46,694", "timestamp_s": 226.0}, {"text": "endpoint, the roots, that we would normally build on the server side.", "timestamp": "00:03:50,194", "timestamp_s": 230.0}, {"text": "We also use natural and human language, in my case English, to", "timestamp": "00:03:54,384", "timestamp_s": 234.0}, {"text": "create code, very much like pseudocode.", "timestamp": "00:03:58,094", "timestamp_s": 238.0}, {"text": "And thirdly, we give a sense of autonomy, in a sense that the LLM", "timestamp": "00:04:00,614", "timestamp_s": 240.0}, {"text": "can direct the flow of the app.", "timestamp": "00:04:04,044", "timestamp_s": 244.0}, {"text": "And that could be within bounds that we have created.", "timestamp": "00:04:05,974", "timestamp_s": 245.0}, {"text": "But basically, the next step can be determined by the", "timestamp": "00:04:08,944", "timestamp_s": 248.0}, {"text": "LLM, which our app will take.", "timestamp": "00:04:11,764", "timestamp_s": 251.0}, {"text": "So before we go into some code examples, why don\u0027t we just refresh ourselves", "timestamp": "00:04:13,984", "timestamp_s": 253.0}, {"text": "on what a REST API is before we start using any library implementations by", "timestamp": "00:04:17,664", "timestamp_s": 257.0}, {"text": "using just the requests library so that we can see that how we actually do a", "timestamp": "00:04:23,074", "timestamp_s": 263.0}, {"text": "POST request to our LLM rather than using, say, an OpenAI library that hides", "timestamp": "00:04:28,874", "timestamp_s": 268.0}, {"text": "the implementation of the requests.", "timestamp": "00:04:34,784", "timestamp_s": 274.0}, {"text": "So we have our model, we have our endpoint, and it\u0027s worth noting that we", "timestamp": "00:04:37,554", "timestamp_s": 277.0}, {"text": "only have one endpoint, one root, and we\u0027ll see why this is apparent later on.", "timestamp": "00:04:41,064", "timestamp_s": 281.0}, {"text": "And we will need to send our token, our API key, in the headers.", "timestamp": "00:04:46,334", "timestamp_s": 286.0}, {"text": "We will send our payload, for example, our model, a list of messages, whether", "timestamp": "00:04:50,464", "timestamp_s": 290.0}, {"text": "we\u0027re streaming, the temperature.", "timestamp": "00:04:54,204", "timestamp_s": 294.0}, {"text": "And here, with requests, what we can do is send POST.", "timestamp": "00:04:56,434", "timestamp_s": 296.0}, {"text": "With all these details to get a JSON response and to bear in mind, the", "timestamp": "00:04:59,919", "timestamp_s": 299.0}, {"text": "request is a string of characters and doesn\u0027t contain any objects or other", "timestamp": "00:05:04,619", "timestamp_s": 304.0}, {"text": "data types, and basically it\u0027s a JSON dumps or if we were in JavaScript,", "timestamp": "00:05:08,379", "timestamp_s": 308.0}, {"text": "it would be a JSON stringify.", "timestamp": "00:05:13,349", "timestamp_s": 313.0}, {"text": "Where all that information is created as a string.", "timestamp": "00:05:15,284", "timestamp_s": 315.0}, {"text": "So what we\u0027ll do now is we\u0027ll look at our very first file, 01, to", "timestamp": "00:05:18,894", "timestamp_s": 318.0}, {"text": "see this in action and to see the basics of an agent application.", "timestamp": "00:05:22,554", "timestamp_s": 322.0}, {"text": "We\u0027re now in our very first file, 01, and the first thing we\u0027re just going to do is", "timestamp": "00:05:28,314", "timestamp_s": 328.0}, {"text": "load in our imports of OS JSON requests, and to read our keys from the env file.", "timestamp": "00:05:32,874", "timestamp_s": 332.0}, {"text": "I\u0027m going to keep mine secret here, but you have a copy here as env.", "timestamp": "00:05:38,954", "timestamp_s": 338.0}, {"text": "sample.", "timestamp": "00:05:42,674", "timestamp_s": 342.0}, {"text": "Just paste your Open AI key here.", "timestamp": "00:05:43,724", "timestamp_s": 343.0}, {"text": "If we come back, what we can then do is load in our AI key.", "timestamp": "00:05:46,669", "timestamp_s": 346.0}, {"text": "And just check that we have it, which we see we have here.", "timestamp": "00:05:52,184", "timestamp_s": 352.0}, {"text": "We\u0027re going to select our model, which in this case is GPT 40mini.", "timestamp": "00:05:55,594", "timestamp_s": 355.0}, {"text": "And we just check that it\u0027s here.", "timestamp": "00:06:00,304", "timestamp_s": 360.0}, {"text": "And for the demonstration, we\u0027re going to create our own class to show how", "timestamp": "00:06:02,124", "timestamp_s": 362.0}, {"text": "we can do a request to the endpoint.", "timestamp": "00:06:06,214", "timestamp_s": 366.0}, {"text": "Later on, we\u0027ll use one of the libraries from OpenAI.", "timestamp": "00:06:10,164", "timestamp_s": 370.0}, {"text": "But we\u0027re just going to see in a very raw form actually how", "timestamp": "00:06:13,584", "timestamp_s": 373.0}, {"text": "we would create our own class.", "timestamp": "00:06:16,134", "timestamp_s": 376.0}, {"text": "request to an LLM directly to its endpoint.", "timestamp": "00:06:18,729", "timestamp_s": 378.0}, {"text": "So we\u0027re going to be making a POST request to this endpoint.", "timestamp": "00:06:21,949", "timestamp_s": 381.0}, {"text": "And what we\u0027re going to do in this class is use this one single endpoint,", "timestamp": "00:06:25,229", "timestamp_s": 385.0}, {"text": "and there\u0027s only ever one endpoint.", "timestamp": "00:06:29,329", "timestamp_s": 389.0}, {"text": "It\u0027s not that we have different routes for different tasks that we want to do.", "timestamp": "00:06:30,829", "timestamp_s": 390.0}, {"text": "We have the temperature, which is a hyperparameter that kind of", "timestamp": "00:06:35,149", "timestamp_s": 395.0}, {"text": "takes into account the probability.", "timestamp": "00:06:38,719", "timestamp_s": 398.0}, {"text": "Zero means it\u0027s very strict and is as deterministic as possible.", "timestamp": "00:06:40,619", "timestamp_s": 400.0}, {"text": "If we want to be more creative in generating text or images, we would", "timestamp": "00:06:45,209", "timestamp_s": 405.0}, {"text": "vary the temperature to, say, 0.", "timestamp": "00:06:50,129", "timestamp_s": 410.0}, {"text": "5, 1, etc. The range is between 0 and 2.", "timestamp": "00:06:52,289", "timestamp_s": 412.0}, {"text": "We have our system prompt.", "timestamp": "00:06:55,819", "timestamp_s": 415.0}, {"text": "We\u0027ll get into that.", "timestamp": "00:06:57,099", "timestamp_s": 417.0}, {"text": "Our API key.", "timestamp": "00:06:57,969", "timestamp_s": 417.0}, {"text": "And here we have our headers so that we get authentication.", "timestamp": "00:06:59,409", "timestamp_s": 419.0}, {"text": "Giving it the content type.", "timestamp": "00:07:02,904", "timestamp_s": 422.0}, {"text": "This is our request.", "timestamp": "00:07:04,644", "timestamp_s": 424.0}, {"text": "And when we want to generate some text, we pass in a prompt, our query, our request.", "timestamp": "00:07:06,674", "timestamp_s": 426.0}, {"text": "We pass in the payload of the model, the messages, the system prompt, our prompt.", "timestamp": "00:07:12,494", "timestamp_s": 432.0}, {"text": "We\u0027ll get into that.", "timestamp": "00:07:18,154", "timestamp_s": 438.0}, {"text": "Whether we\u0027re streaming, in this case false, the temperature.", "timestamp": "00:07:19,359", "timestamp_s": 439.0}, {"text": "And here, we make our requests to that endpoint with headers", "timestamp": "00:07:22,849", "timestamp_s": 442.0}, {"text": "and also with the payload.", "timestamp": "00:07:27,629", "timestamp_s": 447.0}, {"text": "And we can put in here for future reference, URL equals.", "timestamp": "00:07:29,619", "timestamp_s": 449.0}, {"text": "So that\u0027s a little bit clearer.", "timestamp": "00:07:33,499", "timestamp_s": 453.0}, {"text": "So now that we\u0027ve got that class, let\u0027s create an instance.", "timestamp": "00:07:34,949", "timestamp_s": 454.0}, {"text": "We pass in the model we want.", "timestamp": "00:07:37,579", "timestamp_s": 457.0}, {"text": "We pass in the system prompt, which is like, gives the character", "timestamp": "00:07:39,709", "timestamp_s": 459.0}, {"text": "or the personality or the role.", "timestamp": "00:07:43,649", "timestamp_s": 463.0}, {"text": "of our agent.", "timestamp": "00:07:46,084", "timestamp_s": 466.0}, {"text": "And we\u0027re saying in this case, you give concise answers to questions", "timestamp": "00:07:47,814", "timestamp_s": 467.0}, {"text": "with no more than 100 characters.", "timestamp": "00:07:50,704", "timestamp_s": 470.0}, {"text": "We can get into more complex system prompts later, and that\u0027s", "timestamp": "00:07:52,994", "timestamp_s": 472.0}, {"text": "the sort of prompt engineering.", "timestamp": "00:07:56,094", "timestamp_s": 476.0}, {"text": "And we can then make a request to say what is Pydantic.", "timestamp": "00:07:58,414", "timestamp_s": 478.0}, {"text": "And if we print the response that we get back originally, it is a JSON, stringified", "timestamp": "00:08:02,594", "timestamp_s": 482.0}, {"text": "JSON object of all this information.", "timestamp": "00:08:07,714", "timestamp_s": 487.0}, {"text": "But if we drill down through choices, message, and content,", "timestamp": "00:08:10,864", "timestamp_s": 490.0}, {"text": "we will end up with the Pydantic.", "timestamp": "00:08:14,524", "timestamp_s": 494.0}, {"text": "response that we want by Dantic is a data validation and settings", "timestamp": "00:08:16,679", "timestamp_s": 496.0}, {"text": "management library from Python.", "timestamp": "00:08:20,109", "timestamp_s": 500.0}, {"text": "And you can see how it\u0027s respecting the 100 characters.", "timestamp": "00:08:21,329", "timestamp_s": 501.0}, {"text": "So this is our base and we can see that if we were not using any of the AI", "timestamp": "00:08:24,779", "timestamp_s": 504.0}, {"text": "libraries but merely using our Python library of requests and env, that this is", "timestamp": "00:08:28,959", "timestamp_s": 508.0}, {"text": "how we would send a POST request to the LLM with all the details that it needs.", "timestamp": "00:08:33,759", "timestamp_s": 513.0}, {"text": "what we would get back.", "timestamp": "00:08:39,999", "timestamp_s": 519.0}, {"text": "And what we can do is we can just hide this and then show the response.", "timestamp": "00:08:41,119", "timestamp_s": 521.0}, {"text": "We can see it\u0027s quite complicated in quite detail, but it\u0027s choices", "timestamp": "00:08:45,449", "timestamp_s": 525.0}, {"text": "is where the answers come back.", "timestamp": "00:08:49,639", "timestamp_s": 529.0}, {"text": "We want the very first one.", "timestamp": "00:08:51,499", "timestamp_s": 531.0}, {"text": "And as we work through, we see message.", "timestamp": "00:08:53,349", "timestamp_s": 533.0}, {"text": "We want to look for the content where the answer to our query lies.", "timestamp": "00:08:55,489", "timestamp_s": 535.0}, {"text": "So this is our base template.", "timestamp": "00:09:01,539", "timestamp_s": 541.0}, {"text": "We\u0027re going to replace it with OpenAI library for our calls.", "timestamp": "00:09:04,214", "timestamp_s": 544.0}, {"text": "Let\u0027s now move on to the second file, O2 API.", "timestamp": "00:09:07,994", "timestamp_s": 547.0}, {"text": "And what we\u0027re going to do is we\u0027re going to get a joke back, but we\u0027re", "timestamp": "00:09:11,604", "timestamp_s": 551.0}, {"text": "also going to make a little bit of prompt engineering to get the LLM to", "timestamp": "00:09:14,514", "timestamp_s": 554.0}, {"text": "rate the joke and give us the next step.", "timestamp": "00:09:17,974", "timestamp_s": 557.0}, {"text": "So let\u0027s go into that to see an example of that.", "timestamp": "00:09:21,184", "timestamp_s": 561.0}, {"text": "So if we go into O2 API, We again load in our usual imports.", "timestamp": "00:09:23,414", "timestamp_s": 563.0}, {"text": "I\u0027m using rich to colorize the console output.", "timestamp": "00:09:27,834", "timestamp_s": 567.0}, {"text": "And we know we can make a request to some random joke API.", "timestamp": "00:09:32,724", "timestamp_s": 572.0}, {"text": "To get a joke here, using the request library, and we get the joke.", "timestamp": "00:09:36,534", "timestamp_s": 576.0}, {"text": "We won\u0027t go into that.", "timestamp": "00:09:41,884", "timestamp_s": 581.0}, {"text": "We will load in our open API key.", "timestamp": "00:09:43,274", "timestamp_s": 583.0}, {"text": "We\u0027ll get our model as usual.", "timestamp": "00:09:45,534", "timestamp_s": 585.0}, {"text": "And what we\u0027ll do is we\u0027ll then set up a more elaborate, system.", "timestamp": "00:09:47,274", "timestamp_s": 587.0}, {"text": "We\u0027re going to start with our basic system message.", "timestamp": "00:09:52,704", "timestamp_s": 592.0}, {"text": "You are an assistant that is great at telling jokes.", "timestamp": "00:09:55,114", "timestamp_s": 595.0}, {"text": "But we want to get something a little bit more advanced.", "timestamp": "00:09:58,684", "timestamp_s": 598.0}, {"text": "And it\u0027s almost as if we were sending to the endpoint a new endpoint that would", "timestamp": "00:10:01,314", "timestamp_s": 601.0}, {"text": "do something totally different for us.", "timestamp": "00:10:05,494", "timestamp_s": 605.0}, {"text": "But what we will find is actually that we will do this on the client side.", "timestamp": "00:10:07,504", "timestamp_s": 607.0}, {"text": "So.", "timestamp": "00:10:12,964", "timestamp_s": 612.0}, {"text": "Let\u0027s just add this extra prompt here, and I\u0027m just going to call it prompt", "timestamp": "00:10:13,689", "timestamp_s": 613.0}, {"text": "engineering because it\u0027s separate from the system message, totally customizable,", "timestamp": "00:10:16,839", "timestamp_s": 616.0}, {"text": "doesn\u0027t have to be prompt engineering.", "timestamp": "00:10:21,649", "timestamp_s": 621.0}, {"text": "And we\u0027re giving it a set of instructions, and we could almost", "timestamp": "00:10:24,029", "timestamp_s": 624.0}, {"text": "consider this to be pseudocode.", "timestamp": "00:10:26,619", "timestamp_s": 626.0}, {"text": "A joke worthy of publishing is a joke that has a rating of 8.", "timestamp": "00:10:28,359", "timestamp_s": 628.0}, {"text": "5 or 10 or above.", "timestamp": "00:10:31,119", "timestamp_s": 631.0}, {"text": "If the joke is worthy of publishing, also includes the", "timestamp": "00:10:32,989", "timestamp_s": 632.0}, {"text": "next step, whether to publish it.", "timestamp": "00:10:35,749", "timestamp_s": 635.0}, {"text": "Otherwise, tell us what the next step would be, which we should retry.", "timestamp": "00:10:37,859", "timestamp_s": 637.0}, {"text": "We give it an example.", "timestamp": "00:10:42,019", "timestamp_s": 642.0}, {"text": "of what we\u0027re going to get.", "timestamp": "00:10:43,774", "timestamp_s": 643.0}, {"text": "So this is called, one shot prompting or multi shot prompting where", "timestamp": "00:10:45,444", "timestamp_s": 645.0}, {"text": "you give one or more examples.", "timestamp": "00:10:49,934", "timestamp_s": 649.0}, {"text": "And basically we\u0027re giving it an example of what we would like to get back.", "timestamp": "00:10:52,294", "timestamp_s": 652.0}, {"text": "Please supply the response in the following format.", "timestamp": "00:10:55,974", "timestamp_s": 655.0}, {"text": "It\u0027s in JSON format here.", "timestamp": "00:10:58,119", "timestamp_s": 658.0}, {"text": "And to actually help it, let\u0027s actually, although it works, to be more specific,", "timestamp": "00:11:00,609", "timestamp_s": 660.0}, {"text": "we\u0027ll say the following JSON format.", "timestamp": "00:11:04,639", "timestamp_s": 664.0}, {"text": "Because we want to have clear instructions.", "timestamp": "00:11:07,149", "timestamp_s": 667.0}, {"text": "And here we see we get the setup, the punchline, we ask it to give us a rating,", "timestamp": "00:11:09,069", "timestamp_s": 669.0}, {"text": "and we also then tell us that based on the rating and whether it deems it", "timestamp": "00:11:13,649", "timestamp_s": 673.0}, {"text": "worthy of publishing, to either send the next step, that we pass on higher up in", "timestamp": "00:11:17,699", "timestamp_s": 677.0}, {"text": "the app chain to publish or to retry.", "timestamp": "00:11:22,759", "timestamp_s": 682.0}, {"text": "We\u0027re also giving it some further instructions to remove all backticks,", "timestamp": "00:11:26,289", "timestamp_s": 686.0}, {"text": "any unnecessary characters.", "timestamp": "00:11:29,949", "timestamp_s": 689.0}, {"text": "Once again, I did say JSON format here, and using capitals can be a useful", "timestamp": "00:11:32,259", "timestamp_s": 692.0}, {"text": "way to emphasize things for the LLM.", "timestamp": "00:11:35,959", "timestamp_s": 695.0}, {"text": "And also we\u0027re saying that if we do a retry, do not repeat the joke,", "timestamp": "00:11:38,399", "timestamp_s": 698.0}, {"text": "and we can even say thank you.", "timestamp": "00:11:41,629", "timestamp_s": 701.0}, {"text": "So this is our pseudocode.", "timestamp": "00:11:43,699", "timestamp_s": 703.0}, {"text": "This is almost a new endpoint that we would like to have where we", "timestamp": "00:11:46,464", "timestamp_s": 706.0}, {"text": "could send a request and it will process something different rather", "timestamp": "00:11:49,804", "timestamp_s": 709.0}, {"text": "than just returning us a joke.", "timestamp": "00:11:53,614", "timestamp_s": 713.0}, {"text": "But what we do is on the client side, we send that code up to that one endpoint.", "timestamp": "00:11:55,924", "timestamp_s": 715.0}, {"text": "And that\u0027s the mouse being at 180 degrees different.", "timestamp": "00:12:01,164", "timestamp_s": 721.0}, {"text": "It\u0027s like we are now creating our own code.", "timestamp": "00:12:04,114", "timestamp_s": 724.0}, {"text": "REST endpoint R route here.", "timestamp": "00:12:07,024", "timestamp_s": 727.0}, {"text": "So we just add those two together so that it\u0027s now the system message.", "timestamp": "00:12:10,064", "timestamp_s": 730.0}, {"text": "And we also have our user prompt, which is going to tell a light hearted", "timestamp": "00:12:14,534", "timestamp_s": 734.0}, {"text": "joke for an audience of Pythonistas.", "timestamp": "00:12:17,404", "timestamp_s": 737.0}, {"text": "And when we send up to the ALM, we want to send a list of all these messages,", "timestamp": "00:12:19,864", "timestamp_s": 739.0}, {"text": "and the convention is we have prompts.", "timestamp": "00:12:23,634", "timestamp_s": 743.0}, {"text": "We have a role for system, and its content is the system message.", "timestamp": "00:12:25,664", "timestamp_s": 745.0}, {"text": "We have a role for user, and the content is the user prompt.", "timestamp": "00:12:29,584", "timestamp_s": 749.0}, {"text": "We can also have role of the AI assistant, because we may want", "timestamp": "00:12:33,054", "timestamp_s": 753.0}, {"text": "to filter out the messages.", "timestamp": "00:12:36,334", "timestamp_s": 756.0}, {"text": "And this is the practice with these LLMs, is that we have a system message,", "timestamp": "00:12:37,714", "timestamp_s": 757.0}, {"text": "user message, and an AI message, which is the response back from the LLM.", "timestamp": "00:12:41,474", "timestamp_s": 761.0}, {"text": "So we complete as before, but this time we are using the OpenAI.", "timestamp": "00:12:46,724", "timestamp_s": 766.0}, {"text": "If we scroll up to the top, we\u0027re using from OpenAI, one of its libraries.", "timestamp": "00:12:52,414", "timestamp_s": 772.0}, {"text": "So we can just create the client here, An instance of OpenAI.", "timestamp": "00:12:57,404", "timestamp_s": 777.0}, {"text": "And therefore, when we send the request, we don\u0027t need to be as detailed.", "timestamp": "00:13:03,589", "timestamp_s": 783.0}, {"text": "We can use a convenience method.", "timestamp": "00:13:08,059", "timestamp_s": 788.0}, {"text": "Client.", "timestamp": "00:13:09,869", "timestamp_s": 789.0}, {"text": "chat.", "timestamp": "00:13:10,539", "timestamp_s": 790.0}, {"text": "completions.", "timestamp": "00:13:11,059", "timestamp_s": 791.0}, {"text": "create.", "timestamp": "00:13:11,839", "timestamp_s": 791.0}, {"text": "We send the model we want.", "timestamp": "00:13:12,709", "timestamp_s": 792.0}, {"text": "We send all our messages in, which is these prompts.", "timestamp": "00:13:14,099", "timestamp_s": 794.0}, {"text": "And when we get it back, we will get a response.", "timestamp": "00:13:17,359", "timestamp_s": 797.0}, {"text": "Again, it\u0027s in the choices, it\u0027s in the message, in the content,", "timestamp": "00:13:19,999", "timestamp_s": 799.0}, {"text": "and we can display it here.", "timestamp": "00:13:23,309", "timestamp_s": 803.0}, {"text": "So we now get a JSON object here, where it has the setup, the punchline,", "timestamp": "00:13:25,269", "timestamp_s": 805.0}, {"text": "The rating and what it advises to do next in the flow, which", "timestamp": "00:13:32,154", "timestamp_s": 812.0}, {"text": "we publish as opposed to retry.", "timestamp": "00:13:36,244", "timestamp_s": 816.0}, {"text": "Now, in our app, we might have a state object that holds all of this", "timestamp": "00:13:38,994", "timestamp_s": 818.0}, {"text": "information because this is we\u0027ve made the REST API call to the LLM.", "timestamp": "00:13:42,654", "timestamp_s": 822.0}, {"text": "But what we do next is day to day Python.", "timestamp": "00:13:47,524", "timestamp_s": 827.0}, {"text": "It\u0027s any system design, PubSub, ActorModel, FiniteStateMachine.", "timestamp": "00:13:50,544", "timestamp_s": 830.0}, {"text": "We\u0027ve now been given exactly what to do next.", "timestamp": "00:13:56,934", "timestamp_s": 836.0}, {"text": "Now we can pass that over to another agent, or we can act upon it ourselves.", "timestamp": "00:14:00,164", "timestamp_s": 840.0}, {"text": "And what I\u0027m going to do here is I run it out.", "timestamp": "00:14:05,424", "timestamp_s": 845.0}, {"text": "I\u0027ve extracted it out nicely.", "timestamp": "00:14:08,274", "timestamp_s": 848.0}, {"text": "JSON data.", "timestamp": "00:14:10,534", "timestamp_s": 850.0}, {"text": "And when I look at it, I can then basically see that if the result next", "timestamp": "00:14:11,764", "timestamp_s": 851.0}, {"text": "equals publish, for example, I can load it into our state object, and", "timestamp": "00:14:16,274", "timestamp_s": 856.0}, {"text": "I could then go on to publish it, to send a message to another part", "timestamp": "00:14:22,444", "timestamp_s": 862.0}, {"text": "of our program or to another agent.", "timestamp": "00:14:26,484", "timestamp_s": 866.0}, {"text": "The main thing is we\u0027re kind of getting almost like an event driven", "timestamp": "00:14:28,734", "timestamp_s": 868.0}, {"text": "application, but we\u0027ve been asking the LLM to decide the next step.", "timestamp": "00:14:31,754", "timestamp_s": 871.0}, {"text": "And that\u0027s the autonomy, because it could have come back with retry.", "timestamp": "00:14:36,884", "timestamp_s": 876.0}, {"text": "And in which case, the flow of the program would go in a different direction.", "timestamp": "00:14:40,874", "timestamp_s": 880.0}, {"text": "And so what I\u0027ve just done here, I\u0027ve extracted out the", "timestamp": "00:14:44,424", "timestamp_s": 884.0}, {"text": "next step, which is publish.", "timestamp": "00:14:47,164", "timestamp_s": 887.0}, {"text": "So this is how our app would have its flow and its direction directed by the", "timestamp": "00:14:49,244", "timestamp_s": 889.0}, {"text": "LLM, as opposed to us imperatively.", "timestamp": "00:14:55,234", "timestamp_s": 895.0}, {"text": "So to recap with this, the main thing is we\u0027re structuring", "timestamp": "00:14:58,244", "timestamp_s": 898.0}, {"text": "what we would like to get back.", "timestamp": "00:15:02,004", "timestamp_s": 902.0}, {"text": "Rather than just getting a joke, we\u0027re setting back an endpoint.", "timestamp": "00:15:03,364", "timestamp_s": 903.0}, {"text": "We\u0027re setting back instructions.", "timestamp": "00:15:06,564", "timestamp_s": 906.0}, {"text": "This is our pseudocode.", "timestamp": "00:15:07,874", "timestamp_s": 907.0}, {"text": "This is our REST endpoint that we send from the client up to the server", "timestamp": "00:15:09,644", "timestamp_s": 909.0}, {"text": "along with our payload, and then we get the response back as we want.", "timestamp": "00:15:14,444", "timestamp_s": 914.0}, {"text": "And we\u0027re going to see various different forms of these prompts.", "timestamp": "00:15:18,634", "timestamp_s": 918.0}, {"text": "They could be more advanced and more structured as we go along.", "timestamp": "00:15:21,654", "timestamp_s": 921.0}, {"text": "So when we get that back, we can then basically decide what we want to do next.", "timestamp": "00:15:25,804", "timestamp_s": 925.0}, {"text": "If we\u0027re having many agents, we may keep track where each agent is.", "timestamp": "00:15:31,234", "timestamp_s": 931.0}, {"text": "So this was O2 API.", "timestamp": "00:15:36,794", "timestamp_s": 936.0}, {"text": "An initial start into prompt engineering and how we use the client side REST API", "timestamp": "00:15:39,054", "timestamp_s": 939.0}, {"text": "coding in natural language, enabling autonomy to take place in our AI agent.", "timestamp": "00:15:45,144", "timestamp_s": 945.0}, {"text": "We\u0027ve seen the two out of the three steps of the AI reverse process,", "timestamp": "00:15:51,534", "timestamp_s": 951.0}, {"text": "where we kind of created on the client side our route, our endpoint,", "timestamp": "00:15:55,584", "timestamp_s": 955.0}, {"text": "and the use of natural language.", "timestamp": "00:15:59,614", "timestamp_s": 959.0}, {"text": "And we briefly looked at autonomy.", "timestamp": "00:16:01,544", "timestamp_s": 961.0}, {"text": "So how do we handle this autonomy of the flow of an API?", "timestamp": "00:16:03,274", "timestamp_s": 963.0}, {"text": "AI agent app.", "timestamp": "00:16:07,304", "timestamp_s": 967.0}, {"text": "Well, we asked in our LLM to give us not just a rating,", "timestamp": "00:16:08,341", "timestamp_s": 968.0}, {"text": "but to give us that next step.", "timestamp": "00:16:11,974", "timestamp_s": 971.0}, {"text": "And in this case, it was published.", "timestamp": "00:16:13,704", "timestamp_s": 973.0}, {"text": "So what we\u0027d like to do now is to begin to see how we can handle this in our app.", "timestamp": "00:16:15,754", "timestamp_s": 975.0}, {"text": "And we\u0027re going to go on to our next example, which is a sort of an idea is", "timestamp": "00:16:21,124", "timestamp_s": 981.0}, {"text": "leading into the idea of an FAQ or sort of the router pattern, a sort of if else.", "timestamp": "00:16:24,284", "timestamp_s": 984.0}, {"text": "And what I like about the FAQ pattern, which we\u0027re going to see now, is", "timestamp": "00:16:29,724", "timestamp_s": 989.0}, {"text": "the fact that we can introduce a little bit of AI into our app.", "timestamp": "00:16:33,509", "timestamp_s": 993.0}, {"text": "So let\u0027s have a look at this example of OA03FAQ.", "timestamp": "00:16:39,399", "timestamp_s": 999.0}, {"text": "We\u0027re going to see an example of sort of retrieval augmented generation.", "timestamp": "00:16:41,719", "timestamp_s": 1001.0}, {"text": "Now, we\u0027re not querying documents, but RAG is basically supplementing our", "timestamp": "00:16:44,799", "timestamp_s": 1004.0}, {"text": "query with additional information that we haven\u0027t fine tuned our model with", "timestamp": "00:16:49,349", "timestamp_s": 1009.0}, {"text": "or that\u0027s important for our LLM query.", "timestamp": "00:16:53,729", "timestamp_s": 1013.0}, {"text": "So what we\u0027re going to do is we\u0027re going to give it a list of", "timestamp": "00:16:57,439", "timestamp_s": 1017.0}, {"text": "frequently asked questions and have a little chatbot experience.", "timestamp": "00:16:59,449", "timestamp_s": 1019.0}, {"text": "So And this is going to pave way for the next file, which will", "timestamp": "00:17:02,369", "timestamp_s": 1022.0}, {"text": "be a sort of a router pattern.", "timestamp": "00:17:06,369", "timestamp_s": 1026.0}, {"text": "So once again, we load in our imports, we get our key, we get a utility client from", "timestamp": "00:17:08,279", "timestamp_s": 1028.0}, {"text": "the OpenAI library to ease our connection to the LLM, which is picking our model.", "timestamp": "00:17:14,519", "timestamp_s": 1034.0}, {"text": "And we\u0027re going to create now a function that has some history, that has all", "timestamp": "00:17:19,849", "timestamp_s": 1039.0}, {"text": "the previous messages, that has the system message and the user prompt.", "timestamp": "00:17:23,339", "timestamp_s": 1043.0}, {"text": "And the history is important because it gives us a record of what went on before,", "timestamp": "00:17:27,409", "timestamp_s": 1047.0}, {"text": "because every request is stateless.", "timestamp": "00:17:31,109", "timestamp_s": 1051.0}, {"text": "It\u0027s like the LLM is seeing it for the first time.", "timestamp": "00:17:33,499", "timestamp_s": 1053.0}, {"text": "So we need to pass the history back with it so it has context", "timestamp": "00:17:36,169", "timestamp_s": 1056.0}, {"text": "and a certain degree of memory.", "timestamp": "00:17:40,519", "timestamp_s": 1060.0}, {"text": "So in our function chat, We\u0027re having our role, our system.", "timestamp": "00:17:43,469", "timestamp_s": 1063.0}, {"text": "We\u0027re adding in the history of all the previous messages that we have", "timestamp": "00:17:47,239", "timestamp_s": 1067.0}, {"text": "in our chatbot and our user message.", "timestamp": "00:17:50,389", "timestamp_s": 1070.0}, {"text": "We can print out the history, print out the messages.", "timestamp": "00:17:53,199", "timestamp_s": 1073.0}, {"text": "We\u0027re using the stream option for our chatbot, which we\u0027ll see", "timestamp": "00:17:56,509", "timestamp_s": 1076.0}, {"text": "in a minute, and we\u0027re going to then chunk out our responses.", "timestamp": "00:18:00,339", "timestamp_s": 1080.0}, {"text": "So this is where we start to build more context into our chatbot.", "timestamp": "00:18:04,429", "timestamp_s": 1084.0}, {"text": "agent.", "timestamp": "00:18:10,014", "timestamp_s": 1090.0}, {"text": "We\u0027re saying it\u0027s a helpful assistant for a shoe store.", "timestamp": "00:18:11,124", "timestamp_s": 1091.0}, {"text": "And if a user asks a question, please be as helpful as possible and as", "timestamp": "00:18:13,754", "timestamp_s": 1093.0}, {"text": "courteous and professional manner.", "timestamp": "00:18:17,654", "timestamp_s": 1097.0}, {"text": "You are provided with the following facts to help you.", "timestamp": "00:18:19,944", "timestamp_s": 1099.0}, {"text": "Please be verbose and suggestive.", "timestamp": "00:18:22,044", "timestamp_s": 1102.0}, {"text": "So now I\u0027m changing like the character and the nature of the", "timestamp": "00:18:24,114", "timestamp_s": 1104.0}, {"text": "prompt, adding in a little bit more verbosity and suggestiveness.", "timestamp": "00:18:26,764", "timestamp_s": 1106.0}, {"text": "So here is a list of just some basic facts about our shop.", "timestamp": "00:18:29,624", "timestamp_s": 1109.0}, {"text": "And bear in mind that this could be retrieved from the database.", "timestamp": "00:18:34,074", "timestamp_s": 1114.0}, {"text": "This could be a result of selecting from the Options from", "timestamp": "00:18:36,544", "timestamp_s": 1116.0}, {"text": "a form for further information.", "timestamp": "00:18:39,524", "timestamp_s": 1119.0}, {"text": "But we can see here is if we just pass some extra content,", "timestamp": "00:18:41,914", "timestamp_s": 1121.0}, {"text": "some retrieved content.", "timestamp": "00:18:45,464", "timestamp_s": 1125.0}, {"text": "Admittedly, it\u0027s already in the file, but it could have been retrieved from", "timestamp": "00:18:46,894", "timestamp_s": 1126.0}, {"text": "a database or from some other source, and we join it onto the system message.", "timestamp": "00:18:50,114", "timestamp_s": 1130.0}, {"text": "We can now use Gradio to set up a little chatbot interface to see how this works.", "timestamp": "00:18:55,644", "timestamp_s": 1135.0}, {"text": "And if we look at the code while this is working, it hasn\u0027t taken a lot to", "timestamp": "00:19:01,754", "timestamp_s": 1141.0}, {"text": "introduce a fairly sophisticated little chatbot based on very limited information.", "timestamp": "00:19:05,314", "timestamp_s": 1145.0}, {"text": "We could very much increase this greatly in our app.", "timestamp": "00:19:10,024", "timestamp_s": 1150.0}, {"text": "So as that runs through.", "timestamp": "00:19:13,794", "timestamp_s": 1153.0}, {"text": "Almost there.", "timestamp": "00:19:15,424", "timestamp_s": 1155.0}, {"text": "Come back down.", "timestamp": "00:19:16,434", "timestamp_s": 1156.0}, {"text": "We have our Gradio interface,", "timestamp": "00:19:17,734", "timestamp_s": 1157.0}, {"text": "and let\u0027s just open that up.", "timestamp": "00:19:20,334", "timestamp_s": 1160.0}, {"text": "In fact, I\u0027ll put it in.", "timestamp": "00:19:22,004", "timestamp_s": 1162.0}, {"text": "There we go.", "timestamp": "00:19:23,684", "timestamp_s": 1163.0}, {"text": "Here\u0027s our chatbot.", "timestamp": "00:19:24,414", "timestamp_s": 1164.0}, {"text": "Type message.", "timestamp": "00:19:25,434", "timestamp_s": 1165.0}, {"text": "Now, it\u0027s quite simple.", "timestamp": "00:19:26,744", "timestamp_s": 1166.0}, {"text": "I could just say Sunday, and if we look at that, it\u0027s coming through.", "timestamp": "00:19:28,374", "timestamp_s": 1168.0}, {"text": "Thank you for your inquiry.", "timestamp": "00:19:32,544", "timestamp_s": 1172.0}, {"text": "Our store is picked up.", "timestamp": "00:19:33,544", "timestamp_s": 1173.0}, {"text": "It\u0027s about time.", "timestamp": "00:19:34,934", "timestamp_s": 1174.0}, {"text": "It\u0027s Monday to Friday 9 to 5.", "timestamp": "00:19:35,824", "timestamp_s": 1175.0}, {"text": "Unfortunately, we\u0027re closed on Sundays.", "timestamp": "00:19:37,524", "timestamp_s": 1177.0}, {"text": "Notice how it\u0027s quite verbose and suggestive.", "timestamp": "00:19:39,354", "timestamp_s": 1179.0}, {"text": "We look forward to welcoming you soon.", "timestamp": "00:19:41,584", "timestamp_s": 1181.0}, {"text": "If we come back to our code, we can see that here are the facts.", "timestamp": "00:19:43,434", "timestamp_s": 1183.0}, {"text": "We don\u0027t have very many.", "timestamp": "00:19:46,944", "timestamp_s": 1186.0}, {"text": "This could be a much more complex document.", "timestamp": "00:19:48,404", "timestamp_s": 1188.0}, {"text": "It could be an MD markdown file.", "timestamp": "00:19:50,534", "timestamp_s": 1190.0}, {"text": "It could be driven from the database.", "timestamp": "00:19:52,114", "timestamp_s": 1192.0}, {"text": "So if we come back and let\u0027s have another example.", "timestamp": "00:19:54,104", "timestamp_s": 1194.0}, {"text": "I can say green belts.", "timestamp": "00:19:57,159", "timestamp_s": 1197.0}, {"text": "Thank you for reaching out.", "timestamp": "00:19:59,139", "timestamp_s": 1199.0}, {"text": "We don\u0027t do that.", "timestamp": "00:20:00,039", "timestamp_s": 1200.0}, {"text": "Gives us the address.", "timestamp": "00:20:01,289", "timestamp_s": 1201.0}, {"text": "It says that basically exclusively in shoes.", "timestamp": "00:20:02,569", "timestamp_s": 1202.0}, {"text": "So this little example here of basically having an agentic AI with a minimum", "timestamp": "00:20:05,499", "timestamp_s": 1205.0}, {"text": "amount of rag, a minimum amount of extra context can produce a nice small little", "timestamp": "00:20:11,559", "timestamp_s": 1211.0}, {"text": "app in your, small little AI app in your Python app without having to be fully AI.", "timestamp": "00:20:16,449", "timestamp_s": 1216.0}, {"text": "This is what I call a bit of AI programming.", "timestamp": "00:20:24,369", "timestamp_s": 1224.0}, {"text": "Now, this is quite an interesting pattern, because in the next one, it\u0027s", "timestamp": "00:20:26,859", "timestamp_s": 1226.0}, {"text": "the agent router, and it\u0027s something that happened when I was at CodeBar.", "timestamp": "00:20:30,039", "timestamp_s": 1230.0}, {"text": "somebody asked that they would like to get a job in AI.", "timestamp": "00:20:34,069", "timestamp_s": 1234.0}, {"text": "They were doing Python.", "timestamp": "00:20:37,199", "timestamp_s": 1237.0}, {"text": "And I said to them, Do they have an AI department where they work?", "timestamp": "00:20:38,319", "timestamp_s": 1238.0}, {"text": "And they said, No.", "timestamp": "00:20:41,459", "timestamp_s": 1241.0}, {"text": "And I said, What do they do?", "timestamp": "00:20:42,349", "timestamp_s": 1242.0}, {"text": "They said they were insurance.", "timestamp": "00:20:43,319", "timestamp_s": 1243.0}, {"text": "I said, What do you do?", "timestamp": "00:20:44,389", "timestamp_s": 1244.0}, {"text": "And they said they they don\u0027t write the reports that they\u0027re there to go", "timestamp": "00:20:45,719", "timestamp_s": 1245.0}, {"text": "to person that when somebody wants a report, they know which one it", "timestamp": "00:20:49,229", "timestamp_s": 1249.0}, {"text": "is, and they can run it for them.", "timestamp": "00:20:52,529", "timestamp_s": 1252.0}, {"text": "I thought, Brilliant.", "timestamp": "00:20:54,779", "timestamp_s": 1254.0}, {"text": "You can do that.", "timestamp": "00:20:55,509", "timestamp_s": 1255.0}, {"text": "And there were concerns.", "timestamp": "00:20:56,609", "timestamp_s": 1256.0}, {"text": "They said, put me out of a job.", "timestamp": "00:20:57,969", "timestamp_s": 1257.0}, {"text": "I said, yes, but you\u0027ll then be the head of the AI department.", "timestamp": "00:21:00,179", "timestamp_s": 1260.0}, {"text": "And so what we\u0027re going to do is we\u0027re going to have a", "timestamp": "00:21:03,979", "timestamp_s": 1263.0}, {"text": "little variation on this FAQ.", "timestamp": "00:21:05,419", "timestamp_s": 1265.0}, {"text": "It\u0027s a similar type of thing.", "timestamp": "00:21:07,529", "timestamp_s": 1267.0}, {"text": "We\u0027re loading in all the usual imports and setting ourself up the same chat message.", "timestamp": "00:21:09,159", "timestamp_s": 1269.0}, {"text": "And a useful tip is caps and italics and even markdown in one\u0027s prompts", "timestamp": "00:21:15,169", "timestamp_s": 1275.0}, {"text": "actually have an impact with the LLM.", "timestamp": "00:21:20,609", "timestamp_s": 1280.0}, {"text": "It\u0027s being trained on so many of these that it begins to", "timestamp": "00:21:23,009", "timestamp_s": 1283.0}, {"text": "recognize the importance.", "timestamp": "00:21:25,719", "timestamp_s": 1285.0}, {"text": "And I\u0027m just basically setting up a report agent.", "timestamp": "00:21:27,939", "timestamp_s": 1287.0}, {"text": "I\u0027m saying you\u0027re a report selection agent.", "timestamp": "00:21:30,439", "timestamp_s": 1290.0}, {"text": "You\u0027re very good at returning the best report to answer a user\u0027s question.", "timestamp": "00:21:32,199", "timestamp_s": 1292.0}, {"text": "For example, if a user wants a joke, you reply with, and I\u0027m just using", "timestamp": "00:21:35,859", "timestamp_s": 1295.0}, {"text": "this format for demonstration purposes.", "timestamp": "00:21:39,429", "timestamp_s": 1299.0}, {"text": "This will make it nice and bold.", "timestamp": "00:21:41,689", "timestamp_s": 1301.0}, {"text": "The tool they need is the get joke report.", "timestamp": "00:21:43,279", "timestamp_s": 1303.0}, {"text": "If they want total sales, the tool or report they need will be the", "timestamp": "00:21:45,959", "timestamp_s": 1305.0}, {"text": "get sales would be the best report.", "timestamp": "00:21:49,229", "timestamp_s": 1309.0}, {"text": "So I made a list of reports here for whether use the get weather for hotel,", "timestamp": "00:21:52,199", "timestamp_s": 1312.0}, {"text": "the hotel booking, very much like we did in the last FAQ, adding them all in.", "timestamp": "00:21:56,039", "timestamp_s": 1316.0}, {"text": "And if I run all of that, we will see now that we can actually", "timestamp": "00:22:02,309", "timestamp_s": 1322.0}, {"text": "have a report selection agent that will get the right report.", "timestamp": "00:22:05,609", "timestamp_s": 1325.0}, {"text": "And if we combine that with information like the date range or", "timestamp": "00:22:09,639", "timestamp_s": 1329.0}, {"text": "any other properties, we could even run the report for them and send it.", "timestamp": "00:22:12,729", "timestamp_s": 1332.0}, {"text": "All through agents.", "timestamp": "00:22:16,754", "timestamp_s": 1336.0}, {"text": "So let\u0027s check.", "timestamp": "00:22:18,684", "timestamp_s": 1338.0}, {"text": "We\u0027ve got this one working.", "timestamp": "00:22:19,704", "timestamp_s": 1339.0}, {"text": "Lovely.", "timestamp": "00:22:21,344", "timestamp_s": 1341.0}, {"text": "Okay.", "timestamp": "00:22:22,004", "timestamp_s": 1342.0}, {"text": "So say I want to take a plane and notice I didn\u0027t use the word plane.", "timestamp": "00:22:22,744", "timestamp_s": 1342.0}, {"text": "I use flight.", "timestamp": "00:22:25,834", "timestamp_s": 1345.0}, {"text": "Plane to Rome.", "timestamp": "00:22:26,874", "timestamp_s": 1346.0}, {"text": "What report should I get?", "timestamp": "00:22:28,234", "timestamp_s": 1348.0}, {"text": "Get the flight.", "timestamp": "00:22:29,834", "timestamp_s": 1349.0}, {"text": "Plane to Rome and Auto, let\u0027s just check the typos there,", "timestamp": "00:22:30,994", "timestamp_s": 1350.0}, {"text": "to Rome, and auto to Paris.", "timestamp": "00:22:36,394", "timestamp_s": 1356.0}, {"text": "It comes back with those two reports.", "timestamp": "00:22:40,564", "timestamp_s": 1360.0}, {"text": "So straight away, with that, if we had the information, shall we say, sent", "timestamp": "00:22:42,754", "timestamp_s": 1362.0}, {"text": "along with it through a form selection, we could then get the right report.", "timestamp": "00:22:46,304", "timestamp_s": 1366.0}, {"text": "And so this is an example of a sort of router that actually,", "timestamp": "00:22:51,204", "timestamp_s": 1371.0}, {"text": "what do we want to do next?", "timestamp": "00:22:54,554", "timestamp_s": 1374.0}, {"text": "Through a very simple addition of some context to our system message,", "timestamp": "00:22:56,344", "timestamp_s": 1376.0}, {"text": "I\u0027m going to return to this slide a number of times because it\u0027s easy when", "timestamp": "00:23:02,464", "timestamp_s": 1382.0}, {"text": "we\u0027re going through these different patterns and function callings to lose", "timestamp": "00:23:07,084", "timestamp_s": 1387.0}, {"text": "sight of what is the essence of an A.", "timestamp": "00:23:10,874", "timestamp_s": 1390.0}, {"text": "I. agent and A. I. agents are python code with A. P. I. request to L.", "timestamp": "00:23:13,524", "timestamp_s": 1393.0}, {"text": "L. M. s. We can only pass a string in the A. P. I. request and in that", "timestamp": "00:23:18,604", "timestamp_s": 1398.0}, {"text": "string we create a job description.", "timestamp": "00:23:23,294", "timestamp_s": 1403.0}, {"text": "And this could be what the role is, what they do, these are the tools you have,", "timestamp": "00:23:26,634", "timestamp_s": 1406.0}, {"text": "here is the data to work on, this is what we want returned, and in what format.", "timestamp": "00:23:31,454", "timestamp_s": 1411.0}, {"text": "This is prompt or flow engineering.", "timestamp": "00:23:36,204", "timestamp_s": 1416.0}, {"text": "How we make use of this in terms of design patterns is then day to day Python.", "timestamp": "00:23:39,254", "timestamp_s": 1419.0}, {"text": "Fundamentally, it is a function with inputs, LLM magic, and some output", "timestamp": "00:23:44,314", "timestamp_s": 1424.0}, {"text": "returned in the form that we want.", "timestamp": "00:23:50,444", "timestamp_s": 1430.0}, {"text": "And we\u0027ll return to this slide as we proceed through some of", "timestamp": "00:23:52,934", "timestamp_s": 1432.0}, {"text": "the more involved examples.", "timestamp": "00:23:55,814", "timestamp_s": 1435.0}, {"text": "Now we\u0027ve seen how an agent can make decisions.", "timestamp": "00:23:58,734", "timestamp_s": 1438.0}, {"text": "About the next step, the autonomy, how we\u0027ve created our sort of", "timestamp": "00:24:02,354", "timestamp_s": 1442.0}, {"text": "client side API, and we use natural language, but an agent may need tools.", "timestamp": "00:24:05,894", "timestamp_s": 1445.0}, {"text": "It may need to do anything.", "timestamp": "00:24:10,644", "timestamp_s": 1450.0}, {"text": "It may need to make a request to the Internet.", "timestamp": "00:24:11,824", "timestamp_s": 1451.0}, {"text": "It may need to call upon a function that we have in our code base to", "timestamp": "00:24:15,174", "timestamp_s": 1455.0}, {"text": "calculate something and use that it.", "timestamp": "00:24:19,844", "timestamp_s": 1459.0}, {"text": "as part of its response.", "timestamp": "00:24:23,164", "timestamp_s": 1463.0}, {"text": "So what we\u0027re going to do in O5 is we\u0027re going to look at not how we", "timestamp": "00:24:25,144", "timestamp_s": 1465.0}, {"text": "just define tools, but also how an agent can decide which one to use.", "timestamp": "00:24:28,074", "timestamp_s": 1468.0}, {"text": "Now, usually it\u0027s better to have an agent to just one single thing, but", "timestamp": "00:24:32,264", "timestamp_s": 1472.0}, {"text": "sometimes we might have an agent that might need to make a decision for a", "timestamp": "00:24:35,554", "timestamp_s": 1475.0}, {"text": "particular task, which tool to use.", "timestamp": "00:24:38,764", "timestamp_s": 1478.0}, {"text": "So we may need to determine Which tool to use in the agent and", "timestamp": "00:24:41,624", "timestamp_s": 1481.0}, {"text": "basically, what happens is we\u0027re constantly adding new messages.", "timestamp": "00:24:46,514", "timestamp_s": 1486.0}, {"text": "Now, where does those functions run?", "timestamp": "00:24:49,464", "timestamp_s": 1489.0}, {"text": "What\u0027s going to happen is when we\u0027ve given the prompt, which we\u0027re", "timestamp": "00:24:51,924", "timestamp_s": 1491.0}, {"text": "going to have a look at now in 05.", "timestamp": "00:24:55,464", "timestamp_s": 1495.0}, {"text": "What we\u0027re going to do is do the standard setup.", "timestamp": "00:24:58,324", "timestamp_s": 1498.0}, {"text": "But now when we come to our tool prompt, we\u0027re saying you\u0027re in a system", "timestamp": "00:25:02,674", "timestamp_s": 1502.0}, {"text": "that is very good at determining what tool to use to solve a certain query.", "timestamp": "00:25:05,934", "timestamp_s": 1505.0}, {"text": "And our AI programming is we give it in descriptive form, which", "timestamp": "00:25:10,094", "timestamp_s": 1510.0}, {"text": "is saying we\u0027re using Markdown here to emphasize this is tools.", "timestamp": "00:25:14,474", "timestamp_s": 1514.0}, {"text": "We have two tools.", "timestamp": "00:25:17,584", "timestamp_s": 1517.0}, {"text": "We\u0027re describing our calculator tool that does basic arithmetic.", "timestamp": "00:25:19,004", "timestamp_s": 1519.0}, {"text": "And it responds in JSON.", "timestamp": "00:25:22,734", "timestamp_s": 1522.0}, {"text": "And we give it an example of the JSON format, of that, when we pick the", "timestamp": "00:25:24,474", "timestamp_s": 1524.0}, {"text": "particular tool called Calculator, the next signal will be to use the", "timestamp": "00:25:28,734", "timestamp_s": 1528.0}, {"text": "doCalculation function, and, for example, what argument should be passed.", "timestamp": "00:25:33,804", "timestamp_s": 1533.0}, {"text": "This is an example of one scenario.", "timestamp": "00:25:38,224", "timestamp_s": 1538.0}, {"text": "We have a second tool, which is a joke tool, something", "timestamp": "00:25:41,854", "timestamp_s": 1541.0}, {"text": "totally different, JSON format.", "timestamp": "00:25:43,914", "timestamp_s": 1543.0}, {"text": "And this is what the result the tool would look like.", "timestamp": "00:25:46,524", "timestamp_s": 1546.0}, {"text": "So, for example, we can ask it a question.", "timestamp": "00:25:50,244", "timestamp_s": 1550.0}, {"text": "What is 10 times 9?", "timestamp": "00:25:52,214", "timestamp_s": 1552.0}, {"text": "And what we\u0027re going to do is when we send that out, it will actually", "timestamp": "00:25:54,064", "timestamp_s": 1554.0}, {"text": "determine that the tool it needs, the response it\u0027s sending back to us is", "timestamp": "00:25:57,234", "timestamp_s": 1557.0}, {"text": "the calculator, the do calculation, and the arguments that are needed.", "timestamp": "00:26:01,664", "timestamp_s": 1561.0}, {"text": "We can strip all of that out and basically then say if the do next was do", "timestamp": "00:26:06,484", "timestamp_s": 1566.0}, {"text": "calculation, we can run those functions.", "timestamp": "00:26:12,414", "timestamp_s": 1572.0}, {"text": "If it was perhaps, for example, to do the joke, we might do an", "timestamp": "00:26:15,709", "timestamp_s": 1575.0}, {"text": "internet request to get a joke.", "timestamp": "00:26:19,069", "timestamp_s": 1579.0}, {"text": "So let\u0027s just go back to the flow of messages.", "timestamp": "00:26:21,699", "timestamp_s": 1581.0}, {"text": "We\u0027re doing all the usual messages.", "timestamp": "00:26:26,439", "timestamp_s": 1586.0}, {"text": "But when the API, when the LLM decides that it needs a particular", "timestamp": "00:26:28,259", "timestamp_s": 1588.0}, {"text": "tool, what it does is it sends back the tool signature and the arguments", "timestamp": "00:26:31,639", "timestamp_s": 1591.0}, {"text": "that it\u0027s extracted from the query.", "timestamp": "00:26:37,099", "timestamp_s": 1597.0}, {"text": "We then on our own computer box, not the LLM, run that function, get the result.", "timestamp": "00:26:39,769", "timestamp_s": 1599.0}, {"text": "And add that back to the list of messages and send it to the LLM that", "timestamp": "00:26:46,569", "timestamp_s": 1606.0}, {"text": "does the next step, for example.", "timestamp": "00:26:50,479", "timestamp_s": 1610.0}, {"text": "Now, that particular example is one where we\u0027re going to see later where", "timestamp": "00:26:52,609", "timestamp_s": 1612.0}, {"text": "we\u0027re going to use planning, where it\u0027s thinking, making an action, getting a", "timestamp": "00:26:55,339", "timestamp_s": 1615.0}, {"text": "result, putting it back in the loop.", "timestamp": "00:27:00,159", "timestamp_s": 1620.0}, {"text": "This particular example here in 05 tool is just the first step of showing how", "timestamp": "00:27:02,709", "timestamp_s": 1622.0}, {"text": "it can determine which tool to use.", "timestamp": "00:27:08,369", "timestamp_s": 1628.0}, {"text": "So I\u0027m just going to run everything.", "timestamp": "00:27:11,249", "timestamp_s": 1631.0}, {"text": "And as you see, we load in our imports, we get our key.", "timestamp": "00:27:13,339", "timestamp_s": 1633.0}, {"text": "We get our messages.", "timestamp": "00:27:16,614", "timestamp_s": 1636.0}, {"text": "It\u0027s added into the system message at the very end here.", "timestamp": "00:27:18,084", "timestamp_s": 1638.0}, {"text": "Our code.", "timestamp": "00:27:21,964", "timestamp_s": 1641.0}, {"text": "This is our endpoint.", "timestamp": "00:27:22,694", "timestamp_s": 1642.0}, {"text": "On the client side, in natural human language.", "timestamp": "00:27:24,529", "timestamp_s": 1644.0}, {"text": "And it\u0027s like a very clear description you give to somebody when they join a company.", "timestamp": "00:27:28,289", "timestamp_s": 1648.0}, {"text": "This is how you do your job.", "timestamp": "00:27:32,059", "timestamp_s": 1652.0}, {"text": "The more detail, the clearer you can be, the better.", "timestamp": "00:27:33,729", "timestamp_s": 1653.0}, {"text": "So, for example, we asked, what is 10 times 9?", "timestamp": "00:27:37,049", "timestamp_s": 1657.0}, {"text": "We\u0027ve added the messages in.", "timestamp": "00:27:40,289", "timestamp_s": 1660.0}, {"text": "We\u0027ve got back this response that it\u0027s determined the tool", "timestamp": "00:27:42,049", "timestamp_s": 1662.0}, {"text": "it needs is the calculator tool.", "timestamp": "00:27:44,669", "timestamp_s": 1664.0}, {"text": "It knows what to do next.", "timestamp": "00:27:46,489", "timestamp_s": 1666.0}, {"text": "It\u0027s the do calculation, and it knows the arguments.", "timestamp": "00:27:48,289", "timestamp_s": 1668.0}, {"text": "10 and 2, 10 and 9, and the operation is multiplication.", "timestamp": "00:27:50,939", "timestamp_s": 1670.0}, {"text": "What is 10 times 9?", "timestamp": "00:27:56,299", "timestamp_s": 1676.0}, {"text": "Notice how it\u0027s picked up times and multiplication.", "timestamp": "00:27:57,929", "timestamp_s": 1677.0}, {"text": "What we can then do is strip that all out, and if we come back down here, in", "timestamp": "00:28:00,859", "timestamp_s": 1680.0}, {"text": "this particular agent, we can actually run some code, or we could pass this on", "timestamp": "00:28:04,899", "timestamp_s": 1684.0}, {"text": "to another agent, or run it through a loop again, which we\u0027ll see later on.", "timestamp": "00:28:09,139", "timestamp_s": 1689.0}, {"text": "So in this example, because it knows that doNext says doCalculation, it extracts all", "timestamp": "00:28:13,579", "timestamp_s": 1693.0}, {"text": "the information, the tool, the arguments, and then basically just carries out these", "timestamp": "00:28:18,939", "timestamp_s": 1698.0}, {"text": "functions to produce the answer back.", "timestamp": "00:28:24,519", "timestamp_s": 1704.0}, {"text": "So if we do that again, but with some different numbers, 102 times 3,", "timestamp": "00:28:26,959", "timestamp_s": 1706.0}, {"text": "and we run it all, and we see it going back through to clear.", "timestamp": "00:28:31,819", "timestamp_s": 1711.0}, {"text": "It\u0027s now picked out arguments 102 and 3, operation multiply.", "timestamp": "00:28:34,939", "timestamp_s": 1714.0}, {"text": "Let\u0027s do add.", "timestamp": "00:28:40,139", "timestamp_s": 1720.0}, {"text": "What is 102\u0027s plus 3?", "timestamp": "00:28:41,459", "timestamp_s": 1721.0}, {"text": "Let\u0027s run that.", "timestamp": "00:28:44,619", "timestamp_s": 1724.0}, {"text": "We can see that it\u0027s picking out the arguments.", "timestamp": "00:28:45,659", "timestamp_s": 1725.0}, {"text": "It\u0027s knowing which tool it needs.", "timestamp": "00:28:48,839", "timestamp_s": 1728.0}, {"text": "1 0 2 3 and addition.", "timestamp": "00:28:50,344", "timestamp_s": 1730.0}, {"text": "And when we come back down to the answer, it produces the answer 1 0 5.", "timestamp": "00:28:53,094", "timestamp_s": 1733.0}, {"text": "Let\u0027s see if it picks up if it needs the different tool.", "timestamp": "00:28:57,774", "timestamp_s": 1737.0}, {"text": "So, for example, here, I\u0027ll do that.", "timestamp": "00:29:00,324", "timestamp_s": 1740.0}, {"text": "Tell me a joke.", "timestamp": "00:29:03,084", "timestamp_s": 1743.0}, {"text": "I\u0027m doing this at a builders conference.", "timestamp": "00:29:03,784", "timestamp_s": 1743.0}, {"text": "Let\u0027s run it all.", "timestamp": "00:29:05,844", "timestamp_s": 1745.0}, {"text": "So now it should determine that what is the right tool.", "timestamp": "00:29:07,174", "timestamp_s": 1747.0}, {"text": "It\u0027s the do joke tool.", "timestamp": "00:29:09,684", "timestamp_s": 1749.0}, {"text": "It\u0027s a different tool.", "timestamp": "00:29:10,834", "timestamp_s": 1750.0}, {"text": "There it are.", "timestamp": "00:29:12,264", "timestamp_s": 1752.0}, {"text": "The joke, do joke.", "timestamp": "00:29:12,917", "timestamp_s": 1752.0}, {"text": "The audience has picked up one of the arguments.", "timestamp": "00:29:13,803", "timestamp_s": 1753.0}, {"text": "It doesn\u0027t run here.", "timestamp": "00:29:16,134", "timestamp_s": 1756.0}, {"text": "Because the do next is do joke, it just goes off to the internet and gets", "timestamp": "00:29:18,064", "timestamp_s": 1758.0}, {"text": "a joke, and then we see the answer.", "timestamp": "00:29:21,904", "timestamp_s": 1761.0}, {"text": "So you may be thinking they all seem a little bit the same, the router,", "timestamp": "00:29:23,984", "timestamp_s": 1763.0}, {"text": "the tool use, the basic query.", "timestamp": "00:29:28,194", "timestamp_s": 1768.0}, {"text": "And I suppose they are because they\u0027re just function calls.", "timestamp": "00:29:30,394", "timestamp_s": 1770.0}, {"text": "At the end of the day, we\u0027re just doing Python functions, getting", "timestamp": "00:29:33,424", "timestamp_s": 1773.0}, {"text": "a response back from an API, then doing something with that response.", "timestamp": "00:29:36,364", "timestamp_s": 1776.0}, {"text": "The difference now is that we create our endpoint, we create our route on the", "timestamp": "00:29:40,334", "timestamp_s": 1780.0}, {"text": "client side, ship up that pseudocode.", "timestamp": "00:29:44,244", "timestamp_s": 1784.0}, {"text": "ship up the kind of query, and we get the answer back.", "timestamp": "00:29:47,344", "timestamp_s": 1787.0}, {"text": "And we also enable a certain level of autonomy because we can ask the", "timestamp": "00:29:51,214", "timestamp_s": 1791.0}, {"text": "LLM what it should do next based on the prompt that we sent it.", "timestamp": "00:29:55,094", "timestamp_s": 1795.0}, {"text": "So this is what is called tool use, or it\u0027s just function calling.", "timestamp": "00:29:59,694", "timestamp_s": 1799.0}, {"text": "And it\u0027s just a mechanism of how we do the function calling.", "timestamp": "00:30:02,974", "timestamp_s": 1802.0}, {"text": "And once again, the function takes place on our box.", "timestamp": "00:30:06,564", "timestamp_s": 1806.0}, {"text": "We don\u0027t run it on the LLM\u0027s box.", "timestamp": "00:30:10,304", "timestamp_s": 1810.0}, {"text": "And when we get the result of that information, we pass it back to the LLM.", "timestamp": "00:30:12,974", "timestamp_s": 1812.0}, {"text": "And we\u0027re going to see that when we come on to the what\u0027s called the reason act", "timestamp": "00:30:17,544", "timestamp_s": 1817.0}, {"text": "react type pattern for planning agent.", "timestamp": "00:30:23,064", "timestamp_s": 1823.0}, {"text": "So we\u0027ve seen a few little pieces of how we can create simple AI agents,", "timestamp": "00:30:26,644", "timestamp_s": 1826.0}, {"text": "and some can be quite powerful.", "timestamp": "00:30:32,194", "timestamp_s": 1832.0}, {"text": "Your utilities, like we saw in the frequently asked questions or the report", "timestamp": "00:30:33,714", "timestamp_s": 1833.0}, {"text": "selector and What we\u0027re going to do now is actually look at the four main patterns.", "timestamp": "00:30:37,489", "timestamp_s": 1837.0}, {"text": "Andrew Ng, in his lecture, listed here, talked about the four main patterns.", "timestamp": "00:30:43,254", "timestamp_s": 1843.0}, {"text": "Reflection, where the LLM kind of re examines its own work.", "timestamp": "00:30:48,794", "timestamp_s": 1848.0}, {"text": "It sends it back to itself, but with a critique to say, make a", "timestamp": "00:30:53,654", "timestamp_s": 1853.0}, {"text": "critique of what I\u0027ve just sent you.", "timestamp": "00:30:57,744", "timestamp_s": 1857.0}, {"text": "Tool use, we\u0027ve seen an example of that.", "timestamp": "00:31:00,284", "timestamp_s": 1860.0}, {"text": "planning, where it comes up with a plan to execute a multi step goal,", "timestamp": "00:31:03,084", "timestamp_s": 1863.0}, {"text": "and also multi agent collaboration.", "timestamp": "00:31:08,804", "timestamp_s": 1868.0}, {"text": "So we\u0027ve seen a number of these examples, and what we\u0027re going to", "timestamp": "00:31:11,164", "timestamp_s": 1871.0}, {"text": "do now is we\u0027re going to look at the reflection pattern, the tool pattern,", "timestamp": "00:31:13,764", "timestamp_s": 1873.0}, {"text": "planning, and the multi agent pattern.", "timestamp": "00:31:18,244", "timestamp_s": 1878.0}, {"text": "So let\u0027s go into the code to look at the reflection pattern to start with.", "timestamp": "00:31:20,634", "timestamp_s": 1880.0}, {"text": "We\u0027re now going to look at the reflection pattern.", "timestamp": "00:31:26,634", "timestamp_s": 1886.0}, {"text": "And let us not forget in essence what we\u0027re doing.", "timestamp": "00:31:29,694", "timestamp_s": 1889.0}, {"text": "We\u0027re just creating a big string job description that we send.", "timestamp": "00:31:32,654", "timestamp_s": 1892.0}, {"text": "We get some response.", "timestamp": "00:31:36,974", "timestamp_s": 1896.0}, {"text": "We may append that to a new request or start a new request from fresh.", "timestamp": "00:31:39,004", "timestamp_s": 1899.0}, {"text": "But in this reflection pattern, what we do is we generate a response with our", "timestamp": "00:31:44,134", "timestamp_s": 1904.0}, {"text": "first query, then add this content to the request in a second query where we\u0027ve", "timestamp": "00:31:47,214", "timestamp_s": 1907.0}, {"text": "asked it to do something, and in this case to have a critique and further refinement.", "timestamp": "00:31:53,834", "timestamp_s": 1913.0}, {"text": "So in some sense, the first request, it can be considered", "timestamp": "00:31:58,974", "timestamp_s": 1918.0}, {"text": "as actually almost like RAG.", "timestamp": "00:32:01,664", "timestamp_s": 1921.0}, {"text": "That we\u0027re generating some content to add to a new query,", "timestamp": "00:32:03,749", "timestamp_s": 1923.0}, {"text": "augmenting it with a new set of instructions and getting a response.", "timestamp": "00:32:08,629", "timestamp_s": 1928.0}, {"text": "And what we\u0027re going to do in this one is we\u0027re going to ask it to generate", "timestamp": "00:32:13,279", "timestamp_s": 1933.0}, {"text": "some Python code, and then we\u0027re going to ask for it to critique it and make some", "timestamp": "00:32:15,669", "timestamp_s": 1935.0}, {"text": "adjustments to produce a final response.", "timestamp": "00:32:20,529", "timestamp_s": 1940.0}, {"text": "So we use our usual standard opening, getting the key, setting the models.", "timestamp": "00:32:23,059", "timestamp_s": 1943.0}, {"text": "And we can see here that we\u0027re setting the very first system", "timestamp": "00:32:29,499", "timestamp_s": 1949.0}, {"text": "message, first role, as a Python program tasked with generating code.", "timestamp": "00:32:32,789", "timestamp_s": 1952.0}, {"text": "And we\u0027re generating our chat history.", "timestamp": "00:32:37,859", "timestamp_s": 1957.0}, {"text": "We append our system content, the job description, as it were.", "timestamp": "00:32:39,709", "timestamp_s": 1959.0}, {"text": "We then add to that, list, the user query, which in this case is", "timestamp": "00:32:44,639", "timestamp_s": 1964.0}, {"text": "generate a Python implementation of requesting an API with request library.", "timestamp": "00:32:50,429", "timestamp_s": 1970.0}, {"text": "And we then send that to the LLM", "timestamp": "00:32:54,629", "timestamp_s": 1974.0}, {"text": "to get our response back.", "timestamp": "00:32:56,989", "timestamp_s": 1976.0}, {"text": "And here we get our response.", "timestamp": "00:32:59,559", "timestamp_s": 1979.0}, {"text": "And before we do that in for the next step, we also add what we got back.", "timestamp": "00:33:01,719", "timestamp_s": 1981.0}, {"text": "So we can see we get the response from the LLM.", "timestamp": "00:33:06,529", "timestamp_s": 1986.0}, {"text": "It\u0027s giving us some sample code with an explanation.", "timestamp": "00:33:10,319", "timestamp_s": 1990.0}, {"text": "Now we want to reflect on that.", "timestamp": "00:33:14,759", "timestamp_s": 1994.0}, {"text": "We want to send it back again and have a little critique or refinement.", "timestamp": "00:33:16,259", "timestamp_s": 1996.0}, {"text": "So in our chat history, we add in now another system message,", "timestamp": "00:33:20,009", "timestamp_s": 2000.0}, {"text": "and we\u0027re saying you\u0027re an experienced and talented Pythonista.", "timestamp": "00:33:24,429", "timestamp_s": 2004.0}, {"text": "You\u0027re tasked with generating critique and recommendations for the user\u0027s code.", "timestamp": "00:33:27,329", "timestamp_s": 2007.0}, {"text": "All of these messages are attached together and sent, because we", "timestamp": "00:33:31,769", "timestamp_s": 2011.0}, {"text": "must remember that the request is stateless, so it needs to know what", "timestamp": "00:33:35,929", "timestamp_s": 2015.0}, {"text": "went on before, so we must parse in that history of the conversation.", "timestamp": "00:33:39,539", "timestamp_s": 2019.0}, {"text": "And we then get our critique back and display it.", "timestamp": "00:33:43,789", "timestamp_s": 2023.0}, {"text": "And here is the output.", "timestamp": "00:33:48,754", "timestamp_s": 2028.0}, {"text": "Your code demonstrates a solid approach going through it all, all", "timestamp": "00:33:50,034", "timestamp_s": 2030.0}, {"text": "the way down.", "timestamp": "00:33:54,774", "timestamp_s": 2034.0}, {"text": "And then we add this again, this critique to our chat history, send it", "timestamp": "00:33:55,894", "timestamp_s": 2035.0}, {"text": "all in again to get a final summary.", "timestamp": "00:34:00,814", "timestamp_s": 2040.0}, {"text": "And here\u0027s our final summary in form of an essay.", "timestamp": "00:34:03,294", "timestamp_s": 2043.0}, {"text": "You scroll down, and here is our final output as requested.", "timestamp": "00:34:06,214", "timestamp_s": 2046.0}, {"text": "Now, of course, what we do and how many times we go through it is up to us, but", "timestamp": "00:34:11,374", "timestamp_s": 2051.0}, {"text": "the pattern is literally just Making requests, getting a response, taking that", "timestamp": "00:34:16,454", "timestamp_s": 2056.0}, {"text": "response, adding it back into our chat history, adding in a new prompt to do", "timestamp": "00:34:21,659", "timestamp_s": 2061.0}, {"text": "something with that and get a response.", "timestamp": "00:34:27,869", "timestamp_s": 2067.0}, {"text": "It\u0027s one function after another with inputs producing outputs get", "timestamp": "00:34:29,949", "timestamp_s": 2069.0}, {"text": "put into the next function as an input that produces an output.", "timestamp": "00:34:34,459", "timestamp_s": 2074.0}, {"text": "So this is a reflection pattern.", "timestamp": "00:34:38,409", "timestamp_s": 2078.0}, {"text": "And as we scroll down, we see the final answer.", "timestamp": "00:34:40,469", "timestamp_s": 2080.0}, {"text": "Key improvements.", "timestamp": "00:34:43,269", "timestamp_s": 2083.0}, {"text": "And, of course, we can make this a class and make the input come through a", "timestamp": "00:34:45,179", "timestamp_s": 2085.0}, {"text": "certain form field or through a chat bot.", "timestamp": "00:34:48,509", "timestamp_s": 2088.0}, {"text": "And once again, if we go up to the very top, we can remember, in essence,", "timestamp": "00:34:51,009", "timestamp_s": 2091.0}, {"text": "this is really what we\u0027re doing.", "timestamp": "00:34:54,999", "timestamp_s": 2094.0}, {"text": "We\u0027re making a function.", "timestamp": "00:34:56,279", "timestamp_s": 2096.0}, {"text": "We\u0027re passing in inputs, getting an output and really chaining it", "timestamp": "00:34:57,349", "timestamp_s": 2097.0}, {"text": "from one request to the other.", "timestamp": "00:35:01,299", "timestamp_s": 2101.0}, {"text": "In this particular reflection pattern, we\u0027re going to look at using planning", "timestamp": "00:35:03,569", "timestamp_s": 2103.0}, {"text": "or using reflection and tool calling.", "timestamp": "00:35:08,699", "timestamp_s": 2108.0}, {"text": "Once again, let us remind ourselves that we\u0027re looking in this talk to see", "timestamp": "00:35:11,659", "timestamp_s": 2111.0}, {"text": "what AI agents are in terms of their simplicity before we move into frameworks.", "timestamp": "00:35:14,979", "timestamp_s": 2114.0}, {"text": "And what we\u0027re going to do now is use a sort of pattern of react, reason and act.", "timestamp": "00:35:20,029", "timestamp_s": 2120.0}, {"text": "And essentially, we pass the output of each step as an input to the next request", "timestamp": "00:35:25,499", "timestamp_s": 2125.0}, {"text": "to an LLM, like we did in Reflection.", "timestamp": "00:35:30,099", "timestamp_s": 2130.0}, {"text": "But we\u0027re going to add in some tool calling, and we\u0027re also going to be adding", "timestamp": "00:35:32,299", "timestamp_s": 2132.0}, {"text": "in some effectively routing, because the agent will determine which tool to use.", "timestamp": "00:35:35,559", "timestamp_s": 2135.0}, {"text": "And how to use it.", "timestamp": "00:35:41,139", "timestamp_s": 2141.0}, {"text": "We\u0027ll do this with the 20 planning agent with loop dot py But we will also do it", "timestamp": "00:35:42,649", "timestamp_s": 2142.0}, {"text": "with the notebook where we will do this Looping manually so we can see how it", "timestamp": "00:35:48,059", "timestamp_s": 2148.0}, {"text": "works So let\u0027s just go to the python file and we can see that we run in everything", "timestamp": "00:35:52,559", "timestamp_s": 2152.0}, {"text": "As usual and we\u0027re creating an agent class where we\u0027re setting the client the system", "timestamp": "00:35:58,719", "timestamp_s": 2158.0}, {"text": "role We\u0027re using the under core method.", "timestamp": "00:36:04,279", "timestamp_s": 2164.0}, {"text": "We\u0027re using an execute function where we can just invoke the llm to get a response", "timestamp": "00:36:07,309", "timestamp_s": 2167.0}, {"text": "You And what we want to do in this example is calculate the total price for an item.", "timestamp": "00:36:11,339", "timestamp_s": 2171.0}, {"text": "And the two tools we have are calculate the total that", "timestamp": "00:36:19,529", "timestamp_s": 2179.0}, {"text": "given a price adds on the VAT.", "timestamp": "00:36:22,129", "timestamp_s": 2182.0}, {"text": "We also have another tool, function, get product price, that for a given", "timestamp": "00:36:25,009", "timestamp_s": 2185.0}, {"text": "argument gets the price of the product.", "timestamp": "00:36:30,039", "timestamp_s": 2190.0}, {"text": "And if we scroll down, we will see these two functions here.", "timestamp": "00:36:33,194", "timestamp_s": 2193.0}, {"text": "CalculateTotal, GetProductPrice.", "timestamp": "00:36:36,044", "timestamp_s": 2196.0}, {"text": "So let\u0027s look at our system prompt, and we\u0027re telling it how we want it to work.", "timestamp": "00:36:38,534", "timestamp_s": 2198.0}, {"text": "We want it to think, take an action, get an observation, and repeat the loop.", "timestamp": "00:36:43,364", "timestamp_s": 2203.0}, {"text": "So we give examples of what the tools are.", "timestamp": "00:36:49,114", "timestamp_s": 2209.0}, {"text": "What the kind of response we\u0027d like to get back for both the calculate", "timestamp": "00:36:53,929", "timestamp_s": 2213.0}, {"text": "price, total and get product price.", "timestamp": "00:36:58,399", "timestamp_s": 2218.0}, {"text": "And we\u0027re also given an example session.", "timestamp": "00:37:01,879", "timestamp_s": 2221.0}, {"text": "So what\u0027s going to happen is a user is going to say, what is the total", "timestamp": "00:37:04,139", "timestamp_s": 2224.0}, {"text": "cost of a bike, including that?", "timestamp": "00:37:06,679", "timestamp_s": 2226.0}, {"text": "We want the AI response to be in this format.", "timestamp": "00:37:09,029", "timestamp_s": 2229.0}, {"text": "Thought, I need to find the cost of a bike.", "timestamp": "00:37:12,189", "timestamp_s": 2232.0}, {"text": "We\u0027re pipe delimbing it so we can get the function name and the price.", "timestamp": "00:37:15,219", "timestamp_s": 2235.0}, {"text": "arguments, but it\u0027s going to be an action type as opposed to an answer.", "timestamp": "00:37:19,029", "timestamp_s": 2239.0}, {"text": "We\u0027re going to get the tool call and we\u0027re going to get the argument.", "timestamp": "00:37:23,169", "timestamp_s": 2243.0}, {"text": "We will get the response back of an observation of the actual return of", "timestamp": "00:37:27,219", "timestamp_s": 2247.0}, {"text": "that function, which we will then use as an input to the next query", "timestamp": "00:37:31,519", "timestamp_s": 2251.0}, {"text": "where it now needs to calculate the total price including the VAT.", "timestamp": "00:37:35,719", "timestamp_s": 2255.0}, {"text": "It\u0027s an action.", "timestamp": "00:37:39,859", "timestamp_s": 2259.0}, {"text": "It knows to use the calculate total, and it has an argument passed into it.", "timestamp": "00:37:41,034", "timestamp_s": 2261.0}, {"text": "This is just a sample example so it can see the format of what it needs", "timestamp": "00:37:45,204", "timestamp_s": 2265.0}, {"text": "to do, and that we will always be passing in the result of our LLM", "timestamp": "00:37:48,844", "timestamp_s": 2268.0}, {"text": "course as observation pipe 240.", "timestamp": "00:37:53,524", "timestamp_s": 2273.0}, {"text": "That\u0027s what it can expect.", "timestamp": "00:37:56,824", "timestamp_s": 2276.0}, {"text": "Then we tell it that if you have the answer, print out an", "timestamp": "00:37:58,664", "timestamp_s": 2278.0}, {"text": "answer for us in this form.", "timestamp": "00:38:01,244", "timestamp_s": 2281.0}, {"text": "So let\u0027s just run this just to see what it will actually look like.", "timestamp": "00:38:03,144", "timestamp_s": 2283.0}, {"text": "And if we scroll down.", "timestamp": "00:38:06,384", "timestamp_s": 2286.0}, {"text": "We can see we\u0027ve got a loop.", "timestamp": "00:38:07,779", "timestamp_s": 2287.0}, {"text": "We\u0027re not going to get into each line of the code, but basically", "timestamp": "00:38:09,079", "timestamp_s": 2289.0}, {"text": "it\u0027s going to be looping around.", "timestamp": "00:38:11,899", "timestamp_s": 2291.0}, {"text": "If it\u0027s an action, it will run that function.", "timestamp": "00:38:13,799", "timestamp_s": 2293.0}, {"text": "Get a value and pass it back through to be used again.", "timestamp": "00:38:16,589", "timestamp_s": 2296.0}, {"text": "If it determines it has an answer, it\u0027ll print the answer and exit the loop.", "timestamp": "00:38:20,369", "timestamp_s": 2300.0}, {"text": "So, for example, we\u0027ve got three here.", "timestamp": "00:38:25,259", "timestamp_s": 2305.0}, {"text": "One is the cost of a bike, a TV, and a laptop.", "timestamp": "00:38:26,989", "timestamp_s": 2306.0}, {"text": "So, let\u0027s run that and see what happens.", "timestamp": "00:38:29,569", "timestamp_s": 2309.0}, {"text": "It\u0027s starting the loop.", "timestamp": "00:38:32,259", "timestamp_s": 2312.0}, {"text": "It has a thought.", "timestamp": "00:38:33,289", "timestamp_s": 2313.0}, {"text": "It has an observation.", "timestamp": "00:38:34,499", "timestamp_s": 2314.0}, {"text": "It gets passed in.", "timestamp": "00:38:36,619", "timestamp_s": 2316.0}, {"text": "And each time we get the result here, but we can see the summary answers.", "timestamp": "00:38:38,249", "timestamp_s": 2318.0}, {"text": "But if we look at a particular loop starting the loop, it has the thought,", "timestamp": "00:38:42,034", "timestamp_s": 2322.0}, {"text": "I need to find the cost of a TV.", "timestamp": "00:38:44,794", "timestamp_s": 2324.0}, {"text": "We\u0027ve extracted out the action.", "timestamp": "00:38:47,104", "timestamp_s": 2327.0}, {"text": "We\u0027ve extracted out that the function call is going to be get", "timestamp": "00:38:49,474", "timestamp_s": 2329.0}, {"text": "product price and the parameter.", "timestamp": "00:38:51,834", "timestamp_s": 2331.0}, {"text": "We get an observation of 200.", "timestamp": "00:38:54,424", "timestamp_s": 2334.0}, {"text": "That now gets VAT back in, and it now knows it needs to calculate", "timestamp": "00:38:56,494", "timestamp_s": 2336.0}, {"text": "the total, including the VAT.", "timestamp": "00:39:00,404", "timestamp_s": 2340.0}, {"text": "It\u0027s going to use a calculateTotal with the 200 that we passed in.", "timestamp": "00:39:01,974", "timestamp_s": 2341.0}, {"text": "The observation is returned back as 240.", "timestamp": "00:39:05,954", "timestamp_s": 2345.0}, {"text": "That gets put into the loop to get the answer.", "timestamp": "00:39:08,854", "timestamp_s": 2348.0}, {"text": "Now, if you notice, we\u0027ve missed one here, and we have the answer here.", "timestamp": "00:39:11,994", "timestamp_s": 2351.0}, {"text": "And that\u0027s the nature sometimes of LLMs, that things do actually go", "timestamp": "00:39:17,524", "timestamp_s": 2357.0}, {"text": "wrong because it\u0027s probabilistic.", "timestamp": "00:39:21,934", "timestamp_s": 2361.0}, {"text": "But we get the first one, the price of the bike is 120.", "timestamp": "00:39:25,034", "timestamp_s": 2365.0}, {"text": "We do get the result, 240, but we don\u0027t get it printed out in the answer.", "timestamp": "00:39:28,094", "timestamp_s": 2368.0}, {"text": "And if we were to run this again, come", "timestamp": "00:39:32,984", "timestamp_s": 2372.0}, {"text": "back in here and open this up, and if we run it again, we\u0027ll", "timestamp": "00:39:35,254", "timestamp_s": 2375.0}, {"text": "probably see a different answer.", "timestamp": "00:39:39,574", "timestamp_s": 2379.0}, {"text": "Starting the loop, 120, answer found, the price of the bike is 120.", "timestamp": "00:39:41,254", "timestamp_s": 2381.0}, {"text": "Starting again, we found the price of the TV is 240.", "timestamp": "00:39:47,214", "timestamp_s": 2387.0}, {"text": "And now we\u0027ve got the price of the laptop, 360, and we get all three answers.", "timestamp": "00:39:50,804", "timestamp_s": 2390.0}, {"text": "So it can show that we can\u0027t take things for granted because", "timestamp": "00:39:55,024", "timestamp_s": 2395.0}, {"text": "the Things will be missed out.", "timestamp": "00:39:57,884", "timestamp_s": 2397.0}, {"text": "It is not 100 percent deterministic.", "timestamp": "00:39:59,589", "timestamp_s": 2399.0}, {"text": "So we need to put in some checks and balances if we were using these results.", "timestamp": "00:40:02,589", "timestamp_s": 2402.0}, {"text": "But this is an example of actually how we can loop through.", "timestamp": "00:40:06,759", "timestamp_s": 2406.0}, {"text": "So what we\u0027re going to do now is actually go to the notebook version", "timestamp": "00:40:10,839", "timestamp_s": 2410.0}, {"text": "and see how would we do this manually.", "timestamp": "00:40:14,009", "timestamp_s": 2414.0}, {"text": "What\u0027s actually going on?", "timestamp": "00:40:15,799", "timestamp_s": 2415.0}, {"text": "So We use the same setup, use the same system prompt, same functions.", "timestamp": "00:40:17,369", "timestamp_s": 2417.0}, {"text": "But what we do now is we call an instance of this planning agent with", "timestamp": "00:40:24,689", "timestamp_s": 2424.0}, {"text": "the client and the system prompt.", "timestamp": "00:40:28,049", "timestamp_s": 2428.0}, {"text": "We set up an instance of it and we pass through the first question.", "timestamp": "00:40:30,089", "timestamp_s": 2430.0}, {"text": "What is the cost of a laptop, including the VAT?", "timestamp": "00:40:33,729", "timestamp_s": 2433.0}, {"text": "When we run that, we get back the result, which is thought.", "timestamp": "00:40:36,569", "timestamp_s": 2436.0}, {"text": "That we can now extract out by splitting on the pipe exactly whether", "timestamp": "00:40:40,949", "timestamp_s": 2440.0}, {"text": "it\u0027s an action, what the function name is, and what the parameter", "timestamp": "00:40:45,289", "timestamp_s": 2445.0}, {"text": "name is, what the argument is.", "timestamp": "00:40:48,549", "timestamp_s": 2448.0}, {"text": "And as you can see, when we do that, we get the function, we", "timestamp": "00:40:50,509", "timestamp_s": 2450.0}, {"text": "need to run the getProductPrice, and we need to do it on a laptop.", "timestamp": "00:40:53,409", "timestamp_s": 2453.0}, {"text": "So manually, this is what we would do.", "timestamp": "00:40:56,809", "timestamp_s": 2456.0}, {"text": "We\u0027d say, if the next function is, which we\u0027ve just determined here by extracting", "timestamp": "00:40:59,069", "timestamp_s": 2459.0}, {"text": "out this item here in the output.", "timestamp": "00:41:02,949", "timestamp_s": 2462.0}, {"text": "If it\u0027s get product price, run get product price with that value.", "timestamp": "00:41:07,419", "timestamp_s": 2467.0}, {"text": "The argument that we got is the third, fourth item here.", "timestamp": "00:41:11,839", "timestamp_s": 2471.0}, {"text": "So we get 300, and we said in our prompt that we would send it back in in the form", "timestamp": "00:41:15,389", "timestamp_s": 2475.0}, {"text": "of observation, pipe, and the result.", "timestamp": "00:41:19,769", "timestamp_s": 2479.0}, {"text": "So this is what we\u0027re going to send as the next prompt.", "timestamp": "00:41:22,219", "timestamp_s": 2482.0}, {"text": "So we run it with our agent.", "timestamp": "00:41:24,919", "timestamp_s": 2484.0}, {"text": "Next prompt, the result.", "timestamp": "00:41:26,889", "timestamp_s": 2486.0}, {"text": "It now determines it still needs another action, but it\u0027s the calculate", "timestamp": "00:41:28,599", "timestamp_s": 2488.0}, {"text": "total and it needs the price of 300 to be calculated with VAT.", "timestamp": "00:41:32,129", "timestamp_s": 2492.0}, {"text": "So once again, we strip out the function and the parameter.", "timestamp": "00:41:37,809", "timestamp_s": 2497.0}, {"text": "We need to do calculate total and the argument of 300.", "timestamp": "00:41:41,399", "timestamp_s": 2501.0}, {"text": "And once again, we run this manually.", "timestamp": "00:41:45,079", "timestamp_s": 2505.0}, {"text": "You know, it\u0027s to calculate total.", "timestamp": "00:41:46,989", "timestamp_s": 2506.0}, {"text": "And the next arc, there are different ways that we could", "timestamp": "00:41:48,519", "timestamp_s": 2508.0}, {"text": "run that using, an eval method.", "timestamp": "00:41:51,429", "timestamp_s": 2511.0}, {"text": "But for simplicity, we\u0027re just going to run it manually is calculate total.", "timestamp": "00:41:53,639", "timestamp_s": 2513.0}, {"text": "We get the answer 360.", "timestamp": "00:41:57,219", "timestamp_s": 2517.0}, {"text": "Once again, we\u0027re going to send that back in, in the form of observation result.", "timestamp": "00:41:59,179", "timestamp_s": 2519.0}, {"text": "That\u0027s what we specified in the system prompt.", "timestamp": "00:42:03,389", "timestamp_s": 2523.0}, {"text": "So we\u0027re sending this back in as the next query.", "timestamp": "00:42:05,809", "timestamp_s": 2525.0}, {"text": "That\u0027s the next prompt.", "timestamp": "00:42:08,249", "timestamp_s": 2528.0}, {"text": "Pass it in to the agent, the next prompt of observation 360.", "timestamp": "00:42:09,919", "timestamp_s": 2529.0}, {"text": "Print the result.", "timestamp": "00:42:14,079", "timestamp_s": 2534.0}, {"text": "Now that it\u0027s determined it has an answer, it prints it", "timestamp": "00:42:15,269", "timestamp_s": 2535.0}, {"text": "out in the format that we want.", "timestamp": "00:42:17,849", "timestamp_s": 2537.0}, {"text": "Answer.", "timestamp": "00:42:19,799", "timestamp_s": 2539.0}, {"text": "The price of the laptop, including that, is 360.", "timestamp": "00:42:19,999", "timestamp_s": 2539.0}, {"text": "So if we go back up again and basically change it, and now let\u0027s", "timestamp": "00:42:22,849", "timestamp_s": 2542.0}, {"text": "work out for the price of the TV and see exactly the same again.", "timestamp": "00:42:26,329", "timestamp_s": 2546.0}, {"text": "We\u0027re going running through.", "timestamp": "00:42:29,819", "timestamp_s": 2549.0}, {"text": "We", "timestamp": "00:42:31,479", "timestamp_s": 2551.0}, {"text": "now know the action is get product price, but it\u0027s for a TV.", "timestamp": "00:42:32,319", "timestamp_s": 2552.0}, {"text": "It runs the function.", "timestamp": "00:42:35,759", "timestamp_s": 2555.0}, {"text": "It gets the price that it\u0027s 200.", "timestamp": "00:42:36,719", "timestamp_s": 2556.0}, {"text": "We send it back in and saying the observation from", "timestamp": "00:42:39,049", "timestamp_s": 2559.0}, {"text": "the previous action is 200.", "timestamp": "00:42:42,179", "timestamp_s": 2562.0}, {"text": "It then extracts what it needs, and now it\u0027s starting to calculate the total", "timestamp": "00:42:44,189", "timestamp_s": 2564.0}, {"text": "with the VAT, 200, extracts again, runs.", "timestamp": "00:42:48,209", "timestamp_s": 2568.0}, {"text": "We then run the function with the extracted argument, we get 240.", "timestamp": "00:42:52,669", "timestamp_s": 2572.0}, {"text": "We now send this observation back in the form that we said we", "timestamp": "00:42:58,089", "timestamp_s": 2578.0}, {"text": "would, observation pipe result.", "timestamp": "00:43:01,459", "timestamp_s": 2581.0}, {"text": "You send this into the next query, goes into here, you print the result, answer", "timestamp": "00:43:04,004", "timestamp_s": 2584.0}, {"text": "the price of the TV, including VAT is 240.", "timestamp": "00:43:09,264", "timestamp_s": 2589.0}, {"text": "Now, there are many ways you can refactor this and use eval without", "timestamp": "00:43:11,994", "timestamp_s": 2591.0}, {"text": "actually having to specify in a step the actual function name.", "timestamp": "00:43:15,424", "timestamp_s": 2595.0}, {"text": "That\u0027s effectively what we do when we do our loop.", "timestamp": "00:43:18,704", "timestamp_s": 2598.0}, {"text": "We basically extract all that information and execute appropriately in our loop.", "timestamp": "00:43:21,294", "timestamp_s": 2601.0}, {"text": "So I hope this kind of shows that although we can have many different patterns,", "timestamp": "00:43:27,164", "timestamp_s": 2607.0}, {"text": "Essentially, it\u0027s basically involving reflection, sending back a previous output", "timestamp": "00:43:31,314", "timestamp_s": 2611.0}, {"text": "as an input, use of tool call, and also the use of planning of breaking up a task", "timestamp": "00:43:36,604", "timestamp_s": 2616.0}, {"text": "into separate steps that it executes.", "timestamp": "00:43:43,474", "timestamp_s": 2623.0}, {"text": "So they all sort of overlap and are interconnected.", "timestamp": "00:43:46,124", "timestamp_s": 2626.0}, {"text": "And I hope that this actually explains how AI agents work and how if we look at where", "timestamp": "00:43:49,574", "timestamp_s": 2629.0}, {"text": "is the AI in all of this, literally it is just in the execute function to the LLM.", "timestamp": "00:43:54,614", "timestamp_s": 2634.0}, {"text": "The rest of the AI part is just where we invoke the instance of", "timestamp": "00:44:01,464", "timestamp_s": 2641.0}, {"text": "the agent with the next prompt that produces all these results.", "timestamp": "00:44:05,374", "timestamp_s": 2645.0}, {"text": "The rest is we\u0027re just stripping with day to day Python, sending", "timestamp": "00:44:09,274", "timestamp_s": 2649.0}, {"text": "it back in to the loop, and then running again another AI API request.", "timestamp": "00:44:12,564", "timestamp_s": 2652.0}, {"text": "Well, we\u0027ve just been through quite a lot of code.", "timestamp": "00:44:18,924", "timestamp_s": 2658.0}, {"text": "It\u0027s something really to digest offline.", "timestamp": "00:44:21,494", "timestamp_s": 2661.0}, {"text": "As I\u0027ve said before, it\u0027s taken me quite a while just to kind of work through", "timestamp": "00:44:24,614", "timestamp_s": 2664.0}, {"text": "and really get it clear in my own mind to be able to explain it to people.", "timestamp": "00:44:28,034", "timestamp_s": 2668.0}, {"text": "So if you don\u0027t feel it\u0027s kind of sunk in, that\u0027s not a problem at all.", "timestamp": "00:44:31,604", "timestamp_s": 2671.0}, {"text": "That\u0027s probably to be expected.", "timestamp": "00:44:35,754", "timestamp_s": 2675.0}, {"text": "And that\u0027s why I\u0027ve given you the repo and all the code examples here so you", "timestamp": "00:44:37,704", "timestamp_s": 2677.0}, {"text": "can work through them, chop it up, change them around to get understand it.", "timestamp": "00:44:41,044", "timestamp_s": 2681.0}, {"text": "So we saw the sort of four main, we\u0027ve seen sort of reflection, we\u0027ve", "timestamp": "00:44:45,214", "timestamp_s": 2685.0}, {"text": "seen tool use, we\u0027ve seen a sort of planning where we\u0027re sort of", "timestamp": "00:44:48,274", "timestamp_s": 2688.0}, {"text": "basically having to break a problem down into steps and work through them.", "timestamp": "00:44:51,604", "timestamp_s": 2691.0}, {"text": "We haven\u0027t really seen multi agent collaboration.", "timestamp": "00:44:56,204", "timestamp_s": 2696.0}, {"text": "And the reason for this is basically it\u0027s a design pattern of how one", "timestamp": "00:44:59,194", "timestamp_s": 2699.0}, {"text": "wants to use what one gets back from each agent with another agent.", "timestamp": "00:45:03,974", "timestamp_s": 2703.0}, {"text": "Is it the active pattern, the pub sub, finite state machine, as", "timestamp": "00:45:08,534", "timestamp_s": 2708.0}, {"text": "in, for example, like Landgraf?", "timestamp": "00:45:12,554", "timestamp_s": 2712.0}, {"text": "And what I\u0027d like to talk about in closing is the libraries and", "timestamp": "00:45:15,024", "timestamp_s": 2715.0}, {"text": "frameworks that are available.", "timestamp": "00:45:20,124", "timestamp_s": 2720.0}, {"text": "And I like to think of libraries as sort of frameworks without", "timestamp": "00:45:21,564", "timestamp_s": 2721.0}, {"text": "the framework, as it were.", "timestamp": "00:45:24,344", "timestamp_s": 2724.0}, {"text": "Lots of convenience, tools, simplicity, that we can build", "timestamp": "00:45:25,684", "timestamp_s": 2725.0}, {"text": "the overall design pattern.", "timestamp": "00:45:28,804", "timestamp_s": 2728.0}, {"text": "And Pydantic AI is a new one that\u0027s come towards the end of 2024.", "timestamp": "00:45:30,944", "timestamp_s": 2730.0}, {"text": "Pydantic is well known in the Python community.", "timestamp": "00:45:35,484", "timestamp_s": 2735.0}, {"text": "And as you can see, structured output.", "timestamp": "00:45:38,514", "timestamp_s": 2738.0}, {"text": "It\u0027s very important because we want to pass structured output from one agent", "timestamp": "00:45:40,969", "timestamp_s": 2740.0}, {"text": "or from one part of our app to the next.", "timestamp": "00:45:45,369", "timestamp_s": 2745.0}, {"text": "HuggingFace SmallAgents is another very small, lightweight framework", "timestamp": "00:45:48,059", "timestamp_s": 2748.0}, {"text": "library that they implement so that one can create one\u0027s own overall", "timestamp": "00:45:52,809", "timestamp_s": 2752.0}, {"text": "framework or design pattern.", "timestamp": "00:45:57,799", "timestamp_s": 2757.0}, {"text": "Then we see many different crews and swarms and frameworks.", "timestamp": "00:45:59,369", "timestamp_s": 2759.0}, {"text": "And these are basically design patterns for, essentially, intercommunication", "timestamp": "00:46:03,229", "timestamp_s": 2763.0}, {"text": "between bits of code, our AI agents.", "timestamp": "00:46:08,179", "timestamp_s": 2768.0}, {"text": "And we saw before in the AI agents directory, how many different", "timestamp": "00:46:11,299", "timestamp_s": 2771.0}, {"text": "frameworks there are around there.", "timestamp": "00:46:14,769", "timestamp_s": 2774.0}, {"text": "As you can see, if we start with the simplicity as we have been doing, we", "timestamp": "00:46:17,089", "timestamp_s": 2777.0}, {"text": "might refactor it, build our own library.", "timestamp": "00:46:20,699", "timestamp_s": 2780.0}, {"text": "And then realize actually other people could benefit from them, customize", "timestamp": "00:46:23,374", "timestamp_s": 2783.0}, {"text": "it to make it more easy to use.", "timestamp": "00:46:27,024", "timestamp_s": 2787.0}, {"text": "And then we add another framework to the AI agents directory, for example.", "timestamp": "00:46:28,934", "timestamp_s": 2788.0}, {"text": "So in terms of frameworks, there\u0027s LLAMA Index, LLANG Chain, LLANG Graph,", "timestamp": "00:46:33,814", "timestamp_s": 2793.0}, {"text": "AutoGen, Crew AI, and then very many more.", "timestamp": "00:46:37,254", "timestamp_s": 2797.0}, {"text": "All very good, all very useful.", "timestamp": "00:46:40,804", "timestamp_s": 2800.0}, {"text": "And hopefully, having gone through the simplicity of these AI agents,", "timestamp": "00:46:42,794", "timestamp_s": 2802.0}, {"text": "not only could you maybe just design your own mini framework, But when", "timestamp": "00:46:46,904", "timestamp_s": 2806.0}, {"text": "you come to use these frameworks, you will actually understand what it is", "timestamp": "00:46:50,234", "timestamp_s": 2810.0}, {"text": "they\u0027re doing and how they\u0027re doing it.", "timestamp": "00:46:53,484", "timestamp_s": 2813.0}, {"text": "So, in summary, I hope AI agents have been demystified and helped us understand", "timestamp": "00:46:55,464", "timestamp_s": 2815.0}, {"text": "what they can do, enabling us either to build our own frameworks or use", "timestamp": "00:47:00,424", "timestamp_s": 2820.0}, {"text": "existing ones, with a deeper appreciation and understanding of how they work.", "timestamp": "00:47:03,854", "timestamp_s": 2823.0}, {"text": "And Anthropic has, in their blog, has come up with this,", "timestamp": "00:47:08,404", "timestamp_s": 2828.0}, {"text": "when and when not to use agents.", "timestamp": "00:47:12,334", "timestamp_s": 2832.0}, {"text": "And I\u0027ll just read it out loud because it sums it up better than I could.", "timestamp": "00:47:15,554", "timestamp_s": 2835.0}, {"text": "When building applications with LLMs, we recommend finding the", "timestamp": "00:47:19,029", "timestamp_s": 2839.0}, {"text": "simplest solution possible, and only increasing complexity when needed.", "timestamp": "00:47:22,559", "timestamp_s": 2842.0}, {"text": "This might mean not building agentic systems at all.", "timestamp": "00:47:26,899", "timestamp_s": 2846.0}, {"text": "Agentic systems often trade latency and cost for better task", "timestamp": "00:47:29,989", "timestamp_s": 2849.0}, {"text": "performance, and you should consider when this trade off makes sense.", "timestamp": "00:47:32,939", "timestamp_s": 2852.0}, {"text": "When more complexity is warranted, workflows offer predictability and", "timestamp": "00:47:36,799", "timestamp_s": 2856.0}, {"text": "consistency for well defined tasks, whereas agents are the better options", "timestamp": "00:47:40,629", "timestamp_s": 2860.0}, {"text": "when flexibility and model driven decision making are needed at scale.", "timestamp": "00:47:44,909", "timestamp_s": 2864.0}, {"text": "For many applications, however, optimizing single LLM calls with retrieval and", "timestamp": "00:47:49,339", "timestamp_s": 2869.0}, {"text": "in context examples is usually enough.", "timestamp": "00:47:54,429", "timestamp_s": 2874.0}, {"text": "Thank you very much for letting me present this topic on AI agents,", "timestamp": "00:47:56,989", "timestamp_s": 2876.0}, {"text": "their simplicity and their power.", "timestamp": "00:48:01,139", "timestamp_s": 2881.0}, {"text": "My name is Craig West.", "timestamp": "00:48:04,069", "timestamp_s": 2884.0}, {"text": "Thank you very much.", "timestamp": "00:48:05,379", "timestamp_s": 2885.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'BaWTWtKP7nE',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Implementing Agentic AI Solutions in Python from scratch
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>The use of AI and AI Agents in everyday Django can be viewed as <code>AI as API</code> where we can create on the Django side powerful agentic APIs.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./srt/python2025_Craig_West.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:00,200'); seek(0.0)">
              Hello, everyone, and welcome to the implementing agentic AI
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:03,540'); seek(3.0)">
              solutions in Python from scratch.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:06,090'); seek(6.0)">
              Here is the repo, and you have full access to it.
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:09,589'); seek(9.0)">
              We have all the code samples.
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:11,550'); seek(11.0)">
              I'm using my notes here, which I'll have access to, as well as an
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:14,360'); seek(14.0)">
              HTML version, if you so need it.
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:16,910'); seek(16.0)">
              So who am I?
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:18,000'); seek(18.0)">
              I'm one of us, a regular Pythonista.
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:20,400'); seek(20.0)">
              I was in tech in the early 2000s as a business information architect
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:24,250'); seek(24.0)">
              and certified Microsoft SQL Server DBA for about four years.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:28,289'); seek(28.0)">
              And then I returned in 2017 via WordPress and JavaScript.
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:00:31,795'); seek(31.0)">
              Frameworks.
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:00:32,735'); seek(32.0)">
              Moving to Python and machine learning in 2021.
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:00:36,544'); seek(36.0)">
              Currently, I'm working on a project, AI powered knowledge systems, building
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:00:39,824'); seek(39.0)">
              a book framework similar to my Python.
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:00:42,769'); seek(42.0)">
              test full stack, which is here at pytestcookbook.
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:00:45,339'); seek(45.0)">
              com.
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:00:46,110'); seek(46.0)">
              I've also got some useful notes on Django full stack testing, and here
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:00:50,950'); seek(50.0)">
              is my main project at the moment.
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:00:53,809'); seek(53.0)">
              I'm based in Brighton in the UK, down on the south coast,
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:00:56,370'); seek(56.0)">
              and here is our lovely beach.
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:00:58,509'); seek(58.0)">
              And I'm a volunteer coach at Cobar.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:01,319'); seek(61.0)">
              io, which I find very rewarding.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:02,490'); seek(62.0)">
              We meet every two weeks.
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:04,970'); seek(64.0)">
              And I've just got myself a new Red Fox Labrador pup, Leo,
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:08,430'); seek(68.0)">
              much earlier than planned.
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:10,205'); seek(70.0)">
              And locally, we have a red fox that is quite tame, and seems to check
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:15,525'); seek(75.0)">
              Leo out, and they both stare at each other, wondering who's who.
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:01:18,995'); seek(78.0)">
              My first computer was in 1979, and it was a paper tape reader, with
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:01:22,795'); seek(82.0)">
              a teletype printer for output, and cut and paste was cut and paste.
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:01:27,825'); seek(87.0)">
              So what are AI agents?
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:01:30,205'); seek(90.0)">
              The word agent is a subject of much discussion in academic circles, but
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:01:35,565'); seek(95.0)">
              if we're looking at AI agents, we can see that Pydantic has its version, a
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:01:40,325'); seek(100.0)">
              primary interface for interacting with LLMs, and an Anthropic also defines
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:01:44,885'); seek(104.0)">
              workflow and agents, and Hugging Face says AI agents are programs where
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:01:49,105'); seek(109.0)">
              LLM outputs control to workflow.
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:01:51,625'); seek(111.0)">
              What we're going to do is we're going to look at examples of code to see what
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:01:54,325'); seek(114.0)">
              AI agents are and what they can do.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:01:57,235'); seek(117.0)">
              Now, for example, if we look at this link here, we will see The range of
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:02,504'); seek(122.0)">
              AI agents that are being created.
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:04,364'); seek(124.0)">
              There's 46 character categories, 825, all different versions in different areas,
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:11,604'); seek(131.0)">
              so it can be quite overwhelming to know which framework to use and what they are.
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:16,244'); seek(136.0)">
              What's the aim of my talk?
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:18,484'); seek(138.0)">
              Well, my talk is to a chain to achieve that, to demystify AI agents
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:02:23,494'); seek(143.0)">
              and AI programming because it can seem like it's another different
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:02:27,064'); seek(147.0)">
              world of development for us, ISTs.
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:02:29,454'); seek(149.0)">
              And what I'd like to propose is what if AI agents adjust Python code with a REST API
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:02:35,104'); seek(155.0)">
              call, admittedly to a very magical API?
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:02:37,834'); seek(157.0)">
              Then we could use day to day Python design patterns to handle the
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:02:40,944'); seek(160.0)">
              responses we get back from these API calls to the AI, to the LLMs.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:02:45,944'); seek(165.0)">
              And so the main focus of this talk is to demystify and simplify, and not to
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:02:50,684'); seek(170.0)">
              focus on actual real world applications.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:02:54,114'); seek(174.0)">
              And with that in mind, we don't need to fully graph the code this time round.
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:02:57,734'); seek(177.0)">
              It may take a few minutes.
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:02:59,574'); seek(179.0)">
              iterations to fully grasp it, so perhaps to look at the high level view
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:04,114'); seek(184.0)">
              and to see how it is different from just regular Python and to realize
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:08,904'); seek(188.0)">
              that actually AI agents are Python code with REST API calls to an LLM,
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:17,404'); seek(197.0)">
              admittedly a very magical REST API.
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:19,744'); seek(199.0)">
              In fact, what I'd like to propose is that actually there's no real
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:22,444'); seek(202.0)">
              difference between doing our regular day to day Python and Python code.
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:03:26,119'); seek(206.0)">
              It is very much like a mouse that we turn around 180 degrees.
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:03:29,979'); seek(209.0)">
              It's still the same actions, up, down, left, right, but in a different
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:03:34,419'); seek(214.0)">
              way, in a different paradigm.
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:03:36,069'); seek(216.0)">
              And that can be a little bit tricky to get to grips with initially.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:03:39,599'); seek(219.0)">
              And in this regard, there are three areas that I consider to be part of agentic AI.
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:03:46,694'); seek(226.0)">
              First, it seems that we can almost create on the client side the
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:03:50,194'); seek(230.0)">
              endpoint, the roots, that we would normally build on the server side.
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:03:54,384'); seek(234.0)">
              We also use natural and human language, in my case English, to
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:03:58,094'); seek(238.0)">
              create code, very much like pseudocode.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:00,614'); seek(240.0)">
              And thirdly, we give a sense of autonomy, in a sense that the LLM
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:04,044'); seek(244.0)">
              can direct the flow of the app.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:05,974'); seek(245.0)">
              And that could be within bounds that we have created.
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:08,944'); seek(248.0)">
              But basically, the next step can be determined by the
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:11,764'); seek(251.0)">
              LLM, which our app will take.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:13,984'); seek(253.0)">
              So before we go into some code examples, why don't we just refresh ourselves
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:17,664'); seek(257.0)">
              on what a REST API is before we start using any library implementations by
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:04:23,074'); seek(263.0)">
              using just the requests library so that we can see that how we actually do a
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:04:28,874'); seek(268.0)">
              POST request to our LLM rather than using, say, an OpenAI library that hides
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:04:34,784'); seek(274.0)">
              the implementation of the requests.
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:04:37,554'); seek(277.0)">
              So we have our model, we have our endpoint, and it's worth noting that we
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:04:41,064'); seek(281.0)">
              only have one endpoint, one root, and we'll see why this is apparent later on.
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:04:46,334'); seek(286.0)">
              And we will need to send our token, our API key, in the headers.
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:04:50,464'); seek(290.0)">
              We will send our payload, for example, our model, a list of messages, whether
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:04:54,204'); seek(294.0)">
              we're streaming, the temperature.
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:04:56,434'); seek(296.0)">
              And here, with requests, what we can do is send POST.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:04:59,919'); seek(299.0)">
              With all these details to get a JSON response and to bear in mind, the
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:04,619'); seek(304.0)">
              request is a string of characters and doesn't contain any objects or other
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:08,379'); seek(308.0)">
              data types, and basically it's a JSON dumps or if we were in JavaScript,
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:13,349'); seek(313.0)">
              it would be a JSON stringify.
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:15,284'); seek(315.0)">
              Where all that information is created as a string.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:18,894'); seek(318.0)">
              So what we'll do now is we'll look at our very first file, 01, to
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:22,554'); seek(322.0)">
              see this in action and to see the basics of an agent application.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:05:28,314'); seek(328.0)">
              We're now in our very first file, 01, and the first thing we're just going to do is
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:05:32,874'); seek(332.0)">
              load in our imports of OS JSON requests, and to read our keys from the env file.
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:05:38,954'); seek(338.0)">
              I'm going to keep mine secret here, but you have a copy here as env.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:05:42,674'); seek(342.0)">
              sample.
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:05:43,724'); seek(343.0)">
              Just paste your Open AI key here.
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:05:46,669'); seek(346.0)">
              If we come back, what we can then do is load in our AI key.
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:05:52,184'); seek(352.0)">
              And just check that we have it, which we see we have here.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:05:55,594'); seek(355.0)">
              We're going to select our model, which in this case is GPT 40mini.
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:00,304'); seek(360.0)">
              And we just check that it's here.
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:02,124'); seek(362.0)">
              And for the demonstration, we're going to create our own class to show how
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:06,214'); seek(366.0)">
              we can do a request to the endpoint.
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:10,164'); seek(370.0)">
              Later on, we'll use one of the libraries from OpenAI.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:13,584'); seek(373.0)">
              But we're just going to see in a very raw form actually how
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:16,134'); seek(376.0)">
              we would create our own class.
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:18,729'); seek(378.0)">
              request to an LLM directly to its endpoint.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:21,949'); seek(381.0)">
              So we're going to be making a POST request to this endpoint.
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:06:25,229'); seek(385.0)">
              And what we're going to do in this class is use this one single endpoint,
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:06:29,329'); seek(389.0)">
              and there's only ever one endpoint.
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:06:30,829'); seek(390.0)">
              It's not that we have different routes for different tasks that we want to do.
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:06:35,149'); seek(395.0)">
              We have the temperature, which is a hyperparameter that kind of
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:06:38,719'); seek(398.0)">
              takes into account the probability.
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:06:40,619'); seek(400.0)">
              Zero means it's very strict and is as deterministic as possible.
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:06:45,209'); seek(405.0)">
              If we want to be more creative in generating text or images, we would
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:06:50,129'); seek(410.0)">
              vary the temperature to, say, 0.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:06:52,289'); seek(412.0)">
              5, 1, etc. The range is between 0 and 2.
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:06:55,819'); seek(415.0)">
              We have our system prompt.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:06:57,099'); seek(417.0)">
              We'll get into that.
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:06:57,969'); seek(417.0)">
              Our API key.
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:06:59,409'); seek(419.0)">
              And here we have our headers so that we get authentication.
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:02,904'); seek(422.0)">
              Giving it the content type.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:04,644'); seek(424.0)">
              This is our request.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:06,674'); seek(426.0)">
              And when we want to generate some text, we pass in a prompt, our query, our request.
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:07:12,494'); seek(432.0)">
              We pass in the payload of the model, the messages, the system prompt, our prompt.
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:07:18,154'); seek(438.0)">
              We'll get into that.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:07:19,359'); seek(439.0)">
              Whether we're streaming, in this case false, the temperature.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:07:22,849'); seek(442.0)">
              And here, we make our requests to that endpoint with headers
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:07:27,629'); seek(447.0)">
              and also with the payload.
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:07:29,619'); seek(449.0)">
              And we can put in here for future reference, URL equals.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:07:33,499'); seek(453.0)">
              So that's a little bit clearer.
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:07:34,949'); seek(454.0)">
              So now that we've got that class, let's create an instance.
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:07:37,579'); seek(457.0)">
              We pass in the model we want.
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:07:39,709'); seek(459.0)">
              We pass in the system prompt, which is like, gives the character
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:07:43,649'); seek(463.0)">
              or the personality or the role.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:07:46,084'); seek(466.0)">
              of our agent.
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:07:47,814'); seek(467.0)">
              And we're saying in this case, you give concise answers to questions
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:07:50,704'); seek(470.0)">
              with no more than 100 characters.
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:07:52,994'); seek(472.0)">
              We can get into more complex system prompts later, and that's
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:07:56,094'); seek(476.0)">
              the sort of prompt engineering.
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:07:58,414'); seek(478.0)">
              And we can then make a request to say what is Pydantic.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:08:02,594'); seek(482.0)">
              And if we print the response that we get back originally, it is a JSON, stringified
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:08:07,714'); seek(487.0)">
              JSON object of all this information.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:08:10,864'); seek(490.0)">
              But if we drill down through choices, message, and content,
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:08:14,524'); seek(494.0)">
              we will end up with the Pydantic.
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:08:16,679'); seek(496.0)">
              response that we want by Dantic is a data validation and settings
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:08:20,109'); seek(500.0)">
              management library from Python.
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:08:21,329'); seek(501.0)">
              And you can see how it's respecting the 100 characters.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:08:24,779'); seek(504.0)">
              So this is our base and we can see that if we were not using any of the AI
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:08:28,959'); seek(508.0)">
              libraries but merely using our Python library of requests and env, that this is
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:08:33,759'); seek(513.0)">
              how we would send a POST request to the LLM with all the details that it needs.
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:08:39,999'); seek(519.0)">
              what we would get back.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:08:41,119'); seek(521.0)">
              And what we can do is we can just hide this and then show the response.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:08:45,449'); seek(525.0)">
              We can see it's quite complicated in quite detail, but it's choices
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:08:49,639'); seek(529.0)">
              is where the answers come back.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:08:51,499'); seek(531.0)">
              We want the very first one.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:08:53,349'); seek(533.0)">
              And as we work through, we see message.
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:08:55,489'); seek(535.0)">
              We want to look for the content where the answer to our query lies.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:09:01,539'); seek(541.0)">
              So this is our base template.
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:09:04,214'); seek(544.0)">
              We're going to replace it with OpenAI library for our calls.
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:09:07,994'); seek(547.0)">
              Let's now move on to the second file, O2 API.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:09:11,604'); seek(551.0)">
              And what we're going to do is we're going to get a joke back, but we're
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:09:14,514'); seek(554.0)">
              also going to make a little bit of prompt engineering to get the LLM to
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:09:17,974'); seek(557.0)">
              rate the joke and give us the next step.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:09:21,184'); seek(561.0)">
              So let's go into that to see an example of that.
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:09:23,414'); seek(563.0)">
              So if we go into O2 API, We again load in our usual imports.
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:09:27,834'); seek(567.0)">
              I'm using rich to colorize the console output.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:09:32,724'); seek(572.0)">
              And we know we can make a request to some random joke API.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:09:36,534'); seek(576.0)">
              To get a joke here, using the request library, and we get the joke.
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:09:41,884'); seek(581.0)">
              We won't go into that.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:09:43,274'); seek(583.0)">
              We will load in our open API key.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:09:45,534'); seek(585.0)">
              We'll get our model as usual.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:09:47,274'); seek(587.0)">
              And what we'll do is we'll then set up a more elaborate, system.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:09:52,704'); seek(592.0)">
              We're going to start with our basic system message.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:09:55,114'); seek(595.0)">
              You are an assistant that is great at telling jokes.
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:09:58,684'); seek(598.0)">
              But we want to get something a little bit more advanced.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:10:01,314'); seek(601.0)">
              And it's almost as if we were sending to the endpoint a new endpoint that would
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:10:05,494'); seek(605.0)">
              do something totally different for us.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:10:07,504'); seek(607.0)">
              But what we will find is actually that we will do this on the client side.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:10:12,964'); seek(612.0)">
              So.
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:10:13,689'); seek(613.0)">
              Let's just add this extra prompt here, and I'm just going to call it prompt
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:10:16,839'); seek(616.0)">
              engineering because it's separate from the system message, totally customizable,
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:10:21,649'); seek(621.0)">
              doesn't have to be prompt engineering.
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:10:24,029'); seek(624.0)">
              And we're giving it a set of instructions, and we could almost
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:10:26,619'); seek(626.0)">
              consider this to be pseudocode.
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:10:28,359'); seek(628.0)">
              A joke worthy of publishing is a joke that has a rating of 8.
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:10:31,119'); seek(631.0)">
              5 or 10 or above.
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:10:32,989'); seek(632.0)">
              If the joke is worthy of publishing, also includes the
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:10:35,749'); seek(635.0)">
              next step, whether to publish it.
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:10:37,859'); seek(637.0)">
              Otherwise, tell us what the next step would be, which we should retry.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:10:42,019'); seek(642.0)">
              We give it an example.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:10:43,774'); seek(643.0)">
              of what we're going to get.
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:10:45,444'); seek(645.0)">
              So this is called, one shot prompting or multi shot prompting where
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:10:49,934'); seek(649.0)">
              you give one or more examples.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:10:52,294'); seek(652.0)">
              And basically we're giving it an example of what we would like to get back.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:10:55,974'); seek(655.0)">
              Please supply the response in the following format.
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:10:58,119'); seek(658.0)">
              It's in JSON format here.
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:11:00,609'); seek(660.0)">
              And to actually help it, let's actually, although it works, to be more specific,
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:11:04,639'); seek(664.0)">
              we'll say the following JSON format.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:11:07,149'); seek(667.0)">
              Because we want to have clear instructions.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:11:09,069'); seek(669.0)">
              And here we see we get the setup, the punchline, we ask it to give us a rating,
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:11:13,649'); seek(673.0)">
              and we also then tell us that based on the rating and whether it deems it
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:11:17,699'); seek(677.0)">
              worthy of publishing, to either send the next step, that we pass on higher up in
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:11:22,759'); seek(682.0)">
              the app chain to publish or to retry.
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:11:26,289'); seek(686.0)">
              We're also giving it some further instructions to remove all backticks,
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:11:29,949'); seek(689.0)">
              any unnecessary characters.
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:11:32,259'); seek(692.0)">
              Once again, I did say JSON format here, and using capitals can be a useful
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:11:35,959'); seek(695.0)">
              way to emphasize things for the LLM.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:11:38,399'); seek(698.0)">
              And also we're saying that if we do a retry, do not repeat the joke,
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:11:41,629'); seek(701.0)">
              and we can even say thank you.
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:11:43,699'); seek(703.0)">
              So this is our pseudocode.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:11:46,464'); seek(706.0)">
              This is almost a new endpoint that we would like to have where we
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:11:49,804'); seek(709.0)">
              could send a request and it will process something different rather
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:11:53,614'); seek(713.0)">
              than just returning us a joke.
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:11:55,924'); seek(715.0)">
              But what we do is on the client side, we send that code up to that one endpoint.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:12:01,164'); seek(721.0)">
              And that's the mouse being at 180 degrees different.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:12:04,114'); seek(724.0)">
              It's like we are now creating our own code.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:12:07,024'); seek(727.0)">
              REST endpoint R route here.
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:12:10,064'); seek(730.0)">
              So we just add those two together so that it's now the system message.
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:12:14,534'); seek(734.0)">
              And we also have our user prompt, which is going to tell a light hearted
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:12:17,404'); seek(737.0)">
              joke for an audience of Pythonistas.
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:12:19,864'); seek(739.0)">
              And when we send up to the ALM, we want to send a list of all these messages,
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:12:23,634'); seek(743.0)">
              and the convention is we have prompts.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:12:25,664'); seek(745.0)">
              We have a role for system, and its content is the system message.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:12:29,584'); seek(749.0)">
              We have a role for user, and the content is the user prompt.
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:12:33,054'); seek(753.0)">
              We can also have role of the AI assistant, because we may want
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:12:36,334'); seek(756.0)">
              to filter out the messages.
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:12:37,714'); seek(757.0)">
              And this is the practice with these LLMs, is that we have a system message,
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:12:41,474'); seek(761.0)">
              user message, and an AI message, which is the response back from the LLM.
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:12:46,724'); seek(766.0)">
              So we complete as before, but this time we are using the OpenAI.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:12:52,414'); seek(772.0)">
              If we scroll up to the top, we're using from OpenAI, one of its libraries.
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:12:57,404'); seek(777.0)">
              So we can just create the client here, An instance of OpenAI.
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:13:03,589'); seek(783.0)">
              And therefore, when we send the request, we don't need to be as detailed.
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:13:08,059'); seek(788.0)">
              We can use a convenience method.
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:13:09,869'); seek(789.0)">
              Client.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:13:10,539'); seek(790.0)">
              chat.
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:13:11,059'); seek(791.0)">
              completions.
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:13:11,839'); seek(791.0)">
              create.
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:13:12,709'); seek(792.0)">
              We send the model we want.
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:13:14,099'); seek(794.0)">
              We send all our messages in, which is these prompts.
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:13:17,359'); seek(797.0)">
              And when we get it back, we will get a response.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:13:19,999'); seek(799.0)">
              Again, it's in the choices, it's in the message, in the content,
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:13:23,309'); seek(803.0)">
              and we can display it here.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:13:25,269'); seek(805.0)">
              So we now get a JSON object here, where it has the setup, the punchline,
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:13:32,154'); seek(812.0)">
              The rating and what it advises to do next in the flow, which
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:13:36,244'); seek(816.0)">
              we publish as opposed to retry.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:13:38,994'); seek(818.0)">
              Now, in our app, we might have a state object that holds all of this
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:13:42,654'); seek(822.0)">
              information because this is we've made the REST API call to the LLM.
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:13:47,524'); seek(827.0)">
              But what we do next is day to day Python.
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:13:50,544'); seek(830.0)">
              It's any system design, PubSub, ActorModel, FiniteStateMachine.
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:13:56,934'); seek(836.0)">
              We've now been given exactly what to do next.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:14:00,164'); seek(840.0)">
              Now we can pass that over to another agent, or we can act upon it ourselves.
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:14:05,424'); seek(845.0)">
              And what I'm going to do here is I run it out.
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:14:08,274'); seek(848.0)">
              I've extracted it out nicely.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:14:10,534'); seek(850.0)">
              JSON data.
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:14:11,764'); seek(851.0)">
              And when I look at it, I can then basically see that if the result next
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:14:16,274'); seek(856.0)">
              equals publish, for example, I can load it into our state object, and
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:14:22,444'); seek(862.0)">
              I could then go on to publish it, to send a message to another part
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:14:26,484'); seek(866.0)">
              of our program or to another agent.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:14:28,734'); seek(868.0)">
              The main thing is we're kind of getting almost like an event driven
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:14:31,754'); seek(871.0)">
              application, but we've been asking the LLM to decide the next step.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:14:36,884'); seek(876.0)">
              And that's the autonomy, because it could have come back with retry.
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:14:40,874'); seek(880.0)">
              And in which case, the flow of the program would go in a different direction.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:14:44,424'); seek(884.0)">
              And so what I've just done here, I've extracted out the
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:14:47,164'); seek(887.0)">
              next step, which is publish.
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:14:49,244'); seek(889.0)">
              So this is how our app would have its flow and its direction directed by the
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:14:55,234'); seek(895.0)">
              LLM, as opposed to us imperatively.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:14:58,244'); seek(898.0)">
              So to recap with this, the main thing is we're structuring
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:15:02,004'); seek(902.0)">
              what we would like to get back.
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:15:03,364'); seek(903.0)">
              Rather than just getting a joke, we're setting back an endpoint.
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:15:06,564'); seek(906.0)">
              We're setting back instructions.
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:15:07,874'); seek(907.0)">
              This is our pseudocode.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:15:09,644'); seek(909.0)">
              This is our REST endpoint that we send from the client up to the server
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:15:14,444'); seek(914.0)">
              along with our payload, and then we get the response back as we want.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:15:18,634'); seek(918.0)">
              And we're going to see various different forms of these prompts.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:15:21,654'); seek(921.0)">
              They could be more advanced and more structured as we go along.
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:15:25,804'); seek(925.0)">
              So when we get that back, we can then basically decide what we want to do next.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:15:31,234'); seek(931.0)">
              If we're having many agents, we may keep track where each agent is.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:15:36,794'); seek(936.0)">
              So this was O2 API.
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:15:39,054'); seek(939.0)">
              An initial start into prompt engineering and how we use the client side REST API
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:15:45,144'); seek(945.0)">
              coding in natural language, enabling autonomy to take place in our AI agent.
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:15:51,534'); seek(951.0)">
              We've seen the two out of the three steps of the AI reverse process,
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:15:55,584'); seek(955.0)">
              where we kind of created on the client side our route, our endpoint,
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:15:59,614'); seek(959.0)">
              and the use of natural language.
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:16:01,544'); seek(961.0)">
              And we briefly looked at autonomy.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:16:03,274'); seek(963.0)">
              So how do we handle this autonomy of the flow of an API?
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:16:07,304'); seek(967.0)">
              AI agent app.
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:16:08,341'); seek(968.0)">
              Well, we asked in our LLM to give us not just a rating,
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:16:11,974'); seek(971.0)">
              but to give us that next step.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:16:13,704'); seek(973.0)">
              And in this case, it was published.
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:16:15,754'); seek(975.0)">
              So what we'd like to do now is to begin to see how we can handle this in our app.
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:16:21,124'); seek(981.0)">
              And we're going to go on to our next example, which is a sort of an idea is
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:16:24,284'); seek(984.0)">
              leading into the idea of an FAQ or sort of the router pattern, a sort of if else.
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:16:29,724'); seek(989.0)">
              And what I like about the FAQ pattern, which we're going to see now, is
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:16:33,509'); seek(993.0)">
              the fact that we can introduce a little bit of AI into our app.
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:16:39,399'); seek(999.0)">
              So let's have a look at this example of OA03FAQ.
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:16:41,719'); seek(1001.0)">
              We're going to see an example of sort of retrieval augmented generation.
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:16:44,799'); seek(1004.0)">
              Now, we're not querying documents, but RAG is basically supplementing our
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:16:49,349'); seek(1009.0)">
              query with additional information that we haven't fine tuned our model with
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:16:53,729'); seek(1013.0)">
              or that's important for our LLM query.
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:16:57,439'); seek(1017.0)">
              So what we're going to do is we're going to give it a list of
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:16:59,449'); seek(1019.0)">
              frequently asked questions and have a little chatbot experience.
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:17:02,369'); seek(1022.0)">
              So And this is going to pave way for the next file, which will
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:17:06,369'); seek(1026.0)">
              be a sort of a router pattern.
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:17:08,279'); seek(1028.0)">
              So once again, we load in our imports, we get our key, we get a utility client from
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:17:14,519'); seek(1034.0)">
              the OpenAI library to ease our connection to the LLM, which is picking our model.
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:17:19,849'); seek(1039.0)">
              And we're going to create now a function that has some history, that has all
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:17:23,339'); seek(1043.0)">
              the previous messages, that has the system message and the user prompt.
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:17:27,409'); seek(1047.0)">
              And the history is important because it gives us a record of what went on before,
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:17:31,109'); seek(1051.0)">
              because every request is stateless.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:17:33,499'); seek(1053.0)">
              It's like the LLM is seeing it for the first time.
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:17:36,169'); seek(1056.0)">
              So we need to pass the history back with it so it has context
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:17:40,519'); seek(1060.0)">
              and a certain degree of memory.
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:17:43,469'); seek(1063.0)">
              So in our function chat, We're having our role, our system.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:17:47,239'); seek(1067.0)">
              We're adding in the history of all the previous messages that we have
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:17:50,389'); seek(1070.0)">
              in our chatbot and our user message.
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:17:53,199'); seek(1073.0)">
              We can print out the history, print out the messages.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:17:56,509'); seek(1076.0)">
              We're using the stream option for our chatbot, which we'll see
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:18:00,339'); seek(1080.0)">
              in a minute, and we're going to then chunk out our responses.
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:18:04,429'); seek(1084.0)">
              So this is where we start to build more context into our chatbot.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:18:10,014'); seek(1090.0)">
              agent.
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:18:11,124'); seek(1091.0)">
              We're saying it's a helpful assistant for a shoe store.
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:18:13,754'); seek(1093.0)">
              And if a user asks a question, please be as helpful as possible and as
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:18:17,654'); seek(1097.0)">
              courteous and professional manner.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:18:19,944'); seek(1099.0)">
              You are provided with the following facts to help you.
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:18:22,044'); seek(1102.0)">
              Please be verbose and suggestive.
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:18:24,114'); seek(1104.0)">
              So now I'm changing like the character and the nature of the
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:18:26,764'); seek(1106.0)">
              prompt, adding in a little bit more verbosity and suggestiveness.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:18:29,624'); seek(1109.0)">
              So here is a list of just some basic facts about our shop.
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:18:34,074'); seek(1114.0)">
              And bear in mind that this could be retrieved from the database.
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:18:36,544'); seek(1116.0)">
              This could be a result of selecting from the Options from
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:18:39,524'); seek(1119.0)">
              a form for further information.
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:18:41,914'); seek(1121.0)">
              But we can see here is if we just pass some extra content,
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:18:45,464'); seek(1125.0)">
              some retrieved content.
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:18:46,894'); seek(1126.0)">
              Admittedly, it's already in the file, but it could have been retrieved from
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:18:50,114'); seek(1130.0)">
              a database or from some other source, and we join it onto the system message.
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:18:55,644'); seek(1135.0)">
              We can now use Gradio to set up a little chatbot interface to see how this works.
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:19:01,754'); seek(1141.0)">
              And if we look at the code while this is working, it hasn't taken a lot to
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:19:05,314'); seek(1145.0)">
              introduce a fairly sophisticated little chatbot based on very limited information.
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:19:10,024'); seek(1150.0)">
              We could very much increase this greatly in our app.
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:19:13,794'); seek(1153.0)">
              So as that runs through.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:19:15,424'); seek(1155.0)">
              Almost there.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:19:16,434'); seek(1156.0)">
              Come back down.
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:19:17,734'); seek(1157.0)">
              We have our Gradio interface,
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:19:20,334'); seek(1160.0)">
              and let's just open that up.
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:19:22,004'); seek(1162.0)">
              In fact, I'll put it in.
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:19:23,684'); seek(1163.0)">
              There we go.
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:19:24,414'); seek(1164.0)">
              Here's our chatbot.
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:19:25,434'); seek(1165.0)">
              Type message.
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:19:26,744'); seek(1166.0)">
              Now, it's quite simple.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:19:28,374'); seek(1168.0)">
              I could just say Sunday, and if we look at that, it's coming through.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:19:32,544'); seek(1172.0)">
              Thank you for your inquiry.
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:19:33,544'); seek(1173.0)">
              Our store is picked up.
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:19:34,934'); seek(1174.0)">
              It's about time.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:19:35,824'); seek(1175.0)">
              It's Monday to Friday 9 to 5.
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:19:37,524'); seek(1177.0)">
              Unfortunately, we're closed on Sundays.
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:19:39,354'); seek(1179.0)">
              Notice how it's quite verbose and suggestive.
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:19:41,584'); seek(1181.0)">
              We look forward to welcoming you soon.
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:19:43,434'); seek(1183.0)">
              If we come back to our code, we can see that here are the facts.
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:19:46,944'); seek(1186.0)">
              We don't have very many.
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:19:48,404'); seek(1188.0)">
              This could be a much more complex document.
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:19:50,534'); seek(1190.0)">
              It could be an MD markdown file.
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:19:52,114'); seek(1192.0)">
              It could be driven from the database.
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:19:54,104'); seek(1194.0)">
              So if we come back and let's have another example.
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:19:57,159'); seek(1197.0)">
              I can say green belts.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:19:59,139'); seek(1199.0)">
              Thank you for reaching out.
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:20:00,039'); seek(1200.0)">
              We don't do that.
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:20:01,289'); seek(1201.0)">
              Gives us the address.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:20:02,569'); seek(1202.0)">
              It says that basically exclusively in shoes.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:20:05,499'); seek(1205.0)">
              So this little example here of basically having an agentic AI with a minimum
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:20:11,559'); seek(1211.0)">
              amount of rag, a minimum amount of extra context can produce a nice small little
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:20:16,449'); seek(1216.0)">
              app in your, small little AI app in your Python app without having to be fully AI.
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:20:24,369'); seek(1224.0)">
              This is what I call a bit of AI programming.
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:20:26,859'); seek(1226.0)">
              Now, this is quite an interesting pattern, because in the next one, it's
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:20:30,039'); seek(1230.0)">
              the agent router, and it's something that happened when I was at CodeBar.
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:20:34,069'); seek(1234.0)">
              somebody asked that they would like to get a job in AI.
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:20:37,199'); seek(1237.0)">
              They were doing Python.
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:20:38,319'); seek(1238.0)">
              And I said to them, Do they have an AI department where they work?
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:20:41,459'); seek(1241.0)">
              And they said, No.
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:20:42,349'); seek(1242.0)">
              And I said, What do they do?
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:20:43,319'); seek(1243.0)">
              They said they were insurance.
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:20:44,389'); seek(1244.0)">
              I said, What do you do?
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:20:45,719'); seek(1245.0)">
              And they said they they don't write the reports that they're there to go
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:20:49,229'); seek(1249.0)">
              to person that when somebody wants a report, they know which one it
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:20:52,529'); seek(1252.0)">
              is, and they can run it for them.
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:20:54,779'); seek(1254.0)">
              I thought, Brilliant.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:20:55,509'); seek(1255.0)">
              You can do that.
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:20:56,609'); seek(1256.0)">
              And there were concerns.
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:20:57,969'); seek(1257.0)">
              They said, put me out of a job.
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:21:00,179'); seek(1260.0)">
              I said, yes, but you'll then be the head of the AI department.
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:21:03,979'); seek(1263.0)">
              And so what we're going to do is we're going to have a
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:21:05,419'); seek(1265.0)">
              little variation on this FAQ.
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:21:07,529'); seek(1267.0)">
              It's a similar type of thing.
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:21:09,159'); seek(1269.0)">
              We're loading in all the usual imports and setting ourself up the same chat message.
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:21:15,169'); seek(1275.0)">
              And a useful tip is caps and italics and even markdown in one's prompts
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:21:20,609'); seek(1280.0)">
              actually have an impact with the LLM.
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:21:23,009'); seek(1283.0)">
              It's being trained on so many of these that it begins to
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:21:25,719'); seek(1285.0)">
              recognize the importance.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:21:27,939'); seek(1287.0)">
              And I'm just basically setting up a report agent.
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:21:30,439'); seek(1290.0)">
              I'm saying you're a report selection agent.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:21:32,199'); seek(1292.0)">
              You're very good at returning the best report to answer a user's question.
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:21:35,859'); seek(1295.0)">
              For example, if a user wants a joke, you reply with, and I'm just using
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:21:39,429'); seek(1299.0)">
              this format for demonstration purposes.
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:21:41,689'); seek(1301.0)">
              This will make it nice and bold.
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:21:43,279'); seek(1303.0)">
              The tool they need is the get joke report.
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:21:45,959'); seek(1305.0)">
              If they want total sales, the tool or report they need will be the
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:21:49,229'); seek(1309.0)">
              get sales would be the best report.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:21:52,199'); seek(1312.0)">
              So I made a list of reports here for whether use the get weather for hotel,
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:21:56,039'); seek(1316.0)">
              the hotel booking, very much like we did in the last FAQ, adding them all in.
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:22:02,309'); seek(1322.0)">
              And if I run all of that, we will see now that we can actually
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:22:05,609'); seek(1325.0)">
              have a report selection agent that will get the right report.
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:22:09,639'); seek(1329.0)">
              And if we combine that with information like the date range or
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:22:12,729'); seek(1332.0)">
              any other properties, we could even run the report for them and send it.
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:22:16,754'); seek(1336.0)">
              All through agents.
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:22:18,684'); seek(1338.0)">
              So let's check.
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:22:19,704'); seek(1339.0)">
              We've got this one working.
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:22:21,344'); seek(1341.0)">
              Lovely.
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:22:22,004'); seek(1342.0)">
              Okay.
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:22:22,744'); seek(1342.0)">
              So say I want to take a plane and notice I didn't use the word plane.
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:22:25,834'); seek(1345.0)">
              I use flight.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:22:26,874'); seek(1346.0)">
              Plane to Rome.
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:22:28,234'); seek(1348.0)">
              What report should I get?
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:22:29,834'); seek(1349.0)">
              Get the flight.
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:22:30,994'); seek(1350.0)">
              Plane to Rome and Auto, let's just check the typos there,
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:22:36,394'); seek(1356.0)">
              to Rome, and auto to Paris.
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:22:40,564'); seek(1360.0)">
              It comes back with those two reports.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:22:42,754'); seek(1362.0)">
              So straight away, with that, if we had the information, shall we say, sent
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:22:46,304'); seek(1366.0)">
              along with it through a form selection, we could then get the right report.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:22:51,204'); seek(1371.0)">
              And so this is an example of a sort of router that actually,
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:22:54,554'); seek(1374.0)">
              what do we want to do next?
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:22:56,344'); seek(1376.0)">
              Through a very simple addition of some context to our system message,
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:23:02,464'); seek(1382.0)">
              I'm going to return to this slide a number of times because it's easy when
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:23:07,084'); seek(1387.0)">
              we're going through these different patterns and function callings to lose
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:23:10,874'); seek(1390.0)">
              sight of what is the essence of an A.
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:23:13,524'); seek(1393.0)">
              I. agent and A. I. agents are python code with A. P. I. request to L.
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:23:18,604'); seek(1398.0)">
              L. M. s. We can only pass a string in the A. P. I. request and in that
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:23:23,294'); seek(1403.0)">
              string we create a job description.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:23:26,634'); seek(1406.0)">
              And this could be what the role is, what they do, these are the tools you have,
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:23:31,454'); seek(1411.0)">
              here is the data to work on, this is what we want returned, and in what format.
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:23:36,204'); seek(1416.0)">
              This is prompt or flow engineering.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:23:39,254'); seek(1419.0)">
              How we make use of this in terms of design patterns is then day to day Python.
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:23:44,314'); seek(1424.0)">
              Fundamentally, it is a function with inputs, LLM magic, and some output
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:23:50,444'); seek(1430.0)">
              returned in the form that we want.
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:23:52,934'); seek(1432.0)">
              And we'll return to this slide as we proceed through some of
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:23:55,814'); seek(1435.0)">
              the more involved examples.
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:23:58,734'); seek(1438.0)">
              Now we've seen how an agent can make decisions.
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:24:02,354'); seek(1442.0)">
              About the next step, the autonomy, how we've created our sort of
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:24:05,894'); seek(1445.0)">
              client side API, and we use natural language, but an agent may need tools.
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:24:10,644'); seek(1450.0)">
              It may need to do anything.
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:24:11,824'); seek(1451.0)">
              It may need to make a request to the Internet.
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:24:15,174'); seek(1455.0)">
              It may need to call upon a function that we have in our code base to
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:24:19,844'); seek(1459.0)">
              calculate something and use that it.
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:24:23,164'); seek(1463.0)">
              as part of its response.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:24:25,144'); seek(1465.0)">
              So what we're going to do in O5 is we're going to look at not how we
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:24:28,074'); seek(1468.0)">
              just define tools, but also how an agent can decide which one to use.
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:24:32,264'); seek(1472.0)">
              Now, usually it's better to have an agent to just one single thing, but
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:24:35,554'); seek(1475.0)">
              sometimes we might have an agent that might need to make a decision for a
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:24:38,764'); seek(1478.0)">
              particular task, which tool to use.
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:24:41,624'); seek(1481.0)">
              So we may need to determine Which tool to use in the agent and
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:24:46,514'); seek(1486.0)">
              basically, what happens is we're constantly adding new messages.
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:24:49,464'); seek(1489.0)">
              Now, where does those functions run?
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:24:51,924'); seek(1491.0)">
              What's going to happen is when we've given the prompt, which we're
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:24:55,464'); seek(1495.0)">
              going to have a look at now in 05.
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:24:58,324'); seek(1498.0)">
              What we're going to do is do the standard setup.
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:25:02,674'); seek(1502.0)">
              But now when we come to our tool prompt, we're saying you're in a system
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:25:05,934'); seek(1505.0)">
              that is very good at determining what tool to use to solve a certain query.
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:25:10,094'); seek(1510.0)">
              And our AI programming is we give it in descriptive form, which
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:25:14,474'); seek(1514.0)">
              is saying we're using Markdown here to emphasize this is tools.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:25:17,584'); seek(1517.0)">
              We have two tools.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:25:19,004'); seek(1519.0)">
              We're describing our calculator tool that does basic arithmetic.
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:25:22,734'); seek(1522.0)">
              And it responds in JSON.
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:25:24,474'); seek(1524.0)">
              And we give it an example of the JSON format, of that, when we pick the
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:25:28,734'); seek(1528.0)">
              particular tool called Calculator, the next signal will be to use the
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:25:33,804'); seek(1533.0)">
              doCalculation function, and, for example, what argument should be passed.
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:25:38,224'); seek(1538.0)">
              This is an example of one scenario.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:25:41,854'); seek(1541.0)">
              We have a second tool, which is a joke tool, something
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:25:43,914'); seek(1543.0)">
              totally different, JSON format.
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:25:46,524'); seek(1546.0)">
              And this is what the result the tool would look like.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:25:50,244'); seek(1550.0)">
              So, for example, we can ask it a question.
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:25:52,214'); seek(1552.0)">
              What is 10 times 9?
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:25:54,064'); seek(1554.0)">
              And what we're going to do is when we send that out, it will actually
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:25:57,234'); seek(1557.0)">
              determine that the tool it needs, the response it's sending back to us is
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:26:01,664'); seek(1561.0)">
              the calculator, the do calculation, and the arguments that are needed.
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:26:06,484'); seek(1566.0)">
              We can strip all of that out and basically then say if the do next was do
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:26:12,414'); seek(1572.0)">
              calculation, we can run those functions.
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:26:15,709'); seek(1575.0)">
              If it was perhaps, for example, to do the joke, we might do an
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:26:19,069'); seek(1579.0)">
              internet request to get a joke.
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:26:21,699'); seek(1581.0)">
              So let's just go back to the flow of messages.
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:26:26,439'); seek(1586.0)">
              We're doing all the usual messages.
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:26:28,259'); seek(1588.0)">
              But when the API, when the LLM decides that it needs a particular
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:26:31,639'); seek(1591.0)">
              tool, what it does is it sends back the tool signature and the arguments
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:26:37,099'); seek(1597.0)">
              that it's extracted from the query.
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:26:39,769'); seek(1599.0)">
              We then on our own computer box, not the LLM, run that function, get the result.
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:26:46,569'); seek(1606.0)">
              And add that back to the list of messages and send it to the LLM that
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:26:50,479'); seek(1610.0)">
              does the next step, for example.
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:26:52,609'); seek(1612.0)">
              Now, that particular example is one where we're going to see later where
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:26:55,339'); seek(1615.0)">
              we're going to use planning, where it's thinking, making an action, getting a
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:27:00,159'); seek(1620.0)">
              result, putting it back in the loop.
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:27:02,709'); seek(1622.0)">
              This particular example here in 05 tool is just the first step of showing how
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:27:08,369'); seek(1628.0)">
              it can determine which tool to use.
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:27:11,249'); seek(1631.0)">
              So I'm just going to run everything.
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:27:13,339'); seek(1633.0)">
              And as you see, we load in our imports, we get our key.
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:27:16,614'); seek(1636.0)">
              We get our messages.
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:27:18,084'); seek(1638.0)">
              It's added into the system message at the very end here.
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:27:21,964'); seek(1641.0)">
              Our code.
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:27:22,694'); seek(1642.0)">
              This is our endpoint.
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:27:24,529'); seek(1644.0)">
              On the client side, in natural human language.
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:27:28,289'); seek(1648.0)">
              And it's like a very clear description you give to somebody when they join a company.
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:27:32,059'); seek(1652.0)">
              This is how you do your job.
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:27:33,729'); seek(1653.0)">
              The more detail, the clearer you can be, the better.
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:27:37,049'); seek(1657.0)">
              So, for example, we asked, what is 10 times 9?
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:27:40,289'); seek(1660.0)">
              We've added the messages in.
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:27:42,049'); seek(1662.0)">
              We've got back this response that it's determined the tool
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:27:44,669'); seek(1664.0)">
              it needs is the calculator tool.
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:27:46,489'); seek(1666.0)">
              It knows what to do next.
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:27:48,289'); seek(1668.0)">
              It's the do calculation, and it knows the arguments.
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:27:50,939'); seek(1670.0)">
              10 and 2, 10 and 9, and the operation is multiplication.
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:27:56,299'); seek(1676.0)">
              What is 10 times 9?
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:27:57,929'); seek(1677.0)">
              Notice how it's picked up times and multiplication.
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:28:00,859'); seek(1680.0)">
              What we can then do is strip that all out, and if we come back down here, in
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:28:04,899'); seek(1684.0)">
              this particular agent, we can actually run some code, or we could pass this on
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:28:09,139'); seek(1689.0)">
              to another agent, or run it through a loop again, which we'll see later on.
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:28:13,579'); seek(1693.0)">
              So in this example, because it knows that doNext says doCalculation, it extracts all
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:28:18,939'); seek(1698.0)">
              the information, the tool, the arguments, and then basically just carries out these
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:28:24,519'); seek(1704.0)">
              functions to produce the answer back.
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:28:26,959'); seek(1706.0)">
              So if we do that again, but with some different numbers, 102 times 3,
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:28:31,819'); seek(1711.0)">
              and we run it all, and we see it going back through to clear.
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:28:34,939'); seek(1714.0)">
              It's now picked out arguments 102 and 3, operation multiply.
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:28:40,139'); seek(1720.0)">
              Let's do add.
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:28:41,459'); seek(1721.0)">
              What is 102's plus 3?
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:28:44,619'); seek(1724.0)">
              Let's run that.
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:28:45,659'); seek(1725.0)">
              We can see that it's picking out the arguments.
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:28:48,839'); seek(1728.0)">
              It's knowing which tool it needs.
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:28:50,344'); seek(1730.0)">
              1 0 2 3 and addition.
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:28:53,094'); seek(1733.0)">
              And when we come back down to the answer, it produces the answer 1 0 5.
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:28:57,774'); seek(1737.0)">
              Let's see if it picks up if it needs the different tool.
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:29:00,324'); seek(1740.0)">
              So, for example, here, I'll do that.
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:29:03,084'); seek(1743.0)">
              Tell me a joke.
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:29:03,784'); seek(1743.0)">
              I'm doing this at a builders conference.
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:29:05,844'); seek(1745.0)">
              Let's run it all.
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:29:07,174'); seek(1747.0)">
              So now it should determine that what is the right tool.
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:29:09,684'); seek(1749.0)">
              It's the do joke tool.
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:29:10,834'); seek(1750.0)">
              It's a different tool.
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:29:12,264'); seek(1752.0)">
              There it are.
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:29:12,917'); seek(1752.0)">
              The joke, do joke.
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:29:13,803'); seek(1753.0)">
              The audience has picked up one of the arguments.
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:29:16,134'); seek(1756.0)">
              It doesn't run here.
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:29:18,064'); seek(1758.0)">
              Because the do next is do joke, it just goes off to the internet and gets
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:29:21,904'); seek(1761.0)">
              a joke, and then we see the answer.
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:29:23,984'); seek(1763.0)">
              So you may be thinking they all seem a little bit the same, the router,
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:29:28,194'); seek(1768.0)">
              the tool use, the basic query.
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:29:30,394'); seek(1770.0)">
              And I suppose they are because they're just function calls.
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:29:33,424'); seek(1773.0)">
              At the end of the day, we're just doing Python functions, getting
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:29:36,364'); seek(1776.0)">
              a response back from an API, then doing something with that response.
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:29:40,334'); seek(1780.0)">
              The difference now is that we create our endpoint, we create our route on the
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:29:44,244'); seek(1784.0)">
              client side, ship up that pseudocode.
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:29:47,344'); seek(1787.0)">
              ship up the kind of query, and we get the answer back.
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:29:51,214'); seek(1791.0)">
              And we also enable a certain level of autonomy because we can ask the
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:29:55,094'); seek(1795.0)">
              LLM what it should do next based on the prompt that we sent it.
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:29:59,694'); seek(1799.0)">
              So this is what is called tool use, or it's just function calling.
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:30:02,974'); seek(1802.0)">
              And it's just a mechanism of how we do the function calling.
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:30:06,564'); seek(1806.0)">
              And once again, the function takes place on our box.
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:30:10,304'); seek(1810.0)">
              We don't run it on the LLM's box.
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:30:12,974'); seek(1812.0)">
              And when we get the result of that information, we pass it back to the LLM.
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:30:17,544'); seek(1817.0)">
              And we're going to see that when we come on to the what's called the reason act
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:30:23,064'); seek(1823.0)">
              react type pattern for planning agent.
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:30:26,644'); seek(1826.0)">
              So we've seen a few little pieces of how we can create simple AI agents,
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:30:32,194'); seek(1832.0)">
              and some can be quite powerful.
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:30:33,714'); seek(1833.0)">
              Your utilities, like we saw in the frequently asked questions or the report
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:30:37,489'); seek(1837.0)">
              selector and What we're going to do now is actually look at the four main patterns.
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:30:43,254'); seek(1843.0)">
              Andrew Ng, in his lecture, listed here, talked about the four main patterns.
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:30:48,794'); seek(1848.0)">
              Reflection, where the LLM kind of re examines its own work.
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:30:53,654'); seek(1853.0)">
              It sends it back to itself, but with a critique to say, make a
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:30:57,744'); seek(1857.0)">
              critique of what I've just sent you.
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:31:00,284'); seek(1860.0)">
              Tool use, we've seen an example of that.
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:31:03,084'); seek(1863.0)">
              planning, where it comes up with a plan to execute a multi step goal,
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:31:08,804'); seek(1868.0)">
              and also multi agent collaboration.
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:31:11,164'); seek(1871.0)">
              So we've seen a number of these examples, and what we're going to
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:31:13,764'); seek(1873.0)">
              do now is we're going to look at the reflection pattern, the tool pattern,
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:31:18,244'); seek(1878.0)">
              planning, and the multi agent pattern.
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:31:20,634'); seek(1880.0)">
              So let's go into the code to look at the reflection pattern to start with.
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:31:26,634'); seek(1886.0)">
              We're now going to look at the reflection pattern.
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:31:29,694'); seek(1889.0)">
              And let us not forget in essence what we're doing.
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:31:32,654'); seek(1892.0)">
              We're just creating a big string job description that we send.
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:31:36,974'); seek(1896.0)">
              We get some response.
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:31:39,004'); seek(1899.0)">
              We may append that to a new request or start a new request from fresh.
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:31:44,134'); seek(1904.0)">
              But in this reflection pattern, what we do is we generate a response with our
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:31:47,214'); seek(1907.0)">
              first query, then add this content to the request in a second query where we've
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:31:53,834'); seek(1913.0)">
              asked it to do something, and in this case to have a critique and further refinement.
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:31:58,974'); seek(1918.0)">
              So in some sense, the first request, it can be considered
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:32:01,664'); seek(1921.0)">
              as actually almost like RAG.
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:32:03,749'); seek(1923.0)">
              That we're generating some content to add to a new query,
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:32:08,629'); seek(1928.0)">
              augmenting it with a new set of instructions and getting a response.
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:32:13,279'); seek(1933.0)">
              And what we're going to do in this one is we're going to ask it to generate
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:32:15,669'); seek(1935.0)">
              some Python code, and then we're going to ask for it to critique it and make some
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:32:20,529'); seek(1940.0)">
              adjustments to produce a final response.
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:32:23,059'); seek(1943.0)">
              So we use our usual standard opening, getting the key, setting the models.
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:32:29,499'); seek(1949.0)">
              And we can see here that we're setting the very first system
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:32:32,789'); seek(1952.0)">
              message, first role, as a Python program tasked with generating code.
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:32:37,859'); seek(1957.0)">
              And we're generating our chat history.
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:32:39,709'); seek(1959.0)">
              We append our system content, the job description, as it were.
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:32:44,639'); seek(1964.0)">
              We then add to that, list, the user query, which in this case is
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:32:50,429'); seek(1970.0)">
              generate a Python implementation of requesting an API with request library.
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:32:54,629'); seek(1974.0)">
              And we then send that to the LLM
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:32:56,989'); seek(1976.0)">
              to get our response back.
            </span>
            
            <span id="chunk-605" class="transcript-chunks" onclick="console.log('00:32:59,559'); seek(1979.0)">
              And here we get our response.
            </span>
            
            <span id="chunk-606" class="transcript-chunks" onclick="console.log('00:33:01,719'); seek(1981.0)">
              And before we do that in for the next step, we also add what we got back.
            </span>
            
            <span id="chunk-607" class="transcript-chunks" onclick="console.log('00:33:06,529'); seek(1986.0)">
              So we can see we get the response from the LLM.
            </span>
            
            <span id="chunk-608" class="transcript-chunks" onclick="console.log('00:33:10,319'); seek(1990.0)">
              It's giving us some sample code with an explanation.
            </span>
            
            <span id="chunk-609" class="transcript-chunks" onclick="console.log('00:33:14,759'); seek(1994.0)">
              Now we want to reflect on that.
            </span>
            
            <span id="chunk-610" class="transcript-chunks" onclick="console.log('00:33:16,259'); seek(1996.0)">
              We want to send it back again and have a little critique or refinement.
            </span>
            
            <span id="chunk-611" class="transcript-chunks" onclick="console.log('00:33:20,009'); seek(2000.0)">
              So in our chat history, we add in now another system message,
            </span>
            
            <span id="chunk-612" class="transcript-chunks" onclick="console.log('00:33:24,429'); seek(2004.0)">
              and we're saying you're an experienced and talented Pythonista.
            </span>
            
            <span id="chunk-613" class="transcript-chunks" onclick="console.log('00:33:27,329'); seek(2007.0)">
              You're tasked with generating critique and recommendations for the user's code.
            </span>
            
            <span id="chunk-614" class="transcript-chunks" onclick="console.log('00:33:31,769'); seek(2011.0)">
              All of these messages are attached together and sent, because we
            </span>
            
            <span id="chunk-615" class="transcript-chunks" onclick="console.log('00:33:35,929'); seek(2015.0)">
              must remember that the request is stateless, so it needs to know what
            </span>
            
            <span id="chunk-616" class="transcript-chunks" onclick="console.log('00:33:39,539'); seek(2019.0)">
              went on before, so we must parse in that history of the conversation.
            </span>
            
            <span id="chunk-617" class="transcript-chunks" onclick="console.log('00:33:43,789'); seek(2023.0)">
              And we then get our critique back and display it.
            </span>
            
            <span id="chunk-618" class="transcript-chunks" onclick="console.log('00:33:48,754'); seek(2028.0)">
              And here is the output.
            </span>
            
            <span id="chunk-619" class="transcript-chunks" onclick="console.log('00:33:50,034'); seek(2030.0)">
              Your code demonstrates a solid approach going through it all, all
            </span>
            
            <span id="chunk-620" class="transcript-chunks" onclick="console.log('00:33:54,774'); seek(2034.0)">
              the way down.
            </span>
            
            <span id="chunk-621" class="transcript-chunks" onclick="console.log('00:33:55,894'); seek(2035.0)">
              And then we add this again, this critique to our chat history, send it
            </span>
            
            <span id="chunk-622" class="transcript-chunks" onclick="console.log('00:34:00,814'); seek(2040.0)">
              all in again to get a final summary.
            </span>
            
            <span id="chunk-623" class="transcript-chunks" onclick="console.log('00:34:03,294'); seek(2043.0)">
              And here's our final summary in form of an essay.
            </span>
            
            <span id="chunk-624" class="transcript-chunks" onclick="console.log('00:34:06,214'); seek(2046.0)">
              You scroll down, and here is our final output as requested.
            </span>
            
            <span id="chunk-625" class="transcript-chunks" onclick="console.log('00:34:11,374'); seek(2051.0)">
              Now, of course, what we do and how many times we go through it is up to us, but
            </span>
            
            <span id="chunk-626" class="transcript-chunks" onclick="console.log('00:34:16,454'); seek(2056.0)">
              the pattern is literally just Making requests, getting a response, taking that
            </span>
            
            <span id="chunk-627" class="transcript-chunks" onclick="console.log('00:34:21,659'); seek(2061.0)">
              response, adding it back into our chat history, adding in a new prompt to do
            </span>
            
            <span id="chunk-628" class="transcript-chunks" onclick="console.log('00:34:27,869'); seek(2067.0)">
              something with that and get a response.
            </span>
            
            <span id="chunk-629" class="transcript-chunks" onclick="console.log('00:34:29,949'); seek(2069.0)">
              It's one function after another with inputs producing outputs get
            </span>
            
            <span id="chunk-630" class="transcript-chunks" onclick="console.log('00:34:34,459'); seek(2074.0)">
              put into the next function as an input that produces an output.
            </span>
            
            <span id="chunk-631" class="transcript-chunks" onclick="console.log('00:34:38,409'); seek(2078.0)">
              So this is a reflection pattern.
            </span>
            
            <span id="chunk-632" class="transcript-chunks" onclick="console.log('00:34:40,469'); seek(2080.0)">
              And as we scroll down, we see the final answer.
            </span>
            
            <span id="chunk-633" class="transcript-chunks" onclick="console.log('00:34:43,269'); seek(2083.0)">
              Key improvements.
            </span>
            
            <span id="chunk-634" class="transcript-chunks" onclick="console.log('00:34:45,179'); seek(2085.0)">
              And, of course, we can make this a class and make the input come through a
            </span>
            
            <span id="chunk-635" class="transcript-chunks" onclick="console.log('00:34:48,509'); seek(2088.0)">
              certain form field or through a chat bot.
            </span>
            
            <span id="chunk-636" class="transcript-chunks" onclick="console.log('00:34:51,009'); seek(2091.0)">
              And once again, if we go up to the very top, we can remember, in essence,
            </span>
            
            <span id="chunk-637" class="transcript-chunks" onclick="console.log('00:34:54,999'); seek(2094.0)">
              this is really what we're doing.
            </span>
            
            <span id="chunk-638" class="transcript-chunks" onclick="console.log('00:34:56,279'); seek(2096.0)">
              We're making a function.
            </span>
            
            <span id="chunk-639" class="transcript-chunks" onclick="console.log('00:34:57,349'); seek(2097.0)">
              We're passing in inputs, getting an output and really chaining it
            </span>
            
            <span id="chunk-640" class="transcript-chunks" onclick="console.log('00:35:01,299'); seek(2101.0)">
              from one request to the other.
            </span>
            
            <span id="chunk-641" class="transcript-chunks" onclick="console.log('00:35:03,569'); seek(2103.0)">
              In this particular reflection pattern, we're going to look at using planning
            </span>
            
            <span id="chunk-642" class="transcript-chunks" onclick="console.log('00:35:08,699'); seek(2108.0)">
              or using reflection and tool calling.
            </span>
            
            <span id="chunk-643" class="transcript-chunks" onclick="console.log('00:35:11,659'); seek(2111.0)">
              Once again, let us remind ourselves that we're looking in this talk to see
            </span>
            
            <span id="chunk-644" class="transcript-chunks" onclick="console.log('00:35:14,979'); seek(2114.0)">
              what AI agents are in terms of their simplicity before we move into frameworks.
            </span>
            
            <span id="chunk-645" class="transcript-chunks" onclick="console.log('00:35:20,029'); seek(2120.0)">
              And what we're going to do now is use a sort of pattern of react, reason and act.
            </span>
            
            <span id="chunk-646" class="transcript-chunks" onclick="console.log('00:35:25,499'); seek(2125.0)">
              And essentially, we pass the output of each step as an input to the next request
            </span>
            
            <span id="chunk-647" class="transcript-chunks" onclick="console.log('00:35:30,099'); seek(2130.0)">
              to an LLM, like we did in Reflection.
            </span>
            
            <span id="chunk-648" class="transcript-chunks" onclick="console.log('00:35:32,299'); seek(2132.0)">
              But we're going to add in some tool calling, and we're also going to be adding
            </span>
            
            <span id="chunk-649" class="transcript-chunks" onclick="console.log('00:35:35,559'); seek(2135.0)">
              in some effectively routing, because the agent will determine which tool to use.
            </span>
            
            <span id="chunk-650" class="transcript-chunks" onclick="console.log('00:35:41,139'); seek(2141.0)">
              And how to use it.
            </span>
            
            <span id="chunk-651" class="transcript-chunks" onclick="console.log('00:35:42,649'); seek(2142.0)">
              We'll do this with the 20 planning agent with loop dot py But we will also do it
            </span>
            
            <span id="chunk-652" class="transcript-chunks" onclick="console.log('00:35:48,059'); seek(2148.0)">
              with the notebook where we will do this Looping manually so we can see how it
            </span>
            
            <span id="chunk-653" class="transcript-chunks" onclick="console.log('00:35:52,559'); seek(2152.0)">
              works So let's just go to the python file and we can see that we run in everything
            </span>
            
            <span id="chunk-654" class="transcript-chunks" onclick="console.log('00:35:58,719'); seek(2158.0)">
              As usual and we're creating an agent class where we're setting the client the system
            </span>
            
            <span id="chunk-655" class="transcript-chunks" onclick="console.log('00:36:04,279'); seek(2164.0)">
              role We're using the under core method.
            </span>
            
            <span id="chunk-656" class="transcript-chunks" onclick="console.log('00:36:07,309'); seek(2167.0)">
              We're using an execute function where we can just invoke the llm to get a response
            </span>
            
            <span id="chunk-657" class="transcript-chunks" onclick="console.log('00:36:11,339'); seek(2171.0)">
              You And what we want to do in this example is calculate the total price for an item.
            </span>
            
            <span id="chunk-658" class="transcript-chunks" onclick="console.log('00:36:19,529'); seek(2179.0)">
              And the two tools we have are calculate the total that
            </span>
            
            <span id="chunk-659" class="transcript-chunks" onclick="console.log('00:36:22,129'); seek(2182.0)">
              given a price adds on the VAT.
            </span>
            
            <span id="chunk-660" class="transcript-chunks" onclick="console.log('00:36:25,009'); seek(2185.0)">
              We also have another tool, function, get product price, that for a given
            </span>
            
            <span id="chunk-661" class="transcript-chunks" onclick="console.log('00:36:30,039'); seek(2190.0)">
              argument gets the price of the product.
            </span>
            
            <span id="chunk-662" class="transcript-chunks" onclick="console.log('00:36:33,194'); seek(2193.0)">
              And if we scroll down, we will see these two functions here.
            </span>
            
            <span id="chunk-663" class="transcript-chunks" onclick="console.log('00:36:36,044'); seek(2196.0)">
              CalculateTotal, GetProductPrice.
            </span>
            
            <span id="chunk-664" class="transcript-chunks" onclick="console.log('00:36:38,534'); seek(2198.0)">
              So let's look at our system prompt, and we're telling it how we want it to work.
            </span>
            
            <span id="chunk-665" class="transcript-chunks" onclick="console.log('00:36:43,364'); seek(2203.0)">
              We want it to think, take an action, get an observation, and repeat the loop.
            </span>
            
            <span id="chunk-666" class="transcript-chunks" onclick="console.log('00:36:49,114'); seek(2209.0)">
              So we give examples of what the tools are.
            </span>
            
            <span id="chunk-667" class="transcript-chunks" onclick="console.log('00:36:53,929'); seek(2213.0)">
              What the kind of response we'd like to get back for both the calculate
            </span>
            
            <span id="chunk-668" class="transcript-chunks" onclick="console.log('00:36:58,399'); seek(2218.0)">
              price, total and get product price.
            </span>
            
            <span id="chunk-669" class="transcript-chunks" onclick="console.log('00:37:01,879'); seek(2221.0)">
              And we're also given an example session.
            </span>
            
            <span id="chunk-670" class="transcript-chunks" onclick="console.log('00:37:04,139'); seek(2224.0)">
              So what's going to happen is a user is going to say, what is the total
            </span>
            
            <span id="chunk-671" class="transcript-chunks" onclick="console.log('00:37:06,679'); seek(2226.0)">
              cost of a bike, including that?
            </span>
            
            <span id="chunk-672" class="transcript-chunks" onclick="console.log('00:37:09,029'); seek(2229.0)">
              We want the AI response to be in this format.
            </span>
            
            <span id="chunk-673" class="transcript-chunks" onclick="console.log('00:37:12,189'); seek(2232.0)">
              Thought, I need to find the cost of a bike.
            </span>
            
            <span id="chunk-674" class="transcript-chunks" onclick="console.log('00:37:15,219'); seek(2235.0)">
              We're pipe delimbing it so we can get the function name and the price.
            </span>
            
            <span id="chunk-675" class="transcript-chunks" onclick="console.log('00:37:19,029'); seek(2239.0)">
              arguments, but it's going to be an action type as opposed to an answer.
            </span>
            
            <span id="chunk-676" class="transcript-chunks" onclick="console.log('00:37:23,169'); seek(2243.0)">
              We're going to get the tool call and we're going to get the argument.
            </span>
            
            <span id="chunk-677" class="transcript-chunks" onclick="console.log('00:37:27,219'); seek(2247.0)">
              We will get the response back of an observation of the actual return of
            </span>
            
            <span id="chunk-678" class="transcript-chunks" onclick="console.log('00:37:31,519'); seek(2251.0)">
              that function, which we will then use as an input to the next query
            </span>
            
            <span id="chunk-679" class="transcript-chunks" onclick="console.log('00:37:35,719'); seek(2255.0)">
              where it now needs to calculate the total price including the VAT.
            </span>
            
            <span id="chunk-680" class="transcript-chunks" onclick="console.log('00:37:39,859'); seek(2259.0)">
              It's an action.
            </span>
            
            <span id="chunk-681" class="transcript-chunks" onclick="console.log('00:37:41,034'); seek(2261.0)">
              It knows to use the calculate total, and it has an argument passed into it.
            </span>
            
            <span id="chunk-682" class="transcript-chunks" onclick="console.log('00:37:45,204'); seek(2265.0)">
              This is just a sample example so it can see the format of what it needs
            </span>
            
            <span id="chunk-683" class="transcript-chunks" onclick="console.log('00:37:48,844'); seek(2268.0)">
              to do, and that we will always be passing in the result of our LLM
            </span>
            
            <span id="chunk-684" class="transcript-chunks" onclick="console.log('00:37:53,524'); seek(2273.0)">
              course as observation pipe 240.
            </span>
            
            <span id="chunk-685" class="transcript-chunks" onclick="console.log('00:37:56,824'); seek(2276.0)">
              That's what it can expect.
            </span>
            
            <span id="chunk-686" class="transcript-chunks" onclick="console.log('00:37:58,664'); seek(2278.0)">
              Then we tell it that if you have the answer, print out an
            </span>
            
            <span id="chunk-687" class="transcript-chunks" onclick="console.log('00:38:01,244'); seek(2281.0)">
              answer for us in this form.
            </span>
            
            <span id="chunk-688" class="transcript-chunks" onclick="console.log('00:38:03,144'); seek(2283.0)">
              So let's just run this just to see what it will actually look like.
            </span>
            
            <span id="chunk-689" class="transcript-chunks" onclick="console.log('00:38:06,384'); seek(2286.0)">
              And if we scroll down.
            </span>
            
            <span id="chunk-690" class="transcript-chunks" onclick="console.log('00:38:07,779'); seek(2287.0)">
              We can see we've got a loop.
            </span>
            
            <span id="chunk-691" class="transcript-chunks" onclick="console.log('00:38:09,079'); seek(2289.0)">
              We're not going to get into each line of the code, but basically
            </span>
            
            <span id="chunk-692" class="transcript-chunks" onclick="console.log('00:38:11,899'); seek(2291.0)">
              it's going to be looping around.
            </span>
            
            <span id="chunk-693" class="transcript-chunks" onclick="console.log('00:38:13,799'); seek(2293.0)">
              If it's an action, it will run that function.
            </span>
            
            <span id="chunk-694" class="transcript-chunks" onclick="console.log('00:38:16,589'); seek(2296.0)">
              Get a value and pass it back through to be used again.
            </span>
            
            <span id="chunk-695" class="transcript-chunks" onclick="console.log('00:38:20,369'); seek(2300.0)">
              If it determines it has an answer, it'll print the answer and exit the loop.
            </span>
            
            <span id="chunk-696" class="transcript-chunks" onclick="console.log('00:38:25,259'); seek(2305.0)">
              So, for example, we've got three here.
            </span>
            
            <span id="chunk-697" class="transcript-chunks" onclick="console.log('00:38:26,989'); seek(2306.0)">
              One is the cost of a bike, a TV, and a laptop.
            </span>
            
            <span id="chunk-698" class="transcript-chunks" onclick="console.log('00:38:29,569'); seek(2309.0)">
              So, let's run that and see what happens.
            </span>
            
            <span id="chunk-699" class="transcript-chunks" onclick="console.log('00:38:32,259'); seek(2312.0)">
              It's starting the loop.
            </span>
            
            <span id="chunk-700" class="transcript-chunks" onclick="console.log('00:38:33,289'); seek(2313.0)">
              It has a thought.
            </span>
            
            <span id="chunk-701" class="transcript-chunks" onclick="console.log('00:38:34,499'); seek(2314.0)">
              It has an observation.
            </span>
            
            <span id="chunk-702" class="transcript-chunks" onclick="console.log('00:38:36,619'); seek(2316.0)">
              It gets passed in.
            </span>
            
            <span id="chunk-703" class="transcript-chunks" onclick="console.log('00:38:38,249'); seek(2318.0)">
              And each time we get the result here, but we can see the summary answers.
            </span>
            
            <span id="chunk-704" class="transcript-chunks" onclick="console.log('00:38:42,034'); seek(2322.0)">
              But if we look at a particular loop starting the loop, it has the thought,
            </span>
            
            <span id="chunk-705" class="transcript-chunks" onclick="console.log('00:38:44,794'); seek(2324.0)">
              I need to find the cost of a TV.
            </span>
            
            <span id="chunk-706" class="transcript-chunks" onclick="console.log('00:38:47,104'); seek(2327.0)">
              We've extracted out the action.
            </span>
            
            <span id="chunk-707" class="transcript-chunks" onclick="console.log('00:38:49,474'); seek(2329.0)">
              We've extracted out that the function call is going to be get
            </span>
            
            <span id="chunk-708" class="transcript-chunks" onclick="console.log('00:38:51,834'); seek(2331.0)">
              product price and the parameter.
            </span>
            
            <span id="chunk-709" class="transcript-chunks" onclick="console.log('00:38:54,424'); seek(2334.0)">
              We get an observation of 200.
            </span>
            
            <span id="chunk-710" class="transcript-chunks" onclick="console.log('00:38:56,494'); seek(2336.0)">
              That now gets VAT back in, and it now knows it needs to calculate
            </span>
            
            <span id="chunk-711" class="transcript-chunks" onclick="console.log('00:39:00,404'); seek(2340.0)">
              the total, including the VAT.
            </span>
            
            <span id="chunk-712" class="transcript-chunks" onclick="console.log('00:39:01,974'); seek(2341.0)">
              It's going to use a calculateTotal with the 200 that we passed in.
            </span>
            
            <span id="chunk-713" class="transcript-chunks" onclick="console.log('00:39:05,954'); seek(2345.0)">
              The observation is returned back as 240.
            </span>
            
            <span id="chunk-714" class="transcript-chunks" onclick="console.log('00:39:08,854'); seek(2348.0)">
              That gets put into the loop to get the answer.
            </span>
            
            <span id="chunk-715" class="transcript-chunks" onclick="console.log('00:39:11,994'); seek(2351.0)">
              Now, if you notice, we've missed one here, and we have the answer here.
            </span>
            
            <span id="chunk-716" class="transcript-chunks" onclick="console.log('00:39:17,524'); seek(2357.0)">
              And that's the nature sometimes of LLMs, that things do actually go
            </span>
            
            <span id="chunk-717" class="transcript-chunks" onclick="console.log('00:39:21,934'); seek(2361.0)">
              wrong because it's probabilistic.
            </span>
            
            <span id="chunk-718" class="transcript-chunks" onclick="console.log('00:39:25,034'); seek(2365.0)">
              But we get the first one, the price of the bike is 120.
            </span>
            
            <span id="chunk-719" class="transcript-chunks" onclick="console.log('00:39:28,094'); seek(2368.0)">
              We do get the result, 240, but we don't get it printed out in the answer.
            </span>
            
            <span id="chunk-720" class="transcript-chunks" onclick="console.log('00:39:32,984'); seek(2372.0)">
              And if we were to run this again, come
            </span>
            
            <span id="chunk-721" class="transcript-chunks" onclick="console.log('00:39:35,254'); seek(2375.0)">
              back in here and open this up, and if we run it again, we'll
            </span>
            
            <span id="chunk-722" class="transcript-chunks" onclick="console.log('00:39:39,574'); seek(2379.0)">
              probably see a different answer.
            </span>
            
            <span id="chunk-723" class="transcript-chunks" onclick="console.log('00:39:41,254'); seek(2381.0)">
              Starting the loop, 120, answer found, the price of the bike is 120.
            </span>
            
            <span id="chunk-724" class="transcript-chunks" onclick="console.log('00:39:47,214'); seek(2387.0)">
              Starting again, we found the price of the TV is 240.
            </span>
            
            <span id="chunk-725" class="transcript-chunks" onclick="console.log('00:39:50,804'); seek(2390.0)">
              And now we've got the price of the laptop, 360, and we get all three answers.
            </span>
            
            <span id="chunk-726" class="transcript-chunks" onclick="console.log('00:39:55,024'); seek(2395.0)">
              So it can show that we can't take things for granted because
            </span>
            
            <span id="chunk-727" class="transcript-chunks" onclick="console.log('00:39:57,884'); seek(2397.0)">
              the Things will be missed out.
            </span>
            
            <span id="chunk-728" class="transcript-chunks" onclick="console.log('00:39:59,589'); seek(2399.0)">
              It is not 100 percent deterministic.
            </span>
            
            <span id="chunk-729" class="transcript-chunks" onclick="console.log('00:40:02,589'); seek(2402.0)">
              So we need to put in some checks and balances if we were using these results.
            </span>
            
            <span id="chunk-730" class="transcript-chunks" onclick="console.log('00:40:06,759'); seek(2406.0)">
              But this is an example of actually how we can loop through.
            </span>
            
            <span id="chunk-731" class="transcript-chunks" onclick="console.log('00:40:10,839'); seek(2410.0)">
              So what we're going to do now is actually go to the notebook version
            </span>
            
            <span id="chunk-732" class="transcript-chunks" onclick="console.log('00:40:14,009'); seek(2414.0)">
              and see how would we do this manually.
            </span>
            
            <span id="chunk-733" class="transcript-chunks" onclick="console.log('00:40:15,799'); seek(2415.0)">
              What's actually going on?
            </span>
            
            <span id="chunk-734" class="transcript-chunks" onclick="console.log('00:40:17,369'); seek(2417.0)">
              So We use the same setup, use the same system prompt, same functions.
            </span>
            
            <span id="chunk-735" class="transcript-chunks" onclick="console.log('00:40:24,689'); seek(2424.0)">
              But what we do now is we call an instance of this planning agent with
            </span>
            
            <span id="chunk-736" class="transcript-chunks" onclick="console.log('00:40:28,049'); seek(2428.0)">
              the client and the system prompt.
            </span>
            
            <span id="chunk-737" class="transcript-chunks" onclick="console.log('00:40:30,089'); seek(2430.0)">
              We set up an instance of it and we pass through the first question.
            </span>
            
            <span id="chunk-738" class="transcript-chunks" onclick="console.log('00:40:33,729'); seek(2433.0)">
              What is the cost of a laptop, including the VAT?
            </span>
            
            <span id="chunk-739" class="transcript-chunks" onclick="console.log('00:40:36,569'); seek(2436.0)">
              When we run that, we get back the result, which is thought.
            </span>
            
            <span id="chunk-740" class="transcript-chunks" onclick="console.log('00:40:40,949'); seek(2440.0)">
              That we can now extract out by splitting on the pipe exactly whether
            </span>
            
            <span id="chunk-741" class="transcript-chunks" onclick="console.log('00:40:45,289'); seek(2445.0)">
              it's an action, what the function name is, and what the parameter
            </span>
            
            <span id="chunk-742" class="transcript-chunks" onclick="console.log('00:40:48,549'); seek(2448.0)">
              name is, what the argument is.
            </span>
            
            <span id="chunk-743" class="transcript-chunks" onclick="console.log('00:40:50,509'); seek(2450.0)">
              And as you can see, when we do that, we get the function, we
            </span>
            
            <span id="chunk-744" class="transcript-chunks" onclick="console.log('00:40:53,409'); seek(2453.0)">
              need to run the getProductPrice, and we need to do it on a laptop.
            </span>
            
            <span id="chunk-745" class="transcript-chunks" onclick="console.log('00:40:56,809'); seek(2456.0)">
              So manually, this is what we would do.
            </span>
            
            <span id="chunk-746" class="transcript-chunks" onclick="console.log('00:40:59,069'); seek(2459.0)">
              We'd say, if the next function is, which we've just determined here by extracting
            </span>
            
            <span id="chunk-747" class="transcript-chunks" onclick="console.log('00:41:02,949'); seek(2462.0)">
              out this item here in the output.
            </span>
            
            <span id="chunk-748" class="transcript-chunks" onclick="console.log('00:41:07,419'); seek(2467.0)">
              If it's get product price, run get product price with that value.
            </span>
            
            <span id="chunk-749" class="transcript-chunks" onclick="console.log('00:41:11,839'); seek(2471.0)">
              The argument that we got is the third, fourth item here.
            </span>
            
            <span id="chunk-750" class="transcript-chunks" onclick="console.log('00:41:15,389'); seek(2475.0)">
              So we get 300, and we said in our prompt that we would send it back in in the form
            </span>
            
            <span id="chunk-751" class="transcript-chunks" onclick="console.log('00:41:19,769'); seek(2479.0)">
              of observation, pipe, and the result.
            </span>
            
            <span id="chunk-752" class="transcript-chunks" onclick="console.log('00:41:22,219'); seek(2482.0)">
              So this is what we're going to send as the next prompt.
            </span>
            
            <span id="chunk-753" class="transcript-chunks" onclick="console.log('00:41:24,919'); seek(2484.0)">
              So we run it with our agent.
            </span>
            
            <span id="chunk-754" class="transcript-chunks" onclick="console.log('00:41:26,889'); seek(2486.0)">
              Next prompt, the result.
            </span>
            
            <span id="chunk-755" class="transcript-chunks" onclick="console.log('00:41:28,599'); seek(2488.0)">
              It now determines it still needs another action, but it's the calculate
            </span>
            
            <span id="chunk-756" class="transcript-chunks" onclick="console.log('00:41:32,129'); seek(2492.0)">
              total and it needs the price of 300 to be calculated with VAT.
            </span>
            
            <span id="chunk-757" class="transcript-chunks" onclick="console.log('00:41:37,809'); seek(2497.0)">
              So once again, we strip out the function and the parameter.
            </span>
            
            <span id="chunk-758" class="transcript-chunks" onclick="console.log('00:41:41,399'); seek(2501.0)">
              We need to do calculate total and the argument of 300.
            </span>
            
            <span id="chunk-759" class="transcript-chunks" onclick="console.log('00:41:45,079'); seek(2505.0)">
              And once again, we run this manually.
            </span>
            
            <span id="chunk-760" class="transcript-chunks" onclick="console.log('00:41:46,989'); seek(2506.0)">
              You know, it's to calculate total.
            </span>
            
            <span id="chunk-761" class="transcript-chunks" onclick="console.log('00:41:48,519'); seek(2508.0)">
              And the next arc, there are different ways that we could
            </span>
            
            <span id="chunk-762" class="transcript-chunks" onclick="console.log('00:41:51,429'); seek(2511.0)">
              run that using, an eval method.
            </span>
            
            <span id="chunk-763" class="transcript-chunks" onclick="console.log('00:41:53,639'); seek(2513.0)">
              But for simplicity, we're just going to run it manually is calculate total.
            </span>
            
            <span id="chunk-764" class="transcript-chunks" onclick="console.log('00:41:57,219'); seek(2517.0)">
              We get the answer 360.
            </span>
            
            <span id="chunk-765" class="transcript-chunks" onclick="console.log('00:41:59,179'); seek(2519.0)">
              Once again, we're going to send that back in, in the form of observation result.
            </span>
            
            <span id="chunk-766" class="transcript-chunks" onclick="console.log('00:42:03,389'); seek(2523.0)">
              That's what we specified in the system prompt.
            </span>
            
            <span id="chunk-767" class="transcript-chunks" onclick="console.log('00:42:05,809'); seek(2525.0)">
              So we're sending this back in as the next query.
            </span>
            
            <span id="chunk-768" class="transcript-chunks" onclick="console.log('00:42:08,249'); seek(2528.0)">
              That's the next prompt.
            </span>
            
            <span id="chunk-769" class="transcript-chunks" onclick="console.log('00:42:09,919'); seek(2529.0)">
              Pass it in to the agent, the next prompt of observation 360.
            </span>
            
            <span id="chunk-770" class="transcript-chunks" onclick="console.log('00:42:14,079'); seek(2534.0)">
              Print the result.
            </span>
            
            <span id="chunk-771" class="transcript-chunks" onclick="console.log('00:42:15,269'); seek(2535.0)">
              Now that it's determined it has an answer, it prints it
            </span>
            
            <span id="chunk-772" class="transcript-chunks" onclick="console.log('00:42:17,849'); seek(2537.0)">
              out in the format that we want.
            </span>
            
            <span id="chunk-773" class="transcript-chunks" onclick="console.log('00:42:19,799'); seek(2539.0)">
              Answer.
            </span>
            
            <span id="chunk-774" class="transcript-chunks" onclick="console.log('00:42:19,999'); seek(2539.0)">
              The price of the laptop, including that, is 360.
            </span>
            
            <span id="chunk-775" class="transcript-chunks" onclick="console.log('00:42:22,849'); seek(2542.0)">
              So if we go back up again and basically change it, and now let's
            </span>
            
            <span id="chunk-776" class="transcript-chunks" onclick="console.log('00:42:26,329'); seek(2546.0)">
              work out for the price of the TV and see exactly the same again.
            </span>
            
            <span id="chunk-777" class="transcript-chunks" onclick="console.log('00:42:29,819'); seek(2549.0)">
              We're going running through.
            </span>
            
            <span id="chunk-778" class="transcript-chunks" onclick="console.log('00:42:31,479'); seek(2551.0)">
              We
            </span>
            
            <span id="chunk-779" class="transcript-chunks" onclick="console.log('00:42:32,319'); seek(2552.0)">
              now know the action is get product price, but it's for a TV.
            </span>
            
            <span id="chunk-780" class="transcript-chunks" onclick="console.log('00:42:35,759'); seek(2555.0)">
              It runs the function.
            </span>
            
            <span id="chunk-781" class="transcript-chunks" onclick="console.log('00:42:36,719'); seek(2556.0)">
              It gets the price that it's 200.
            </span>
            
            <span id="chunk-782" class="transcript-chunks" onclick="console.log('00:42:39,049'); seek(2559.0)">
              We send it back in and saying the observation from
            </span>
            
            <span id="chunk-783" class="transcript-chunks" onclick="console.log('00:42:42,179'); seek(2562.0)">
              the previous action is 200.
            </span>
            
            <span id="chunk-784" class="transcript-chunks" onclick="console.log('00:42:44,189'); seek(2564.0)">
              It then extracts what it needs, and now it's starting to calculate the total
            </span>
            
            <span id="chunk-785" class="transcript-chunks" onclick="console.log('00:42:48,209'); seek(2568.0)">
              with the VAT, 200, extracts again, runs.
            </span>
            
            <span id="chunk-786" class="transcript-chunks" onclick="console.log('00:42:52,669'); seek(2572.0)">
              We then run the function with the extracted argument, we get 240.
            </span>
            
            <span id="chunk-787" class="transcript-chunks" onclick="console.log('00:42:58,089'); seek(2578.0)">
              We now send this observation back in the form that we said we
            </span>
            
            <span id="chunk-788" class="transcript-chunks" onclick="console.log('00:43:01,459'); seek(2581.0)">
              would, observation pipe result.
            </span>
            
            <span id="chunk-789" class="transcript-chunks" onclick="console.log('00:43:04,004'); seek(2584.0)">
              You send this into the next query, goes into here, you print the result, answer
            </span>
            
            <span id="chunk-790" class="transcript-chunks" onclick="console.log('00:43:09,264'); seek(2589.0)">
              the price of the TV, including VAT is 240.
            </span>
            
            <span id="chunk-791" class="transcript-chunks" onclick="console.log('00:43:11,994'); seek(2591.0)">
              Now, there are many ways you can refactor this and use eval without
            </span>
            
            <span id="chunk-792" class="transcript-chunks" onclick="console.log('00:43:15,424'); seek(2595.0)">
              actually having to specify in a step the actual function name.
            </span>
            
            <span id="chunk-793" class="transcript-chunks" onclick="console.log('00:43:18,704'); seek(2598.0)">
              That's effectively what we do when we do our loop.
            </span>
            
            <span id="chunk-794" class="transcript-chunks" onclick="console.log('00:43:21,294'); seek(2601.0)">
              We basically extract all that information and execute appropriately in our loop.
            </span>
            
            <span id="chunk-795" class="transcript-chunks" onclick="console.log('00:43:27,164'); seek(2607.0)">
              So I hope this kind of shows that although we can have many different patterns,
            </span>
            
            <span id="chunk-796" class="transcript-chunks" onclick="console.log('00:43:31,314'); seek(2611.0)">
              Essentially, it's basically involving reflection, sending back a previous output
            </span>
            
            <span id="chunk-797" class="transcript-chunks" onclick="console.log('00:43:36,604'); seek(2616.0)">
              as an input, use of tool call, and also the use of planning of breaking up a task
            </span>
            
            <span id="chunk-798" class="transcript-chunks" onclick="console.log('00:43:43,474'); seek(2623.0)">
              into separate steps that it executes.
            </span>
            
            <span id="chunk-799" class="transcript-chunks" onclick="console.log('00:43:46,124'); seek(2626.0)">
              So they all sort of overlap and are interconnected.
            </span>
            
            <span id="chunk-800" class="transcript-chunks" onclick="console.log('00:43:49,574'); seek(2629.0)">
              And I hope that this actually explains how AI agents work and how if we look at where
            </span>
            
            <span id="chunk-801" class="transcript-chunks" onclick="console.log('00:43:54,614'); seek(2634.0)">
              is the AI in all of this, literally it is just in the execute function to the LLM.
            </span>
            
            <span id="chunk-802" class="transcript-chunks" onclick="console.log('00:44:01,464'); seek(2641.0)">
              The rest of the AI part is just where we invoke the instance of
            </span>
            
            <span id="chunk-803" class="transcript-chunks" onclick="console.log('00:44:05,374'); seek(2645.0)">
              the agent with the next prompt that produces all these results.
            </span>
            
            <span id="chunk-804" class="transcript-chunks" onclick="console.log('00:44:09,274'); seek(2649.0)">
              The rest is we're just stripping with day to day Python, sending
            </span>
            
            <span id="chunk-805" class="transcript-chunks" onclick="console.log('00:44:12,564'); seek(2652.0)">
              it back in to the loop, and then running again another AI API request.
            </span>
            
            <span id="chunk-806" class="transcript-chunks" onclick="console.log('00:44:18,924'); seek(2658.0)">
              Well, we've just been through quite a lot of code.
            </span>
            
            <span id="chunk-807" class="transcript-chunks" onclick="console.log('00:44:21,494'); seek(2661.0)">
              It's something really to digest offline.
            </span>
            
            <span id="chunk-808" class="transcript-chunks" onclick="console.log('00:44:24,614'); seek(2664.0)">
              As I've said before, it's taken me quite a while just to kind of work through
            </span>
            
            <span id="chunk-809" class="transcript-chunks" onclick="console.log('00:44:28,034'); seek(2668.0)">
              and really get it clear in my own mind to be able to explain it to people.
            </span>
            
            <span id="chunk-810" class="transcript-chunks" onclick="console.log('00:44:31,604'); seek(2671.0)">
              So if you don't feel it's kind of sunk in, that's not a problem at all.
            </span>
            
            <span id="chunk-811" class="transcript-chunks" onclick="console.log('00:44:35,754'); seek(2675.0)">
              That's probably to be expected.
            </span>
            
            <span id="chunk-812" class="transcript-chunks" onclick="console.log('00:44:37,704'); seek(2677.0)">
              And that's why I've given you the repo and all the code examples here so you
            </span>
            
            <span id="chunk-813" class="transcript-chunks" onclick="console.log('00:44:41,044'); seek(2681.0)">
              can work through them, chop it up, change them around to get understand it.
            </span>
            
            <span id="chunk-814" class="transcript-chunks" onclick="console.log('00:44:45,214'); seek(2685.0)">
              So we saw the sort of four main, we've seen sort of reflection, we've
            </span>
            
            <span id="chunk-815" class="transcript-chunks" onclick="console.log('00:44:48,274'); seek(2688.0)">
              seen tool use, we've seen a sort of planning where we're sort of
            </span>
            
            <span id="chunk-816" class="transcript-chunks" onclick="console.log('00:44:51,604'); seek(2691.0)">
              basically having to break a problem down into steps and work through them.
            </span>
            
            <span id="chunk-817" class="transcript-chunks" onclick="console.log('00:44:56,204'); seek(2696.0)">
              We haven't really seen multi agent collaboration.
            </span>
            
            <span id="chunk-818" class="transcript-chunks" onclick="console.log('00:44:59,194'); seek(2699.0)">
              And the reason for this is basically it's a design pattern of how one
            </span>
            
            <span id="chunk-819" class="transcript-chunks" onclick="console.log('00:45:03,974'); seek(2703.0)">
              wants to use what one gets back from each agent with another agent.
            </span>
            
            <span id="chunk-820" class="transcript-chunks" onclick="console.log('00:45:08,534'); seek(2708.0)">
              Is it the active pattern, the pub sub, finite state machine, as
            </span>
            
            <span id="chunk-821" class="transcript-chunks" onclick="console.log('00:45:12,554'); seek(2712.0)">
              in, for example, like Landgraf?
            </span>
            
            <span id="chunk-822" class="transcript-chunks" onclick="console.log('00:45:15,024'); seek(2715.0)">
              And what I'd like to talk about in closing is the libraries and
            </span>
            
            <span id="chunk-823" class="transcript-chunks" onclick="console.log('00:45:20,124'); seek(2720.0)">
              frameworks that are available.
            </span>
            
            <span id="chunk-824" class="transcript-chunks" onclick="console.log('00:45:21,564'); seek(2721.0)">
              And I like to think of libraries as sort of frameworks without
            </span>
            
            <span id="chunk-825" class="transcript-chunks" onclick="console.log('00:45:24,344'); seek(2724.0)">
              the framework, as it were.
            </span>
            
            <span id="chunk-826" class="transcript-chunks" onclick="console.log('00:45:25,684'); seek(2725.0)">
              Lots of convenience, tools, simplicity, that we can build
            </span>
            
            <span id="chunk-827" class="transcript-chunks" onclick="console.log('00:45:28,804'); seek(2728.0)">
              the overall design pattern.
            </span>
            
            <span id="chunk-828" class="transcript-chunks" onclick="console.log('00:45:30,944'); seek(2730.0)">
              And Pydantic AI is a new one that's come towards the end of 2024.
            </span>
            
            <span id="chunk-829" class="transcript-chunks" onclick="console.log('00:45:35,484'); seek(2735.0)">
              Pydantic is well known in the Python community.
            </span>
            
            <span id="chunk-830" class="transcript-chunks" onclick="console.log('00:45:38,514'); seek(2738.0)">
              And as you can see, structured output.
            </span>
            
            <span id="chunk-831" class="transcript-chunks" onclick="console.log('00:45:40,969'); seek(2740.0)">
              It's very important because we want to pass structured output from one agent
            </span>
            
            <span id="chunk-832" class="transcript-chunks" onclick="console.log('00:45:45,369'); seek(2745.0)">
              or from one part of our app to the next.
            </span>
            
            <span id="chunk-833" class="transcript-chunks" onclick="console.log('00:45:48,059'); seek(2748.0)">
              HuggingFace SmallAgents is another very small, lightweight framework
            </span>
            
            <span id="chunk-834" class="transcript-chunks" onclick="console.log('00:45:52,809'); seek(2752.0)">
              library that they implement so that one can create one's own overall
            </span>
            
            <span id="chunk-835" class="transcript-chunks" onclick="console.log('00:45:57,799'); seek(2757.0)">
              framework or design pattern.
            </span>
            
            <span id="chunk-836" class="transcript-chunks" onclick="console.log('00:45:59,369'); seek(2759.0)">
              Then we see many different crews and swarms and frameworks.
            </span>
            
            <span id="chunk-837" class="transcript-chunks" onclick="console.log('00:46:03,229'); seek(2763.0)">
              And these are basically design patterns for, essentially, intercommunication
            </span>
            
            <span id="chunk-838" class="transcript-chunks" onclick="console.log('00:46:08,179'); seek(2768.0)">
              between bits of code, our AI agents.
            </span>
            
            <span id="chunk-839" class="transcript-chunks" onclick="console.log('00:46:11,299'); seek(2771.0)">
              And we saw before in the AI agents directory, how many different
            </span>
            
            <span id="chunk-840" class="transcript-chunks" onclick="console.log('00:46:14,769'); seek(2774.0)">
              frameworks there are around there.
            </span>
            
            <span id="chunk-841" class="transcript-chunks" onclick="console.log('00:46:17,089'); seek(2777.0)">
              As you can see, if we start with the simplicity as we have been doing, we
            </span>
            
            <span id="chunk-842" class="transcript-chunks" onclick="console.log('00:46:20,699'); seek(2780.0)">
              might refactor it, build our own library.
            </span>
            
            <span id="chunk-843" class="transcript-chunks" onclick="console.log('00:46:23,374'); seek(2783.0)">
              And then realize actually other people could benefit from them, customize
            </span>
            
            <span id="chunk-844" class="transcript-chunks" onclick="console.log('00:46:27,024'); seek(2787.0)">
              it to make it more easy to use.
            </span>
            
            <span id="chunk-845" class="transcript-chunks" onclick="console.log('00:46:28,934'); seek(2788.0)">
              And then we add another framework to the AI agents directory, for example.
            </span>
            
            <span id="chunk-846" class="transcript-chunks" onclick="console.log('00:46:33,814'); seek(2793.0)">
              So in terms of frameworks, there's LLAMA Index, LLANG Chain, LLANG Graph,
            </span>
            
            <span id="chunk-847" class="transcript-chunks" onclick="console.log('00:46:37,254'); seek(2797.0)">
              AutoGen, Crew AI, and then very many more.
            </span>
            
            <span id="chunk-848" class="transcript-chunks" onclick="console.log('00:46:40,804'); seek(2800.0)">
              All very good, all very useful.
            </span>
            
            <span id="chunk-849" class="transcript-chunks" onclick="console.log('00:46:42,794'); seek(2802.0)">
              And hopefully, having gone through the simplicity of these AI agents,
            </span>
            
            <span id="chunk-850" class="transcript-chunks" onclick="console.log('00:46:46,904'); seek(2806.0)">
              not only could you maybe just design your own mini framework, But when
            </span>
            
            <span id="chunk-851" class="transcript-chunks" onclick="console.log('00:46:50,234'); seek(2810.0)">
              you come to use these frameworks, you will actually understand what it is
            </span>
            
            <span id="chunk-852" class="transcript-chunks" onclick="console.log('00:46:53,484'); seek(2813.0)">
              they're doing and how they're doing it.
            </span>
            
            <span id="chunk-853" class="transcript-chunks" onclick="console.log('00:46:55,464'); seek(2815.0)">
              So, in summary, I hope AI agents have been demystified and helped us understand
            </span>
            
            <span id="chunk-854" class="transcript-chunks" onclick="console.log('00:47:00,424'); seek(2820.0)">
              what they can do, enabling us either to build our own frameworks or use
            </span>
            
            <span id="chunk-855" class="transcript-chunks" onclick="console.log('00:47:03,854'); seek(2823.0)">
              existing ones, with a deeper appreciation and understanding of how they work.
            </span>
            
            <span id="chunk-856" class="transcript-chunks" onclick="console.log('00:47:08,404'); seek(2828.0)">
              And Anthropic has, in their blog, has come up with this,
            </span>
            
            <span id="chunk-857" class="transcript-chunks" onclick="console.log('00:47:12,334'); seek(2832.0)">
              when and when not to use agents.
            </span>
            
            <span id="chunk-858" class="transcript-chunks" onclick="console.log('00:47:15,554'); seek(2835.0)">
              And I'll just read it out loud because it sums it up better than I could.
            </span>
            
            <span id="chunk-859" class="transcript-chunks" onclick="console.log('00:47:19,029'); seek(2839.0)">
              When building applications with LLMs, we recommend finding the
            </span>
            
            <span id="chunk-860" class="transcript-chunks" onclick="console.log('00:47:22,559'); seek(2842.0)">
              simplest solution possible, and only increasing complexity when needed.
            </span>
            
            <span id="chunk-861" class="transcript-chunks" onclick="console.log('00:47:26,899'); seek(2846.0)">
              This might mean not building agentic systems at all.
            </span>
            
            <span id="chunk-862" class="transcript-chunks" onclick="console.log('00:47:29,989'); seek(2849.0)">
              Agentic systems often trade latency and cost for better task
            </span>
            
            <span id="chunk-863" class="transcript-chunks" onclick="console.log('00:47:32,939'); seek(2852.0)">
              performance, and you should consider when this trade off makes sense.
            </span>
            
            <span id="chunk-864" class="transcript-chunks" onclick="console.log('00:47:36,799'); seek(2856.0)">
              When more complexity is warranted, workflows offer predictability and
            </span>
            
            <span id="chunk-865" class="transcript-chunks" onclick="console.log('00:47:40,629'); seek(2860.0)">
              consistency for well defined tasks, whereas agents are the better options
            </span>
            
            <span id="chunk-866" class="transcript-chunks" onclick="console.log('00:47:44,909'); seek(2864.0)">
              when flexibility and model driven decision making are needed at scale.
            </span>
            
            <span id="chunk-867" class="transcript-chunks" onclick="console.log('00:47:49,339'); seek(2869.0)">
              For many applications, however, optimizing single LLM calls with retrieval and
            </span>
            
            <span id="chunk-868" class="transcript-chunks" onclick="console.log('00:47:54,429'); seek(2874.0)">
              in context examples is usually enough.
            </span>
            
            <span id="chunk-869" class="transcript-chunks" onclick="console.log('00:47:56,989'); seek(2876.0)">
              Thank you very much for letting me present this topic on AI agents,
            </span>
            
            <span id="chunk-870" class="transcript-chunks" onclick="console.log('00:48:01,139'); seek(2881.0)">
              their simplicity and their power.
            </span>
            
            <span id="chunk-871" class="transcript-chunks" onclick="console.log('00:48:04,069'); seek(2884.0)">
              My name is Craig West.
            </span>
            
            <span id="chunk-872" class="transcript-chunks" onclick="console.log('00:48:05,379'); seek(2885.0)">
              Thank you very much.
            </span>
            
            </div>
          </div>
          
          

          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/python2025" class="btn btn-sm btn-danger shadow lift" style="background-color: #69811f;">
                <i class="fe fe-grid me-2"></i>
                See all 53 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Craig%20West_python.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Craig West
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Backend Pythonista 
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://github.com/Python-Test-Engineer" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Craig West's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Craig West"
                  data-url="https://www.conf42.com/python2025"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/python2025"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Python"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>